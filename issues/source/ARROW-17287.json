{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13474745",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745",
    "key": "ARROW-17287",
    "fields": {
        "parent": {
            "id": "13436593",
            "key": "ARROW-16072",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13436593",
            "fields": {
                "summary": "[C++] Migrate scanner logic to ExecPlan, remove merged generator",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                    "description": "The issue is open and ready for the assignee to start work on it.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                    "name": "Open",
                    "id": "1",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                        "id": 2,
                        "key": "new",
                        "colorName": "blue-gray",
                        "name": "To Do"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                    "id": "4",
                    "description": "An improvement or enhancement to an existing feature or task.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                    "name": "Improvement",
                    "subtask": false,
                    "avatarId": 21140
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351947",
                "id": "12351947",
                "description": "",
                "name": "10.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-10-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
            "name": "westonpace",
            "key": "westonpace",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Weston Pace",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 19800,
            "total": 19800,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 19800,
            "total": 19800,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17287/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 33,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797336",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace opened a new pull request, #13782:\nURL: https://github.com/apache/arrow/pull/13782\n\n   **Primary Goal:** Create a scanner that \"cancels\" properly.  In other words, when the scan node is marked finished then all scan-related thread tasks will be finished.  This is different than the current model where I/O tasks are allowed to keep parts of the scan alive via captures of shared_ptr state.\r\n   \r\n   **Secondary Goal:** Remove our dependency on the merged generator and make the scanner more accessible.  The merged generator is complicated and does not support cancellation, and it currently only understood by a very small set of people.\r\n   \r\n   Performance isn't a goal for this rework but ideally this should not degrade performance.\n\n\n",
                    "created": "2022-08-02T18:22:36.615+0000",
                    "updated": "2022-08-02T18:22:36.615+0000",
                    "started": "2022-08-02T18:22:36.614+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797336",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797337",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#issuecomment-1203068987\n\n   https://issues.apache.org/jira/browse/ARROW-17287\n\n\n",
                    "created": "2022-08-02T18:23:04.133+0000",
                    "updated": "2022-08-02T18:23:04.133+0000",
                    "started": "2022-08-02T18:23:04.133+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797337",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797338",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#issuecomment-1203069034\n\n   :warning: Ticket **has not been started in JIRA**, please click 'Start Progress'.\n\n\n",
                    "created": "2022-08-02T18:23:06.374+0000",
                    "updated": "2022-08-02T18:23:06.374+0000",
                    "started": "2022-08-02T18:23:06.373+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797338",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797340",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#issuecomment-1203071080\n\n   This is still very much a draft.  However, I have the basic path working.  I will need to migrate over the existing scanner tests and test the various failure paths (as well as stress test for race conditions, there is potential for plenty here).\r\n   \r\n   I'm very curious what people think about the new scan options as well as the new evolution interfaces.\n\n\n",
                    "created": "2022-08-02T18:25:29.133+0000",
                    "updated": "2022-08-02T18:25:29.133+0000",
                    "started": "2022-08-02T18:25:29.132+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797340",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797342",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#issuecomment-1203072225\n\n   CC @save-buffer @marsupialtail who might be interested in this as well as we have spoken on similar topics\n\n\n",
                    "created": "2022-08-02T18:26:44.052+0000",
                    "updated": "2022-08-02T18:26:44.052+0000",
                    "started": "2022-08-02T18:26:44.051+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797342",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/797730",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r936983246\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n\nReview Comment:\n   ```suggestion\r\n   struct ARROW_DS_EXPORT FragmentSelectionColumn {\r\n   ```\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n\nReview Comment:\n   ```suggestion\r\n   struct ARROW_DS_EXPORT FragmentScanRequest {\r\n   ```\n\n\n\n##########\ncpp/src/arrow/dataset/scan_node.cc:\n##########\n@@ -0,0 +1,653 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dataset/scanner.h\"\n+\n+#include <functional>\n+#include <iostream>\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+#include \"arrow/util/unreachable.h\"\n+\n+namespace cp = arrow::compute;\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace dataset {\n+\n+namespace {\n+\n+Result<std::shared_ptr<Schema>> OutputSchemaFromOptions(const ScanV2Options& options) {\n+  return FieldPath::GetAll(*options.dataset->schema(), options.columns);\n+}\n+\n+class BatchOutputStrategy {\n+ public:\n+  BatchOutputStrategy(compute::ExecNode* self, compute::ExecNode* output)\n+      : self_(self), output_(output) {}\n+  virtual ~BatchOutputStrategy() = default;\n+  virtual void DeliverBatch(compute::ExecBatch batch) = 0;\n+  virtual void InputFinished() {\n+    // std::cout << \"BatchOutputFinished: \" + std::to_string(batch_count_.load()) + \"\\n\";\n+    output_->InputFinished(self_, batch_count_.load());\n+  }\n+\n+ protected:\n+  void SliceAndDoDeliverBatch(compute::ExecBatch batch) {\n+    // FIXME - Add slicing\n+    batch_count_++;\n+    output_->InputReceived(self_, std::move(batch));\n+  }\n+\n+  std::atomic<int32_t> batch_count_{0};\n+  compute::ExecNode* self_;\n+  compute::ExecNode* output_;\n+};\n+\n+class UnorderedBatchOutputStrategy : public BatchOutputStrategy {\n+ public:\n+  UnorderedBatchOutputStrategy(compute::ExecNode* self, compute::ExecNode* output)\n+      : BatchOutputStrategy(self, output){};\n+  ~UnorderedBatchOutputStrategy() = default;\n+  void DeliverBatch(compute::ExecBatch batch) override {\n+    return SliceAndDoDeliverBatch(std::move(batch));\n+  }\n+};\n+\n+// Unlike other nodes the scanner has to synchronize a number of resources\n+// that are external to the plan itself.  These things may start tasks that\n+// are invisible to the exec plan.\n+class ScannerSynchronization {\n+ public:\n+  ScannerSynchronization(int fragment_readahead)\n+      : fragment_readahead_(fragment_readahead) {}\n+\n+  bool MarkListingCompleted() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    finished_listing_ = true;\n+    return fragments_active_ == 0;\n+  }\n+\n+  void MarkListingFailed(Status err) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    finished_listing_ = true;\n+    cancelled_ = true;\n+    error_ = std::move(err);\n+  }\n+\n+  void Cancel() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (!cancelled_) {\n+      cancelled_ = true;\n+      error_ = Status::Cancelled(\"Plan cancelled before scan completed\");\n+    }\n+  }\n+\n+  void Pause() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    paused_ = true;\n+  }\n+\n+  void ReportFailedFragment(Status st) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (!cancelled_) {\n+      cancelled_ = true;\n+      error_ = std::move(st);\n+      fragments_active_--;\n+    }\n+  }\n+\n+  struct NextFragment {\n+    std::shared_ptr<Fragment> fragment;\n+    bool should_mark_input_finished;\n+  };\n+\n+  NextFragment FinishFragmentAndGetNext() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (cancelled_ || paused_ || fragments_to_scan_.empty()) {\n+      fragments_active_--;\n+      // std::cout << \"Not continuing fragment inline fragments_active=\" +\n+      //                  std::to_string(fragments_active_) + \"\\n\";\n+      return {nullptr, /*should_mark_input_finished=*/finished_listing_};\n+    }\n+    // std::cout << \"Continuing fragment inline \" + std::to_string(fragments_active_) +\n+    // \"\\n\";\n+    std::shared_ptr<Fragment> next = std::move(fragments_to_scan_.front());\n+    fragments_to_scan_.pop();\n+    return {next, /*should_mark_input_finished=*/false};\n+  }\n+\n+  bool QueueFragment(std::shared_ptr<Fragment> fragment) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (cancelled_) {\n+      return false;\n+    }\n+    if (!paused_ && fragments_active_ < fragment_readahead_) {\n+      // If we have space start scanning the fragment immediately\n+      fragments_active_++;\n+      // std::cout << \"Start scanning immediately: \" + std::to_string(fragments_active_) +\n+      //                  \"\\n\";\n+      return true;\n+    }\n+    fragments_to_scan_.push(std::move(fragment));\n+    return false;\n+  }\n+\n+ private:\n+  int fragment_readahead_;\n+  std::mutex mutex_;\n+  std::queue<std::shared_ptr<Fragment>> fragments_to_scan_;\n+  Status error_;\n+  bool cancelled_ = false;\n+  bool finished_listing_ = false;\n+  int fragments_active_ = 0;\n+  bool paused_ = false;\n+};\n+\n+// In the future we should support async scanning of fragments.  The\n+// Dataset class doesn't support this yet but we pretend it does here to\n+// ease future adoption of the feature.\n+AsyncGenerator<std::shared_ptr<Fragment>> GetFragments(Dataset* dataset,\n+                                                       cp::Expression predicate) {\n+  // In the future the dataset should be responsible for figuring out\n+  // the I/O context.  This will allow different I/O contexts to be used\n+  // when scanning different datasets.  For example, if we are scanning a\n+  // union of a remote dataset and a local dataset.\n+  const auto& io_context = io::default_io_context();\n+  auto io_executor = io_context.executor();\n+  Future<std::shared_ptr<FragmentIterator>> fragments_it_fut =\n+      DeferNotOk(io_executor->Submit(\n+          [dataset, predicate]() -> Result<std::shared_ptr<FragmentIterator>> {\n+            // std::cout << \"dataset->GetFragments\\n\";\n+            ARROW_ASSIGN_OR_RAISE(FragmentIterator fragments_iter,\n+                                  dataset->GetFragments(predicate));\n+            return std::make_shared<FragmentIterator>(std::move(fragments_iter));\n+          }));\n+  Future<AsyncGenerator<std::shared_ptr<Fragment>>> fragments_gen_fut =\n+      fragments_it_fut.Then([](const std::shared_ptr<FragmentIterator>& fragments_it)\n+                                -> Result<AsyncGenerator<std::shared_ptr<Fragment>>> {\n+        ARROW_ASSIGN_OR_RAISE(std::vector<std::shared_ptr<Fragment>> fragments,\n+                              fragments_it->ToVector());\n+        return MakeVectorGenerator(std::move(fragments));\n+      });\n+  return MakeFromFuture(std::move(fragments_gen_fut));\n+}\n+\n+/// \\brief A node that scans a dataset\n+///\n+/// The scan node has three groups of io-tasks and one task.\n+///\n+/// The first io-task (listing) fetches the fragments from the dataset.  This may be a\n+/// simple iteration of paths or, if the dataset is described with wildcards, this may\n+/// involve I/O for listing and walking directory paths.  There is one listing io-task per\n+/// dataset.\n+///\n+/// Ths next step is to fetch the metadata for the fragment.  For some formats (e.g. CSV)\n+/// this may be quite simple (get the size of the file).  For other formats (e.g. parquet)\n+/// this is more involved and requires reading data.  There is one metadata io-task per\n+/// fragment.  The metadata io-task creates an AsyncGenerator<RecordBatch> from the\n+/// fragment.\n+///\n+/// Once the metadata io-task is done we can issue read io-tasks.  Each read io-task\n+/// requests a single batch of data from the disk by pulling the next Future from the\n+/// generator.\n+///\n+/// Finally, when the future is fulfilled, we issue a pipeline task to drive the batch\n+/// through the pipeline.\n+///\n+/// Most of these tasks are io-tasks.  They take very few CPU resources and they run on\n+/// the I/O thread pool.  These io-tasks are invisible to the exec plan and so we need to\n+/// do some custom scheduling.  We limit how many fragments we read from at any one time.\n+/// This is referred to as \"fragment readahead\".\n+///\n+/// Within a fragment there is usually also some amount of \"row readahead\".  This row\n+/// readahead is handled by the fragment (and not the scanner) because the exact details\n+/// of how it is performed depend on the underlying format.\n+///\n+/// When a scan node is aborted (StopProducing) we send a cancel signal to any active\n+/// fragments.  On destruction we continue consuming the fragments until they complete\n+/// (which should be fairly quick since we cancelled the fragment).  This ensures the\n+/// I/O work is completely finished before the node is destroyed.\n+class ScanNode : public cp::ExecNode {\n+ public:\n+  ScanNode(cp::ExecPlan* plan, ScanV2Options options,\n+           std::shared_ptr<Schema> output_schema)\n+      : cp::ExecNode(plan, {}, {}, std::move(output_schema),\n+                     /*num_outputs=*/1),\n+        options_(options),\n+        synchronization_(options_.fragment_readahead) {}\n+\n+  static Result<ScanV2Options> NormalizeAndValidate(const ScanV2Options& options,\n+                                                    compute::ExecContext* ctx) {\n+    ScanV2Options normalized(options);\n+    if (!normalized.dataset) {\n+      return Status::Invalid(\"Scan options must include a dataset\");\n+    }\n+\n+    if (normalized.filter.IsBound()) {\n+      // There is no easy way to make sure a filter was bound agaisnt the same\n+      // function registry as the one in ctx so we just require it to be unbound\n+      // FIXME - Do we care if it was bound to a different function registry?\n+      return Status::Invalid(\"Scan filter must be unbound\");\n+    } else {\n+      ARROW_ASSIGN_OR_RAISE(normalized.filter,\n+                            normalized.filter.Bind(*options.dataset->schema(), ctx));\n+    }\n+\n+    return std::move(normalized);\n+  }\n+\n+  static Result<cp::ExecNode*> Make(cp::ExecPlan* plan, std::vector<cp::ExecNode*> inputs,\n+                                    const cp::ExecNodeOptions& options) {\n+    RETURN_NOT_OK(ValidateExecNodeInputs(plan, inputs, 0, \"ScanNode\"));\n+    const auto& scan_options = checked_cast<const ScanV2Options&>(options);\n+    ARROW_ASSIGN_OR_RAISE(ScanV2Options normalized_options,\n+                          NormalizeAndValidate(scan_options, plan->exec_context()));\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Schema> output_schema,\n+                          OutputSchemaFromOptions(normalized_options));\n+    return plan->EmplaceNode<ScanNode>(plan, std::move(normalized_options),\n+                                       std::move(output_schema));\n+  }\n+\n+  const char* kind_name() const override { return \"ScanNode\"; }\n+\n+  [[noreturn]] static void NoInputs() {\n+    Unreachable(\"no inputs; this should never be called\");\n+  }\n+  [[noreturn]] void InputReceived(cp::ExecNode*, cp::ExecBatch) override { NoInputs(); }\n+  [[noreturn]] void ErrorReceived(cp::ExecNode*, Status) override { NoInputs(); }\n+  [[noreturn]] void InputFinished(cp::ExecNode*, int) override { NoInputs(); }\n+\n+  Status Init() override {\n+    batch_output_ =\n+        ::arrow::internal::make_unique<UnorderedBatchOutputStrategy>(this, outputs_[0]);\n+    return Status::OK();\n+  }\n+\n+  Status StartProducing() override {\n+    START_COMPUTE_SPAN(span_, std::string(kind_name()) + \":\" + label(),\n+                       {{\"node.kind\", kind_name()},\n+                        {\"node.label\", label()},\n+                        {\"node.output_schema\", output_schema()->ToString()},\n+                        {\"node.detail\", ToString()}});\n+    END_SPAN_ON_FUTURE_COMPLETION(span_, finished_);\n+    // std::cout << \"StartProducing\\n\";\n+    ARROW_ASSIGN_OR_RAISE(Future<> list_task, plan_->BeginExternalTask());\n+    if (!list_task.is_valid()) {\n+      // Cancelled before we even started\n+      return Status::OK();\n+    }\n+    // std::cout << \"LIST TASK: BEGIN\\n\";\n+\n+    AsyncGenerator<std::shared_ptr<Fragment>> frag_gen =\n+        GetFragments(options_.dataset.get(), options_.filter);\n+    Future<> visit_task = VisitAsyncGenerator(\n+        std::move(frag_gen), [this](const std::shared_ptr<Fragment>& fragment) {\n+          // std::cout << \"Queuing fragment: \" + fragment->ToString() + \"\\n\";\n+          QueueOrStartFragment(fragment);\n+          return Status::OK();\n+        });\n+    visit_task\n+        .Then(\n+            [this]() {\n+              // std::cout << \"Marking listing completed\\n\";\n+              if (synchronization_.MarkListingCompleted()) {\n+                // It's possible, but probably unlikely, that all fragment scans have\n+                // already finished and so we need to report the total batch count here in\n+                // this case\n+                batch_output_->InputFinished();\n+                finished_.MarkFinished();\n+              }\n+            },\n+            [this](const Status& err) {\n+              synchronization_.MarkListingFailed(err);\n+              outputs_[0]->ErrorReceived(this, err);\n+            })\n+        .AddCallback([list_task](const Status& st) mutable {\n+          // std::cout << \"LIST TASK: END\\n\";\n+          list_task.MarkFinished();\n+        });\n+\n+    return Status::OK();\n+  }\n+\n+  void PauseProducing(ExecNode* output, int32_t counter) override {\n+    // FIXME(TODO)\n+    // Need to ressurect AsyncToggle and then all fragment scanners\n+    // should share the same toggle\n+  }\n+\n+  void ResumeProducing(ExecNode* output, int32_t counter) override {\n+    // FIXME(TODO)\n+  }\n+\n+  void StopProducing(ExecNode* output) override {\n+    DCHECK_EQ(output, outputs_[0]);\n+    StopProducing();\n+  }\n+\n+  void StopProducing() override { synchronization_.Cancel(); }\n+\n+ private:\n+  struct FragmentScanTask;\n+  using ScanTaskPtr = std::list<FragmentScanTask>::iterator;\n+  struct FragmentScanTask {\n+    std::unique_ptr<FragmentEvolutionStrategy> fragment_evolution;\n+    std::shared_ptr<FragmentScanner> fragment_scanner;\n+    uint64_t unused_readahead_bytes;\n+    bool iterating_tasks = false;\n+    // Transitions to true when we've finished listing fragments for this task\n+    bool finished = false;\n+    // Transitions to true when we receieve an error or cancellation is requested\n\nReview Comment:\n   Might it make sense to have an enum instead of individual flags?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.cc:\n##########\n@@ -238,5 +277,112 @@ Result<FragmentIterator> UnionDataset::GetFragmentsImpl(compute::Expression pred\n   return GetFragmentsFromDatasets(children_, predicate);\n }\n \n+namespace {\n+\n+class BasicFragmentEvolution : public FragmentEvolutionStrategy {\n+ public:\n+  BasicFragmentEvolution(std::vector<int> ds_to_frag_map, Schema* dataset_schema)\n+      : ds_to_frag_map(std::move(ds_to_frag_map)), dataset_schema(dataset_schema) {}\n+\n+  virtual Result<compute::Expression> GetGuarantee(\n+      const std::vector<FieldPath>& dataset_schema_selection) const override {\n+    std::vector<compute::Expression> missing_fields;\n+    for (const FieldPath& path : dataset_schema_selection) {\n+      int top_level_field_idx = path[0];\n+      if (ds_to_frag_map[top_level_field_idx] < 0) {\n+        missing_fields.push_back(compute::equal(\n\nReview Comment:\n   I think the expression simplification only recognizes `is_null(x)`\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+/// \\brief A collection of reads issued by a FragmentScanner\n+struct ReadTask {\n+  /// \\brief A future which will complete when the read is finished\n+  Future<std::shared_ptr<RecordBatch>> task;\n+  /// \\brief An estimate of the number of data bytes that will\n+  ///        be read when all the reads have finished\n+  ///\n+  /// This may be lower than the requested number of data bytes\n+  /// if the fragment is almost finished\n+  ///\n+  /// This may be higher than the requested number of data bytes\n+  /// if the fragment had to read more to respect record batch\n+  /// boundaries\n+  uint64_t num_bytes;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan further ahead into the fragment\n+  /// \\param target_num_bytes The number of data bytes the scanner is ready to receive\n+  /// \\return Futures for each read operation that has been started\n+  ///\n+  /// `target_num_bytes` is a hint only and fragments do not need to match this\n+  /// exactly.  For example, when scanning a parquet file, reads have to align\n+  /// with pages and so it is often neccesary to read more than asked for.\n+  /// See ScanV2Options for more details.\n+  ///\n+  /// It is acceptable for a future to complete with an empty batch.\n+  ///\n+  /// This may be called before the previous read task has finished and the caller\n+  /// should always check HasUnscannedData before each call.\n+  virtual ReadTask ScanMore(uint64_t target_num_bytes) = 0;\n+\n+  /// \\brief True if there is still data that has not been scanned\n+  ///\n+  /// This should return true when there is no more data to start scanning.\n+  /// Some scan operations may still be in progress and that is fine.\n+  virtual bool HasUnscannedData() = 0;\n+};\n+\n+struct InspectedFragment {\n\nReview Comment:\n   nit: docstring?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.cc:\n##########\n@@ -238,5 +277,112 @@ Result<FragmentIterator> UnionDataset::GetFragmentsImpl(compute::Expression pred\n   return GetFragmentsFromDatasets(children_, predicate);\n }\n \n+namespace {\n+\n+class BasicFragmentEvolution : public FragmentEvolutionStrategy {\n+ public:\n+  BasicFragmentEvolution(std::vector<int> ds_to_frag_map, Schema* dataset_schema)\n+      : ds_to_frag_map(std::move(ds_to_frag_map)), dataset_schema(dataset_schema) {}\n+\n+  virtual Result<compute::Expression> GetGuarantee(\n+      const std::vector<FieldPath>& dataset_schema_selection) const override {\n+    std::vector<compute::Expression> missing_fields;\n+    for (const FieldPath& path : dataset_schema_selection) {\n+      int top_level_field_idx = path[0];\n+      if (ds_to_frag_map[top_level_field_idx] < 0) {\n+        missing_fields.push_back(compute::equal(\n+            compute::field_ref(top_level_field_idx),\n+            compute::literal(\n+                MakeNullScalar(dataset_schema->fields()[top_level_field_idx]->type()))));\n+      }\n+    }\n+    if (missing_fields.empty()) {\n+      return compute::literal(true);\n+    }\n+    if (missing_fields.size() == 1) {\n+      return missing_fields[0];\n+    }\n+    return compute::and_(missing_fields);\n\nReview Comment:\n   nit: std::move?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n\nReview Comment:\n   Hmm, wouldn't we want to keep this responsibility in a more common place?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n\nReview Comment:\n   etc.\n\n\n\n##########\ncpp/src/arrow/dataset/scan_node.cc:\n##########\n@@ -0,0 +1,653 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dataset/scanner.h\"\n+\n+#include <functional>\n+#include <iostream>\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+#include \"arrow/util/unreachable.h\"\n+\n+namespace cp = arrow::compute;\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace dataset {\n+\n+namespace {\n+\n+Result<std::shared_ptr<Schema>> OutputSchemaFromOptions(const ScanV2Options& options) {\n+  return FieldPath::GetAll(*options.dataset->schema(), options.columns);\n+}\n+\n+class BatchOutputStrategy {\n+ public:\n+  BatchOutputStrategy(compute::ExecNode* self, compute::ExecNode* output)\n+      : self_(self), output_(output) {}\n+  virtual ~BatchOutputStrategy() = default;\n+  virtual void DeliverBatch(compute::ExecBatch batch) = 0;\n+  virtual void InputFinished() {\n+    // std::cout << \"BatchOutputFinished: \" + std::to_string(batch_count_.load()) + \"\\n\";\n+    output_->InputFinished(self_, batch_count_.load());\n+  }\n+\n+ protected:\n+  void SliceAndDoDeliverBatch(compute::ExecBatch batch) {\n+    // FIXME - Add slicing\n+    batch_count_++;\n+    output_->InputReceived(self_, std::move(batch));\n+  }\n+\n+  std::atomic<int32_t> batch_count_{0};\n+  compute::ExecNode* self_;\n+  compute::ExecNode* output_;\n+};\n+\n+class UnorderedBatchOutputStrategy : public BatchOutputStrategy {\n+ public:\n+  UnorderedBatchOutputStrategy(compute::ExecNode* self, compute::ExecNode* output)\n+      : BatchOutputStrategy(self, output){};\n+  ~UnorderedBatchOutputStrategy() = default;\n+  void DeliverBatch(compute::ExecBatch batch) override {\n+    return SliceAndDoDeliverBatch(std::move(batch));\n+  }\n+};\n+\n+// Unlike other nodes the scanner has to synchronize a number of resources\n+// that are external to the plan itself.  These things may start tasks that\n+// are invisible to the exec plan.\n+class ScannerSynchronization {\n+ public:\n+  ScannerSynchronization(int fragment_readahead)\n+      : fragment_readahead_(fragment_readahead) {}\n+\n+  bool MarkListingCompleted() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    finished_listing_ = true;\n+    return fragments_active_ == 0;\n+  }\n+\n+  void MarkListingFailed(Status err) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    finished_listing_ = true;\n+    cancelled_ = true;\n+    error_ = std::move(err);\n+  }\n+\n+  void Cancel() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (!cancelled_) {\n+      cancelled_ = true;\n+      error_ = Status::Cancelled(\"Plan cancelled before scan completed\");\n+    }\n+  }\n+\n+  void Pause() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    paused_ = true;\n+  }\n+\n+  void ReportFailedFragment(Status st) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (!cancelled_) {\n+      cancelled_ = true;\n+      error_ = std::move(st);\n+      fragments_active_--;\n+    }\n+  }\n+\n+  struct NextFragment {\n+    std::shared_ptr<Fragment> fragment;\n+    bool should_mark_input_finished;\n+  };\n+\n+  NextFragment FinishFragmentAndGetNext() {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (cancelled_ || paused_ || fragments_to_scan_.empty()) {\n+      fragments_active_--;\n+      // std::cout << \"Not continuing fragment inline fragments_active=\" +\n+      //                  std::to_string(fragments_active_) + \"\\n\";\n+      return {nullptr, /*should_mark_input_finished=*/finished_listing_};\n+    }\n+    // std::cout << \"Continuing fragment inline \" + std::to_string(fragments_active_) +\n+    // \"\\n\";\n+    std::shared_ptr<Fragment> next = std::move(fragments_to_scan_.front());\n+    fragments_to_scan_.pop();\n+    return {next, /*should_mark_input_finished=*/false};\n+  }\n+\n+  bool QueueFragment(std::shared_ptr<Fragment> fragment) {\n+    std::lock_guard<std::mutex> lg(mutex_);\n+    if (cancelled_) {\n+      return false;\n+    }\n+    if (!paused_ && fragments_active_ < fragment_readahead_) {\n+      // If we have space start scanning the fragment immediately\n+      fragments_active_++;\n+      // std::cout << \"Start scanning immediately: \" + std::to_string(fragments_active_) +\n+      //                  \"\\n\";\n+      return true;\n+    }\n+    fragments_to_scan_.push(std::move(fragment));\n+    return false;\n+  }\n+\n+ private:\n+  int fragment_readahead_;\n+  std::mutex mutex_;\n+  std::queue<std::shared_ptr<Fragment>> fragments_to_scan_;\n+  Status error_;\n+  bool cancelled_ = false;\n+  bool finished_listing_ = false;\n+  int fragments_active_ = 0;\n+  bool paused_ = false;\n+};\n+\n+// In the future we should support async scanning of fragments.  The\n+// Dataset class doesn't support this yet but we pretend it does here to\n+// ease future adoption of the feature.\n+AsyncGenerator<std::shared_ptr<Fragment>> GetFragments(Dataset* dataset,\n+                                                       cp::Expression predicate) {\n+  // In the future the dataset should be responsible for figuring out\n+  // the I/O context.  This will allow different I/O contexts to be used\n+  // when scanning different datasets.  For example, if we are scanning a\n+  // union of a remote dataset and a local dataset.\n+  const auto& io_context = io::default_io_context();\n+  auto io_executor = io_context.executor();\n+  Future<std::shared_ptr<FragmentIterator>> fragments_it_fut =\n+      DeferNotOk(io_executor->Submit(\n+          [dataset, predicate]() -> Result<std::shared_ptr<FragmentIterator>> {\n+            // std::cout << \"dataset->GetFragments\\n\";\n+            ARROW_ASSIGN_OR_RAISE(FragmentIterator fragments_iter,\n+                                  dataset->GetFragments(predicate));\n+            return std::make_shared<FragmentIterator>(std::move(fragments_iter));\n+          }));\n+  Future<AsyncGenerator<std::shared_ptr<Fragment>>> fragments_gen_fut =\n+      fragments_it_fut.Then([](const std::shared_ptr<FragmentIterator>& fragments_it)\n+                                -> Result<AsyncGenerator<std::shared_ptr<Fragment>>> {\n+        ARROW_ASSIGN_OR_RAISE(std::vector<std::shared_ptr<Fragment>> fragments,\n+                              fragments_it->ToVector());\n+        return MakeVectorGenerator(std::move(fragments));\n+      });\n+  return MakeFromFuture(std::move(fragments_gen_fut));\n+}\n+\n+/// \\brief A node that scans a dataset\n\nReview Comment:\n   Thanks for the great explanation here!\n\n\n\n##########\ncpp/src/arrow/dataset/scan_node.cc:\n##########\n@@ -0,0 +1,653 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/dataset/scanner.h\"\n+\n+#include <functional>\n+#include <iostream>\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+#include \"arrow/util/unreachable.h\"\n+\n+namespace cp = arrow::compute;\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace dataset {\n+\n+namespace {\n+\n+Result<std::shared_ptr<Schema>> OutputSchemaFromOptions(const ScanV2Options& options) {\n+  return FieldPath::GetAll(*options.dataset->schema(), options.columns);\n+}\n+\n+class BatchOutputStrategy {\n+ public:\n+  BatchOutputStrategy(compute::ExecNode* self, compute::ExecNode* output)\n+      : self_(self), output_(output) {}\n+  virtual ~BatchOutputStrategy() = default;\n+  virtual void DeliverBatch(compute::ExecBatch batch) = 0;\n+  virtual void InputFinished() {\n+    // std::cout << \"BatchOutputFinished: \" + std::to_string(batch_count_.load()) + \"\\n\";\n\nReview Comment:\n   nit: commented code (well, I guess there's lots of that for debugging)\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -59,6 +156,17 @@ class ARROW_DS_EXPORT Fragment : public std::enable_shared_from_this<Fragment> {\n   virtual Result<RecordBatchGenerator> ScanBatchesAsync(\n       const std::shared_ptr<ScanOptions>& options) = 0;\n \n+  /// \\brief Inspect a fragment to learn basic information\n\nReview Comment:\n   This is basically to read the footer ahead of time? \r\n   \r\n   So, ParquetFileFragment would attach the actual Parquet footer in the structure it returns, the Arrow fragment would return the schema, the CSV fragment would throw its hands up, etc.?\n\n\n\n",
                    "created": "2022-08-03T18:51:31.451+0000",
                    "updated": "2022-08-03T18:51:31.451+0000",
                    "started": "2022-08-03T18:51:31.451+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "797730",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798052",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937894460\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n\nReview Comment:\n   I suppose the scanner can do this \"field pruning\" just before the batch is sent for evolution.  I'm not entirely sure how the scanner would know which fields need to be pruned though.  The fragment scanner could maybe have a boolean flag whether it will do the pruning or not do the pruning.  \n\n\n\n",
                    "created": "2022-08-04T14:54:48.756+0000",
                    "updated": "2022-08-04T14:54:48.756+0000",
                    "started": "2022-08-04T14:54:48.756+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798052",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798063",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937901142\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,11 +38,107 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath column_reference;\n+  /// \\brief The requested type of the column\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  In this case the scan node will cast the column\n+  /// to the appropriate type later.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n\nReview Comment:\n   Could it be treated as part of the 'schema evolution' functionality, since that's already being used to fill in missing fields?\n\n\n\n",
                    "created": "2022-08-04T15:00:23.966+0000",
                    "updated": "2022-08-04T15:00:23.966+0000",
                    "started": "2022-08-04T15:00:23.965+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798063",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798064",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937901786\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -59,6 +156,17 @@ class ARROW_DS_EXPORT Fragment : public std::enable_shared_from_this<Fragment> {\n   virtual Result<RecordBatchGenerator> ScanBatchesAsync(\n       const std::shared_ptr<ScanOptions>& options) = 0;\n \n+  /// \\brief Inspect a fragment to learn basic information\n\nReview Comment:\n   Yes, exactly.  I'm not a huge fan of this because now fragment scanning is broken into three steps...\r\n   \r\n   FragmentScanner: Inspect fragment (new step)\r\n   Scanner: Create evolution\r\n   FragmentScanner: Open reader (where we previously determined column names / schema)\r\n   Scanner: Start iteration task\r\n   FragmentScanner: Scan\r\n   \r\n   I could push the evolution creation into the fragment scanner but that makes a new step that every fragment scanner has to do and so I think that would be worse.\r\n   \r\n   Your point on \"actual parquet footer\" and \"the schema\" is also correct.  My \"prototypical future evolution strategy\" that I am trying to make sure we plan for is to resolve field references by parquet column id.  So it would work in this fashion:\r\n   \r\n    * User specifies numeric field references against the dataset schema\r\n    * Evolution strategy has a lookup table from numeric position in dataset schema  to column id\r\n    * On inspection, physical column ids are inserted into the schema and passed to the evolution\r\n    * Evolution uses the lookup table combined with the physical column ids to determine the physical columns that are being asked for using\r\n    \r\n    So this way the inspect step would need to return something with the column IDs.  This could be the parquet footer.  However, it also could be the schema extracted from the footer as we have a mechanism for encoding those column IDs into the schema today.  This way we could leave the door open to someone adding column IDs to IPC files as well.\n\n\n\n",
                    "created": "2022-08-04T15:00:54.363+0000",
                    "updated": "2022-08-04T15:00:54.363+0000",
                    "started": "2022-08-04T15:00:54.362+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798064",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798065",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937901786\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -59,6 +156,17 @@ class ARROW_DS_EXPORT Fragment : public std::enable_shared_from_this<Fragment> {\n   virtual Result<RecordBatchGenerator> ScanBatchesAsync(\n       const std::shared_ptr<ScanOptions>& options) = 0;\n \n+  /// \\brief Inspect a fragment to learn basic information\n\nReview Comment:\n   Yes, exactly.  I'm not a huge fan of this because now fragment scanning is broken into three steps...\r\n   \r\n   FragmentScanner: Inspect fragment (new step)\r\n   Scanner: Create evolution\r\n   FragmentScanner: Open reader (where we previously determined column names / schema)\r\n   Scanner: Start iteration task\r\n   FragmentScanner: Scan\r\n   \r\n   I could push the evolution creation into the fragment scanner but that makes a new step that every fragment scanner has to do and so I think that would be worse.\r\n   \r\n   Your point on \"actual parquet footer\" and \"the schema\" is also correct.  My \"prototypical future evolution strategy\" that I am trying to make sure we plan for is to resolve field references by parquet column id.  So it would work in this fashion:\r\n   \r\n    * User specifies numeric field references against the dataset schema\r\n    * Evolution strategy has a lookup table from numeric position in dataset schema  to column id\r\n    * On inspection, physical column ids are inserted into the schema and passed to the evolution\r\n    * Evolution uses the lookup table combined with the physical column ids to determine the physical columns that are being asked for\r\n    \r\n    So this way the inspect step would need to return something with the column IDs.  This could be the parquet footer.  However, it also could be the schema extracted from the footer as we have a mechanism for encoding those column IDs into the schema today.  This way we could leave the door open to someone adding column IDs to IPC files as well.\n\n\n\n",
                    "created": "2022-08-04T15:01:44.455+0000",
                    "updated": "2022-08-04T15:01:44.455+0000",
                    "started": "2022-08-04T15:01:44.455+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798065",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798067",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937904517\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -59,6 +156,17 @@ class ARROW_DS_EXPORT Fragment : public std::enable_shared_from_this<Fragment> {\n   virtual Result<RecordBatchGenerator> ScanBatchesAsync(\n       const std::shared_ptr<ScanOptions>& options) = 0;\n \n+  /// \\brief Inspect a fragment to learn basic information\n\nReview Comment:\n   Sounds good to me.\r\n   \r\n   The Parquet/Arrow readers, IIRC, let you open a file and provide an already-parsed footer to at least skip that step. \n\n\n\n",
                    "created": "2022-08-04T15:03:24.237+0000",
                    "updated": "2022-08-04T15:03:24.237+0000",
                    "started": "2022-08-04T15:03:24.237+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798067",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/798073",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r937908586\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -59,6 +156,17 @@ class ARROW_DS_EXPORT Fragment : public std::enable_shared_from_this<Fragment> {\n   virtual Result<RecordBatchGenerator> ScanBatchesAsync(\n       const std::shared_ptr<ScanOptions>& options) = 0;\n \n+  /// \\brief Inspect a fragment to learn basic information\n\nReview Comment:\n   I don't see the problem with splitting scanning into these steps, FWIW. I think this would also let us down the line support schemes where metadata (footers) are cached in an index separately from the actual data (which would let us skip a synchronous I/O step)\n\n\n\n",
                    "created": "2022-08-04T15:06:59.362+0000",
                    "updated": "2022-08-04T15:06:59.362+0000",
                    "started": "2022-08-04T15:06:59.361+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "798073",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/809780",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r973584495\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n\nReview Comment:\n   ARROW_DS_EXPORT/docstring here?\n\n\n\n##########\ncpp/src/arrow/dataset/scanner_test.cc:\n##########\n@@ -54,6 +57,751 @@ using internal::Iota;\n \n namespace dataset {\n \n+// The basic evolution strategy doesn't really need any info from the dataset\n+// or the fragment other than the schema so we just make a dummy dataset/fragment\n+// here.\n+std::unique_ptr<Dataset> MakeDatasetFromSchema(std::shared_ptr<Schema> sch) {\n+  return ::arrow::internal::make_unique<InMemoryDataset>(std::move(sch),\n\nReview Comment:\n   We can use std::make_unique now!\n\n\n\n##########\ncpp/src/arrow/compute/exec/expression.h:\n##########\n@@ -277,6 +279,53 @@ ARROW_EXPORT Expression or_(Expression lhs, Expression rhs);\n ARROW_EXPORT Expression or_(const std::vector<Expression>&);\n ARROW_EXPORT Expression not_(Expression operand);\n \n+/// Modify an Expression with pre-order and post-order visitation.\n+/// `pre` will be invoked on each Expression. `pre` will visit Calls before their\n+/// arguments, `post_call` will visit Calls (and no other Expressions) after their\n+/// arguments. Visitors should return the Identical expression to indicate no change; this\n+/// will prevent unnecessary construction in the common case where a modification is not\n+/// possible/necessary/...\n+///\n+/// If an argument was modified, `post_call` visits a reconstructed Call with the modified\n+/// arguments but also receives a pointer to the unmodified Expression as a second\n+/// argument. If no arguments were modified the unmodified Expression* will be nullptr.\n+template <typename PreVisit, typename PostVisitCall>\n+Result<Expression> Modify(Expression expr, const PreVisit& pre,\n\nReview Comment:\n   nit: did we have to move this out of the internal header?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   How would something like CSV implement this?\n\n\n\n##########\ncpp/src/arrow/dataset/scan_node.cc:\n##########\n@@ -0,0 +1,372 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <functional>\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+#include \"arrow/util/unreachable.h\"\n+\n+namespace cp = arrow::compute;\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace dataset {\n+\n+namespace {\n+\n+Result<std::shared_ptr<Schema>> OutputSchemaFromOptions(const ScanV2Options& options) {\n+  return FieldPath::GetAll(*options.dataset->schema(), options.columns);\n+}\n+\n+// In the future we should support async scanning of fragments.  The\n+// Dataset class doesn't support this yet but we pretend it does here to\n+// ease future adoption of the feature.\n+AsyncGenerator<std::shared_ptr<Fragment>> GetFragments(Dataset* dataset,\n+                                                       cp::Expression predicate) {\n+  // In the future the dataset should be responsible for figuring out\n+  // the I/O context.  This will allow different I/O contexts to be used\n+  // when scanning different datasets.  For example, if we are scanning a\n+  // union of a remote dataset and a local dataset.\n+  const auto& io_context = io::default_io_context();\n+  auto io_executor = io_context.executor();\n+  Future<std::shared_ptr<FragmentIterator>> fragments_it_fut =\n+      DeferNotOk(io_executor->Submit(\n+          [dataset, predicate]() -> Result<std::shared_ptr<FragmentIterator>> {\n+            ARROW_ASSIGN_OR_RAISE(FragmentIterator fragments_iter,\n+                                  dataset->GetFragments(predicate));\n+            return std::make_shared<FragmentIterator>(std::move(fragments_iter));\n+          }));\n+  Future<AsyncGenerator<std::shared_ptr<Fragment>>> fragments_gen_fut =\n+      fragments_it_fut.Then([](const std::shared_ptr<FragmentIterator>& fragments_it)\n+                                -> Result<AsyncGenerator<std::shared_ptr<Fragment>>> {\n+        ARROW_ASSIGN_OR_RAISE(std::vector<std::shared_ptr<Fragment>> fragments,\n+                              fragments_it->ToVector());\n+        return MakeVectorGenerator(std::move(fragments));\n+      });\n+  return MakeFromFuture(std::move(fragments_gen_fut));\n+}\n+\n+/// \\brief A node that scans a dataset\n+///\n+/// The scan node has three groups of io-tasks and one task.\n+///\n+/// The first io-task (listing) fetches the fragments from the dataset.  This may be a\n+/// simple iteration of paths or, if the dataset is described with wildcards, this may\n+/// involve I/O for listing and walking directory paths.  There is one listing io-task per\n+/// dataset.\n+///\n+/// Ths next step is to fetch the metadata for the fragment.  For some formats (e.g. CSV)\n+/// this may be quite simple (get the size of the file).  For other formats (e.g. parquet)\n+/// this is more involved and requires reading data.  There is one metadata io-task per\n+/// fragment.  The metadata io-task creates an AsyncGenerator<RecordBatch> from the\n+/// fragment.\n+///\n+/// Once the metadata io-task is done we can issue read io-tasks.  Each read io-task\n+/// requests a single batch of data from the disk by pulling the next Future from the\n+/// generator.\n+///\n+/// Finally, when the future is fulfilled, we issue a pipeline task to drive the batch\n+/// through the pipeline.\n+///\n+/// Most of these tasks are io-tasks.  They take very few CPU resources and they run on\n+/// the I/O thread pool.  These io-tasks are invisible to the exec plan and so we need to\n+/// do some custom scheduling.  We limit how many fragments we read from at any one time.\n+/// This is referred to as \"fragment readahead\".\n+///\n+/// Within a fragment there is usually also some amount of \"row readahead\".  This row\n+/// readahead is handled by the fragment (and not the scanner) because the exact details\n+/// of how it is performed depend on the underlying format.\n+///\n+/// When a scan node is aborted (StopProducing) we send a cancel signal to any active\n+/// fragments.  On destruction we continue consuming the fragments until they complete\n+/// (which should be fairly quick since we cancelled the fragment).  This ensures the\n+/// I/O work is completely finished before the node is destroyed.\n+class ScanNode : public cp::ExecNode {\n+ public:\n+  ScanNode(cp::ExecPlan* plan, ScanV2Options options,\n+           std::shared_ptr<Schema> output_schema)\n+      : cp::ExecNode(plan, {}, {}, std::move(output_schema),\n+                     /*num_outputs=*/1),\n+        options_(options),\n+        fragments_throttle_(\n+            util::AsyncTaskScheduler::MakeThrottle(options_.fragment_readahead + 1)),\n+        batches_throttle_(\n+            util::AsyncTaskScheduler::MakeThrottle(options_.target_bytes_readahead + 1)) {\n+  }\n+\n+  static Result<ScanV2Options> NormalizeAndValidate(const ScanV2Options& options,\n+                                                    compute::ExecContext* ctx) {\n+    ScanV2Options normalized(options);\n+    if (!normalized.dataset) {\n+      return Status::Invalid(\"Scan options must include a dataset\");\n+    }\n+\n+    if (options.fragment_readahead < 0) {\n+      return Status::Invalid(\n+          \"Fragment readahead may not be less than 0.  Set to 0 to disable readahead\");\n+    }\n+\n+    if (options.target_bytes_readahead < 0) {\n+      return Status::Invalid(\n+          \"Batch readahead may not be less than 0.  Set to 0 to disable readahead\");\n+    }\n+\n+    if (!normalized.filter.is_valid()) {\n+      normalized.filter = compute::literal(true);\n+    }\n+\n+    if (normalized.filter.call() && normalized.filter.IsBound()) {\n+      // There is no easy way to make sure a filter was bound agaisnt the same\n+      // function registry as the one in ctx so we just require it to be unbound\n+      // FIXME - Do we care if it was bound to a different function registry?\n+      return Status::Invalid(\"Scan filter must be unbound\");\n+    } else if (!normalized.filter.IsBound()) {\n+      ARROW_ASSIGN_OR_RAISE(normalized.filter,\n+                            normalized.filter.Bind(*options.dataset->schema(), ctx));\n+    }  // Else we must have some simple filter like literal(true) which might be bound\n+       // but we don't care\n+\n+    return std::move(normalized);\n+  }\n+\n+  static Result<cp::ExecNode*> Make(cp::ExecPlan* plan, std::vector<cp::ExecNode*> inputs,\n+                                    const cp::ExecNodeOptions& options) {\n+    RETURN_NOT_OK(ValidateExecNodeInputs(plan, inputs, 0, \"ScanNode\"));\n+    const auto& scan_options = checked_cast<const ScanV2Options&>(options);\n+    ARROW_ASSIGN_OR_RAISE(ScanV2Options normalized_options,\n+                          NormalizeAndValidate(scan_options, plan->exec_context()));\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Schema> output_schema,\n+                          OutputSchemaFromOptions(normalized_options));\n+    return plan->EmplaceNode<ScanNode>(plan, std::move(normalized_options),\n+                                       std::move(output_schema));\n+  }\n+\n+  const char* kind_name() const override { return \"ScanNode\"; }\n+\n+  [[noreturn]] static void NoInputs() {\n+    Unreachable(\"no inputs; this should never be called\");\n+  }\n+  [[noreturn]] void InputReceived(cp::ExecNode*, cp::ExecBatch) override { NoInputs(); }\n+  [[noreturn]] void ErrorReceived(cp::ExecNode*, Status) override { NoInputs(); }\n+  [[noreturn]] void InputFinished(cp::ExecNode*, int) override { NoInputs(); }\n+\n+  Status Init() override {\n+    // batch_output_ =\n+    //     ::arrow::internal::make_unique<UnorderedBatchOutputStrategy>(this,\n+    //     outputs_[0]);\n+    return Status::OK();\n+  }\n+\n+  struct ScanState {\n+    std::mutex mutex;\n+    std::shared_ptr<FragmentScanner> fragment_scanner;\n+    std::unique_ptr<FragmentEvolutionStrategy> fragment_evolution;\n+    FragmentScanRequest scan_request;\n+  };\n+\n+  struct ScanBatchTask : util::AsyncTaskScheduler::Task {\n+    ScanBatchTask(ScanNode* node, ScanState* scan_state, int batch_index)\n+        : node_(node), scan_(scan_state), batch_index_(batch_index) {\n+      int64_t cost = scan_state->fragment_scanner->EstimatedDataBytes(batch_index_);\n+      // It's possible, though probably a bad idea, for a single batch of a fragment\n+      // to be larger than 2GiB.  In that case, it doesn't matter much if we underestimate\n+      // because the largest the throttle can be is 2GiB and thus we will be in \"one batch\n+      // at a time\" mode anyways which is the best we can do in this case.\n+      cost_ = static_cast<int>(\n+          std::min(cost, static_cast<int64_t>(std::numeric_limits<int>::max())));\n+    }\n+\n+    Result<Future<>> operator()(util::AsyncTaskScheduler* scheduler) override {\n+      // Prevent concurrent calls to ScanBatch which might not be thread safe\n+      std::lock_guard<std::mutex> lk(scan_->mutex);\n+      return scan_->fragment_scanner->ScanBatch(batch_index_)\n+          .Then([this](const std::shared_ptr<RecordBatch> batch) {\n\nReview Comment:\n   nit: was this meant to be const&?\n\n\n\n##########\ncpp/src/arrow/util/async_util.cc:\n##########\n@@ -274,7 +282,9 @@ class AsyncTaskSchedulerImpl : public AsyncTaskScheduler {\n       AbortUnlocked(submit_result.status(), std::move(lk));\n       return;\n     }\n-    submit_result->AddCallback([this, cost](const Status& st) {\n+    // FIXME(C++17, move into lambda?)\n\nReview Comment:\n   should be able to do this now? \n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   Should NumBatches and EstimatedDataBytes be `const`?\n\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n+};\n+\n+struct InspectedFragment {\n+  explicit InspectedFragment(std::vector<std::string> column_names)\n+      : column_names(std::move(column_names)) {}\n+  std::vector<std::string> column_names;\n\nReview Comment:\n   Hmm, why don't we care about the types here?\n\n\n\n##########\ncpp/src/arrow/dataset/scan_node.cc:\n##########\n@@ -0,0 +1,372 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include <functional>\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/compute/exec/exec_plan.h\"\n+#include \"arrow/compute/exec/expression.h\"\n+#include \"arrow/dataset/scanner.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/make_unique.h\"\n+#include \"arrow/util/tracing_internal.h\"\n+#include \"arrow/util/unreachable.h\"\n+\n+namespace cp = arrow::compute;\n+\n+namespace arrow {\n+\n+using internal::checked_cast;\n+\n+namespace dataset {\n+\n+namespace {\n+\n+Result<std::shared_ptr<Schema>> OutputSchemaFromOptions(const ScanV2Options& options) {\n+  return FieldPath::GetAll(*options.dataset->schema(), options.columns);\n+}\n+\n+// In the future we should support async scanning of fragments.  The\n+// Dataset class doesn't support this yet but we pretend it does here to\n+// ease future adoption of the feature.\n+AsyncGenerator<std::shared_ptr<Fragment>> GetFragments(Dataset* dataset,\n+                                                       cp::Expression predicate) {\n+  // In the future the dataset should be responsible for figuring out\n+  // the I/O context.  This will allow different I/O contexts to be used\n+  // when scanning different datasets.  For example, if we are scanning a\n+  // union of a remote dataset and a local dataset.\n+  const auto& io_context = io::default_io_context();\n+  auto io_executor = io_context.executor();\n+  Future<std::shared_ptr<FragmentIterator>> fragments_it_fut =\n+      DeferNotOk(io_executor->Submit(\n+          [dataset, predicate]() -> Result<std::shared_ptr<FragmentIterator>> {\n+            ARROW_ASSIGN_OR_RAISE(FragmentIterator fragments_iter,\n+                                  dataset->GetFragments(predicate));\n+            return std::make_shared<FragmentIterator>(std::move(fragments_iter));\n+          }));\n+  Future<AsyncGenerator<std::shared_ptr<Fragment>>> fragments_gen_fut =\n+      fragments_it_fut.Then([](const std::shared_ptr<FragmentIterator>& fragments_it)\n+                                -> Result<AsyncGenerator<std::shared_ptr<Fragment>>> {\n+        ARROW_ASSIGN_OR_RAISE(std::vector<std::shared_ptr<Fragment>> fragments,\n+                              fragments_it->ToVector());\n+        return MakeVectorGenerator(std::move(fragments));\n+      });\n+  return MakeFromFuture(std::move(fragments_gen_fut));\n+}\n+\n+/// \\brief A node that scans a dataset\n+///\n+/// The scan node has three groups of io-tasks and one task.\n+///\n+/// The first io-task (listing) fetches the fragments from the dataset.  This may be a\n+/// simple iteration of paths or, if the dataset is described with wildcards, this may\n+/// involve I/O for listing and walking directory paths.  There is one listing io-task per\n+/// dataset.\n+///\n+/// Ths next step is to fetch the metadata for the fragment.  For some formats (e.g. CSV)\n+/// this may be quite simple (get the size of the file).  For other formats (e.g. parquet)\n+/// this is more involved and requires reading data.  There is one metadata io-task per\n+/// fragment.  The metadata io-task creates an AsyncGenerator<RecordBatch> from the\n+/// fragment.\n+///\n+/// Once the metadata io-task is done we can issue read io-tasks.  Each read io-task\n+/// requests a single batch of data from the disk by pulling the next Future from the\n+/// generator.\n+///\n+/// Finally, when the future is fulfilled, we issue a pipeline task to drive the batch\n+/// through the pipeline.\n+///\n+/// Most of these tasks are io-tasks.  They take very few CPU resources and they run on\n+/// the I/O thread pool.  These io-tasks are invisible to the exec plan and so we need to\n+/// do some custom scheduling.  We limit how many fragments we read from at any one time.\n+/// This is referred to as \"fragment readahead\".\n+///\n+/// Within a fragment there is usually also some amount of \"row readahead\".  This row\n+/// readahead is handled by the fragment (and not the scanner) because the exact details\n+/// of how it is performed depend on the underlying format.\n+///\n+/// When a scan node is aborted (StopProducing) we send a cancel signal to any active\n+/// fragments.  On destruction we continue consuming the fragments until they complete\n+/// (which should be fairly quick since we cancelled the fragment).  This ensures the\n+/// I/O work is completely finished before the node is destroyed.\n+class ScanNode : public cp::ExecNode {\n+ public:\n+  ScanNode(cp::ExecPlan* plan, ScanV2Options options,\n+           std::shared_ptr<Schema> output_schema)\n+      : cp::ExecNode(plan, {}, {}, std::move(output_schema),\n+                     /*num_outputs=*/1),\n+        options_(options),\n+        fragments_throttle_(\n+            util::AsyncTaskScheduler::MakeThrottle(options_.fragment_readahead + 1)),\n+        batches_throttle_(\n+            util::AsyncTaskScheduler::MakeThrottle(options_.target_bytes_readahead + 1)) {\n+  }\n+\n+  static Result<ScanV2Options> NormalizeAndValidate(const ScanV2Options& options,\n+                                                    compute::ExecContext* ctx) {\n+    ScanV2Options normalized(options);\n+    if (!normalized.dataset) {\n+      return Status::Invalid(\"Scan options must include a dataset\");\n+    }\n+\n+    if (options.fragment_readahead < 0) {\n+      return Status::Invalid(\n+          \"Fragment readahead may not be less than 0.  Set to 0 to disable readahead\");\n+    }\n+\n+    if (options.target_bytes_readahead < 0) {\n+      return Status::Invalid(\n+          \"Batch readahead may not be less than 0.  Set to 0 to disable readahead\");\n+    }\n+\n+    if (!normalized.filter.is_valid()) {\n+      normalized.filter = compute::literal(true);\n+    }\n+\n+    if (normalized.filter.call() && normalized.filter.IsBound()) {\n+      // There is no easy way to make sure a filter was bound agaisnt the same\n+      // function registry as the one in ctx so we just require it to be unbound\n+      // FIXME - Do we care if it was bound to a different function registry?\n+      return Status::Invalid(\"Scan filter must be unbound\");\n+    } else if (!normalized.filter.IsBound()) {\n+      ARROW_ASSIGN_OR_RAISE(normalized.filter,\n+                            normalized.filter.Bind(*options.dataset->schema(), ctx));\n+    }  // Else we must have some simple filter like literal(true) which might be bound\n+       // but we don't care\n+\n+    return std::move(normalized);\n+  }\n+\n+  static Result<cp::ExecNode*> Make(cp::ExecPlan* plan, std::vector<cp::ExecNode*> inputs,\n+                                    const cp::ExecNodeOptions& options) {\n+    RETURN_NOT_OK(ValidateExecNodeInputs(plan, inputs, 0, \"ScanNode\"));\n+    const auto& scan_options = checked_cast<const ScanV2Options&>(options);\n+    ARROW_ASSIGN_OR_RAISE(ScanV2Options normalized_options,\n+                          NormalizeAndValidate(scan_options, plan->exec_context()));\n+    ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Schema> output_schema,\n+                          OutputSchemaFromOptions(normalized_options));\n+    return plan->EmplaceNode<ScanNode>(plan, std::move(normalized_options),\n+                                       std::move(output_schema));\n+  }\n+\n+  const char* kind_name() const override { return \"ScanNode\"; }\n+\n+  [[noreturn]] static void NoInputs() {\n+    Unreachable(\"no inputs; this should never be called\");\n+  }\n+  [[noreturn]] void InputReceived(cp::ExecNode*, cp::ExecBatch) override { NoInputs(); }\n+  [[noreturn]] void ErrorReceived(cp::ExecNode*, Status) override { NoInputs(); }\n+  [[noreturn]] void InputFinished(cp::ExecNode*, int) override { NoInputs(); }\n+\n+  Status Init() override {\n+    // batch_output_ =\n\nReview Comment:\n   Commented code?\n\n\n\n",
                    "created": "2022-09-17T13:31:22.985+0000",
                    "updated": "2022-09-17T13:31:22.985+0000",
                    "started": "2022-09-17T13:31:22.984+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "809780",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810014",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974180407\n\n\n##########\ncpp/src/arrow/compute/exec/expression.h:\n##########\n@@ -277,6 +279,53 @@ ARROW_EXPORT Expression or_(Expression lhs, Expression rhs);\n ARROW_EXPORT Expression or_(const std::vector<Expression>&);\n ARROW_EXPORT Expression not_(Expression operand);\n \n+/// Modify an Expression with pre-order and post-order visitation.\n+/// `pre` will be invoked on each Expression. `pre` will visit Calls before their\n+/// arguments, `post_call` will visit Calls (and no other Expressions) after their\n+/// arguments. Visitors should return the Identical expression to indicate no change; this\n+/// will prevent unnecessary construction in the common case where a modification is not\n+/// possible/necessary/...\n+///\n+/// If an argument was modified, `post_call` visits a reconstructed Call with the modified\n+/// arguments but also receives a pointer to the unmodified Expression as a second\n+/// argument. If no arguments were modified the unmodified Expression* will be nullptr.\n+template <typename PreVisit, typename PostVisitCall>\n+Result<Expression> Modify(Expression expr, const PreVisit& pre,\n\nReview Comment:\n   If I didn't then I ended up needing `expression_internal.h` in `partition.cc`.  I'm not entirely sure why but including `expression_internal.h` in two different .cc files in the same module caused duplicate symbol errors.  Not on `Modify` (which should be fine because it is templated) but on things like `KnownFieldValues` and `CallNotNull`.  Adding `Modify` didn't require any additional includes in `expression.h` and so it doesn't seem too heavyweight an addition but I could be missing some reason we didn't want it exposed.\n\n\n\n",
                    "created": "2022-09-19T12:17:00.265+0000",
                    "updated": "2022-09-19T12:17:00.265+0000",
                    "started": "2022-09-19T12:17:00.265+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810014",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810016",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974188037\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   > How would something like CSV implement this?\r\n   \r\n   It's # of batches instead of # of rows so it should just be `ceil(file_size_bytes / block_size)`.  That being said, this is a change from the previous fragment implementation which didn't require fragments to know the # of batches ahead of time.  It will be a problem for something like the python iterator-based scanner.\r\n   \r\n   It is possible to make this work without knowing the # of batches up front.  However, it creates a bit more complexity for two reasons:\r\n   \r\n    1. It makes it harder to know the batch index (I'm also working on ordered execution) because you can't know the batch index until previous fragments have been fully scanned (where this implementation just requires that previous fragments have been inspected).\r\n    2. It makes it difficult to know when we should stop spawning tasks to read further in the current fragment and start spawning tasks to read the next fragment.  If we do not know `NumBatches` we have to rely on a fragment scanner returning an end-of-fragment future quickly.  However, this should also be possible.\r\n   \r\n   My hope is that an iterator-based dataset could easily just be an iterator-based source node and so it's ok for the scan node to have this expectation.  If we were adapting flight to the scan node do you know if this is something that could cause a problem?  Does a flight stream have a known # of batches before we even start consuming it?  Or do we not know the # of batches until we have finished reading it?\n\n\n\n",
                    "created": "2022-09-19T12:25:51.600+0000",
                    "updated": "2022-09-19T12:25:51.600+0000",
                    "started": "2022-09-19T12:25:51.600+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810016",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810021",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974198545\n\n\n##########\ncpp/src/arrow/compute/exec/expression.h:\n##########\n@@ -277,6 +279,53 @@ ARROW_EXPORT Expression or_(Expression lhs, Expression rhs);\n ARROW_EXPORT Expression or_(const std::vector<Expression>&);\n ARROW_EXPORT Expression not_(Expression operand);\n \n+/// Modify an Expression with pre-order and post-order visitation.\n+/// `pre` will be invoked on each Expression. `pre` will visit Calls before their\n+/// arguments, `post_call` will visit Calls (and no other Expressions) after their\n+/// arguments. Visitors should return the Identical expression to indicate no change; this\n+/// will prevent unnecessary construction in the common case where a modification is not\n+/// possible/necessary/...\n+///\n+/// If an argument was modified, `post_call` visits a reconstructed Call with the modified\n+/// arguments but also receives a pointer to the unmodified Expression as a second\n+/// argument. If no arguments were modified the unmodified Expression* will be nullptr.\n+template <typename PreVisit, typename PostVisitCall>\n+Result<Expression> Modify(Expression expr, const PreVisit& pre,\n\nReview Comment:\n   Probably because those weren't declared `static`, only `inline`\n\n\n\n",
                    "created": "2022-09-19T12:37:37.242+0000",
                    "updated": "2022-09-19T12:37:37.242+0000",
                    "started": "2022-09-19T12:37:37.241+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810021",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810022",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974199255\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   There is no way to know the number of batches, no. And I'd like to avoid reimplementing parallel scans for Flight/ADBC data sources\u2026\n\n\n\n",
                    "created": "2022-09-19T12:38:27.296+0000",
                    "updated": "2022-09-19T12:38:27.296+0000",
                    "started": "2022-09-19T12:38:27.295+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810022",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810023",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974201441\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   Hmm, and I guess `(fragment_index, batch_index, is_last)` pushes too much complexity into the ordered execution bits?\n\n\n\n",
                    "created": "2022-09-19T12:40:47.560+0000",
                    "updated": "2022-09-19T12:40:47.560+0000",
                    "started": "2022-09-19T12:40:47.559+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810023",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810027",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974206038\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   Actually I guess I'm curious how this will work with iterator-based sources in general. Flight/ADBC would give you a set of iterators, where there is no parallelism within an iterator. And if you wanted to support Arrow stream files for some reason, that'd have the same structure. \r\n   \r\n   On the other hand, Flight/ADBC have a fixed number of fragments up front.\r\n   \r\n   Maybe a separate source node for those sources might make sense, with an optional 'merge' step if you want to use ordered execution with them? It would be kind of a shame to force them outside of the Dataset framework entirely, IMO, but having two different interfaces would also be annoying. I'm kind of just typing out things on the fly here\u2026\n\n\n\n",
                    "created": "2022-09-19T12:46:02.982+0000",
                    "updated": "2022-09-19T12:46:02.982+0000",
                    "started": "2022-09-19T12:46:02.982+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810027",
                    "issueId": "13474745"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/worklog/810193",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on code in PR #13782:\nURL: https://github.com/apache/arrow/pull/13782#discussion_r974736847\n\n\n##########\ncpp/src/arrow/dataset/dataset.h:\n##########\n@@ -37,17 +38,91 @@ namespace dataset {\n \n using RecordBatchGenerator = std::function<Future<std::shared_ptr<RecordBatch>>()>;\n \n+/// \\brief Description of a column to scan\n+struct FragmentSelectionColumn {\n+  /// \\brief The path to the column to load\n+  FieldPath path;\n+  /// \\brief The type of the column in the dataset schema\n+  ///\n+  /// A format may choose to ignore this field completely.  For example, when\n+  /// reading from IPC the reader can just return the column in the data type\n+  /// that is stored on disk.  There is no point in doing anything special.\n+  ///\n+  /// However, some formats may be capable of casting on the fly.  For example,\n+  /// when reading from CSV, if we know the target type of the column, we can\n+  /// convert from string to the target type as we read.\n+  DataType* requested_type;\n+  /// \\brief The index in the output selection of this column\n+  int selection_index;\n+};\n+/// \\brief Instructions for scanning a particular fragment\n+///\n+/// The fragment scan request is dervied from ScanV2Options.  The main\n+/// difference is that the scan options are based on the dataset schema\n+/// while the fragment request is based on the fragment schema.\n+struct FragmentScanRequest {\n+  /// \\brief A row filter\n+  ///\n+  /// The filter expression should be written against the fragment schema.\n+  ///\n+  /// \\see ScanV2Options for details on how this filter should be applied\n+  compute::Expression filter = compute::literal(true);\n+\n+  /// \\brief The columns to scan\n+  ///\n+  /// These indices refer to the fragment schema\n+  ///\n+  /// Note: This is NOT a simple list of top-level column indices.\n+  /// For more details \\see ScanV2Options\n+  ///\n+  /// If possible a fragment should only read from disk the data needed\n+  /// to satisfy these columns.  If a format cannot partially read a nested\n+  /// column (e.g. JSON) then it must apply the column selection (in memory)\n+  /// before returning the scanned batch.\n+  std::vector<FragmentSelectionColumn> columns;\n+  /// \\brief Options specific to the format being scanned\n+  FragmentScanOptions* format_scan_options;\n+};\n+\n+class FragmentScanner {\n+ public:\n+  /// This instance will only be destroyed after all ongoing scan futures\n+  /// have been completed.\n+  ///\n+  /// This means any callbacks created as part of the scan can safely\n+  /// capture `this`\n+  virtual ~FragmentScanner() = default;\n+  /// \\brief Scan a batch of data from the file\n+  /// \\param batch_number The index of the batch to read\n+  virtual Future<std::shared_ptr<RecordBatch>> ScanBatch(int batch_number) = 0;\n+  /// \\brief Calculate an estimate of how many data bytes the given batch will represent\n+  ///\n+  /// \"Data bytes\" should be the total size of all the buffers once the data has been\n+  /// decoded into the Arrow format.\n+  virtual int64_t EstimatedDataBytes(int batch_number) = 0;\n+  /// \\brief The number of batches in the fragment to scan\n+  virtual int NumBatches() = 0;\n\nReview Comment:\n   I think the worst case would be if the fragments are large and there is no parallelism within an iterator and the user wants ordered execution.  This will be a problem regardless of approach.  We will need to queue a large number of batches in memory for any kind of resequencing and, as a result, will probably fallback to serial execution.\r\n   \r\n   > Hmm, and I guess (fragment_index, batch_index, is_last) pushes too much complexity into the ordered execution bits?\r\n   \r\n   It might not be too bad.  I'll take another look.\n\n\n\n",
                    "created": "2022-09-19T22:26:23.650+0000",
                    "updated": "2022-09-19T22:26:23.650+0000",
                    "started": "2022-09-19T22:26:23.649+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "810193",
                    "issueId": "13474745"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 19800,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@7ca1edb6[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@104a7f16[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@53cc9adf[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@10bded7f[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7ca127d9[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4413eda8[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@733e9d97[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@c97509a[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@79fd213e[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@2258ed04[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@250d1efd[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@13a89946[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 19800,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Oct 03 12:02:38 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-10-03T12:02:38.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17287/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2022-08-02T18:16:27.000+0000",
        "updated": "2022-10-05T20:05:34.000+0000",
        "timeoriginalestimate": null,
        "description": "See the parent task for more details.  This sub-task is to create the initial scan node.  It appears that some changes will be needed in each of the format classes.  Those changes can be done as follow-on sub-tasks.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5.5h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 19800
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Create scan node that doesn't rely on the merged generator",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13474745/comment/17612264",
                    "id": "17612264",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 13782\n[https://github.com/apache/arrow/pull/13782]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2022-10-03T12:02:38.864+0000",
                    "updated": "2022-10-03T12:02:38.864+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z17ekg:",
        "customfield_12314139": null
    }
}