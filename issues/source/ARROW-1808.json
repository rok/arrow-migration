{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13118133",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133",
    "key": "ARROW-1808",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12341352",
                "id": "12341352",
                "name": "0.8.0",
                "archived": false,
                "released": true,
                "releaseDate": "2017-12-18"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": null,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": null,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1808/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 0,
            "worklogs": []
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": null,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2c023ed[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2d7e2e1d[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@766c364f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@2abef059[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@e076e39[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@7e25f9f2[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@69ec6637[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@348d6f6b[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@14eb965[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@50f52ad7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5a223320[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@58be938[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": null,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Nov 22 00:17:10 UTC 2017",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2017-11-22T00:02:17.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-1808/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2017-11-14T03:07:53.000+0000",
        "updated": "2017-11-22T00:17:10.000+0000",
        "timeoriginalestimate": null,
        "description": "This should be looked at soon to prevent having to define a different virtual interface for record batches. There are places where we are using the record batch constructor directly, and in some third party code (like MapD), so this might be good to get done for 0.8.0",
        "customfield_10010": null,
        "timetracking": {},
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++] Make RecordBatch interface virtual to permit record batches that lazy-materialize columns",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16251621",
                    "id": "16251621",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "[~kou] I am going to start looking at this soon, since it may cause a little bit of disruption in the glib bindings. This is a moderately disruptive API change, but long-term it will be for the best. The idea is that the current {{arrow::RecordBatch}} is a \"simple in-memory record batch\". But the object-boxing requirements to produce a vector of {{std::shared_ptr<arrow::ArrayData>}} can be quite expensive for large record batches. \r\n\r\nInstead, we could have {{arrow::RecordBatch}} as an abstract interface with virtual function for column access, with the current incarnation of RecordBatch as a subclass. So we could also create an {{arrow::IpcRecordBatch}} that does late-materialization of the {{arrow::Array}} objects. So if you have 1000 columns, you do not pay the cost of creating array objects for all of them if you only end up accessing a few columns in some analytics algorithm",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-14T16:10:49.250+0000",
                    "updated": "2017-11-14T16:10:49.250+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16251633",
                    "id": "16251633",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "An alternative to changing the current {{arrow::RecordBatch}} implementation is to create a new {{arrow::VirtualRecordBatch}} class as the base class, but then we would need to change various APIs for classes that return {{std::shared_ptr<RecordBatch>}} (like the IPC loaders). ",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-14T16:16:44.560+0000",
                    "updated": "2017-11-14T16:16:44.560+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16253069",
                    "id": "16253069",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "body": "Thanks for notifying it. It's very useful information.\r\nI like your idea. GLib bindings will follow the API changes soon when they are done.\r\n\r\nI'm not sure that the following is related to this topic but I share:\r\n\r\nIt may be useful that we can use arrow::ipc modules such as RecordBatchStreamReader and RecordBatchStreamWriter for RecordBatch on GPU.\r\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "created": "2017-11-15T07:41:13.425+0000",
                    "updated": "2017-11-15T07:41:13.425+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16253681",
                    "id": "16253681",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Yes, that would be useful. We should create a separate JIRA about this",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-15T15:54:07.185+0000",
                    "updated": "2017-11-15T15:54:07.185+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16254767",
                    "id": "16254767",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "body": "OK. I created ARROW-1824.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "created": "2017-11-16T05:10:41.082+0000",
                    "updated": "2017-11-16T05:10:41.082+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259426",
                    "id": "16259426",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm opened a new pull request #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337\n \n \n   While this will cause some minor API breakage in parquet-cpp and some other downstream users, this is reasonably long overdue. It will permit implementations of the RecordBatch or Table interface that do lazy IO / data loading or lazy materialization of columns.\r\n   \r\n   I will write a patch to fix up parquet-cpp, and will look to see if glib is easy to fix. There's no good way to go about merging this patch since a green build is not possible, so once we're happy with the patch, I can merge this patch and then work on getting a green build in parquet-cpp so we don't have a broken build there for too long\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T16:18:07.529+0000",
                    "updated": "2017-11-20T16:18:07.529+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259620",
                    "id": "16259620",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345787659\n \n \n   @kou I fixed the glib compilation, but I added DCHECKs to the record batch constructor to assert that the schema is the same size as the columns, but this isn't being checked it seems in the Glib bindings:\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T18:37:52.347+0000",
                    "updated": "2017-11-20T18:37:52.347+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259622",
                    "id": "16259622",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345787659\n \n \n   @kou I fixed the glib compilation, but I added DCHECKs to the record batch constructor to assert that the schema is the same size as the columns, but this isn't being checked it seems in the Glib bindings:\r\n   \r\n   ```\r\n   TestFileWriter: \r\n     test_write_record_batch:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t/home/wesm/code/arrow/cpp/src/arrow/record_batch.cc:39 Check failed: (static_cast<int>(columns.size())) == (schema->num_fields()) \r\n   ```\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T18:38:03.532+0000",
                    "updated": "2017-11-20T18:38:03.532+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259627",
                    "id": "16259627",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345787659\n \n \n   @kou I fixed the glib compilation, but I added DCHECKs to the record batch constructor to assert that the schema is the same size as the columns, but this isn't being checked it seems in the Glib bindings:\r\n   \r\n   ```\r\n   TestFileWriter:\r\n     test_write_record_batch:\r\n   /home/wesm/code/arrow/cpp/src/arrow/record_batch.cc:39 Check failed: (static_cast<int>(columns.size())) == (schema->num_fields()) \r\n   ```\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T18:38:17.309+0000",
                    "updated": "2017-11-20T18:38:17.309+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259806",
                    "id": "16259806",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "xhochy commented on a change in pull request #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#discussion_r152104789\n \n \n\n ##########\n File path: cpp/src/arrow/record_batch.h\n ##########\n @@ -0,0 +1,154 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_RECORD_BATCH_H\n+#define ARROW_RECORD_BATCH_H\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class KeyValueMetadata;\n+class Status;\n+\n+/// \\class RecordBatch\n+/// \\brief Collection of equal-length arrays matching a particular Schema\n+///\n+/// A record batch is table-like data structure that is semantically a sequence\n+/// of fields, each a contiguous Arrow array\n+class ARROW_EXPORT RecordBatch {\n+ public:\n+  virtual ~RecordBatch() = default;\n+\n+  /// \\param[in] schema The record batch schema\n+  /// \\param[in] num_rows length of fields in the record batch. Each array\n+  /// should have the same length as num_rows\n+  /// \\param[in] columns the record batch fields as vector of arrays\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<Array>>& columns);\n+\n+  /// \\brief Move-based constructor for a vector of Array instances\n+  static std::shared_ptr<RecordBatch> Make(const std::shared_ptr<Schema>& schema,\n+                                           int64_t num_rows,\n+                                           std::vector<std::shared_ptr<Array>>&& columns);\n+\n+  /// \\brief Construct record batch from vector of internal data structures\n+  /// \\since 0.5.0\n+  ///\n+  /// This class is only provided with an rvalue-reference for the input data,\n+  /// and is intended for internal use, or advanced users.\n+  ///\n+  /// \\param schema the record batch schema\n+  /// \\param num_rows the number of semantic rows in the record batch. This\n+  /// should be equal to the length of each field\n+  /// \\param columns the data for the batch's columns\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      std::vector<std::shared_ptr<ArrayData>>&& columns);\n+\n+  /// \\brief Construct record batch by copying vector of array data\n+  /// \\since 0.5.0\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<ArrayData>>& columns);\n+\n+  /// \\brief Determine if two record batches are exactly equal\n+  /// \\return true if batches are equal\n+  bool Equals(const RecordBatch& other) const;\n+\n+  /// \\brief Determine if two record batches are approximately equal\n+  bool ApproxEquals(const RecordBatch& other) const;\n+\n+  // \\return the table's schema\n+  /// \\return true if batches are equal\n+  std::shared_ptr<Schema> schema() const { return schema_; }\n+\n+  /// \\brief Retrieve an array from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an Array object\n+  virtual std::shared_ptr<Array> column(int i) const = 0;\n+\n+  /// \\brief Retrieve an array's internaldata from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an internal ArrayData object\n+  virtual std::shared_ptr<ArrayData> column_data(int i) const = 0;\n+\n+  virtual std::shared_ptr<RecordBatch> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const = 0;\n+\n+  /// \\brief Name in i-th column\n+  const std::string& column_name(int i) const;\n \n Review comment:\n   Shouldn't this be `std::string`?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T20:42:27.760+0000",
                    "updated": "2017-11-20T20:42:27.760+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259834",
                    "id": "16259834",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on a change in pull request #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#discussion_r152109752\n \n \n\n ##########\n File path: cpp/src/arrow/record_batch.h\n ##########\n @@ -0,0 +1,154 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_RECORD_BATCH_H\n+#define ARROW_RECORD_BATCH_H\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class KeyValueMetadata;\n+class Status;\n+\n+/// \\class RecordBatch\n+/// \\brief Collection of equal-length arrays matching a particular Schema\n+///\n+/// A record batch is table-like data structure that is semantically a sequence\n+/// of fields, each a contiguous Arrow array\n+class ARROW_EXPORT RecordBatch {\n+ public:\n+  virtual ~RecordBatch() = default;\n+\n+  /// \\param[in] schema The record batch schema\n+  /// \\param[in] num_rows length of fields in the record batch. Each array\n+  /// should have the same length as num_rows\n+  /// \\param[in] columns the record batch fields as vector of arrays\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<Array>>& columns);\n+\n+  /// \\brief Move-based constructor for a vector of Array instances\n+  static std::shared_ptr<RecordBatch> Make(const std::shared_ptr<Schema>& schema,\n+                                           int64_t num_rows,\n+                                           std::vector<std::shared_ptr<Array>>&& columns);\n+\n+  /// \\brief Construct record batch from vector of internal data structures\n+  /// \\since 0.5.0\n+  ///\n+  /// This class is only provided with an rvalue-reference for the input data,\n+  /// and is intended for internal use, or advanced users.\n+  ///\n+  /// \\param schema the record batch schema\n+  /// \\param num_rows the number of semantic rows in the record batch. This\n+  /// should be equal to the length of each field\n+  /// \\param columns the data for the batch's columns\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      std::vector<std::shared_ptr<ArrayData>>&& columns);\n+\n+  /// \\brief Construct record batch by copying vector of array data\n+  /// \\since 0.5.0\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<ArrayData>>& columns);\n+\n+  /// \\brief Determine if two record batches are exactly equal\n+  /// \\return true if batches are equal\n+  bool Equals(const RecordBatch& other) const;\n+\n+  /// \\brief Determine if two record batches are approximately equal\n+  bool ApproxEquals(const RecordBatch& other) const;\n+\n+  // \\return the table's schema\n+  /// \\return true if batches are equal\n+  std::shared_ptr<Schema> schema() const { return schema_; }\n+\n+  /// \\brief Retrieve an array from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an Array object\n+  virtual std::shared_ptr<Array> column(int i) const = 0;\n+\n+  /// \\brief Retrieve an array's internaldata from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an internal ArrayData object\n+  virtual std::shared_ptr<ArrayData> column_data(int i) const = 0;\n+\n+  virtual std::shared_ptr<RecordBatch> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const = 0;\n+\n+  /// \\brief Name in i-th column\n+  const std::string& column_name(int i) const;\n \n Review comment:\n   The name is coming from the schema, so a copy not strictly necessary\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T21:02:03.186+0000",
                    "updated": "2017-11-20T21:02:03.186+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16259850",
                    "id": "16259850",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345832486\n \n \n   @kou I removed the dchecks from the ctor that I mentioned in favor of validating in `RecordBatch::Validate`. The tests are segfaulting, though, I stepped into gdb to look at the failing test, it looks like a record batch in the test suite might be malformed (the schema is larger than the actual number of columns)\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-20T21:08:52.284+0000",
                    "updated": "2017-11-20T21:08:52.284+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16260263",
                    "id": "16260263",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "kou commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345919956\n \n \n   @wesm I confirmed. The test creates 0 rows record batch with empty columns. It causes the segmentation fault. The following patch fixes this:\r\n   \r\n   ```diff\r\n   diff --git a/c_glib/test/test-file-writer.rb b/c_glib/test/test-file-writer.rb\r\n   index 3de8e5cf..67aed85f 100644\r\n   --- a/c_glib/test/test-file-writer.rb\r\n   +++ b/c_glib/test/test-file-writer.rb\r\n   @@ -19,14 +19,18 @@ class TestFileWriter < Test::Unit::TestCase\r\n      include Helper::Buildable\r\n    \r\n      def test_write_record_batch\r\n   +    data = [true]\r\n   +    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   +    schema = Arrow::Schema.new([field])\r\n   +\r\n        tempfile = Tempfile.open(\"arrow-ipc-file-writer\")\r\n        output = Arrow::FileOutputStream.new(tempfile.path, false)\r\n        begin\r\n   -      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   -      schema = Arrow::Schema.new([field])\r\n          file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\r\n          begin\r\n   -        record_batch = Arrow::RecordBatch.new(schema, 0, [])\r\n   +        record_batch = Arrow::RecordBatch.new(schema,\r\n   +                                              data.size,\r\n   +                                              [build_boolean_array(data)])\r\n            file_writer.write_record_batch(record_batch)\r\n          ensure\r\n            file_writer.close\r\n   @@ -38,8 +42,12 @@ class TestFileWriter < Test::Unit::TestCase\r\n        input = Arrow::MemoryMappedInputStream.new(tempfile.path)\r\n        begin\r\n          file_reader = Arrow::RecordBatchFileReader.new(input)\r\n   -      assert_equal([\"enabled\"],\r\n   +      assert_equal([field.name],\r\n                       file_reader.schema.fields.collect(&:name))\r\n   +      assert_equal(Arrow::RecordBatch.new(schema,\r\n   +                                          data.size,\r\n   +                                          [build_boolean_array(data)]),\r\n   +                   file_reader.read_record_batch(0))\r\n        ensure\r\n          input.close\r\n        end\r\n   diff --git a/c_glib/test/test-gio-input-stream.rb b/c_glib/test/test-gio-input-stream.rb\r\n   index a71a3704..2adf25b3 100644\r\n   --- a/c_glib/test/test-gio-input-stream.rb\r\n   +++ b/c_glib/test/test-gio-input-stream.rb\r\n   @@ -16,15 +16,21 @@\r\n    # under the License.\r\n    \r\n    class TestGIOInputStream < Test::Unit::TestCase\r\n   +  include Helper::Buildable\r\n   +\r\n      def test_reader_backend\r\n   +    data = [true]\r\n   +    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   +    schema = Arrow::Schema.new([field])\r\n   +\r\n        tempfile = Tempfile.open(\"arrow-gio-input-stream\")\r\n        output = Arrow::FileOutputStream.new(tempfile.path, false)\r\n        begin\r\n   -      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   -      schema = Arrow::Schema.new([field])\r\n          file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\r\n          begin\r\n   -        record_batch = Arrow::RecordBatch.new(schema, 0, [])\r\n   +        record_batch = Arrow::RecordBatch.new(schema,\r\n   +                                              data.size,\r\n   +                                              [build_boolean_array(data)])\r\n            file_writer.write_record_batch(record_batch)\r\n          ensure\r\n            file_writer.close\r\n   @@ -38,8 +44,12 @@ class TestGIOInputStream < Test::Unit::TestCase\r\n        input = Arrow::GIOInputStream.new(input_stream)\r\n        begin\r\n          file_reader = Arrow::RecordBatchFileReader.new(input)\r\n   -      assert_equal([\"enabled\"],\r\n   +      assert_equal([field.name],\r\n                       file_reader.schema.fields.collect(&:name))\r\n   +      assert_equal(Arrow::RecordBatch.new(schema,\r\n   +                                          data.size,\r\n   +                                          [build_boolean_array(data)]),\r\n   +                   file_reader.read_record_batch(0))\r\n        ensure\r\n          input.close\r\n        end\r\n   diff --git a/c_glib/test/test-gio-output-stream.rb b/c_glib/test/test-gio-output-stream.rb\r\n   index adaa8c1b..c77598ed 100644\r\n   --- a/c_glib/test/test-gio-output-stream.rb\r\n   +++ b/c_glib/test/test-gio-output-stream.rb\r\n   @@ -16,17 +16,23 @@\r\n    # under the License.\r\n    \r\n    class TestGIOOutputStream < Test::Unit::TestCase\r\n   +  include Helper::Buildable\r\n   +\r\n      def test_writer_backend\r\n   +    data = [true]\r\n   +    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   +    schema = Arrow::Schema.new([field])\r\n   +\r\n        tempfile = Tempfile.open(\"arrow-gio-output-stream\")\r\n        file = Gio::File.new_for_path(tempfile.path)\r\n        output_stream = file.append_to(:none)\r\n        output = Arrow::GIOOutputStream.new(output_stream)\r\n        begin\r\n   -      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   -      schema = Arrow::Schema.new([field])\r\n          file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\r\n          begin\r\n   -        record_batch = Arrow::RecordBatch.new(schema, 0, [])\r\n   +        record_batch = Arrow::RecordBatch.new(schema,\r\n   +                                              data.size,\r\n   +                                              [build_boolean_array(data)])\r\n            file_writer.write_record_batch(record_batch)\r\n          ensure\r\n            file_writer.close\r\n   @@ -38,8 +44,12 @@ class TestGIOOutputStream < Test::Unit::TestCase\r\n        input = Arrow::MemoryMappedInputStream.new(tempfile.path)\r\n        begin\r\n          file_reader = Arrow::RecordBatchFileReader.new(input)\r\n   -      assert_equal([\"enabled\"],\r\n   +      assert_equal([field.name],\r\n                       file_reader.schema.fields.collect(&:name))\r\n   +      assert_equal(Arrow::RecordBatch.new(schema,\r\n   +                                          data.size,\r\n   +                                          [build_boolean_array(data)]),\r\n   +                   file_reader.read_record_batch(0))\r\n        ensure\r\n          input.close\r\n        end\r\n   diff --git a/c_glib/test/test-stream-writer.rb b/c_glib/test/test-stream-writer.rb\r\n   index c3d0e149..32754e20 100644\r\n   --- a/c_glib/test/test-stream-writer.rb\r\n   +++ b/c_glib/test/test-stream-writer.rb\r\n   @@ -19,17 +19,19 @@ class TestStreamWriter < Test::Unit::TestCase\r\n      include Helper::Buildable\r\n    \r\n      def test_write_record_batch\r\n   +    data = [true]\r\n   +    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   +    schema = Arrow::Schema.new([field])\r\n   +\r\n        tempfile = Tempfile.open(\"arrow-ipc-stream-writer\")\r\n        output = Arrow::FileOutputStream.new(tempfile.path, false)\r\n        begin\r\n   -      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\r\n   -      schema = Arrow::Schema.new([field])\r\n          stream_writer = Arrow::RecordBatchStreamWriter.new(output, schema)\r\n          begin\r\n            columns = [\r\n   -          build_boolean_array([true]),\r\n   +          build_boolean_array(data),\r\n            ]\r\n   -        record_batch = Arrow::RecordBatch.new(schema, 1, columns)\r\n   +        record_batch = Arrow::RecordBatch.new(schema, data.size, columns)\r\n            stream_writer.write_record_batch(record_batch)\r\n          ensure\r\n            stream_writer.close\r\n   @@ -41,10 +43,12 @@ class TestStreamWriter < Test::Unit::TestCase\r\n        input = Arrow::MemoryMappedInputStream.new(tempfile.path)\r\n        begin\r\n          stream_reader = Arrow::RecordBatchStreamReader.new(input)\r\n   -      assert_equal([\"enabled\"],\r\n   +      assert_equal([field.name],\r\n                       stream_reader.schema.fields.collect(&:name))\r\n   -      assert_equal(true,\r\n   -                   stream_reader.read_next.get_column(0).get_value(0))\r\n   +      assert_equal(Arrow::RecordBatch.new(schema,\r\n   +                                          data.size,\r\n   +                                          [build_boolean_array(data)]),\r\n   +                   stream_reader.read_next)\r\n          assert_nil(stream_reader.read_next)\r\n        ensure\r\n          input.close\r\n   \r\n   ```\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-21T05:19:28.374+0000",
                    "updated": "2017-11-21T05:19:28.374+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16260269",
                    "id": "16260269",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "kou commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-345920440\n \n \n   If we allow empty record batch (no columns record batch), the following check is needed:\r\n   \r\n   ```diff\r\n   diff --git a/cpp/src/arrow/ipc/writer.cc b/cpp/src/arrow/ipc/writer.cc\r\n   index 3c1db061..6af9bc4b 100644\r\n   --- a/cpp/src/arrow/ipc/writer.cc\r\n   +++ b/cpp/src/arrow/ipc/writer.cc\r\n   @@ -139,9 +139,11 @@ class RecordBatchSerializer : public ArrayVisitor {\r\n          buffers_.clear();\r\n        }\r\n    \r\n   -    // Perform depth-first traversal of the row-batch\r\n   -    for (int i = 0; i < batch.num_columns(); ++i) {\r\n   -      RETURN_NOT_OK(VisitArray(*batch.column(i)));\r\n   +    if (batch.num_rows() > 0) {\r\n   +      // Perform depth-first traversal of the row-batch\r\n   +      for (int i = 0; i < batch.num_columns(); ++i) {\r\n   +        RETURN_NOT_OK(VisitArray(*batch.column(i)));\r\n   +      }\r\n        }\r\n    \r\n        // The position for the start of a buffer relative to the passed frame of\r\n   ```\r\n   \r\n   I'm OK that we deny no columns record batch. It'll simplify our code.\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-21T05:23:04.631+0000",
                    "updated": "2017-11-21T05:23:04.631+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16260841",
                    "id": "16260841",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-346050608\n \n \n   @kou it seems like there are two different issues here. \r\n   \r\n   Here, a schema with 1 field was passed along with a list of 0 columns:\r\n   \r\n   ```diff\r\n   -        record_batch = Arrow::RecordBatch.new(schema, 0, [])\r\n   +        record_batch = Arrow::RecordBatch.new(schema,\r\n   +                                              data.size,\r\n   +                                              [build_boolean_array(data)])\r\n   ```\r\n   \r\n   I believe this would result in segfaults even if the number of rows is non-zero. So having empty / length-0 record batches in the IPC writer code path is fine so long as the columns matches the schema. \r\n   \r\n   The reason this bug was not caught before was that the `RecordBatch::columns_` member was being used to determine `RecordBatch::num_columns()`, whereas now we are using the schema. It seems like respecting the schema is the right approach. I could add boundschecking to `SimpleRecordBatch::column(i)` and return null if the index is out of bounds, would that help at all?\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-21T14:53:43.183+0000",
                    "updated": "2017-11-21T14:53:43.183+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16261707",
                    "id": "16261707",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-346200509\n \n \n   Merging, since now the Linux build has failed only when reaching parquet-cpp. I will update the parquet-cpp patch and then merge that once its build passes\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-22T00:01:35.956+0000",
                    "updated": "2017-11-22T00:01:35.956+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16261710",
                    "id": "16261710",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/.travis.yml b/.travis.yml\nindex 9c714a689..ddadf739a 100644\n--- a/.travis.yml\n+++ b/.travis.yml\n@@ -55,6 +55,7 @@ matrix:\n     - export ARROW_TRAVIS_VALGRIND=1\n     - export ARROW_TRAVIS_PLASMA=1\n     - export ARROW_TRAVIS_CLANG_FORMAT=1\n+    - export ARROW_BUILD_WARNING_LEVEL=CHECKIN\n     - export CC=\"clang-4.0\"\n     - export CXX=\"clang++-4.0\"\n     - $TRAVIS_BUILD_DIR/ci/travis_install_clang_tools.sh\n@@ -74,6 +75,7 @@ matrix:\n     before_script:\n     - export ARROW_TRAVIS_USE_TOOLCHAIN=1\n     - export ARROW_TRAVIS_PLASMA=1\n+    - export ARROW_BUILD_WARNING_LEVEL=CHECKIN\n     - travis_wait 50 $TRAVIS_BUILD_DIR/ci/travis_before_script_cpp.sh\n     script:\n     - $TRAVIS_BUILD_DIR/ci/travis_script_cpp.sh\ndiff --git a/c_glib/arrow-glib/record-batch.cpp b/c_glib/arrow-glib/record-batch.cpp\nindex f381af0a2..f23a0cf75 100644\n--- a/c_glib/arrow-glib/record-batch.cpp\n+++ b/c_glib/arrow-glib/record-batch.cpp\n@@ -150,9 +150,8 @@ garrow_record_batch_new(GArrowSchema *schema,\n   }\n \n   auto arrow_record_batch =\n-    std::make_shared<arrow::RecordBatch>(garrow_schema_get_raw(schema),\n-                                         n_rows,\n-                                         arrow_columns);\n+    arrow::RecordBatch::Make(garrow_schema_get_raw(schema),\n+                             n_rows, arrow_columns);\n   return garrow_record_batch_new_raw(&arrow_record_batch);\n }\n \ndiff --git a/c_glib/arrow-glib/table.cpp b/c_glib/arrow-glib/table.cpp\nindex 779f2ef62..e086396f8 100644\n--- a/c_glib/arrow-glib/table.cpp\n+++ b/c_glib/arrow-glib/table.cpp\n@@ -143,8 +143,7 @@ garrow_table_new(GArrowSchema *schema,\n   }\n \n   auto arrow_table =\n-    std::make_shared<arrow::Table>(garrow_schema_get_raw(schema),\n-                                   arrow_columns);\n+    arrow::Table::Make(garrow_schema_get_raw(schema), arrow_columns);\n   return garrow_table_new_raw(&arrow_table);\n }\n \ndiff --git a/c_glib/test/test-file-writer.rb b/c_glib/test/test-file-writer.rb\nindex 3de8e5cf3..67aed85f7 100644\n--- a/c_glib/test/test-file-writer.rb\n+++ b/c_glib/test/test-file-writer.rb\n@@ -19,14 +19,18 @@ class TestFileWriter < Test::Unit::TestCase\n   include Helper::Buildable\n \n   def test_write_record_batch\n+    data = [true]\n+    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n+    schema = Arrow::Schema.new([field])\n+\n     tempfile = Tempfile.open(\"arrow-ipc-file-writer\")\n     output = Arrow::FileOutputStream.new(tempfile.path, false)\n     begin\n-      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n-      schema = Arrow::Schema.new([field])\n       file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\n       begin\n-        record_batch = Arrow::RecordBatch.new(schema, 0, [])\n+        record_batch = Arrow::RecordBatch.new(schema,\n+                                              data.size,\n+                                              [build_boolean_array(data)])\n         file_writer.write_record_batch(record_batch)\n       ensure\n         file_writer.close\n@@ -38,8 +42,12 @@ def test_write_record_batch\n     input = Arrow::MemoryMappedInputStream.new(tempfile.path)\n     begin\n       file_reader = Arrow::RecordBatchFileReader.new(input)\n-      assert_equal([\"enabled\"],\n+      assert_equal([field.name],\n                    file_reader.schema.fields.collect(&:name))\n+      assert_equal(Arrow::RecordBatch.new(schema,\n+                                          data.size,\n+                                          [build_boolean_array(data)]),\n+                   file_reader.read_record_batch(0))\n     ensure\n       input.close\n     end\ndiff --git a/c_glib/test/test-gio-input-stream.rb b/c_glib/test/test-gio-input-stream.rb\nindex a71a37043..2adf25b3a 100644\n--- a/c_glib/test/test-gio-input-stream.rb\n+++ b/c_glib/test/test-gio-input-stream.rb\n@@ -16,15 +16,21 @@\n # under the License.\n \n class TestGIOInputStream < Test::Unit::TestCase\n+  include Helper::Buildable\n+\n   def test_reader_backend\n+    data = [true]\n+    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n+    schema = Arrow::Schema.new([field])\n+\n     tempfile = Tempfile.open(\"arrow-gio-input-stream\")\n     output = Arrow::FileOutputStream.new(tempfile.path, false)\n     begin\n-      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n-      schema = Arrow::Schema.new([field])\n       file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\n       begin\n-        record_batch = Arrow::RecordBatch.new(schema, 0, [])\n+        record_batch = Arrow::RecordBatch.new(schema,\n+                                              data.size,\n+                                              [build_boolean_array(data)])\n         file_writer.write_record_batch(record_batch)\n       ensure\n         file_writer.close\n@@ -38,8 +44,12 @@ def test_reader_backend\n     input = Arrow::GIOInputStream.new(input_stream)\n     begin\n       file_reader = Arrow::RecordBatchFileReader.new(input)\n-      assert_equal([\"enabled\"],\n+      assert_equal([field.name],\n                    file_reader.schema.fields.collect(&:name))\n+      assert_equal(Arrow::RecordBatch.new(schema,\n+                                          data.size,\n+                                          [build_boolean_array(data)]),\n+                   file_reader.read_record_batch(0))\n     ensure\n       input.close\n     end\ndiff --git a/c_glib/test/test-gio-output-stream.rb b/c_glib/test/test-gio-output-stream.rb\nindex adaa8c1b7..c77598ed1 100644\n--- a/c_glib/test/test-gio-output-stream.rb\n+++ b/c_glib/test/test-gio-output-stream.rb\n@@ -16,17 +16,23 @@\n # under the License.\n \n class TestGIOOutputStream < Test::Unit::TestCase\n+  include Helper::Buildable\n+\n   def test_writer_backend\n+    data = [true]\n+    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n+    schema = Arrow::Schema.new([field])\n+\n     tempfile = Tempfile.open(\"arrow-gio-output-stream\")\n     file = Gio::File.new_for_path(tempfile.path)\n     output_stream = file.append_to(:none)\n     output = Arrow::GIOOutputStream.new(output_stream)\n     begin\n-      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n-      schema = Arrow::Schema.new([field])\n       file_writer = Arrow::RecordBatchFileWriter.new(output, schema)\n       begin\n-        record_batch = Arrow::RecordBatch.new(schema, 0, [])\n+        record_batch = Arrow::RecordBatch.new(schema,\n+                                              data.size,\n+                                              [build_boolean_array(data)])\n         file_writer.write_record_batch(record_batch)\n       ensure\n         file_writer.close\n@@ -38,8 +44,12 @@ def test_writer_backend\n     input = Arrow::MemoryMappedInputStream.new(tempfile.path)\n     begin\n       file_reader = Arrow::RecordBatchFileReader.new(input)\n-      assert_equal([\"enabled\"],\n+      assert_equal([field.name],\n                    file_reader.schema.fields.collect(&:name))\n+      assert_equal(Arrow::RecordBatch.new(schema,\n+                                          data.size,\n+                                          [build_boolean_array(data)]),\n+                   file_reader.read_record_batch(0))\n     ensure\n       input.close\n     end\ndiff --git a/c_glib/test/test-stream-writer.rb b/c_glib/test/test-stream-writer.rb\nindex c3d0e1490..32754e208 100644\n--- a/c_glib/test/test-stream-writer.rb\n+++ b/c_glib/test/test-stream-writer.rb\n@@ -19,17 +19,19 @@ class TestStreamWriter < Test::Unit::TestCase\n   include Helper::Buildable\n \n   def test_write_record_batch\n+    data = [true]\n+    field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n+    schema = Arrow::Schema.new([field])\n+\n     tempfile = Tempfile.open(\"arrow-ipc-stream-writer\")\n     output = Arrow::FileOutputStream.new(tempfile.path, false)\n     begin\n-      field = Arrow::Field.new(\"enabled\", Arrow::BooleanDataType.new)\n-      schema = Arrow::Schema.new([field])\n       stream_writer = Arrow::RecordBatchStreamWriter.new(output, schema)\n       begin\n         columns = [\n-          build_boolean_array([true]),\n+          build_boolean_array(data),\n         ]\n-        record_batch = Arrow::RecordBatch.new(schema, 1, columns)\n+        record_batch = Arrow::RecordBatch.new(schema, data.size, columns)\n         stream_writer.write_record_batch(record_batch)\n       ensure\n         stream_writer.close\n@@ -41,10 +43,12 @@ def test_write_record_batch\n     input = Arrow::MemoryMappedInputStream.new(tempfile.path)\n     begin\n       stream_reader = Arrow::RecordBatchStreamReader.new(input)\n-      assert_equal([\"enabled\"],\n+      assert_equal([field.name],\n                    stream_reader.schema.fields.collect(&:name))\n-      assert_equal(true,\n-                   stream_reader.read_next.get_column(0).get_value(0))\n+      assert_equal(Arrow::RecordBatch.new(schema,\n+                                          data.size,\n+                                          [build_boolean_array(data)]),\n+                   stream_reader.read_next)\n       assert_nil(stream_reader.read_next)\n     ensure\n       input.close\ndiff --git a/ci/travis_before_script_cpp.sh b/ci/travis_before_script_cpp.sh\nindex 4998f190f..664f7ce5f 100755\n--- a/ci/travis_before_script_cpp.sh\n+++ b/ci/travis_before_script_cpp.sh\n@@ -91,12 +91,14 @@ fi\n if [ $TRAVIS_OS_NAME == \"linux\" ]; then\n     cmake $CMAKE_COMMON_FLAGS \\\n           $CMAKE_LINUX_FLAGS \\\n-          -DBUILD_WARNING_LEVEL=CHECKIN \\\n+          -DCMAKE_BUILD_TYPE=$ARROW_BUILD_TYPE \\\n+          -DBUILD_WARNING_LEVEL=$ARROW_BUILD_WARNING_LEVEL \\\n           $ARROW_CPP_DIR\n else\n     cmake $CMAKE_COMMON_FLAGS \\\n           $CMAKE_OSX_FLAGS \\\n-          -DBUILD_WARNING_LEVEL=CHECKIN \\\n+          -DCMAKE_BUILD_TYPE=$ARROW_BUILD_TYPE \\\n+          -DBUILD_WARNING_LEVEL=$ARROW_BUILD_WARNING_LEVEL \\\n           $ARROW_CPP_DIR\n fi\n \ndiff --git a/ci/travis_env_common.sh b/ci/travis_env_common.sh\nindex 52c7da4e0..21b6e266e 100755\n--- a/ci/travis_env_common.sh\n+++ b/ci/travis_env_common.sh\n@@ -38,6 +38,9 @@ export ARROW_PYTHON_PARQUET_HOME=$TRAVIS_BUILD_DIR/parquet-env\n \n export CMAKE_EXPORT_COMPILE_COMMANDS=1\n \n+export ARROW_BUILD_TYPE=${ARROW_BUILD_TYPE:=debug}\n+export ARROW_BUILD_WARNING_LEVEL=${ARROW_BUILD_WARNING_LEVEL:=Production}\n+\n if [ \"$ARROW_TRAVIS_USE_TOOLCHAIN\" == \"1\" ]; then\n   # C++ toolchain\n   export CPP_TOOLCHAIN=$TRAVIS_BUILD_DIR/cpp-toolchain\ndiff --git a/ci/travis_script_python.sh b/ci/travis_script_python.sh\nindex 603201bcc..5f7b0a9a1 100755\n--- a/ci/travis_script_python.sh\n+++ b/ci/travis_script_python.sh\n@@ -63,6 +63,7 @@ cmake -GNinja \\\n       -DARROW_BUILD_UTILITIES=off \\\n       -DARROW_PLASMA=on \\\n       -DARROW_PYTHON=on \\\n+      -DCMAKE_BUILD_TYPE=$ARROW_BUILD_TYPE \\\n       -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \\\n       $ARROW_CPP_DIR\n \n@@ -78,6 +79,8 @@ if [ \"$PYTHON_VERSION\" == \"2.7\" ]; then\n   pip install futures\n fi\n \n+export PYARROW_BUILD_TYPE=$ARROW_BUILD_TYPE\n+\n pip install -r requirements.txt\n python setup.py build_ext --with-parquet --with-plasma \\\n        install --single-version-externally-managed --record=record.text\ndiff --git a/cpp/src/arrow/CMakeLists.txt b/cpp/src/arrow/CMakeLists.txt\nindex 496e0da9d..94705781f 100644\n--- a/cpp/src/arrow/CMakeLists.txt\n+++ b/cpp/src/arrow/CMakeLists.txt\n@@ -22,6 +22,7 @@ set(ARROW_SRCS\n   compare.cc\n   memory_pool.cc\n   pretty_print.cc\n+  record_batch.cc\n   status.cc\n   table.cc\n   table_builder.cc\n@@ -144,6 +145,7 @@ install(FILES\n   compare.h\n   memory_pool.h\n   pretty_print.h\n+  record_batch.h\n   status.h\n   table.h\n   table_builder.h\ndiff --git a/cpp/src/arrow/api.h b/cpp/src/arrow/api.h\nindex 5d2e859f3..7cae8414a 100644\n--- a/cpp/src/arrow/api.h\n+++ b/cpp/src/arrow/api.h\n@@ -26,6 +26,7 @@\n #include \"arrow/compare.h\"\n #include \"arrow/memory_pool.h\"\n #include \"arrow/pretty_print.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/table_builder.h\"\ndiff --git a/cpp/src/arrow/array.h b/cpp/src/arrow/array.h\nindex 28756a6ab..dda9dd38b 100644\n--- a/cpp/src/arrow/array.h\n+++ b/cpp/src/arrow/array.h\n@@ -279,6 +279,8 @@ class ARROW_EXPORT Array {\n   ARROW_DISALLOW_COPY_AND_ASSIGN(Array);\n };\n \n+using ArrayVector = std::vector<std::shared_ptr<Array>>;\n+\n static inline std::ostream& operator<<(std::ostream& os, const Array& x) {\n   os << x.ToString();\n   return os;\ndiff --git a/cpp/src/arrow/builder.cc b/cpp/src/arrow/builder.cc\nindex 3e213fcd5..a42f90245 100644\n--- a/cpp/src/arrow/builder.cc\n+++ b/cpp/src/arrow/builder.cc\n@@ -28,7 +28,6 @@\n #include \"arrow/buffer.h\"\n #include \"arrow/compare.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bit-util.h\"\ndiff --git a/cpp/src/arrow/builder.h b/cpp/src/arrow/builder.h\nindex 32741b53a..e59e16658 100644\n--- a/cpp/src/arrow/builder.h\n+++ b/cpp/src/arrow/builder.h\n@@ -29,7 +29,6 @@\n #include \"arrow/buffer.h\"\n #include \"arrow/memory_pool.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bit-util.h\"\ndiff --git a/cpp/src/arrow/column-benchmark.cc b/cpp/src/arrow/column-benchmark.cc\nindex e50ddf6d7..af2c368c3 100644\n--- a/cpp/src/arrow/column-benchmark.cc\n+++ b/cpp/src/arrow/column-benchmark.cc\n@@ -19,6 +19,7 @@\n \n #include \"arrow/array.h\"\n #include \"arrow/memory_pool.h\"\n+#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n \n namespace arrow {\ndiff --git a/cpp/src/arrow/compute/kernel.h b/cpp/src/arrow/compute/kernel.h\nindex 0037245d6..e160d9c80 100644\n--- a/cpp/src/arrow/compute/kernel.h\n+++ b/cpp/src/arrow/compute/kernel.h\n@@ -22,6 +22,7 @@\n #include <vector>\n \n #include \"arrow/array.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/table.h\"\n #include \"arrow/util/macros.h\"\n #include \"arrow/util/variant.h\"\ndiff --git a/cpp/src/arrow/gpu/cuda_arrow_ipc.cc b/cpp/src/arrow/gpu/cuda_arrow_ipc.cc\nindex 022268e03..a7262c8b4 100644\n--- a/cpp/src/arrow/gpu/cuda_arrow_ipc.cc\n+++ b/cpp/src/arrow/gpu/cuda_arrow_ipc.cc\n@@ -27,8 +27,8 @@\n #include \"arrow/ipc/message.h\"\n #include \"arrow/ipc/reader.h\"\n #include \"arrow/ipc/writer.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/util/visibility.h\"\n \n #include \"arrow/gpu/cuda_context.h\"\ndiff --git a/cpp/src/arrow/ipc/feather-test.cc b/cpp/src/arrow/ipc/feather-test.cc\nindex 6bd16462d..e3de17f1f 100644\n--- a/cpp/src/arrow/ipc/feather-test.cc\n+++ b/cpp/src/arrow/ipc/feather-test.cc\n@@ -29,6 +29,7 @@\n #include \"arrow/ipc/feather.h\"\n #include \"arrow/ipc/test-common.h\"\n #include \"arrow/pretty_print.h\"\n+#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n \n namespace arrow {\n@@ -376,8 +377,8 @@ TEST_F(TestTableWriter, TimeTypes) {\n         schema->field(i)->type(), values->length(), buffers, values->null_count(), 0));\n   }\n \n-  RecordBatch batch(schema, values->length(), std::move(arrays));\n-  CheckBatch(batch);\n+  auto batch = RecordBatch::Make(schema, values->length(), std::move(arrays));\n+  CheckBatch(*batch);\n }\n \n TEST_F(TestTableWriter, VLenPrimitiveRoundTrip) {\ndiff --git a/cpp/src/arrow/ipc/feather.cc b/cpp/src/arrow/ipc/feather.cc\nindex cea720bd0..077dc3930 100644\n--- a/cpp/src/arrow/ipc/feather.cc\n+++ b/cpp/src/arrow/ipc/feather.cc\n@@ -32,6 +32,7 @@\n #include \"arrow/ipc/feather-internal.h\"\n #include \"arrow/ipc/feather_generated.h\"\n #include \"arrow/ipc/util.h\"  // IWYU pragma: keep\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/type.h\"\ndiff --git a/cpp/src/arrow/ipc/ipc-json-test.cc b/cpp/src/arrow/ipc/ipc-json-test.cc\nindex a560f09d6..e496826f9 100644\n--- a/cpp/src/arrow/ipc/ipc-json-test.cc\n+++ b/cpp/src/arrow/ipc/ipc-json-test.cc\n@@ -31,8 +31,8 @@\n #include \"arrow/ipc/json.h\"\n #include \"arrow/ipc/test-common.h\"\n #include \"arrow/memory_pool.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n@@ -269,7 +269,7 @@ TEST(TestJsonFileReadWrite, BasicRoundTrip) {\n     std::vector<std::shared_ptr<Array>> arrays;\n \n     MakeBatchArrays(schema, num_rows, &arrays);\n-    auto batch = std::make_shared<RecordBatch>(schema, num_rows, arrays);\n+    auto batch = RecordBatch::Make(schema, num_rows, arrays);\n     batches.push_back(batch);\n     ASSERT_OK(writer->WriteRecordBatch(*batch));\n   }\ndiff --git a/cpp/src/arrow/ipc/ipc-read-write-benchmark.cc b/cpp/src/arrow/ipc/ipc-read-write-benchmark.cc\nindex 9ed0abde6..8561fb860 100644\n--- a/cpp/src/arrow/ipc/ipc-read-write-benchmark.cc\n+++ b/cpp/src/arrow/ipc/ipc-read-write-benchmark.cc\n@@ -63,7 +63,7 @@ std::shared_ptr<RecordBatch> MakeRecordBatch(int64_t total_size, int64_t num_fie\n   }\n \n   auto schema = std::make_shared<Schema>(fields);\n-  return std::make_shared<RecordBatch>(schema, length, arrays);\n+  return RecordBatch::Make(schema, length, arrays);\n }\n \n static void BM_WriteRecordBatch(benchmark::State& state) {  // NOLINT non-const reference\ndiff --git a/cpp/src/arrow/ipc/ipc-read-write-test.cc b/cpp/src/arrow/ipc/ipc-read-write-test.cc\nindex 40cd3f0ee..1fcbdac5e 100644\n--- a/cpp/src/arrow/ipc/ipc-read-write-test.cc\n+++ b/cpp/src/arrow/ipc/ipc-read-write-test.cc\n@@ -197,8 +197,8 @@ class IpcTestFixture : public io::MemoryMapFixture {\n     std::vector<std::shared_ptr<Field>> fields = {f0};\n     auto schema = std::make_shared<Schema>(fields);\n \n-    RecordBatch batch(schema, 0, {array});\n-    CheckRoundtrip(batch, buffer_size);\n+    auto batch = RecordBatch::Make(schema, 0, {array});\n+    CheckRoundtrip(*batch, buffer_size);\n   }\n \n  protected:\n@@ -292,13 +292,13 @@ TEST_F(TestWriteRecordBatch, SliceTruncatesBuffers) {\n   auto CheckArray = [this](const std::shared_ptr<Array>& array) {\n     auto f0 = field(\"f0\", array->type());\n     auto schema = ::arrow::schema({f0});\n-    RecordBatch batch(schema, array->length(), {array});\n-    auto sliced_batch = batch.Slice(0, 5);\n+    auto batch = RecordBatch::Make(schema, array->length(), {array});\n+    auto sliced_batch = batch->Slice(0, 5);\n \n     int64_t full_size;\n     int64_t sliced_size;\n \n-    ASSERT_OK(GetRecordBatchSize(batch, &full_size));\n+    ASSERT_OK(GetRecordBatchSize(*batch, &full_size));\n     ASSERT_OK(GetRecordBatchSize(*sliced_batch, &sliced_size));\n     ASSERT_TRUE(sliced_size < full_size) << sliced_size << \" \" << full_size;\n \n@@ -411,8 +411,7 @@ class RecursionLimits : public ::testing::Test, public io::MemoryMapFixture {\n \n     *schema = ::arrow::schema({f0});\n \n-    std::vector<std::shared_ptr<Array>> arrays = {array};\n-    *batch = std::make_shared<RecordBatch>(*schema, batch_length, arrays);\n+    *batch = RecordBatch::Make(*schema, batch_length, {array});\n \n     std::stringstream ss;\n     ss << \"test-write-past-max-recursion-\" << g_file_number++;\n@@ -632,7 +631,7 @@ TEST_F(TestIpcRoundTrip, LargeRecordBatch) {\n   std::vector<std::shared_ptr<Field>> fields = {f0};\n   auto schema = std::make_shared<Schema>(fields);\n \n-  RecordBatch batch(schema, length, {array});\n+  auto batch = RecordBatch::Make(schema, length, {array});\n \n   std::string path = \"test-write-large-record_batch\";\n \n@@ -641,8 +640,8 @@ TEST_F(TestIpcRoundTrip, LargeRecordBatch) {\n   ASSERT_OK(io::MemoryMapFixture::InitMemoryMap(kBufferSize, path, &mmap_));\n \n   std::shared_ptr<RecordBatch> result;\n-  ASSERT_OK(DoLargeRoundTrip(batch, false, &result));\n-  CheckReadResult(*result, batch);\n+  ASSERT_OK(DoLargeRoundTrip(*batch, false, &result));\n+  CheckReadResult(*result, *batch);\n \n   ASSERT_EQ(length, result->num_rows());\n }\ndiff --git a/cpp/src/arrow/ipc/json-integration-test.cc b/cpp/src/arrow/ipc/json-integration-test.cc\nindex c7530a467..f487487df 100644\n--- a/cpp/src/arrow/ipc/json-integration-test.cc\n+++ b/cpp/src/arrow/ipc/json-integration-test.cc\n@@ -34,8 +34,8 @@\n #include \"arrow/ipc/reader.h\"\n #include \"arrow/ipc/writer.h\"\n #include \"arrow/pretty_print.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n #include \"arrow/type.h\"\n \ndiff --git a/cpp/src/arrow/ipc/json-internal.cc b/cpp/src/arrow/ipc/json-internal.cc\nindex bdf1ef52b..bfb3d282d 100644\n--- a/cpp/src/arrow/ipc/json-internal.cc\n+++ b/cpp/src/arrow/ipc/json-internal.cc\n@@ -28,8 +28,8 @@\n #include \"arrow/array.h\"\n #include \"arrow/builder.h\"\n #include \"arrow/ipc/dictionary.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bit-util.h\"\n@@ -125,8 +125,8 @@ class SchemaWriter {\n \n     // Make a dummy record batch. A bit tedious as we have to make a schema\n     auto schema = ::arrow::schema({arrow::field(\"dictionary\", dictionary->type())});\n-    RecordBatch batch(schema, dictionary->length(), {dictionary});\n-    RETURN_NOT_OK(WriteRecordBatch(batch, writer_));\n+    auto batch = RecordBatch::Make(schema, dictionary->length(), {dictionary});\n+    RETURN_NOT_OK(WriteRecordBatch(*batch, writer_));\n     writer_->EndObject();\n     return Status::OK();\n   }\n@@ -1435,7 +1435,7 @@ Status ReadRecordBatch(const rj::Value& json_obj, const std::shared_ptr<Schema>&\n     RETURN_NOT_OK(ReadArray(pool, json_columns[i], type, &columns[i]));\n   }\n \n-  *batch = std::make_shared<RecordBatch>(schema, num_rows, columns);\n+  *batch = RecordBatch::Make(schema, num_rows, columns);\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/arrow/ipc/json.cc b/cpp/src/arrow/ipc/json.cc\nindex 30a1bb81e..ea2947d5d 100644\n--- a/cpp/src/arrow/ipc/json.cc\n+++ b/cpp/src/arrow/ipc/json.cc\n@@ -24,8 +24,8 @@\n #include \"arrow/buffer.h\"\n #include \"arrow/ipc/json-internal.h\"\n #include \"arrow/memory_pool.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/util/logging.h\"\n \ndiff --git a/cpp/src/arrow/ipc/reader.cc b/cpp/src/arrow/ipc/reader.cc\nindex 8e10d7d66..5960e8188 100644\n--- a/cpp/src/arrow/ipc/reader.cc\n+++ b/cpp/src/arrow/ipc/reader.cc\n@@ -37,8 +37,8 @@\n #include \"arrow/ipc/message.h\"\n #include \"arrow/ipc/metadata-internal.h\"\n #include \"arrow/ipc/util.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/tensor.h\"\n #include \"arrow/type.h\"\n #include \"arrow/util/bit-util.h\"\n@@ -307,7 +307,7 @@ static Status LoadRecordBatchFromSource(const std::shared_ptr<Schema>& schema,\n     arrays[i] = std::move(arr);\n   }\n \n-  *out = std::make_shared<RecordBatch>(schema, num_rows, std::move(arrays));\n+  *out = RecordBatch::Make(schema, num_rows, std::move(arrays));\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/arrow/ipc/reader.h b/cpp/src/arrow/ipc/reader.h\nindex 7581fbda5..627f67e25 100644\n--- a/cpp/src/arrow/ipc/reader.h\n+++ b/cpp/src/arrow/ipc/reader.h\n@@ -24,13 +24,12 @@\n #include <memory>\n \n #include \"arrow/ipc/message.h\"\n-#include \"arrow/table.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/util/visibility.h\"\n \n namespace arrow {\n \n class Buffer;\n-class RecordBatch;\n class Schema;\n class Status;\n class Tensor;\ndiff --git a/cpp/src/arrow/ipc/test-common.h b/cpp/src/arrow/ipc/test-common.h\nindex 7fc139381..6f8a0dcc6 100644\n--- a/cpp/src/arrow/ipc/test-common.h\n+++ b/cpp/src/arrow/ipc/test-common.h\n@@ -30,8 +30,8 @@\n #include \"arrow/builder.h\"\n #include \"arrow/memory_pool.h\"\n #include \"arrow/pretty_print.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n #include \"arrow/type.h\"\n #include \"arrow/util/bit-util.h\"\n@@ -184,7 +184,7 @@ Status MakeBooleanBatchSized(const int length, std::shared_ptr<RecordBatch>* out\n   std::shared_ptr<Array> a0, a1;\n   RETURN_NOT_OK(MakeRandomBooleanArray(length, true, &a0));\n   RETURN_NOT_OK(MakeRandomBooleanArray(length, false, &a1));\n-  out->reset(new RecordBatch(schema, length, {a0, a1}));\n+  *out = RecordBatch::Make(schema, length, {a0, a1});\n   return Status::OK();\n }\n \n@@ -203,7 +203,7 @@ Status MakeIntBatchSized(int length, std::shared_ptr<RecordBatch>* out) {\n   MemoryPool* pool = default_memory_pool();\n   RETURN_NOT_OK(MakeRandomInt32Array(length, false, pool, &a0));\n   RETURN_NOT_OK(MakeRandomInt32Array(length, true, pool, &a1));\n-  out->reset(new RecordBatch(schema, length, {a0, a1}));\n+  *out = RecordBatch::Make(schema, length, {a0, a1});\n   return Status::OK();\n }\n \n@@ -252,7 +252,7 @@ Status MakeStringTypesRecordBatch(std::shared_ptr<RecordBatch>* out) {\n     auto s = MakeRandomBinaryArray<BinaryBuilder, uint8_t>(length, true, pool, &a1);\n     RETURN_NOT_OK(s);\n   }\n-  out->reset(new RecordBatch(schema, length, {a0, a1}));\n+  *out = RecordBatch::Make(schema, length, {a0, a1});\n   return Status::OK();\n }\n \n@@ -261,7 +261,7 @@ Status MakeNullRecordBatch(std::shared_ptr<RecordBatch>* out) {\n   auto f0 = field(\"f0\", null());\n   auto schema = ::arrow::schema({f0});\n   std::shared_ptr<Array> a0 = std::make_shared<NullArray>(length);\n-  out->reset(new RecordBatch(schema, length, {a0}));\n+  *out = RecordBatch::Make(schema, length, {a0});\n   return Status::OK();\n }\n \n@@ -284,7 +284,7 @@ Status MakeListRecordBatch(std::shared_ptr<RecordBatch>* out) {\n   RETURN_NOT_OK(\n       MakeRandomListArray(list_array, length, include_nulls, pool, &list_list_array));\n   RETURN_NOT_OK(MakeRandomInt32Array(length, include_nulls, pool, &flat_array));\n-  out->reset(new RecordBatch(schema, length, {list_array, list_list_array, flat_array}));\n+  *out = RecordBatch::Make(schema, length, {list_array, list_list_array, flat_array});\n   return Status::OK();\n }\n \n@@ -304,7 +304,7 @@ Status MakeZeroLengthRecordBatch(std::shared_ptr<RecordBatch>* out) {\n   RETURN_NOT_OK(\n       MakeRandomListArray(list_array, 0, include_nulls, pool, &list_list_array));\n   RETURN_NOT_OK(MakeRandomInt32Array(0, include_nulls, pool, &flat_array));\n-  out->reset(new RecordBatch(schema, 0, {list_array, list_list_array, flat_array}));\n+  *out = RecordBatch::Make(schema, 0, {list_array, list_list_array, flat_array});\n   return Status::OK();\n }\n \n@@ -327,7 +327,7 @@ Status MakeNonNullRecordBatch(std::shared_ptr<RecordBatch>* out) {\n   RETURN_NOT_OK(\n       MakeRandomListArray(list_array, length, include_nulls, pool, &list_list_array));\n   RETURN_NOT_OK(MakeRandomInt32Array(length, include_nulls, pool, &flat_array));\n-  out->reset(new RecordBatch(schema, length, {list_array, list_list_array, flat_array}));\n+  *out = RecordBatch::Make(schema, length, {list_array, list_list_array, flat_array});\n   return Status::OK();\n }\n \n@@ -347,7 +347,7 @@ Status MakeDeeplyNestedList(std::shared_ptr<RecordBatch>* out) {\n   auto f0 = field(\"f0\", type);\n   auto schema = ::arrow::schema({f0});\n   std::vector<std::shared_ptr<Array>> arrays = {array};\n-  out->reset(new RecordBatch(schema, batch_length, arrays));\n+  *out = RecordBatch::Make(schema, batch_length, arrays);\n   return Status::OK();\n }\n \n@@ -377,7 +377,7 @@ Status MakeStruct(std::shared_ptr<RecordBatch>* out) {\n \n   // construct batch\n   std::vector<std::shared_ptr<Array>> arrays = {no_nulls, with_nulls};\n-  out->reset(new RecordBatch(schema, list_batch->num_rows(), arrays));\n+  *out = RecordBatch::Make(schema, list_batch->num_rows(), arrays);\n   return Status::OK();\n }\n \n@@ -445,7 +445,7 @@ Status MakeUnion(std::shared_ptr<RecordBatch>* out) {\n \n   // construct batch\n   std::vector<std::shared_ptr<Array>> arrays = {sparse_no_nulls, sparse, dense};\n-  out->reset(new RecordBatch(schema, length, arrays));\n+  *out = RecordBatch::Make(schema, length, arrays);\n   return Status::OK();\n }\n \n@@ -526,7 +526,7 @@ Status MakeDictionary(std::shared_ptr<RecordBatch>* out) {\n \n   std::vector<std::shared_ptr<Array>> arrays = {a0, a1, a2, a3, a4};\n \n-  out->reset(new RecordBatch(schema, length, arrays));\n+  *out = RecordBatch::Make(schema, length, arrays);\n   return Status::OK();\n }\n \n@@ -564,7 +564,7 @@ Status MakeDictionaryFlat(std::shared_ptr<RecordBatch>* out) {\n       {field(\"dict1\", f0_type), field(\"sparse\", f1_type), field(\"dense\", f2_type)});\n \n   std::vector<std::shared_ptr<Array>> arrays = {a0, a1, a2};\n-  out->reset(new RecordBatch(schema, length, arrays));\n+  *out = RecordBatch::Make(schema, length, arrays);\n   return Status::OK();\n }\n \n@@ -584,8 +584,7 @@ Status MakeDates(std::shared_ptr<RecordBatch>* out) {\n   std::shared_ptr<Array> date64_array;\n   ArrayFromVector<Date64Type, int64_t>(is_valid, date64_values, &date64_array);\n \n-  std::vector<std::shared_ptr<Array>> arrays = {date32_array, date64_array};\n-  *out = std::make_shared<RecordBatch>(schema, date32_array->length(), arrays);\n+  *out = RecordBatch::Make(schema, date32_array->length(), {date32_array, date64_array});\n   return Status::OK();\n }\n \n@@ -604,8 +603,7 @@ Status MakeTimestamps(std::shared_ptr<RecordBatch>* out) {\n   ArrayFromVector<TimestampType, int64_t>(f1->type(), is_valid, ts_values, &a1);\n   ArrayFromVector<TimestampType, int64_t>(f2->type(), is_valid, ts_values, &a2);\n \n-  ArrayVector arrays = {a0, a1, a2};\n-  *out = std::make_shared<RecordBatch>(schema, a0->length(), arrays);\n+  *out = RecordBatch::Make(schema, a0->length(), {a0, a1, a2});\n   return Status::OK();\n }\n \n@@ -628,8 +626,7 @@ Status MakeTimes(std::shared_ptr<RecordBatch>* out) {\n   ArrayFromVector<Time32Type, int32_t>(f2->type(), is_valid, t32_values, &a2);\n   ArrayFromVector<Time64Type, int64_t>(f3->type(), is_valid, t64_values, &a3);\n \n-  ArrayVector arrays = {a0, a1, a2, a3};\n-  *out = std::make_shared<RecordBatch>(schema, a0->length(), arrays);\n+  *out = RecordBatch::Make(schema, a0->length(), {a0, a1, a2, a3});\n   return Status::OK();\n }\n \n@@ -665,8 +662,7 @@ Status MakeFWBinary(std::shared_ptr<RecordBatch>* out) {\n   RETURN_NOT_OK(b1.Finish(&a1));\n   RETURN_NOT_OK(b2.Finish(&a2));\n \n-  ArrayVector arrays = {a1, a2};\n-  *out = std::make_shared<RecordBatch>(schema, a1->length(), arrays);\n+  *out = RecordBatch::Make(schema, a1->length(), {a1, a2});\n   return Status::OK();\n }\n \n@@ -695,8 +691,7 @@ Status MakeDecimal(std::shared_ptr<RecordBatch>* out) {\n \n   auto a2 = std::make_shared<Decimal128Array>(f1->type(), length, data);\n \n-  ArrayVector arrays = {a1, a2};\n-  *out = std::make_shared<RecordBatch>(schema, length, arrays);\n+  *out = RecordBatch::Make(schema, length, {a1, a2});\n   return Status::OK();\n }\n \n@@ -716,8 +711,7 @@ Status MakeNull(std::shared_ptr<RecordBatch>* out) {\n   std::shared_ptr<Array> a2;\n   ArrayFromVector<Int64Type, int64_t>(f1->type(), is_valid, int_values, &a2);\n \n-  ArrayVector arrays = {a1, a2};\n-  *out = std::make_shared<RecordBatch>(schema, a1->length(), arrays);\n+  *out = RecordBatch::Make(schema, a1->length(), {a1, a2});\n   return Status::OK();\n }\n \ndiff --git a/cpp/src/arrow/ipc/writer.cc b/cpp/src/arrow/ipc/writer.cc\nindex 323116f58..3c1db0615 100644\n--- a/cpp/src/arrow/ipc/writer.cc\n+++ b/cpp/src/arrow/ipc/writer.cc\n@@ -32,6 +32,7 @@\n #include \"arrow/ipc/metadata-internal.h\"\n #include \"arrow/ipc/util.h\"\n #include \"arrow/memory_pool.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/tensor.h\"\n@@ -508,12 +509,9 @@ class DictionaryWriter : public RecordBatchSerializer {\n     dictionary_id_ = dictionary_id;\n \n     // Make a dummy record batch. A bit tedious as we have to make a schema\n-    std::vector<std::shared_ptr<Field>> fields = {\n-        arrow::field(\"dictionary\", dictionary->type())};\n-    auto schema = std::make_shared<Schema>(fields);\n-    RecordBatch batch(schema, dictionary->length(), {dictionary});\n-\n-    return RecordBatchSerializer::Write(batch, dst, metadata_length, body_length);\n+    auto schema = arrow::schema({arrow::field(\"dictionary\", dictionary->type())});\n+    auto batch = RecordBatch::Make(schema, dictionary->length(), {dictionary});\n+    return RecordBatchSerializer::Write(*batch, dst, metadata_length, body_length);\n   }\n \n  private:\ndiff --git a/cpp/src/arrow/pretty_print.cc b/cpp/src/arrow/pretty_print.cc\nindex cfbc30315..bd5f8ce10 100644\n--- a/cpp/src/arrow/pretty_print.cc\n+++ b/cpp/src/arrow/pretty_print.cc\n@@ -22,8 +22,8 @@\n \n #include \"arrow/array.h\"\n #include \"arrow/pretty_print.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/logging.h\"\ndiff --git a/cpp/src/arrow/python/python-test.cc b/cpp/src/arrow/python/python-test.cc\nindex 86391a185..3b7d7d884 100644\n--- a/cpp/src/arrow/python/python-test.cc\n+++ b/cpp/src/arrow/python/python-test.cc\n@@ -23,6 +23,7 @@\n \n #include \"arrow/array.h\"\n #include \"arrow/builder.h\"\n+#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n \n #include \"arrow/python/arrow_to_pandas.h\"\n@@ -81,8 +82,8 @@ TEST(PandasConversionTest, TestObjectBlockWriteFails) {\n   std::vector<std::shared_ptr<Field>> fields = {f1, f2, f3};\n   std::vector<std::shared_ptr<Array>> cols = {arr, arr, arr};\n \n-  auto schema = std::make_shared<Schema>(fields);\n-  auto table = std::make_shared<Table>(schema, cols);\n+  auto schema = ::arrow::schema(fields);\n+  auto table = Table::Make(schema, cols);\n \n   PyObject* out;\n   Py_BEGIN_ALLOW_THREADS;\ndiff --git a/cpp/src/arrow/python/python_to_arrow.cc b/cpp/src/arrow/python/python_to_arrow.cc\nindex b0c6287f0..72cc5b6e1 100644\n--- a/cpp/src/arrow/python/python_to_arrow.cc\n+++ b/cpp/src/arrow/python/python_to_arrow.cc\n@@ -32,13 +32,15 @@\n #include \"arrow/builder.h\"\n #include \"arrow/io/interfaces.h\"\n #include \"arrow/ipc/writer.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/tensor.h\"\n+#include \"arrow/util/logging.h\"\n+\n #include \"arrow/python/common.h\"\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/numpy_convert.h\"\n #include \"arrow/python/platform.h\"\n #include \"arrow/python/util/datetime.h\"\n-#include \"arrow/tensor.h\"\n-#include \"arrow/util/logging.h\"\n \n constexpr int32_t kMaxRecursionDepth = 100;\n \n@@ -694,7 +696,7 @@ Status SerializeDict(PyObject* context, std::vector<PyObject*> dicts,\n std::shared_ptr<RecordBatch> MakeBatch(std::shared_ptr<Array> data) {\n   auto field = std::make_shared<Field>(\"list\", data->type());\n   auto schema = ::arrow::schema({field});\n-  return std::shared_ptr<RecordBatch>(new RecordBatch(schema, data->length(), {data}));\n+  return RecordBatch::Make(schema, data->length(), {data});\n }\n \n Status SerializeObject(PyObject* context, PyObject* sequence, SerializedPyObject* out) {\ndiff --git a/cpp/src/arrow/record_batch.cc b/cpp/src/arrow/record_batch.cc\nnew file mode 100644\nindex 000000000..60932bdf3\n--- /dev/null\n+++ b/cpp/src/arrow/record_batch.cc\n@@ -0,0 +1,206 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/record_batch.h\"\n+\n+#include <algorithm>\n+#include <cstdlib>\n+#include <memory>\n+#include <sstream>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/logging.h\"\n+\n+namespace arrow {\n+\n+/// \\class SimpleRecordBatch\n+/// \\brief A basic, non-lazy in-memory record batch\n+class SimpleRecordBatch : public RecordBatch {\n+ public:\n+  SimpleRecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+                    const std::vector<std::shared_ptr<Array>>& columns)\n+      : RecordBatch(schema, num_rows) {\n+    columns_.resize(columns.size());\n+    boxed_columns_.resize(schema->num_fields());\n+    for (size_t i = 0; i < columns.size(); ++i) {\n+      columns_[i] = columns[i]->data();\n+    }\n+  }\n+\n+  SimpleRecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+                    std::vector<std::shared_ptr<Array>>&& columns)\n+      : RecordBatch(schema, num_rows) {\n+    columns_.resize(columns.size());\n+    boxed_columns_.resize(schema->num_fields());\n+    for (size_t i = 0; i < columns.size(); ++i) {\n+      columns_[i] = columns[i]->data();\n+    }\n+  }\n+\n+  SimpleRecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+                    std::vector<std::shared_ptr<ArrayData>>&& columns)\n+      : RecordBatch(schema, num_rows) {\n+    columns_ = std::move(columns);\n+    boxed_columns_.resize(schema->num_fields());\n+  }\n+\n+  SimpleRecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+                    const std::vector<std::shared_ptr<ArrayData>>& columns)\n+      : RecordBatch(schema, num_rows) {\n+    columns_ = columns;\n+    boxed_columns_.resize(schema->num_fields());\n+  }\n+\n+  std::shared_ptr<Array> column(int i) const override {\n+    if (!boxed_columns_[i]) {\n+      boxed_columns_[i] = MakeArray(columns_[i]);\n+    }\n+    DCHECK(boxed_columns_[i]);\n+    return boxed_columns_[i];\n+  }\n+\n+  std::shared_ptr<ArrayData> column_data(int i) const override { return columns_[i]; }\n+\n+  std::shared_ptr<RecordBatch> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const override {\n+    auto new_schema = schema_->AddMetadata(metadata);\n+    return RecordBatch::Make(new_schema, num_rows_, columns_);\n+  }\n+\n+  std::shared_ptr<RecordBatch> Slice(int64_t offset, int64_t length) const override {\n+    std::vector<std::shared_ptr<ArrayData>> arrays;\n+    arrays.reserve(num_columns());\n+    for (const auto& field : columns_) {\n+      int64_t col_length = std::min(field->length - offset, length);\n+      int64_t col_offset = field->offset + offset;\n+\n+      auto new_data = std::make_shared<ArrayData>(*field);\n+      new_data->length = col_length;\n+      new_data->offset = col_offset;\n+      new_data->null_count = kUnknownNullCount;\n+      arrays.emplace_back(new_data);\n+    }\n+    int64_t num_rows = std::min(num_rows_ - offset, length);\n+    return std::make_shared<SimpleRecordBatch>(schema_, num_rows, std::move(arrays));\n+  }\n+\n+  Status Validate() const override {\n+    if (static_cast<int>(columns_.size()) != schema_->num_fields()) {\n+      return Status::Invalid(\"Number of columns did not match schema\");\n+    }\n+    return RecordBatch::Validate();\n+  }\n+\n+ private:\n+  std::vector<std::shared_ptr<ArrayData>> columns_;\n+\n+  // Caching boxed array data\n+  mutable std::vector<std::shared_ptr<Array>> boxed_columns_;\n+};\n+\n+RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows)\n+    : schema_(schema), num_rows_(num_rows) {}\n+\n+std::shared_ptr<RecordBatch> RecordBatch::Make(\n+    const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+    const std::vector<std::shared_ptr<Array>>& columns) {\n+  return std::make_shared<SimpleRecordBatch>(schema, num_rows, columns);\n+}\n+\n+std::shared_ptr<RecordBatch> RecordBatch::Make(\n+    const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+    std::vector<std::shared_ptr<Array>>&& columns) {\n+  return std::make_shared<SimpleRecordBatch>(schema, num_rows, std::move(columns));\n+}\n+\n+std::shared_ptr<RecordBatch> RecordBatch::Make(\n+    const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+    std::vector<std::shared_ptr<ArrayData>>&& columns) {\n+  return std::make_shared<SimpleRecordBatch>(schema, num_rows, std::move(columns));\n+}\n+\n+std::shared_ptr<RecordBatch> RecordBatch::Make(\n+    const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+    const std::vector<std::shared_ptr<ArrayData>>& columns) {\n+  return std::make_shared<SimpleRecordBatch>(schema, num_rows, columns);\n+}\n+\n+const std::string& RecordBatch::column_name(int i) const {\n+  return schema_->field(i)->name();\n+}\n+\n+bool RecordBatch::Equals(const RecordBatch& other) const {\n+  if (num_columns() != other.num_columns() || num_rows_ != other.num_rows()) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < num_columns(); ++i) {\n+    if (!column(i)->Equals(other.column(i))) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool RecordBatch::ApproxEquals(const RecordBatch& other) const {\n+  if (num_columns() != other.num_columns() || num_rows_ != other.num_rows()) {\n+    return false;\n+  }\n+\n+  for (int i = 0; i < num_columns(); ++i) {\n+    if (!column(i)->ApproxEquals(other.column(i))) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+std::shared_ptr<RecordBatch> RecordBatch::Slice(int64_t offset) const {\n+  return Slice(offset, this->num_rows() - offset);\n+}\n+\n+Status RecordBatch::Validate() const {\n+  for (int i = 0; i < num_columns(); ++i) {\n+    auto arr_shared = this->column_data(i);\n+    const ArrayData& arr = *arr_shared;\n+    if (arr.length != num_rows_) {\n+      std::stringstream ss;\n+      ss << \"Number of rows in column \" << i << \" did not match batch: \" << arr.length\n+         << \" vs \" << num_rows_;\n+      return Status::Invalid(ss.str());\n+    }\n+    const auto& schema_type = *schema_->field(i)->type();\n+    if (!arr.type->Equals(schema_type)) {\n+      std::stringstream ss;\n+      ss << \"Column \" << i << \" type not match schema: \" << arr.type->ToString() << \" vs \"\n+         << schema_type.ToString();\n+      return Status::Invalid(ss.str());\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+// ----------------------------------------------------------------------\n+// Base record batch reader\n+\n+RecordBatchReader::~RecordBatchReader() {}\n+\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/record_batch.h b/cpp/src/arrow/record_batch.h\nnew file mode 100644\nindex 000000000..b2c4c76b3\n--- /dev/null\n+++ b/cpp/src/arrow/record_batch.h\n@@ -0,0 +1,154 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_RECORD_BATCH_H\n+#define ARROW_RECORD_BATCH_H\n+\n+#include <cstdint>\n+#include <memory>\n+#include <string>\n+#include <vector>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/macros.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+namespace arrow {\n+\n+class KeyValueMetadata;\n+class Status;\n+\n+/// \\class RecordBatch\n+/// \\brief Collection of equal-length arrays matching a particular Schema\n+///\n+/// A record batch is table-like data structure that is semantically a sequence\n+/// of fields, each a contiguous Arrow array\n+class ARROW_EXPORT RecordBatch {\n+ public:\n+  virtual ~RecordBatch() = default;\n+\n+  /// \\param[in] schema The record batch schema\n+  /// \\param[in] num_rows length of fields in the record batch. Each array\n+  /// should have the same length as num_rows\n+  /// \\param[in] columns the record batch fields as vector of arrays\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<Array>>& columns);\n+\n+  /// \\brief Move-based constructor for a vector of Array instances\n+  static std::shared_ptr<RecordBatch> Make(const std::shared_ptr<Schema>& schema,\n+                                           int64_t num_rows,\n+                                           std::vector<std::shared_ptr<Array>>&& columns);\n+\n+  /// \\brief Construct record batch from vector of internal data structures\n+  /// \\since 0.5.0\n+  ///\n+  /// This class is only provided with an rvalue-reference for the input data,\n+  /// and is intended for internal use, or advanced users.\n+  ///\n+  /// \\param schema the record batch schema\n+  /// \\param num_rows the number of semantic rows in the record batch. This\n+  /// should be equal to the length of each field\n+  /// \\param columns the data for the batch's columns\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      std::vector<std::shared_ptr<ArrayData>>&& columns);\n+\n+  /// \\brief Construct record batch by copying vector of array data\n+  /// \\since 0.5.0\n+  static std::shared_ptr<RecordBatch> Make(\n+      const std::shared_ptr<Schema>& schema, int64_t num_rows,\n+      const std::vector<std::shared_ptr<ArrayData>>& columns);\n+\n+  /// \\brief Determine if two record batches are exactly equal\n+  /// \\return true if batches are equal\n+  bool Equals(const RecordBatch& other) const;\n+\n+  /// \\brief Determine if two record batches are approximately equal\n+  bool ApproxEquals(const RecordBatch& other) const;\n+\n+  // \\return the table's schema\n+  /// \\return true if batches are equal\n+  std::shared_ptr<Schema> schema() const { return schema_; }\n+\n+  /// \\brief Retrieve an array from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an Array object\n+  virtual std::shared_ptr<Array> column(int i) const = 0;\n+\n+  /// \\brief Retrieve an array's internaldata from the record batch\n+  /// \\param[in] i field index, does not boundscheck\n+  /// \\return an internal ArrayData object\n+  virtual std::shared_ptr<ArrayData> column_data(int i) const = 0;\n+\n+  virtual std::shared_ptr<RecordBatch> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const = 0;\n+\n+  /// \\brief Name in i-th column\n+  const std::string& column_name(int i) const;\n+\n+  /// \\return the number of columns in the table\n+  int num_columns() const { return schema_->num_fields(); }\n+\n+  /// \\return the number of rows (the corresponding length of each column)\n+  int64_t num_rows() const { return num_rows_; }\n+\n+  /// \\brief Slice each of the arrays in the record batch\n+  /// \\param[in] offset the starting offset to slice, through end of batch\n+  /// \\return new record batch\n+  virtual std::shared_ptr<RecordBatch> Slice(int64_t offset) const;\n+\n+  /// \\brief Slice each of the arrays in the record batch\n+  /// \\param[in] offset the starting offset to slice\n+  /// \\param[in] length the number of elements to slice from offset\n+  /// \\return new record batch\n+  virtual std::shared_ptr<RecordBatch> Slice(int64_t offset, int64_t length) const = 0;\n+\n+  /// \\brief Check for schema or length inconsistencies\n+  /// \\return Status\n+  virtual Status Validate() const;\n+\n+ protected:\n+  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows);\n+\n+  std::shared_ptr<Schema> schema_;\n+  int64_t num_rows_;\n+\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(RecordBatch);\n+};\n+\n+/// \\brief Abstract interface for reading stream of record batches\n+class ARROW_EXPORT RecordBatchReader {\n+ public:\n+  virtual ~RecordBatchReader();\n+\n+  /// \\return the shared schema of the record batches in the stream\n+  virtual std::shared_ptr<Schema> schema() const = 0;\n+\n+  /// Read the next record batch in the stream. Return null for batch when\n+  /// reaching end of stream\n+  ///\n+  /// \\param[out] batch the next loaded batch, null at end of stream\n+  /// \\return Status\n+  virtual Status ReadNext(std::shared_ptr<RecordBatch>* batch) = 0;\n+};\n+\n+}  // namespace arrow\n+\n+#endif  // ARROW_RECORD_BATCH_H\ndiff --git a/cpp/src/arrow/table-test.cc b/cpp/src/arrow/table-test.cc\nindex b490310c2..e77d3aa8b 100644\n--- a/cpp/src/arrow/table-test.cc\n+++ b/cpp/src/arrow/table-test.cc\n@@ -22,6 +22,7 @@\n #include \"gtest/gtest.h\"\n \n #include \"arrow/array.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/test-common.h\"\n@@ -216,8 +217,8 @@ class TestTable : public TestBase {\n \n TEST_F(TestTable, EmptySchema) {\n   auto empty_schema = ::arrow::schema({});\n-  table_.reset(new Table(empty_schema, columns_));\n-  ASSERT_OK(table_->ValidateColumns());\n+  table_ = Table::Make(empty_schema, columns_);\n+  ASSERT_OK(table_->Validate());\n   ASSERT_EQ(0, table_->num_rows());\n   ASSERT_EQ(0, table_->num_columns());\n }\n@@ -226,20 +227,20 @@ TEST_F(TestTable, Ctors) {\n   const int length = 100;\n   MakeExample1(length);\n \n-  table_.reset(new Table(schema_, columns_));\n-  ASSERT_OK(table_->ValidateColumns());\n+  table_ = Table::Make(schema_, columns_);\n+  ASSERT_OK(table_->Validate());\n   ASSERT_EQ(length, table_->num_rows());\n   ASSERT_EQ(3, table_->num_columns());\n \n-  auto array_ctor = std::make_shared<Table>(schema_, arrays_);\n+  auto array_ctor = Table::Make(schema_, arrays_);\n   ASSERT_TRUE(table_->Equals(*array_ctor));\n \n-  table_.reset(new Table(schema_, columns_, length));\n-  ASSERT_OK(table_->ValidateColumns());\n+  table_ = Table::Make(schema_, columns_, length);\n+  ASSERT_OK(table_->Validate());\n   ASSERT_EQ(length, table_->num_rows());\n \n-  ASSERT_OK(MakeTable(schema_, arrays_, &table_));\n-  ASSERT_OK(table_->ValidateColumns());\n+  table_ = Table::Make(schema_, arrays_);\n+  ASSERT_OK(table_->Validate());\n   ASSERT_EQ(length, table_->num_rows());\n   ASSERT_EQ(3, table_->num_columns());\n }\n@@ -248,7 +249,7 @@ TEST_F(TestTable, Metadata) {\n   const int length = 100;\n   MakeExample1(length);\n \n-  table_.reset(new Table(schema_, columns_));\n+  table_ = Table::Make(schema_, columns_);\n \n   ASSERT_TRUE(table_->schema()->Equals(*schema_));\n \n@@ -262,14 +263,14 @@ TEST_F(TestTable, InvalidColumns) {\n   const int length = 100;\n   MakeExample1(length);\n \n-  table_.reset(new Table(schema_, columns_, length - 1));\n-  ASSERT_RAISES(Invalid, table_->ValidateColumns());\n+  table_ = Table::Make(schema_, columns_, length - 1);\n+  ASSERT_RAISES(Invalid, table_->Validate());\n \n   columns_.clear();\n \n   // Wrong number of columns\n-  table_.reset(new Table(schema_, columns_, length));\n-  ASSERT_RAISES(Invalid, table_->ValidateColumns());\n+  table_ = Table::Make(schema_, columns_, length);\n+  ASSERT_RAISES(Invalid, table_->Validate());\n \n   columns_ = {\n       std::make_shared<Column>(schema_->field(0), MakeRandomArray<Int32Array>(length)),\n@@ -277,15 +278,15 @@ TEST_F(TestTable, InvalidColumns) {\n       std::make_shared<Column>(schema_->field(2),\n                                MakeRandomArray<Int16Array>(length - 1))};\n \n-  table_.reset(new Table(schema_, columns_, length));\n-  ASSERT_RAISES(Invalid, table_->ValidateColumns());\n+  table_ = Table::Make(schema_, columns_, length);\n+  ASSERT_RAISES(Invalid, table_->Validate());\n }\n \n TEST_F(TestTable, Equals) {\n   const int length = 100;\n   MakeExample1(length);\n \n-  table_.reset(new Table(schema_, columns_));\n+  table_ = Table::Make(schema_, columns_);\n \n   ASSERT_TRUE(table_->Equals(*table_));\n   // Differing schema\n@@ -294,7 +295,8 @@ TEST_F(TestTable, Equals) {\n   auto f2 = field(\"f5\", int16());\n   vector<shared_ptr<Field>> fields = {f0, f1, f2};\n   auto other_schema = std::make_shared<Schema>(fields);\n-  ASSERT_FALSE(table_->Equals(Table(other_schema, columns_)));\n+  auto other = Table::Make(other_schema, columns_);\n+  ASSERT_FALSE(table_->Equals(*other));\n   // Differing columns\n   std::vector<std::shared_ptr<Column>> other_columns = {\n       std::make_shared<Column>(schema_->field(0),\n@@ -303,19 +305,21 @@ TEST_F(TestTable, Equals) {\n                                MakeRandomArray<UInt8Array>(length, 10)),\n       std::make_shared<Column>(schema_->field(2),\n                                MakeRandomArray<Int16Array>(length, 10))};\n-  ASSERT_FALSE(table_->Equals(Table(schema_, other_columns)));\n+\n+  other = Table::Make(schema_, other_columns);\n+  ASSERT_FALSE(table_->Equals(*other));\n }\n \n TEST_F(TestTable, FromRecordBatches) {\n   const int64_t length = 10;\n   MakeExample1(length);\n \n-  auto batch1 = std::make_shared<RecordBatch>(schema_, length, arrays_);\n+  auto batch1 = RecordBatch::Make(schema_, length, arrays_);\n \n   std::shared_ptr<Table> result, expected;\n   ASSERT_OK(Table::FromRecordBatches({batch1}, &result));\n \n-  expected = std::make_shared<Table>(schema_, columns_);\n+  expected = Table::Make(schema_, columns_);\n   ASSERT_TRUE(result->Equals(*expected));\n \n   std::vector<std::shared_ptr<Column>> other_columns;\n@@ -325,18 +329,17 @@ TEST_F(TestTable, FromRecordBatches) {\n   }\n \n   ASSERT_OK(Table::FromRecordBatches({batch1, batch1}, &result));\n-  expected = std::make_shared<Table>(schema_, other_columns);\n+  expected = Table::Make(schema_, other_columns);\n   ASSERT_TRUE(result->Equals(*expected));\n \n   // Error states\n   std::vector<std::shared_ptr<RecordBatch>> empty_batches;\n   ASSERT_RAISES(Invalid, Table::FromRecordBatches(empty_batches, &result));\n \n-  std::vector<std::shared_ptr<Field>> fields = {schema_->field(0), schema_->field(1)};\n-  auto other_schema = std::make_shared<Schema>(fields);\n+  auto other_schema = ::arrow::schema({schema_->field(0), schema_->field(1)});\n \n   std::vector<std::shared_ptr<Array>> other_arrays = {arrays_[0], arrays_[1]};\n-  auto batch2 = std::make_shared<RecordBatch>(other_schema, length, other_arrays);\n+  auto batch2 = RecordBatch::Make(other_schema, length, other_arrays);\n   ASSERT_RAISES(Invalid, Table::FromRecordBatches({batch1, batch2}, &result));\n }\n \n@@ -344,11 +347,11 @@ TEST_F(TestTable, ConcatenateTables) {\n   const int64_t length = 10;\n \n   MakeExample1(length);\n-  auto batch1 = std::make_shared<RecordBatch>(schema_, length, arrays_);\n+  auto batch1 = RecordBatch::Make(schema_, length, arrays_);\n \n   // generate different data\n   MakeExample1(length);\n-  auto batch2 = std::make_shared<RecordBatch>(schema_, length, arrays_);\n+  auto batch2 = RecordBatch::Make(schema_, length, arrays_);\n \n   std::shared_ptr<Table> t1, t2, t3, result, expected;\n   ASSERT_OK(Table::FromRecordBatches({batch1}, &t1));\n@@ -362,11 +365,10 @@ TEST_F(TestTable, ConcatenateTables) {\n   std::vector<std::shared_ptr<Table>> empty_tables;\n   ASSERT_RAISES(Invalid, ConcatenateTables(empty_tables, &result));\n \n-  std::vector<std::shared_ptr<Field>> fields = {schema_->field(0), schema_->field(1)};\n-  auto other_schema = std::make_shared<Schema>(fields);\n+  auto other_schema = ::arrow::schema({schema_->field(0), schema_->field(1)});\n \n   std::vector<std::shared_ptr<Array>> other_arrays = {arrays_[0], arrays_[1]};\n-  auto batch3 = std::make_shared<RecordBatch>(other_schema, length, other_arrays);\n+  auto batch3 = RecordBatch::Make(other_schema, length, other_arrays);\n   ASSERT_OK(Table::FromRecordBatches({batch3}, &t3));\n \n   ASSERT_RAISES(Invalid, ConcatenateTables({t1, t3}, &result));\n@@ -376,31 +378,38 @@ TEST_F(TestTable, RemoveColumn) {\n   const int64_t length = 10;\n   MakeExample1(length);\n \n-  Table table(schema_, columns_);\n+  auto table_sp = Table::Make(schema_, columns_);\n+  const Table& table = *table_sp;\n \n   std::shared_ptr<Table> result;\n   ASSERT_OK(table.RemoveColumn(0, &result));\n \n   auto ex_schema = ::arrow::schema({schema_->field(1), schema_->field(2)});\n   std::vector<std::shared_ptr<Column>> ex_columns = {table.column(1), table.column(2)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+\n+  auto expected = Table::Make(ex_schema, ex_columns);\n+  ASSERT_TRUE(result->Equals(*expected));\n \n   ASSERT_OK(table.RemoveColumn(1, &result));\n   ex_schema = ::arrow::schema({schema_->field(0), schema_->field(2)});\n   ex_columns = {table.column(0), table.column(2)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+\n+  expected = Table::Make(ex_schema, ex_columns);\n+  ASSERT_TRUE(result->Equals(*expected));\n \n   ASSERT_OK(table.RemoveColumn(2, &result));\n   ex_schema = ::arrow::schema({schema_->field(0), schema_->field(1)});\n   ex_columns = {table.column(0), table.column(1)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+  expected = Table::Make(ex_schema, ex_columns);\n+  ASSERT_TRUE(result->Equals(*expected));\n }\n \n TEST_F(TestTable, AddColumn) {\n   const int64_t length = 10;\n   MakeExample1(length);\n \n-  Table table(schema_, columns_);\n+  auto table_sp = Table::Make(schema_, columns_);\n+  const Table& table = *table_sp;\n \n   std::shared_ptr<Table> result;\n   // Some negative tests with invalid index\n@@ -419,50 +428,32 @@ TEST_F(TestTable, AddColumn) {\n   ASSERT_OK(table.AddColumn(0, columns_[0], &result));\n   auto ex_schema = ::arrow::schema(\n       {schema_->field(0), schema_->field(0), schema_->field(1), schema_->field(2)});\n-  std::vector<std::shared_ptr<Column>> ex_columns = {table.column(0), table.column(0),\n-                                                     table.column(1), table.column(2)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+\n+  auto expected = Table::Make(\n+      ex_schema, {table.column(0), table.column(0), table.column(1), table.column(2)});\n+  ASSERT_TRUE(result->Equals(*expected));\n \n   ASSERT_OK(table.AddColumn(1, columns_[0], &result));\n   ex_schema = ::arrow::schema(\n       {schema_->field(0), schema_->field(0), schema_->field(1), schema_->field(2)});\n-  ex_columns = {table.column(0), table.column(0), table.column(1), table.column(2)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+\n+  expected = Table::Make(\n+      ex_schema, {table.column(0), table.column(0), table.column(1), table.column(2)});\n+  ASSERT_TRUE(result->Equals(*expected));\n \n   ASSERT_OK(table.AddColumn(2, columns_[0], &result));\n   ex_schema = ::arrow::schema(\n       {schema_->field(0), schema_->field(1), schema_->field(0), schema_->field(2)});\n-  ex_columns = {table.column(0), table.column(1), table.column(0), table.column(2)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n+  expected = Table::Make(\n+      ex_schema, {table.column(0), table.column(1), table.column(0), table.column(2)});\n+  ASSERT_TRUE(result->Equals(*expected));\n \n   ASSERT_OK(table.AddColumn(3, columns_[0], &result));\n   ex_schema = ::arrow::schema(\n       {schema_->field(0), schema_->field(1), schema_->field(2), schema_->field(0)});\n-  ex_columns = {table.column(0), table.column(1), table.column(2), table.column(0)};\n-  ASSERT_TRUE(result->Equals(Table(ex_schema, ex_columns)));\n-}\n-\n-TEST_F(TestTable, IsChunked) {\n-  ArrayVector c1, c2;\n-\n-  auto a1 = MakeRandomArray<Int32Array>(10);\n-  auto a2 = MakeRandomArray<Int32Array>(20);\n-\n-  auto sch1 = arrow::schema({field(\"f1\", int32()), field(\"f2\", int32())});\n-\n-  std::vector<std::shared_ptr<Column>> columns;\n-\n-  std::shared_ptr<RecordBatch> batch;\n-\n-  columns = {column(sch1->field(0), {a1}), column(sch1->field(1), {a1})};\n-  auto t1 = std::make_shared<Table>(sch1, columns);\n-\n-  ASSERT_FALSE(t1->IsChunked());\n-\n-  columns = {column(sch1->field(0), {a2}), column(sch1->field(1), {a1, a1})};\n-  auto t2 = std::make_shared<Table>(sch1, columns);\n-\n-  ASSERT_TRUE(t2->IsChunked());\n+  expected = Table::Make(\n+      ex_schema, {table.column(0), table.column(1), table.column(2), table.column(0)});\n+  ASSERT_TRUE(result->Equals(*expected));\n }\n \n class TestRecordBatch : public TestBase {};\n@@ -475,24 +466,22 @@ TEST_F(TestRecordBatch, Equals) {\n   auto f2 = field(\"f2\", int16());\n \n   vector<shared_ptr<Field>> fields = {f0, f1, f2};\n-  auto schema = std::make_shared<Schema>(fields);\n+  auto schema = ::arrow::schema({f0, f1, f2});\n+  auto schema2 = ::arrow::schema({f0, f1});\n \n   auto a0 = MakeRandomArray<Int32Array>(length);\n   auto a1 = MakeRandomArray<UInt8Array>(length);\n   auto a2 = MakeRandomArray<Int16Array>(length);\n \n-  RecordBatch b1(schema, length, {a0, a1, a2});\n-  RecordBatch b3(schema, length, {a0, a1});\n-  RecordBatch b4(schema, length, {a0, a1, a1});\n+  auto b1 = RecordBatch::Make(schema, length, {a0, a1, a2});\n+  auto b3 = RecordBatch::Make(schema2, length, {a0, a1});\n+  auto b4 = RecordBatch::Make(schema, length, {a0, a1, a1});\n \n-  ASSERT_TRUE(b1.Equals(b1));\n-  ASSERT_FALSE(b1.Equals(b3));\n-  ASSERT_FALSE(b1.Equals(b4));\n+  ASSERT_TRUE(b1->Equals(*b1));\n+  ASSERT_FALSE(b1->Equals(*b3));\n+  ASSERT_FALSE(b1->Equals(*b4));\n }\n \n-#ifdef NDEBUG\n-// In debug builds, RecordBatch ctor aborts if you construct an invalid one\n-\n TEST_F(TestRecordBatch, Validate) {\n   const int length = 10;\n \n@@ -507,21 +496,19 @@ TEST_F(TestRecordBatch, Validate) {\n   auto a2 = MakeRandomArray<Int16Array>(length);\n   auto a3 = MakeRandomArray<Int16Array>(5);\n \n-  RecordBatch b1(schema, length, {a0, a1, a2});\n+  auto b1 = RecordBatch::Make(schema, length, {a0, a1, a2});\n \n-  ASSERT_OK(b1.Validate());\n+  ASSERT_OK(b1->Validate());\n \n   // Length mismatch\n-  RecordBatch b2(schema, length, {a0, a1, a3});\n-  ASSERT_RAISES(Invalid, b2.Validate());\n+  auto b2 = RecordBatch::Make(schema, length, {a0, a1, a3});\n+  ASSERT_RAISES(Invalid, b2->Validate());\n \n   // Type mismatch\n-  RecordBatch b3(schema, length, {a0, a1, a0});\n-  ASSERT_RAISES(Invalid, b3.Validate());\n+  auto b3 = RecordBatch::Make(schema, length, {a0, a1, a0});\n+  ASSERT_RAISES(Invalid, b3->Validate());\n }\n \n-#endif\n-\n TEST_F(TestRecordBatch, Slice) {\n   const int length = 10;\n \n@@ -529,19 +516,19 @@ TEST_F(TestRecordBatch, Slice) {\n   auto f1 = field(\"f1\", uint8());\n \n   vector<shared_ptr<Field>> fields = {f0, f1};\n-  auto schema = std::make_shared<Schema>(fields);\n+  auto schema = ::arrow::schema(fields);\n \n   auto a0 = MakeRandomArray<Int32Array>(length);\n   auto a1 = MakeRandomArray<UInt8Array>(length);\n \n-  RecordBatch batch(schema, length, {a0, a1});\n+  auto batch = RecordBatch::Make(schema, length, {a0, a1});\n \n-  auto batch_slice = batch.Slice(2);\n-  auto batch_slice2 = batch.Slice(1, 5);\n+  auto batch_slice = batch->Slice(2);\n+  auto batch_slice2 = batch->Slice(1, 5);\n \n-  ASSERT_EQ(batch_slice->num_rows(), batch.num_rows() - 2);\n+  ASSERT_EQ(batch_slice->num_rows(), batch->num_rows() - 2);\n \n-  for (int i = 0; i < batch.num_columns(); ++i) {\n+  for (int i = 0; i < batch->num_columns(); ++i) {\n     ASSERT_EQ(2, batch_slice->column(i)->offset());\n     ASSERT_EQ(length - 2, batch_slice->column(i)->length());\n \n@@ -567,9 +554,9 @@ TEST_F(TestTableBatchReader, ReadNext) {\n   std::shared_ptr<RecordBatch> batch;\n \n   columns = {column(sch1->field(0), {a1, a4, a2}), column(sch1->field(1), {a2, a2})};\n-  Table t1(sch1, columns);\n+  auto t1 = Table::Make(sch1, columns);\n \n-  TableBatchReader i1(t1);\n+  TableBatchReader i1(*t1);\n \n   ASSERT_OK(i1.ReadNext(&batch));\n   ASSERT_EQ(10, batch->num_rows());\n@@ -584,9 +571,9 @@ TEST_F(TestTableBatchReader, ReadNext) {\n   ASSERT_EQ(nullptr, batch);\n \n   columns = {column(sch1->field(0), {a1}), column(sch1->field(1), {a4})};\n-  Table t2(sch1, columns);\n+  auto t2 = Table::Make(sch1, columns);\n \n-  TableBatchReader i2(t2);\n+  TableBatchReader i2(*t2);\n \n   ASSERT_OK(i2.ReadNext(&batch));\n   ASSERT_EQ(10, batch->num_rows());\ndiff --git a/cpp/src/arrow/table.cc b/cpp/src/arrow/table.cc\nindex fe19bf4ce..8f3f19576 100644\n--- a/cpp/src/arrow/table.cc\n+++ b/cpp/src/arrow/table.cc\n@@ -23,6 +23,7 @@\n #include <sstream>\n \n #include \"arrow/array.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/type.h\"\n #include \"arrow/util/logging.h\"\n@@ -153,171 +154,126 @@ Status Column::ValidateData() {\n }\n \n // ----------------------------------------------------------------------\n-// RecordBatch methods\n-\n-RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows)\n-    : schema_(schema), num_rows_(num_rows) {\n-  boxed_columns_.resize(schema->num_fields());\n-}\n-\n-RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-                         const std::vector<std::shared_ptr<Array>>& columns)\n-    : RecordBatch(schema, num_rows) {\n-  columns_.resize(columns.size());\n-  for (size_t i = 0; i < columns.size(); ++i) {\n-    columns_[i] = columns[i]->data();\n-  }\n-}\n-\n-RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-                         std::vector<std::shared_ptr<Array>>&& columns)\n-    : RecordBatch(schema, num_rows) {\n-  columns_.resize(columns.size());\n-  for (size_t i = 0; i < columns.size(); ++i) {\n-    columns_[i] = columns[i]->data();\n-  }\n-}\n-\n-RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-                         std::vector<std::shared_ptr<ArrayData>>&& columns)\n-    : RecordBatch(schema, num_rows) {\n-  columns_ = std::move(columns);\n-}\n-\n-RecordBatch::RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-                         const std::vector<std::shared_ptr<ArrayData>>& columns)\n-    : RecordBatch(schema, num_rows) {\n-  columns_ = columns;\n-}\n-\n-std::shared_ptr<Array> RecordBatch::column(int i) const {\n-  if (!boxed_columns_[i]) {\n-    boxed_columns_[i] = MakeArray(columns_[i]);\n-  }\n-  DCHECK(boxed_columns_[i]);\n-  return boxed_columns_[i];\n-}\n-\n-const std::string& RecordBatch::column_name(int i) const {\n-  return schema_->field(i)->name();\n-}\n-\n-bool RecordBatch::Equals(const RecordBatch& other) const {\n-  if (num_columns() != other.num_columns() || num_rows_ != other.num_rows()) {\n-    return false;\n-  }\n+// Table methods\n \n-  for (int i = 0; i < num_columns(); ++i) {\n-    if (!column(i)->Equals(other.column(i))) {\n-      return false;\n+/// \\class SimpleTable\n+/// \\brief A basic, non-lazy in-memory table, like SimpleRecordBatch\n+class SimpleTable : public Table {\n+ public:\n+  SimpleTable(const std::shared_ptr<Schema>& schema,\n+              const std::vector<std::shared_ptr<Column>>& columns, int64_t num_rows = -1)\n+      : columns_(columns) {\n+    schema_ = schema;\n+    if (num_rows < 0) {\n+      if (columns.size() == 0) {\n+        num_rows_ = 0;\n+      } else {\n+        num_rows_ = columns[0]->length();\n+      }\n+    } else {\n+      num_rows_ = num_rows;\n     }\n   }\n \n-  return true;\n-}\n-\n-bool RecordBatch::ApproxEquals(const RecordBatch& other) const {\n-  if (num_columns() != other.num_columns() || num_rows_ != other.num_rows()) {\n-    return false;\n-  }\n+  SimpleTable(const std::shared_ptr<Schema>& schema,\n+              const std::vector<std::shared_ptr<Array>>& columns, int64_t num_rows = -1) {\n+    schema_ = schema;\n+    if (num_rows < 0) {\n+      if (columns.size() == 0) {\n+        num_rows_ = 0;\n+      } else {\n+        num_rows_ = columns[0]->length();\n+      }\n+    } else {\n+      num_rows_ = num_rows;\n+    }\n \n-  for (int i = 0; i < num_columns(); ++i) {\n-    if (!column(i)->ApproxEquals(other.column(i))) {\n-      return false;\n+    columns_.resize(columns.size());\n+    for (size_t i = 0; i < columns.size(); ++i) {\n+      columns_[i] =\n+          std::make_shared<Column>(schema->field(static_cast<int>(i)), columns[i]);\n     }\n   }\n \n-  return true;\n-}\n-\n-std::shared_ptr<RecordBatch> RecordBatch::ReplaceSchemaMetadata(\n-    const std::shared_ptr<const KeyValueMetadata>& metadata) const {\n-  auto new_schema = schema_->AddMetadata(metadata);\n-  return std::make_shared<RecordBatch>(new_schema, num_rows_, columns_);\n-}\n+  std::shared_ptr<Column> column(int i) const override { return columns_[i]; }\n \n-std::shared_ptr<RecordBatch> RecordBatch::Slice(int64_t offset) const {\n-  return Slice(offset, this->num_rows() - offset);\n-}\n+  Status RemoveColumn(int i, std::shared_ptr<Table>* out) const override {\n+    std::shared_ptr<Schema> new_schema;\n+    RETURN_NOT_OK(schema_->RemoveField(i, &new_schema));\n \n-std::shared_ptr<RecordBatch> RecordBatch::Slice(int64_t offset, int64_t length) const {\n-  std::vector<std::shared_ptr<ArrayData>> arrays;\n-  arrays.reserve(num_columns());\n-  for (const auto& field : columns_) {\n-    int64_t col_length = std::min(field->length - offset, length);\n-    int64_t col_offset = field->offset + offset;\n-\n-    auto new_data = std::make_shared<ArrayData>(*field);\n-    new_data->length = col_length;\n-    new_data->offset = col_offset;\n-    new_data->null_count = kUnknownNullCount;\n-    arrays.emplace_back(new_data);\n+    *out = Table::Make(new_schema, internal::DeleteVectorElement(columns_, i));\n+    return Status::OK();\n   }\n-  int64_t num_rows = std::min(num_rows_ - offset, length);\n-  return std::make_shared<RecordBatch>(schema_, num_rows, std::move(arrays));\n-}\n \n-Status RecordBatch::Validate() const {\n-  for (int i = 0; i < num_columns(); ++i) {\n-    const ArrayData& arr = *columns_[i];\n-    if (arr.length != num_rows_) {\n+  Status AddColumn(int i, const std::shared_ptr<Column>& col,\n+                   std::shared_ptr<Table>* out) const override {\n+    if (i < 0 || i > num_columns() + 1) {\n+      return Status::Invalid(\"Invalid column index.\");\n+    }\n+    if (col == nullptr) {\n       std::stringstream ss;\n-      ss << \"Number of rows in column \" << i << \" did not match batch: \" << arr.length\n-         << \" vs \" << num_rows_;\n+      ss << \"Column \" << i << \" was null\";\n       return Status::Invalid(ss.str());\n     }\n-    const auto& schema_type = *schema_->field(i)->type();\n-    if (!arr.type->Equals(schema_type)) {\n+    if (col->length() != num_rows_) {\n       std::stringstream ss;\n-      ss << \"Column \" << i << \" type not match schema: \" << arr.type->ToString() << \" vs \"\n-         << schema_type.ToString();\n+      ss << \"Added column's length must match table's length. Expected length \"\n+         << num_rows_ << \" but got length \" << col->length();\n       return Status::Invalid(ss.str());\n     }\n+\n+    std::shared_ptr<Schema> new_schema;\n+    RETURN_NOT_OK(schema_->AddField(i, col->field(), &new_schema));\n+\n+    *out = Table::Make(new_schema, internal::AddVectorElement(columns_, i, col));\n+    return Status::OK();\n   }\n-  return Status::OK();\n-}\n \n-// ----------------------------------------------------------------------\n-// Table methods\n+  std::shared_ptr<Table> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const override {\n+    auto new_schema = schema_->AddMetadata(metadata);\n+    return Table::Make(new_schema, columns_);\n+  }\n \n-Table::Table(const std::shared_ptr<Schema>& schema,\n-             const std::vector<std::shared_ptr<Column>>& columns, int64_t num_rows)\n-    : schema_(schema), columns_(columns) {\n-  if (num_rows < 0) {\n-    if (columns.size() == 0) {\n-      num_rows_ = 0;\n-    } else {\n-      num_rows_ = columns[0]->length();\n+  Status Validate() const override {\n+    if (static_cast<int>(columns_.size()) != schema_->num_fields()) {\n+      return Status::Invalid(\"Number of columns did not match schema\");\n     }\n-  } else {\n-    num_rows_ = num_rows;\n-  }\n-}\n \n-Table::Table(const std::shared_ptr<Schema>& schema,\n-             const std::vector<std::shared_ptr<Array>>& columns, int64_t num_rows)\n-    : schema_(schema) {\n-  if (num_rows < 0) {\n-    if (columns.size() == 0) {\n-      num_rows_ = 0;\n-    } else {\n-      num_rows_ = columns[0]->length();\n+    // Make sure columns are all the same length\n+    for (int i = 0; i < num_columns(); ++i) {\n+      const Column* col = columns_[i].get();\n+      if (col == nullptr) {\n+        std::stringstream ss;\n+        ss << \"Column \" << i << \" was null\";\n+        return Status::Invalid(ss.str());\n+      }\n+      if (col->length() != num_rows_) {\n+        std::stringstream ss;\n+        ss << \"Column \" << i << \" named \" << col->name() << \" expected length \"\n+           << num_rows_ << \" but got length \" << col->length();\n+        return Status::Invalid(ss.str());\n+      }\n     }\n-  } else {\n-    num_rows_ = num_rows;\n+    return Status::OK();\n   }\n \n-  columns_.resize(columns.size());\n-  for (size_t i = 0; i < columns.size(); ++i) {\n-    columns_[i] =\n-        std::make_shared<Column>(schema->field(static_cast<int>(i)), columns[i]);\n-  }\n+ private:\n+  std::vector<std::shared_ptr<Column>> columns_;\n+};\n+\n+Table::Table() {}\n+\n+std::shared_ptr<Table> Table::Make(const std::shared_ptr<Schema>& schema,\n+                                   const std::vector<std::shared_ptr<Column>>& columns,\n+                                   int64_t num_rows) {\n+  return std::make_shared<SimpleTable>(schema, columns, num_rows);\n }\n \n-std::shared_ptr<Table> Table::ReplaceSchemaMetadata(\n-    const std::shared_ptr<const KeyValueMetadata>& metadata) const {\n-  auto new_schema = schema_->AddMetadata(metadata);\n-  return std::make_shared<Table>(new_schema, columns_);\n+std::shared_ptr<Table> Table::Make(const std::shared_ptr<Schema>& schema,\n+                                   const std::vector<std::shared_ptr<Array>>& arrays,\n+                                   int64_t num_rows) {\n+  return std::make_shared<SimpleTable>(schema, arrays, num_rows);\n }\n \n Status Table::FromRecordBatches(const std::vector<std::shared_ptr<RecordBatch>>& batches,\n@@ -351,7 +307,7 @@ Status Table::FromRecordBatches(const std::vector<std::shared_ptr<RecordBatch>>&\n     columns[i] = std::make_shared<Column>(schema->field(i), column_arrays);\n   }\n \n-  *table = std::make_shared<Table>(schema, columns);\n+  *table = Table::Make(schema, columns);\n   return Status::OK();\n }\n \n@@ -388,7 +344,7 @@ Status ConcatenateTables(const std::vector<std::shared_ptr<Table>>& tables,\n     }\n     columns[i] = std::make_shared<Column>(schema->field(i), column_arrays);\n   }\n-  *table = std::make_shared<Table>(schema, columns);\n+  *table = Table::Make(schema, columns);\n   return Status::OK();\n }\n \n@@ -399,82 +355,19 @@ bool Table::Equals(const Table& other) const {\n   if (!schema_->Equals(*other.schema())) {\n     return false;\n   }\n-  if (static_cast<int64_t>(columns_.size()) != other.num_columns()) {\n+  if (this->num_columns() != other.num_columns()) {\n     return false;\n   }\n \n-  for (int i = 0; i < static_cast<int>(columns_.size()); i++) {\n-    if (!columns_[i]->Equals(other.column(i))) {\n+  for (int i = 0; i < this->num_columns(); i++) {\n+    if (!this->column(i)->Equals(other.column(i))) {\n       return false;\n     }\n   }\n   return true;\n }\n \n-Status Table::RemoveColumn(int i, std::shared_ptr<Table>* out) const {\n-  std::shared_ptr<Schema> new_schema;\n-  RETURN_NOT_OK(schema_->RemoveField(i, &new_schema));\n-\n-  *out = std::make_shared<Table>(new_schema, internal::DeleteVectorElement(columns_, i));\n-  return Status::OK();\n-}\n-\n-Status Table::AddColumn(int i, const std::shared_ptr<Column>& col,\n-                        std::shared_ptr<Table>* out) const {\n-  if (i < 0 || i > num_columns() + 1) {\n-    return Status::Invalid(\"Invalid column index.\");\n-  }\n-  if (col == nullptr) {\n-    std::stringstream ss;\n-    ss << \"Column \" << i << \" was null\";\n-    return Status::Invalid(ss.str());\n-  }\n-  if (col->length() != num_rows_) {\n-    std::stringstream ss;\n-    ss << \"Added column's length must match table's length. Expected length \" << num_rows_\n-       << \" but got length \" << col->length();\n-    return Status::Invalid(ss.str());\n-  }\n-\n-  std::shared_ptr<Schema> new_schema;\n-  RETURN_NOT_OK(schema_->AddField(i, col->field(), &new_schema));\n-\n-  *out =\n-      std::make_shared<Table>(new_schema, internal::AddVectorElement(columns_, i, col));\n-  return Status::OK();\n-}\n-\n-Status Table::ValidateColumns() const {\n-  if (num_columns() != schema_->num_fields()) {\n-    return Status::Invalid(\"Number of columns did not match schema\");\n-  }\n-\n-  // Make sure columns are all the same length\n-  for (size_t i = 0; i < columns_.size(); ++i) {\n-    const Column* col = columns_[i].get();\n-    if (col == nullptr) {\n-      std::stringstream ss;\n-      ss << \"Column \" << i << \" was null\";\n-      return Status::Invalid(ss.str());\n-    }\n-    if (col->length() != num_rows_) {\n-      std::stringstream ss;\n-      ss << \"Column \" << i << \" named \" << col->name() << \" expected length \" << num_rows_\n-         << \" but got length \" << col->length();\n-      return Status::Invalid(ss.str());\n-    }\n-  }\n-  return Status::OK();\n-}\n-\n-bool Table::IsChunked() const {\n-  for (size_t i = 0; i < columns_.size(); ++i) {\n-    if (columns_[i]->data()->num_chunks() > 1) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n+#ifndef ARROW_NO_DEPRECATED_API\n \n Status MakeTable(const std::shared_ptr<Schema>& schema,\n                  const std::vector<std::shared_ptr<Array>>& arrays,\n@@ -493,15 +386,12 @@ Status MakeTable(const std::shared_ptr<Schema>& schema,\n     columns.emplace_back(std::make_shared<Column>(schema->field(i), arrays[i]));\n   }\n \n-  *table = std::make_shared<Table>(schema, columns);\n+  *table = Table::Make(schema, columns);\n \n   return Status::OK();\n }\n \n-// ----------------------------------------------------------------------\n-// Base record batch reader\n-\n-RecordBatchReader::~RecordBatchReader() {}\n+#endif  // ARROW_NO_DEPRECATED_API\n \n // ----------------------------------------------------------------------\n // Convert a table to a sequence of record batches\n@@ -565,8 +455,7 @@ class TableBatchReader::TableBatchReaderImpl {\n     }\n \n     absolute_row_position_ += chunksize;\n-    *out =\n-        std::make_shared<RecordBatch>(table_.schema(), chunksize, std::move(batch_data));\n+    *out = RecordBatch::Make(table_.schema(), chunksize, std::move(batch_data));\n \n     return Status::OK();\n   }\ndiff --git a/cpp/src/arrow/table.h b/cpp/src/arrow/table.h\nindex 2cff32f74..d0312d93c 100644\n--- a/cpp/src/arrow/table.h\n+++ b/cpp/src/arrow/table.h\n@@ -24,6 +24,7 @@\n #include <vector>\n \n #include \"arrow/array.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/type.h\"\n #include \"arrow/util/macros.h\"\n #include \"arrow/util/visibility.h\"\n@@ -33,8 +34,6 @@ namespace arrow {\n class KeyValueMetadata;\n class Status;\n \n-using ArrayVector = std::vector<std::shared_ptr<Array>>;\n-\n /// \\class ChunkedArray\n /// \\brief A data structure managing a list of primitive Arrow arrays logically\n /// as one large array\n@@ -113,123 +112,28 @@ class ARROW_EXPORT Column {\n   ARROW_DISALLOW_COPY_AND_ASSIGN(Column);\n };\n \n-/// \\class RecordBatch\n-/// \\brief Collection of equal-length arrays matching a particular Schema\n-///\n-/// A record batch is table-like data structure consisting of an internal\n-/// sequence of fields, each a contiguous Arrow array\n-class ARROW_EXPORT RecordBatch {\n- public:\n-  /// \\param[in] schema The record batch schema\n-  /// \\param[in] num_rows length of fields in the record batch. Each array\n-  /// should have the same length as num_rows\n-  /// \\param[in] columns the record batch fields as vector of arrays\n-  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-              const std::vector<std::shared_ptr<Array>>& columns);\n-\n-  /// \\brief Move-based constructor for a vector of Array instances\n-  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-              std::vector<std::shared_ptr<Array>>&& columns);\n-\n-  /// \\brief Construct record batch from vector of internal data structures\n-  /// \\since 0.5.0\n-  ///\n-  /// This class is only provided with an rvalue-reference for the input data,\n-  /// and is intended for internal use, or advanced users.\n-  ///\n-  /// \\param schema the record batch schema\n-  /// \\param num_rows the number of semantic rows in the record batch. This\n-  /// should be equal to the length of each field\n-  /// \\param columns the data for the batch's columns\n-  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-              std::vector<std::shared_ptr<ArrayData>>&& columns);\n-\n-  /// \\brief Construct record batch by copying vector of array data\n-  /// \\since 0.5.0\n-  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows,\n-              const std::vector<std::shared_ptr<ArrayData>>& columns);\n-\n-  /// \\brief Determine if two record batches are exactly equal\n-  /// \\return true if batches are equal\n-  bool Equals(const RecordBatch& other) const;\n-\n-  /// \\brief Determine if two record batches are approximately equal\n-  bool ApproxEquals(const RecordBatch& other) const;\n-\n-  // \\return the table's schema\n-  /// \\return true if batches are equal\n-  std::shared_ptr<Schema> schema() const { return schema_; }\n-\n-  /// \\brief Retrieve an array from the record batch\n-  /// \\param[in] i field index, does not boundscheck\n-  /// \\return an Array object\n-  std::shared_ptr<Array> column(int i) const;\n-\n-  std::shared_ptr<ArrayData> column_data(int i) const { return columns_[i]; }\n-\n-  /// \\brief Name in i-th column\n-  const std::string& column_name(int i) const;\n-\n-  /// \\return the number of columns in the table\n-  int num_columns() const { return static_cast<int>(columns_.size()); }\n-\n-  /// \\return the number of rows (the corresponding length of each column)\n-  int64_t num_rows() const { return num_rows_; }\n-\n-  /// \\brief Replace schema key-value metadata with new metadata (EXPERIMENTAL)\n-  /// \\since 0.5.0\n-  ///\n-  /// \\param[in] metadata new KeyValueMetadata\n-  /// \\return new RecordBatch\n-  std::shared_ptr<RecordBatch> ReplaceSchemaMetadata(\n-      const std::shared_ptr<const KeyValueMetadata>& metadata) const;\n-\n-  /// \\brief Slice each of the arrays in the record batch\n-  /// \\param[in] offset the starting offset to slice, through end of batch\n-  /// \\return new record batch\n-  std::shared_ptr<RecordBatch> Slice(int64_t offset) const;\n-\n-  /// \\brief Slice each of the arrays in the record batch\n-  /// \\param[in] offset the starting offset to slice\n-  /// \\param[in] length the number of elements to slice from offset\n-  /// \\return new record batch\n-  std::shared_ptr<RecordBatch> Slice(int64_t offset, int64_t length) const;\n-\n-  /// \\brief Check for schema or length inconsistencies\n-  /// \\return Status\n-  Status Validate() const;\n-\n- private:\n-  ARROW_DISALLOW_COPY_AND_ASSIGN(RecordBatch);\n-\n-  RecordBatch(const std::shared_ptr<Schema>& schema, int64_t num_rows);\n-\n-  std::shared_ptr<Schema> schema_;\n-  int64_t num_rows_;\n-  std::vector<std::shared_ptr<ArrayData>> columns_;\n-\n-  // Caching boxed array data\n-  mutable std::vector<std::shared_ptr<Array>> boxed_columns_;\n-};\n-\n /// \\class Table\n /// \\brief Logical table as sequence of chunked arrays\n class ARROW_EXPORT Table {\n  public:\n+  virtual ~Table() = default;\n+\n   /// \\brief Construct Table from schema and columns\n   /// If columns is zero-length, the table's number of rows is zero\n   /// \\param schema The table schema (column types)\n   /// \\param columns The table's columns\n   /// \\param num_rows number of rows in table, -1 (default) to infer from columns\n-  Table(const std::shared_ptr<Schema>& schema,\n-        const std::vector<std::shared_ptr<Column>>& columns, int64_t num_rows = -1);\n+  static std::shared_ptr<Table> Make(const std::shared_ptr<Schema>& schema,\n+                                     const std::vector<std::shared_ptr<Column>>& columns,\n+                                     int64_t num_rows = -1);\n \n   /// \\brief Construct Table from schema and arrays\n   /// \\param schema The table schema (column types)\n   /// \\param arrays The table's columns as arrays\n   /// \\param num_rows number of rows in table, -1 (default) to infer from columns\n-  Table(const std::shared_ptr<Schema>& schema,\n-        const std::vector<std::shared_ptr<Array>>& arrays, int64_t num_rows = -1);\n+  static std::shared_ptr<Table> Make(const std::shared_ptr<Schema>& schema,\n+                                     const std::vector<std::shared_ptr<Array>>& arrays,\n+                                     int64_t num_rows = -1);\n \n   // Construct table from RecordBatch, but only if all of the batch schemas are\n   // equal. Returns Status::Invalid if there is some problem\n@@ -242,25 +146,28 @@ class ARROW_EXPORT Table {\n \n   /// \\param[in] i column index, does not boundscheck\n   /// \\return the i-th column\n-  std::shared_ptr<Column> column(int i) const { return columns_[i]; }\n+  virtual std::shared_ptr<Column> column(int i) const = 0;\n \n   /// \\brief Remove column from the table, producing a new Table\n-  Status RemoveColumn(int i, std::shared_ptr<Table>* out) const;\n+  virtual Status RemoveColumn(int i, std::shared_ptr<Table>* out) const = 0;\n \n   /// \\brief Add column to the table, producing a new Table\n-  Status AddColumn(int i, const std::shared_ptr<Column>& column,\n-                   std::shared_ptr<Table>* out) const;\n+  virtual Status AddColumn(int i, const std::shared_ptr<Column>& column,\n+                           std::shared_ptr<Table>* out) const = 0;\n \n   /// \\brief Replace schema key-value metadata with new metadata (EXPERIMENTAL)\n   /// \\since 0.5.0\n   ///\n   /// \\param[in] metadata new KeyValueMetadata\n   /// \\return new Table\n-  std::shared_ptr<Table> ReplaceSchemaMetadata(\n-      const std::shared_ptr<const KeyValueMetadata>& metadata) const;\n+  virtual std::shared_ptr<Table> ReplaceSchemaMetadata(\n+      const std::shared_ptr<const KeyValueMetadata>& metadata) const = 0;\n+\n+  /// \\brief Perform any checks to validate the input arguments\n+  virtual Status Validate() const = 0;\n \n   /// \\return the number of columns in the table\n-  int num_columns() const { return static_cast<int>(columns_.size()); }\n+  int num_columns() const { return schema_->num_fields(); }\n \n   /// \\return the number of rows (the corresponding length of each column)\n   int64_t num_rows() const { return num_rows_; }\n@@ -268,35 +175,14 @@ class ARROW_EXPORT Table {\n   /// \\brief Determine if semantic contents of tables are exactly equal\n   bool Equals(const Table& other) const;\n \n-  /// \\brief Perform any checks to validate the input arguments\n-  Status ValidateColumns() const;\n-\n-  /// \\brief Return true if any column has multiple chunks\n-  bool IsChunked() const;\n-\n- private:\n-  ARROW_DISALLOW_COPY_AND_ASSIGN(Table);\n+ protected:\n+  Table();\n \n   std::shared_ptr<Schema> schema_;\n-  std::vector<std::shared_ptr<Column>> columns_;\n-\n   int64_t num_rows_;\n-};\n-\n-/// \\brief Abstract interface for reading stream of record batches\n-class ARROW_EXPORT RecordBatchReader {\n- public:\n-  virtual ~RecordBatchReader();\n \n-  /// \\return the shared schema of the record batches in the stream\n-  virtual std::shared_ptr<Schema> schema() const = 0;\n-\n-  /// Read the next record batch in the stream. Return null for batch when\n-  /// reaching end of stream\n-  ///\n-  /// \\param[out] batch the next loaded batch, null at end of stream\n-  /// \\return Status\n-  virtual Status ReadNext(std::shared_ptr<RecordBatch>* batch) = 0;\n+ private:\n+  ARROW_DISALLOW_COPY_AND_ASSIGN(Table);\n };\n \n /// \\brief Compute a sequence of record batches from a (possibly chunked) Table\n@@ -322,13 +208,18 @@ ARROW_EXPORT\n Status ConcatenateTables(const std::vector<std::shared_ptr<Table>>& tables,\n                          std::shared_ptr<Table>* table);\n \n+#ifndef ARROW_NO_DEPRECATED_API\n+\n /// \\brief Construct table from multiple input tables.\n /// \\return Status, fails if any schemas are different\n+/// \\note Deprecated since 0.8.0\n ARROW_EXPORT\n Status MakeTable(const std::shared_ptr<Schema>& schema,\n                  const std::vector<std::shared_ptr<Array>>& arrays,\n                  std::shared_ptr<Table>* table);\n \n+#endif\n+\n }  // namespace arrow\n \n #endif  // ARROW_TABLE_H\ndiff --git a/cpp/src/arrow/table_builder-test.cc b/cpp/src/arrow/table_builder-test.cc\nindex 07d9b6b2d..8167577e9 100644\n--- a/cpp/src/arrow/table_builder-test.cc\n+++ b/cpp/src/arrow/table_builder-test.cc\n@@ -22,6 +22,7 @@\n #include \"gtest/gtest.h\"\n \n #include \"arrow/array.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/table_builder.h\"\n@@ -98,7 +99,7 @@ TEST_F(TestRecordBatchBuilder, Basics) {\n   ASSERT_OK(ex_b1.Finish(&a1));\n   ASSERT_OK(ex_b2.Finish(&a2));\n \n-  RecordBatch expected(schema, 4, {a0, a1, a2});\n+  auto expected = RecordBatch::Make(schema, 4, {a0, a1, a2});\n \n   // Builder attributes\n   ASSERT_EQ(3, builder->num_fields());\n@@ -119,7 +120,7 @@ TEST_F(TestRecordBatchBuilder, Basics) {\n       ASSERT_OK(builder->Flush(&batch));\n     }\n \n-    ASSERT_BATCHES_EQUAL(expected, *batch);\n+    ASSERT_BATCHES_EQUAL(*expected, *batch);\n   }\n \n   // Test setting initial capacity\ndiff --git a/cpp/src/arrow/table_builder.cc b/cpp/src/arrow/table_builder.cc\nindex a1bd95940..379d886de 100644\n--- a/cpp/src/arrow/table_builder.cc\n+++ b/cpp/src/arrow/table_builder.cc\n@@ -24,6 +24,7 @@\n \n #include \"arrow/array.h\"\n #include \"arrow/builder.h\"\n+#include \"arrow/record_batch.h\"\n #include \"arrow/status.h\"\n #include \"arrow/table.h\"\n #include \"arrow/type.h\"\n@@ -64,7 +65,7 @@ Status RecordBatchBuilder::Flush(bool reset_builders,\n     }\n     length = fields[i]->length();\n   }\n-  *batch = std::make_shared<RecordBatch>(schema_, length, std::move(fields));\n+  *batch = RecordBatch::Make(schema_, length, std::move(fields));\n   if (reset_builders) {\n     return InitBuilders();\n   } else {\ndiff --git a/cpp/src/arrow/test-common.h b/cpp/src/arrow/test-common.h\nindex a4c4fddff..911adf7b6 100644\n--- a/cpp/src/arrow/test-common.h\n+++ b/cpp/src/arrow/test-common.h\n@@ -30,7 +30,6 @@\n #include \"arrow/buffer.h\"\n #include \"arrow/builder.h\"\n #include \"arrow/memory_pool.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/test-util.h\"\n \n namespace arrow {\ndiff --git a/cpp/src/arrow/test-util.h b/cpp/src/arrow/test-util.h\nindex 77f489ab1..1a3480848 100644\n--- a/cpp/src/arrow/test-util.h\n+++ b/cpp/src/arrow/test-util.h\n@@ -35,7 +35,6 @@\n #include \"arrow/memory_pool.h\"\n #include \"arrow/pretty_print.h\"\n #include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n #include \"arrow/type.h\"\n #include \"arrow/type_traits.h\"\n #include \"arrow/util/bit-util.h\"\n@@ -375,7 +374,7 @@ void AssertArraysEqual(const Array& expected, const Array& actual) {\n \n #define ASSERT_BATCHES_EQUAL(LEFT, RIGHT)    \\\n   do {                                       \\\n-    if (!LEFT.ApproxEquals(RIGHT)) {         \\\n+    if (!(LEFT).ApproxEquals(RIGHT)) {       \\\n       std::stringstream ss;                  \\\n       ss << \"Left:\\n\";                       \\\n       ASSERT_OK(PrettyPrint(LEFT, 0, &ss));  \\\ndiff --git a/cpp/src/arrow/type.h b/cpp/src/arrow/type.h\nindex 70f275c0f..8dcc1592d 100644\n--- a/cpp/src/arrow/type.h\n+++ b/cpp/src/arrow/type.h\n@@ -498,9 +498,9 @@ class ARROW_EXPORT StructType : public NestedType {\n   std::vector<BufferDescr> GetBufferLayout() const override;\n };\n \n-class ARROW_EXPORT DecimalBaseType : public FixedSizeBinaryType {\n+class ARROW_EXPORT DecimalType : public FixedSizeBinaryType {\n  public:\n-  explicit DecimalBaseType(int32_t byte_width, int32_t precision, int32_t scale)\n+  explicit DecimalType(int32_t byte_width, int32_t precision, int32_t scale)\n       : FixedSizeBinaryType(byte_width, Type::DECIMAL),\n         precision_(precision),\n         scale_(scale) {}\n@@ -513,21 +513,18 @@ class ARROW_EXPORT DecimalBaseType : public FixedSizeBinaryType {\n   int32_t scale_;\n };\n \n-class ARROW_EXPORT Decimal128Type : public DecimalBaseType {\n+class ARROW_EXPORT Decimal128Type : public DecimalType {\n  public:\n   static constexpr Type::type type_id = Type::DECIMAL;\n \n   explicit Decimal128Type(int32_t precision, int32_t scale)\n-      : DecimalBaseType(16, precision, scale) {}\n+      : DecimalType(16, precision, scale) {}\n \n   Status Accept(TypeVisitor* visitor) const override;\n   std::string ToString() const override;\n   std::string name() const override { return \"decimal\"; }\n };\n \n-// TODO(wesm): Remove this\n-using DecimalType = Decimal128Type;\n-\n struct UnionMode {\n   enum type { SPARSE, DENSE };\n };\ndiff --git a/python/pyarrow/includes/libarrow.pxd b/python/pyarrow/includes/libarrow.pxd\nindex dbfd89cc3..73e34c7b2 100644\n--- a/python/pyarrow/includes/libarrow.pxd\n+++ b/python/pyarrow/includes/libarrow.pxd\n@@ -403,8 +403,10 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n         shared_ptr[CChunkedArray] data()\n \n     cdef cppclass CRecordBatch\" arrow::RecordBatch\":\n-        CRecordBatch(const shared_ptr[CSchema]& schema, int64_t num_rows,\n-                     const vector[shared_ptr[CArray]]& columns)\n+        @staticmethod\n+        shared_ptr[CRecordBatch] Make(\n+            const shared_ptr[CSchema]& schema, int64_t num_rows,\n+            const vector[shared_ptr[CArray]]& columns)\n \n         c_bool Equals(const CRecordBatch& other)\n \n@@ -427,6 +429,11 @@ cdef extern from \"arrow/api.h\" namespace \"arrow\" nogil:\n         CTable(const shared_ptr[CSchema]& schema,\n                const vector[shared_ptr[CColumn]]& columns)\n \n+        @staticmethod\n+        shared_ptr[CTable] Make(\n+            const shared_ptr[CSchema]& schema,\n+            const vector[shared_ptr[CColumn]]& columns)\n+\n         @staticmethod\n         CStatus FromRecordBatches(\n             const vector[shared_ptr[CRecordBatch]]& batches,\ndiff --git a/python/pyarrow/table.pxi b/python/pyarrow/table.pxi\nindex 591f32975..8c5b8bbc3 100644\n--- a/python/pyarrow/table.pxi\n+++ b/python/pyarrow/table.pxi\n@@ -724,7 +724,6 @@ cdef class RecordBatch:\n             Array arr\n             c_string c_name\n             shared_ptr[CSchema] schema\n-            shared_ptr[CRecordBatch] batch\n             vector[shared_ptr[CArray]] c_arrays\n             int64_t num_rows\n             int64_t i\n@@ -740,8 +739,8 @@ cdef class RecordBatch:\n         for arr in arrays:\n             c_arrays.push_back(arr.sp_array)\n \n-        batch.reset(new CRecordBatch(schema, num_rows, c_arrays))\n-        return pyarrow_wrap_batch(batch)\n+        return pyarrow_wrap_batch(\n+            CRecordBatch.Make(schema, num_rows, c_arrays))\n \n \n def table_to_blocks(PandasOptions options, Table table, int nthreads,\n@@ -946,8 +945,7 @@ cdef class Table:\n             else:\n                 raise ValueError(type(arrays[i]))\n \n-        table.reset(new CTable(c_schema, columns))\n-        return pyarrow_wrap_table(table)\n+        return pyarrow_wrap_table(CTable.Make(c_schema, columns))\n \n     @staticmethod\n     def from_batches(batches):\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-22T00:01:57.008+0000",
                    "updated": "2017-11-22T00:01:57.008+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16261711",
                    "id": "16261711",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 1337\n[https://github.com/apache/arrow/pull/1337]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-11-22T00:01:57.016+0000",
                    "updated": "2017-11-22T00:01:57.016+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13118133/comment/16261734",
                    "id": "16261734",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "kou commented on issue #1337: ARROW-1808: [C++] Make RecordBatch, Table virtual interfaces for column access\nURL: https://github.com/apache/arrow/pull/1337#issuecomment-346203089\n \n \n   > It seems like respecting the schema is the right approach.\r\n   \r\n   I agree with you.\r\n   \r\n   > I could add boundschecking to `SimpleRecordBatch::column(i)` and return null if the index is out of bounds, would that help at all?\r\n   \r\n   I think that it's better that we do it in more higher layer such as GLib bindings layer. I think that we don't do needless checks in C++ layer for simplicity and performance.\r\n   \r\n   I'll add boundschecking in GLib bindings later. For now, I think that it's better that we always validate a newly created record batch. If we always validate it, we can assume that all record batches always have valid data.\r\n   \r\n   ```patch\r\n   From d9260c09765b1cd337cda5a09497ee1b985ef623 Mon Sep 17 00:00:00 2001\r\n   From: Kouhei Sutou <kou@clear-code.com>\r\n   Date: Wed, 22 Nov 2017 09:09:30 +0900\r\n   Subject: [PATCH] [GLib] Always validate on creating new record batch\r\n   \r\n   ---\r\n    c_glib/arrow-glib/record-batch.cpp | 13 ++++++++---\r\n    c_glib/arrow-glib/record-batch.h   |  3 ++-\r\n    c_glib/example/go/write-batch.go   | 10 +++++++--\r\n    c_glib/example/go/write-stream.go  | 10 +++++++--\r\n    c_glib/test/test-record-batch.rb   | 46 +++++++++++++++++++++++++-------------\r\n    5 files changed, 58 insertions(+), 24 deletions(-)\r\n   \r\n   diff --git a/c_glib/arrow-glib/record-batch.cpp b/c_glib/arrow-glib/record-batch.cpp\r\n   index f23a0cf7..73de6eeb 100644\r\n   --- a/c_glib/arrow-glib/record-batch.cpp\r\n   +++ b/c_glib/arrow-glib/record-batch.cpp\r\n   @@ -135,13 +135,15 @@ garrow_record_batch_class_init(GArrowRecordBatchClass *klass)\r\n     * @schema: The schema of the record batch.\r\n     * @n_rows: The number of the rows in the record batch.\r\n     * @columns: (element-type GArrowArray): The columns in the record batch.\r\n   + * @error: (nullable): Return location for a #GError or %NULL.\r\n     *\r\n   - * Returns: A newly created #GArrowRecordBatch.\r\n   + * Returns: (nullable): A newly created #GArrowRecordBatch or %NULL on error.\r\n     */\r\n    GArrowRecordBatch *\r\n    garrow_record_batch_new(GArrowSchema *schema,\r\n                            guint32 n_rows,\r\n   -                        GList *columns)\r\n   +                        GList *columns,\r\n   +                        GError **error)\r\n    {\r\n      std::vector<std::shared_ptr<arrow::Array>> arrow_columns;\r\n      for (GList *node = columns; node; node = node->next) {\r\n   @@ -152,7 +154,12 @@ garrow_record_batch_new(GArrowSchema *schema,\r\n      auto arrow_record_batch =\r\n        arrow::RecordBatch::Make(garrow_schema_get_raw(schema),\r\n                                 n_rows, arrow_columns);\r\n   -  return garrow_record_batch_new_raw(&arrow_record_batch);\r\n   +  auto status = arrow_record_batch->Validate();\r\n   +  if (garrow_error_check(error, status, \"[record-batch][new]\")) {\r\n   +    return garrow_record_batch_new_raw(&arrow_record_batch);\r\n   +  } else {\r\n   +    return NULL;\r\n   +  }\r\n    }\r\n    \r\n    /**\r\n   diff --git a/c_glib/arrow-glib/record-batch.h b/c_glib/arrow-glib/record-batch.h\r\n   index 021f894f..823a42bb 100644\r\n   --- a/c_glib/arrow-glib/record-batch.h\r\n   +++ b/c_glib/arrow-glib/record-batch.h\r\n   @@ -68,7 +68,8 @@ GType garrow_record_batch_get_type(void) G_GNUC_CONST;\r\n    \r\n    GArrowRecordBatch *garrow_record_batch_new(GArrowSchema *schema,\r\n                                               guint32 n_rows,\r\n   -                                           GList *columns);\r\n   +                                           GList *columns,\r\n   +                                           GError **error);\r\n    \r\n    gboolean garrow_record_batch_equal(GArrowRecordBatch *record_batch,\r\n                                       GArrowRecordBatch *other_record_batch);\r\n   diff --git a/c_glib/example/go/write-batch.go b/c_glib/example/go/write-batch.go\r\n   index 9dbc3c00..f4d03ed9 100644\r\n   --- a/c_glib/example/go/write-batch.go\r\n   +++ b/c_glib/example/go/write-batch.go\r\n   @@ -188,7 +188,10 @@ func main() {\r\n    \t\tBuildDoubleArray(),\r\n    \t}\r\n    \r\n   -\trecordBatch := arrow.NewRecordBatch(schema, 4, columns)\r\n   +\trecordBatch, err := arrow.NewRecordBatch(schema, 4, columns)\r\n   +\tif err != nil {\r\n   +\t\tlog.Fatalf(\"Failed to create record batch #1: %v\", err)\r\n   +\t}\r\n    \t_, err = writer.WriteRecordBatch(recordBatch)\r\n    \tif err != nil {\r\n    \t\tlog.Fatalf(\"Failed to write record batch #1: %v\", err)\r\n   @@ -198,7 +201,10 @@ func main() {\r\n    \tfor i, column := range columns {\r\n    \t\tslicedColumns[i] = column.Slice(1, 3)\r\n    \t}\r\n   -\trecordBatch = arrow.NewRecordBatch(schema, 3, slicedColumns)\r\n   +\trecordBatch, err = arrow.NewRecordBatch(schema, 3, slicedColumns)\r\n   +\tif err != nil {\r\n   +\t\tlog.Fatalf(\"Failed to create record batch #2: %v\", err)\r\n   +\t}\r\n    \t_, err = writer.WriteRecordBatch(recordBatch)\r\n    \tif err != nil {\r\n    \t\tlog.Fatalf(\"Failed to write record batch #2: %v\", err)\r\n   diff --git a/c_glib/example/go/write-stream.go b/c_glib/example/go/write-stream.go\r\n   index 244741e8..7225156a 100644\r\n   --- a/c_glib/example/go/write-stream.go\r\n   +++ b/c_glib/example/go/write-stream.go\r\n   @@ -188,7 +188,10 @@ func main() {\r\n    \t\tBuildDoubleArray(),\r\n    \t}\r\n    \r\n   -\trecordBatch := arrow.NewRecordBatch(schema, 4, columns)\r\n   +\trecordBatch, err := arrow.NewRecordBatch(schema, 4, columns)\r\n   +\tif err != nil {\r\n   +\t\tlog.Fatalf(\"Failed to create record batch #1: %v\", err)\r\n   +\t}\r\n    \t_, err = writer.WriteRecordBatch(recordBatch)\r\n    \tif err != nil {\r\n    \t\tlog.Fatalf(\"Failed to write record batch #1: %v\", err)\r\n   @@ -198,7 +201,10 @@ func main() {\r\n    \tfor i, column := range columns {\r\n    \t\tslicedColumns[i] = column.Slice(1, 3)\r\n    \t}\r\n   -\trecordBatch = arrow.NewRecordBatch(schema, 3, slicedColumns)\r\n   +\trecordBatch, err = arrow.NewRecordBatch(schema, 3, slicedColumns)\r\n   +\tif err != nil {\r\n   +\t\tlog.Fatalf(\"Failed to create record batch #2: %v\", err)\r\n   +\t}\r\n    \twriter.WriteRecordBatch(recordBatch)\r\n    \t_, err = writer.WriteRecordBatch(recordBatch)\r\n    \tif err != nil {\r\n   diff --git a/c_glib/test/test-record-batch.rb b/c_glib/test/test-record-batch.rb\r\n   index 9fd34b7d..325944b8 100644\r\n   --- a/c_glib/test/test-record-batch.rb\r\n   +++ b/c_glib/test/test-record-batch.rb\r\n   @@ -18,18 +18,32 @@\r\n    class TestTable < Test::Unit::TestCase\r\n      include Helper::Buildable\r\n    \r\n   -  def test_new\r\n   -    fields = [\r\n   -      Arrow::Field.new(\"visible\", Arrow::BooleanDataType.new),\r\n   -      Arrow::Field.new(\"valid\", Arrow::BooleanDataType.new),\r\n   -    ]\r\n   -    schema = Arrow::Schema.new(fields)\r\n   -    columns = [\r\n   -      build_boolean_array([true]),\r\n   -      build_boolean_array([false]),\r\n   -    ]\r\n   -    record_batch = Arrow::RecordBatch.new(schema, 1, columns)\r\n   -    assert_equal(1, record_batch.n_rows)\r\n   +  sub_test_case(\".new\") do\r\n   +    def test_valid\r\n   +      fields = [\r\n   +        Arrow::Field.new(\"visible\", Arrow::BooleanDataType.new),\r\n   +        Arrow::Field.new(\"valid\", Arrow::BooleanDataType.new),\r\n   +      ]\r\n   +      schema = Arrow::Schema.new(fields)\r\n   +      columns = [\r\n   +        build_boolean_array([true]),\r\n   +        build_boolean_array([false]),\r\n   +      ]\r\n   +      record_batch = Arrow::RecordBatch.new(schema, 1, columns)\r\n   +      assert_equal(1, record_batch.n_rows)\r\n   +    end\r\n   +\r\n   +    def test_no_columns\r\n   +      fields = [\r\n   +        Arrow::Field.new(\"visible\", Arrow::BooleanDataType.new),\r\n   +      ]\r\n   +      schema = Arrow::Schema.new(fields)\r\n   +      message = \"[record-batch][new]: \" +\r\n   +        \"Invalid: Number of columns did not match schema\"\r\n   +      assert_raise(Arrow::Error::Invalid.new(message)) do\r\n   +        Arrow::RecordBatch.new(schema, 0, [])\r\n   +      end\r\n   +    end\r\n      end\r\n    \r\n      sub_test_case(\"instance methods\") do\r\n   @@ -40,7 +54,7 @@ class TestTable < Test::Unit::TestCase\r\n          ]\r\n          schema = Arrow::Schema.new(fields)\r\n          columns = [\r\n   -        build_boolean_array([true, false, true, false, true, false]),\r\n   +        build_boolean_array([true, false, true, false, true]),\r\n            build_boolean_array([false, true, false, true, false]),\r\n          ]\r\n          @record_batch = Arrow::RecordBatch.new(schema, 5, columns)\r\n   @@ -53,7 +67,7 @@ class TestTable < Test::Unit::TestCase\r\n          ]\r\n          schema = Arrow::Schema.new(fields)\r\n          columns = [\r\n   -        build_boolean_array([true, false, true, false, true, false]),\r\n   +        build_boolean_array([true, false, true, false, true]),\r\n            build_boolean_array([false, true, false, true, false]),\r\n          ]\r\n          other_record_batch = Arrow::RecordBatch.new(schema, 5, columns)\r\n   @@ -71,7 +85,7 @@ class TestTable < Test::Unit::TestCase\r\n        end\r\n    \r\n        def test_columns\r\n   -      assert_equal([6, 5],\r\n   +      assert_equal([5, 5],\r\n                       @record_batch.columns.collect(&:length))\r\n        end\r\n    \r\n   @@ -94,7 +108,7 @@ class TestTable < Test::Unit::TestCase\r\n    \r\n        def test_to_s\r\n          assert_equal(<<-PRETTY_PRINT, @record_batch.to_s)\r\n   -visible: [true, false, true, false, true, false]\r\n   +visible: [true, false, true, false, true]\r\n    valid: [false, true, false, true, false]\r\n          PRETTY_PRINT\r\n        end\r\n   -- \r\n   2.15.0\r\n   \r\n   ```\r\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-11-22T00:17:10.387+0000",
                    "updated": "2017-11-22T00:17:10.387+0000"
                }
            ],
            "maxResults": 19,
            "total": 19,
            "startAt": 0
        },
        "customfield_12311820": "0|i3mqnz:",
        "customfield_12314139": null
    }
}