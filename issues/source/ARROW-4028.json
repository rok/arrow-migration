{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13204408",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408",
    "key": "ARROW-4028",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343858",
                "id": "12343858",
                "description": "",
                "name": "0.12.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-01-20"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=csun",
            "name": "csun",
            "key": "csun",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"
            },
            "displayName": "Chao Sun",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12333773",
                "id": "12333773",
                "name": "Rust"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=csun",
            "name": "csun",
            "key": "csun",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"
            },
            "displayName": "Chao Sun",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=csun",
            "name": "csun",
            "key": "csun",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=csun&avatarId=23340",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=csun&avatarId=23340",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=csun&avatarId=23340",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=csun&avatarId=23340"
            },
            "displayName": "Chao Sun",
            "active": true,
            "timeZone": "America/Los_Angeles"
        },
        "aggregateprogress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4028/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 4,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408/worklog/176219",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #3050: ARROW-4028: [Rust] Merge parquet-rs codebase\nURL: https://github.com/apache/arrow/pull/3050#issuecomment-447977528\n \n \n   @sunchao can you fix the broken build? It's complaining about the data files\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-12-17T19:56:26.534+0000",
                    "updated": "2018-12-17T19:56:26.534+0000",
                    "started": "2018-12-17T19:56:26.532+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "176219",
                    "issueId": "13204408"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408/worklog/176233",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on issue #3050: ARROW-4028: [Rust] Merge parquet-rs codebase\nURL: https://github.com/apache/arrow/pull/3050#issuecomment-447990372\n \n \n   Sure. We need to update the `parquet-testing` git submodule.\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-12-17T20:38:12.611+0000",
                    "updated": "2018-12-17T20:38:12.611+0000",
                    "started": "2018-12-17T20:38:12.608+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "176233",
                    "issueId": "13204408"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408/worklog/176255",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm closed pull request #3050: ARROW-4028: [Rust] Merge parquet-rs codebase\nURL: https://github.com/apache/arrow/pull/3050\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/ci/rust-build-main.bat b/ci/rust-build-main.bat\nindex c8a51fef6e..e338f7e172 100644\n--- a/ci/rust-build-main.bat\n+++ b/ci/rust-build-main.bat\n@@ -17,6 +17,9 @@\n \n @rem The \"main\" Rust build script for Windows CI\n \n+@rem Retrieve git submodules, configure env var for Parquet unit tests\n+git submodule update --init || exit /B\n+set PARQUET_TEST_DATA=%CD%\\cpp\\submodules\\parquet-testing\\data\n pushd rust\n \n @echo ===================================\ndiff --git a/ci/travis_script_rust.sh b/ci/travis_script_rust.sh\nindex 55cce8f354..4b09bc22e4 100755\n--- a/ci/travis_script_rust.sh\n+++ b/ci/travis_script_rust.sh\n@@ -19,6 +19,8 @@\n \n set -e\n \n+source $TRAVIS_BUILD_DIR/ci/travis_env_common.sh\n+\n RUST_DIR=${TRAVIS_BUILD_DIR}/rust\n \n pushd $RUST_DIR\ndiff --git a/cpp/submodules/parquet-testing b/cpp/submodules/parquet-testing\nindex 46ae2605c2..92a8e6c2ef 160000\n--- a/cpp/submodules/parquet-testing\n+++ b/cpp/submodules/parquet-testing\n@@ -1 +1 @@\n-Subproject commit 46ae2605c2de306f5740587107dcf333a527f2d1\n+Subproject commit 92a8e6c2efdce1925c605d6313994db2c94478fb\ndiff --git a/docker-compose.yml b/docker-compose.yml\nindex 0a01a7cbe9..b61511ee56 100644\n--- a/docker-compose.yml\n+++ b/docker-compose.yml\n@@ -152,6 +152,8 @@ services:\n     build:\n       context: .\n       dockerfile: rust/Dockerfile\n+    environment:\n+      PARQUET_TEST_DATA: /arrow/cpp/submodules/parquet-testing/data\n     volumes: *ubuntu-volumes\n \n   r:\ndiff --git a/rust/Cargo.toml b/rust/Cargo.toml\nindex aa23815f74..49e8a9d9c8 100644\n--- a/rust/Cargo.toml\n+++ b/rust/Cargo.toml\n@@ -42,10 +42,22 @@ serde_derive = \"1.0.80\"\n serde_json = \"1.0.13\"\n rand = \"0.5\"\n csv = \"1.0.0\"\n+parquet-format = \"2.5.0\"\n+quick-error = \"1.2.2\"\n+byteorder = \"1\"\n+thrift = \"0.0.4\"\n+snap = \"0.2\"\n+brotli = \"2.5\"\n+flate2 = \"1.0.2\"\n+lz4 = \"1.23\"\n+zstd = \"0.4\"\n+chrono = \"0.4\"\n+num-bigint = \"0.2\"\n num = \"0.2\"\n \n [dev-dependencies]\n criterion = \"0.2\"\n+lazy_static = \"1\"\n \n [[bench]]\n name = \"array_from_vec\"\ndiff --git a/rust/benches/array_from_vec.rs b/rust/benches/array_from_vec.rs\nindex 669b88eaa4..f935714092 100644\n--- a/rust/benches/array_from_vec.rs\n+++ b/rust/benches/array_from_vec.rs\n@@ -17,7 +17,6 @@\n \n #[macro_use]\n extern crate criterion;\n-\n use criterion::Criterion;\n \n extern crate arrow;\ndiff --git a/rust/benches/builder.rs b/rust/benches/builder.rs\nindex 04f8a33b5b..90fd75a0da 100644\n--- a/rust/benches/builder.rs\n+++ b/rust/benches/builder.rs\n@@ -19,11 +19,13 @@ extern crate arrow;\n extern crate criterion;\n extern crate rand;\n \n-use arrow::builder::*;\n+use std::mem::size_of;\n+\n use criterion::*;\n use rand::distributions::Standard;\n use rand::{thread_rng, Rng};\n-use std::mem::size_of;\n+\n+use arrow::builder::*;\n \n // Build arrays with 512k elements.\n const BATCH_SIZE: usize = 8 << 10;\ndiff --git a/rust/build.rs b/rust/build.rs\nnew file mode 100644\nindex 0000000000..b42b2a4bab\n--- /dev/null\n+++ b/rust/build.rs\n@@ -0,0 +1,43 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+use std::process::Command;\n+\n+fn main() {\n+    // Set Parquet version, build hash and \"created by\" string.\n+    let version = env!(\"CARGO_PKG_VERSION\");\n+    let mut created_by = format!(\"parquet-rs version {}\", version);\n+    if let Ok(git_hash) = run(Command::new(\"git\").arg(\"rev-parse\").arg(\"HEAD\")) {\n+        created_by.push_str(format!(\" (build {})\", git_hash).as_str());\n+        println!(\"cargo:rustc-env=PARQUET_BUILD={}\", git_hash);\n+    }\n+    println!(\"cargo:rustc-env=PARQUET_VERSION={}\", version);\n+    println!(\"cargo:rustc-env=PARQUET_CREATED_BY={}\", created_by);\n+}\n+\n+/// Runs command and returns either content of stdout for successful execution,\n+/// or an error message otherwise.\n+fn run(command: &mut Command) -> Result<String, String> {\n+    println!(\"Running: `{:?}`\", command);\n+    match command.output() {\n+        Ok(ref output) if output.status.success() => {\n+            Ok(String::from_utf8_lossy(&output.stdout).trim().to_string())\n+        }\n+        Ok(ref output) => Err(format!(\"Failed: `{:?}` ({})\", command, output.status)),\n+        Err(error) => Err(format!(\"Failed: `{:?}` ({})\", command, error)),\n+    }\n+}\ndiff --git a/rust/examples/read_csv.rs b/rust/examples/read_csv.rs\nindex df66a8112e..147d2f9c23 100644\n--- a/rust/examples/read_csv.rs\n+++ b/rust/examples/read_csv.rs\n@@ -17,11 +17,12 @@\n \n extern crate arrow;\n \n+use std::fs::File;\n+use std::sync::Arc;\n+\n use arrow::array::{BinaryArray, Float64Array};\n use arrow::csv;\n use arrow::datatypes::{DataType, Field, Schema};\n-use std::fs::File;\n-use std::sync::Arc;\n \n fn main() {\n     let schema = Schema::new(vec![\ndiff --git a/rust/rustfmt.toml b/rust/rustfmt.toml\nnew file mode 100644\nindex 0000000000..72eeee0af1\n--- /dev/null\n+++ b/rust/rustfmt.toml\n@@ -0,0 +1,18 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+format_doc_comments = true\n\\ No newline at end of file\ndiff --git a/rust/src/array.rs b/rust/src/array.rs\nindex 11e732a126..251dd35eea 100644\n--- a/rust/src/array.rs\n+++ b/rust/src/array.rs\n@@ -657,12 +657,14 @@ impl From<Vec<(Field, ArrayRef)>> for StructArray {\n #[cfg(test)]\n mod tests {\n     use super::*;\n+\n+    use std::sync::Arc;\n+    use std::thread;\n+\n     use crate::array_data::ArrayData;\n     use crate::buffer::Buffer;\n-    use crate::datatypes::{DataType, Field, ToByteSlice};\n+    use crate::datatypes::{DataType, Field};\n     use crate::memory;\n-    use std::sync::Arc;\n-    use std::thread;\n \n     #[test]\n     fn test_primitive_array_from_vec() {\ndiff --git a/rust/src/array_data.rs b/rust/src/array_data.rs\nindex 36a817ee57..9ea01a402a 100644\n--- a/rust/src/array_data.rs\n+++ b/rust/src/array_data.rs\n@@ -225,9 +225,10 @@ impl ArrayDataBuilder {\n \n #[cfg(test)]\n mod tests {\n+    use super::*;\n+\n     use std::sync::Arc;\n \n-    use super::{ArrayData, DataType};\n     use crate::buffer::Buffer;\n     use crate::util::bit_util;\n \ndiff --git a/rust/src/builder.rs b/rust/src/builder.rs\nindex fc781ffa50..d5d222d006 100644\n--- a/rust/src/builder.rs\n+++ b/rust/src/builder.rs\n@@ -456,10 +456,10 @@ impl BinaryArrayBuilder {\n \n #[cfg(test)]\n mod tests {\n-    use crate::array::Array;\n-\n     use super::*;\n \n+    use crate::array::Array;\n+\n     #[test]\n     fn test_builder_i32_empty() {\n         let b = Int32BufferBuilder::new(5);\n@@ -825,7 +825,6 @@ mod tests {\n \n     #[test]\n     fn test_binary_array_builder() {\n-        use crate::array::BinaryArray;\n         let mut builder = BinaryArrayBuilder::new(20);\n \n         builder.push(b'h').unwrap();\n@@ -860,7 +859,6 @@ mod tests {\n \n     #[test]\n     fn test_binary_array_builder_push_string() {\n-        use crate::array::BinaryArray;\n         let mut builder = BinaryArrayBuilder::new(20);\n \n         let var = \"hello\".to_owned();\ndiff --git a/rust/src/csv/reader.rs b/rust/src/csv/reader.rs\nindex 956408e4a4..632aa7ae79 100644\n--- a/rust/src/csv/reader.rs\n+++ b/rust/src/csv/reader.rs\n@@ -29,16 +29,16 @@\n //! use std::sync::Arc;\n //!\n //! let schema = Schema::new(vec![\n-//!   Field::new(\"city\", DataType::Utf8, false),\n-//!   Field::new(\"lat\", DataType::Float64, false),\n-//!   Field::new(\"lng\", DataType::Float64, false),\n+//!     Field::new(\"city\", DataType::Utf8, false),\n+//!     Field::new(\"lat\", DataType::Float64, false),\n+//!     Field::new(\"lng\", DataType::Float64, false),\n //! ]);\n //!\n //! let file = File::open(\"test/data/uk_cities.csv\").unwrap();\n //!\n //! let mut csv = csv::Reader::new(file, Arc::new(schema), false, 1024, None);\n //! let batch = csv.next().unwrap().unwrap();\n-//!```\n+//! ```\n \n use std::fs::File;\n use std::io::BufReader;\n@@ -195,8 +195,8 @@ impl Reader {\n \n #[cfg(test)]\n mod tests {\n-\n     use super::*;\n+\n     use crate::array::*;\n     use crate::datatypes::Field;\n \ndiff --git a/rust/src/lib.rs b/rust/src/lib.rs\nindex f41d08f142..d5708b1050 100644\n--- a/rust/src/lib.rs\n+++ b/rust/src/lib.rs\n@@ -15,7 +15,12 @@\n // specific language governing permissions and limitations\n // under the License.\n \n+#![feature(type_ascription)]\n+#![feature(rustc_private)]\n #![feature(specialization)]\n+#![feature(try_from)]\n+#![allow(dead_code)]\n+#![allow(non_camel_case_types)]\n \n pub mod array;\n pub mod array_data;\n@@ -27,6 +32,7 @@ pub mod csv;\n pub mod datatypes;\n pub mod error;\n pub mod memory;\n+pub mod parquet;\n pub mod record_batch;\n pub mod tensor;\n pub mod util;\ndiff --git a/rust/src/mod.rs b/rust/src/mod.rs\nnew file mode 100644\nindex 0000000000..b9fa43ab81\n--- /dev/null\n+++ b/rust/src/mod.rs\n@@ -0,0 +1,28 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+pub mod array;\n+pub mod array_data;\n+pub mod bitmap;\n+pub mod buffer;\n+pub mod builder;\n+pub mod csv;\n+pub mod datatypes;\n+pub mod error;\n+pub mod memory;\n+pub mod record_batch;\n+pub mod tensor;\ndiff --git a/rust/src/parquet/basic.rs b/rust/src/parquet/basic.rs\nnew file mode 100644\nindex 0000000000..22e16347dc\n--- /dev/null\n+++ b/rust/src/parquet/basic.rs\n@@ -0,0 +1,1497 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains Rust mappings for Thrift definition.\n+//! Refer to `parquet.thrift` file to see raw definitions.\n+\n+use std::{convert, fmt, result, str};\n+\n+use parquet_format as parquet;\n+\n+use crate::parquet::errors::ParquetError;\n+\n+// ----------------------------------------------------------------------\n+// Types from the Thrift definition\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::Type`\n+\n+/// Types supported by Parquet.\n+/// These physical types are intended to be used in combination with the encodings to\n+/// control the on disk storage format.\n+/// For example INT16 is not included as a type since a good encoding of INT32\n+/// would handle this.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum Type {\n+    BOOLEAN,\n+    INT32,\n+    INT64,\n+    INT96,\n+    FLOAT,\n+    DOUBLE,\n+    BYTE_ARRAY,\n+    FIXED_LEN_BYTE_ARRAY,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::ConvertedType`\n+\n+/// Common types (logical types) used by frameworks when using Parquet.\n+/// This helps map between types in those frameworks to the base types in Parquet.\n+/// This is only metadata and not needed to read or write the data.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum LogicalType {\n+    NONE,\n+    /// A BYTE_ARRAY actually contains UTF8 encoded chars.\n+    UTF8,\n+\n+    /// A map is converted as an optional field containing a repeated key/value pair.\n+    MAP,\n+\n+    /// A key/value pair is converted into a group of two fields.\n+    MAP_KEY_VALUE,\n+\n+    /// A list is converted into an optional field containing a repeated field for its\n+    /// values.\n+    LIST,\n+\n+    /// An enum is converted into a binary field\n+    ENUM,\n+\n+    /// A decimal value.\n+    /// This may be used to annotate binary or fixed primitive types. The\n+    /// underlying byte array stores the unscaled value encoded as two's\n+    /// complement using big-endian byte order (the most significant byte is the\n+    /// zeroth element).\n+    ///\n+    /// This must be accompanied by a (maximum) precision and a scale in the\n+    /// SchemaElement. The precision specifies the number of digits in the decimal\n+    /// and the scale stores the location of the decimal point. For example 1.23\n+    /// would have precision 3 (3 total digits) and scale 2 (the decimal point is\n+    /// 2 digits over).\n+    DECIMAL,\n+\n+    /// A date stored as days since Unix epoch, encoded as the INT32 physical type.\n+    DATE,\n+\n+    /// The total number of milliseconds since midnight. The value is stored as an INT32\n+    /// physical type.\n+    TIME_MILLIS,\n+\n+    /// The total number of microseconds since midnight. The value is stored as an INT64\n+    /// physical type.\n+    TIME_MICROS,\n+\n+    /// Date and time recorded as milliseconds since the Unix epoch.\n+    /// Recorded as a physical type of INT64.\n+    TIMESTAMP_MILLIS,\n+\n+    /// Date and time recorded as microseconds since the Unix epoch.\n+    /// The value is stored as an INT64 physical type.\n+    TIMESTAMP_MICROS,\n+\n+    /// An unsigned 8 bit integer value stored as INT32 physical type.\n+    UINT_8,\n+\n+    /// An unsigned 16 bit integer value stored as INT32 physical type.\n+    UINT_16,\n+\n+    /// An unsigned 32 bit integer value stored as INT32 physical type.\n+    UINT_32,\n+\n+    /// An unsigned 64 bit integer value stored as INT64 physical type.\n+    UINT_64,\n+\n+    /// A signed 8 bit integer value stored as INT32 physical type.\n+    INT_8,\n+\n+    /// A signed 16 bit integer value stored as INT32 physical type.\n+    INT_16,\n+\n+    /// A signed 32 bit integer value stored as INT32 physical type.\n+    INT_32,\n+\n+    /// A signed 64 bit integer value stored as INT64 physical type.\n+    INT_64,\n+\n+    /// A JSON document embedded within a single UTF8 column.\n+    JSON,\n+\n+    /// A BSON document embedded within a single BINARY column.\n+    BSON,\n+\n+    /// An interval of time.\n+    ///\n+    /// This type annotates data stored as a FIXED_LEN_BYTE_ARRAY of length 12.\n+    /// This data is composed of three separate little endian unsigned integers.\n+    /// Each stores a component of a duration of time. The first integer identifies\n+    /// the number of months associated with the duration, the second identifies\n+    /// the number of days associated with the duration and the third identifies\n+    /// the number of milliseconds associated with the provided duration.\n+    /// This duration of time is independent of any particular timezone or date.\n+    INTERVAL,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::FieldRepetitionType`\n+\n+/// Representation of field types in schema.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum Repetition {\n+    /// Field is required (can not be null) and each record has exactly 1 value.\n+    REQUIRED,\n+    /// Field is optional (can be null) and each record has 0 or 1 values.\n+    OPTIONAL,\n+    /// Field is repeated and can contain 0 or more values.\n+    REPEATED,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::Encoding`\n+\n+/// Encodings supported by Parquet.\n+/// Not all encodings are valid for all types. These enums are also used to specify the\n+/// encoding of definition and repetition levels.\n+#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]\n+pub enum Encoding {\n+    /// Default byte encoding.\n+    /// - BOOLEAN - 1 bit per value, 0 is false; 1 is true.\n+    /// - INT32 - 4 bytes per value, stored as little-endian.\n+    /// - INT64 - 8 bytes per value, stored as little-endian.\n+    /// - FLOAT - 4 bytes per value, stored as little-endian.\n+    /// - DOUBLE - 8 bytes per value, stored as little-endian.\n+    /// - BYTE_ARRAY - 4 byte length stored as little endian, followed by bytes.\n+    /// - FIXED_LEN_BYTE_ARRAY - just the bytes are stored.\n+    PLAIN,\n+\n+    /// **Deprecated** dictionary encoding.\n+    ///\n+    /// The values in the dictionary are encoded using PLAIN encoding.\n+    /// Since it is deprecated, RLE_DICTIONARY encoding is used for a data page, and PLAIN\n+    /// encoding is used for dictionary page.\n+    PLAIN_DICTIONARY,\n+\n+    /// Group packed run length encoding.\n+    ///\n+    /// Usable for definition/repetition levels encoding and boolean values.\n+    RLE,\n+\n+    /// Bit packed encoding.\n+    ///\n+    /// This can only be used if the data has a known max width.\n+    /// Usable for definition/repetition levels encoding.\n+    BIT_PACKED,\n+\n+    /// Delta encoding for integers, either INT32 or INT64.\n+    ///\n+    /// Works best on sorted data.\n+    DELTA_BINARY_PACKED,\n+\n+    /// Encoding for byte arrays to separate the length values and the data.\n+    ///\n+    /// The lengths are encoded using DELTA_BINARY_PACKED encoding.\n+    DELTA_LENGTH_BYTE_ARRAY,\n+\n+    /// Incremental encoding for byte arrays.\n+    ///\n+    /// Prefix lengths are encoded using DELTA_BINARY_PACKED encoding.\n+    /// Suffixes are stored using DELTA_LENGTH_BYTE_ARRAY encoding.\n+    DELTA_BYTE_ARRAY,\n+\n+    /// Dictionary encoding.\n+    ///\n+    /// The ids are encoded using the RLE encoding.\n+    RLE_DICTIONARY,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::CompressionCodec`\n+\n+/// Supported compression algorithms.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum Compression {\n+    UNCOMPRESSED,\n+    SNAPPY,\n+    GZIP,\n+    LZO,\n+    BROTLI,\n+    LZ4,\n+    ZSTD,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::PageType`\n+\n+/// Available data pages for Parquet file format.\n+/// Note that some of the page types may not be supported.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum PageType {\n+    DATA_PAGE,\n+    INDEX_PAGE,\n+    DICTIONARY_PAGE,\n+    DATA_PAGE_V2,\n+}\n+\n+// ----------------------------------------------------------------------\n+// Mirrors `parquet::ColumnOrder`\n+\n+/// Sort order for page and column statistics.\n+///\n+/// Types are associated with sort orders and column stats are aggregated using a sort\n+/// order, and a sort order should be considered when comparing values with statistics\n+/// min/max.\n+///\n+/// See reference in\n+/// https://github.com/apache/parquet-cpp/blob/master/src/parquet/types.h\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum SortOrder {\n+    /// Signed (either value or legacy byte-wise) comparison.\n+    SIGNED,\n+    /// Unsigned (depending on physical type either value or byte-wise) comparison.\n+    UNSIGNED,\n+    /// Comparison is undefined.\n+    UNDEFINED,\n+}\n+\n+/// Column order that specifies what method was used to aggregate min/max values for\n+/// statistics.\n+///\n+/// If column order is undefined, then it is the legacy behaviour and all values should\n+/// be compared as signed values/bytes.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum ColumnOrder {\n+    /// Column uses the order defined by its logical or physical type\n+    /// (if there is no logical type), parquet-format 2.4.0+.\n+    TYPE_DEFINED_ORDER(SortOrder),\n+    /// Undefined column order, means legacy behaviour before parquet-format 2.4.0.\n+    /// Sort order is always SIGNED.\n+    UNDEFINED,\n+}\n+\n+impl ColumnOrder {\n+    /// Returns sort order for a physical/logical type.\n+    pub fn get_sort_order(logical_type: LogicalType, physical_type: Type) -> SortOrder {\n+        match logical_type {\n+            // Unsigned byte-wise comparison.\n+            LogicalType::UTF8 | LogicalType::JSON | LogicalType::BSON | LogicalType::ENUM => {\n+                SortOrder::UNSIGNED\n+            }\n+\n+            LogicalType::INT_8\n+            | LogicalType::INT_16\n+            | LogicalType::INT_32\n+            | LogicalType::INT_64 => SortOrder::SIGNED,\n+\n+            LogicalType::UINT_8\n+            | LogicalType::UINT_16\n+            | LogicalType::UINT_32\n+            | LogicalType::UINT_64 => SortOrder::UNSIGNED,\n+\n+            // Signed comparison of the represented value.\n+            LogicalType::DECIMAL => SortOrder::SIGNED,\n+\n+            LogicalType::DATE => SortOrder::SIGNED,\n+\n+            LogicalType::TIME_MILLIS\n+            | LogicalType::TIME_MICROS\n+            | LogicalType::TIMESTAMP_MILLIS\n+            | LogicalType::TIMESTAMP_MICROS => SortOrder::SIGNED,\n+\n+            LogicalType::INTERVAL => SortOrder::UNSIGNED,\n+\n+            LogicalType::LIST | LogicalType::MAP | LogicalType::MAP_KEY_VALUE => {\n+                SortOrder::UNDEFINED\n+            }\n+\n+            // Fall back to physical type.\n+            LogicalType::NONE => Self::get_default_sort_order(physical_type),\n+        }\n+    }\n+\n+    /// Returns default sort order based on physical type.\n+    fn get_default_sort_order(physical_type: Type) -> SortOrder {\n+        match physical_type {\n+            // Order: false, true\n+            Type::BOOLEAN => SortOrder::UNSIGNED,\n+            Type::INT32 | Type::INT64 => SortOrder::SIGNED,\n+            Type::INT96 => SortOrder::UNDEFINED,\n+            // Notes to remember when comparing float/double values:\n+            // If the min is a NaN, it should be ignored.\n+            // If the max is a NaN, it should be ignored.\n+            // If the min is +0, the row group may contain -0 values as well.\n+            // If the max is -0, the row group may contain +0 values as well.\n+            // When looking for NaN values, min and max should be ignored.\n+            Type::FLOAT | Type::DOUBLE => SortOrder::SIGNED,\n+            // unsigned byte-wise comparison\n+            Type::BYTE_ARRAY | Type::FIXED_LEN_BYTE_ARRAY => SortOrder::UNSIGNED,\n+        }\n+    }\n+\n+    /// Returns sort order associated with this column order.\n+    pub fn sort_order(&self) -> SortOrder {\n+        match *self {\n+            ColumnOrder::TYPE_DEFINED_ORDER(order) => order,\n+            ColumnOrder::UNDEFINED => SortOrder::SIGNED,\n+        }\n+    }\n+}\n+\n+impl fmt::Display for Type {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for LogicalType {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for Repetition {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for Encoding {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for Compression {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for PageType {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for SortOrder {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+impl fmt::Display for ColumnOrder {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{:?}\", self)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::Type <=> Type conversion\n+\n+impl convert::From<parquet::Type> for Type {\n+    fn from(value: parquet::Type) -> Self {\n+        match value {\n+            parquet::Type::BOOLEAN => Type::BOOLEAN,\n+            parquet::Type::INT32 => Type::INT32,\n+            parquet::Type::INT64 => Type::INT64,\n+            parquet::Type::INT96 => Type::INT96,\n+            parquet::Type::FLOAT => Type::FLOAT,\n+            parquet::Type::DOUBLE => Type::DOUBLE,\n+            parquet::Type::BYTE_ARRAY => Type::BYTE_ARRAY,\n+            parquet::Type::FIXED_LEN_BYTE_ARRAY => Type::FIXED_LEN_BYTE_ARRAY,\n+        }\n+    }\n+}\n+\n+impl convert::From<Type> for parquet::Type {\n+    fn from(value: Type) -> Self {\n+        match value {\n+            Type::BOOLEAN => parquet::Type::BOOLEAN,\n+            Type::INT32 => parquet::Type::INT32,\n+            Type::INT64 => parquet::Type::INT64,\n+            Type::INT96 => parquet::Type::INT96,\n+            Type::FLOAT => parquet::Type::FLOAT,\n+            Type::DOUBLE => parquet::Type::DOUBLE,\n+            Type::BYTE_ARRAY => parquet::Type::BYTE_ARRAY,\n+            Type::FIXED_LEN_BYTE_ARRAY => parquet::Type::FIXED_LEN_BYTE_ARRAY,\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::ConvertedType <=> LogicalType conversion\n+\n+impl convert::From<Option<parquet::ConvertedType>> for LogicalType {\n+    fn from(option: Option<parquet::ConvertedType>) -> Self {\n+        match option {\n+            None => LogicalType::NONE,\n+            Some(value) => match value {\n+                parquet::ConvertedType::UTF8 => LogicalType::UTF8,\n+                parquet::ConvertedType::MAP => LogicalType::MAP,\n+                parquet::ConvertedType::MAP_KEY_VALUE => LogicalType::MAP_KEY_VALUE,\n+                parquet::ConvertedType::LIST => LogicalType::LIST,\n+                parquet::ConvertedType::ENUM => LogicalType::ENUM,\n+                parquet::ConvertedType::DECIMAL => LogicalType::DECIMAL,\n+                parquet::ConvertedType::DATE => LogicalType::DATE,\n+                parquet::ConvertedType::TIME_MILLIS => LogicalType::TIME_MILLIS,\n+                parquet::ConvertedType::TIME_MICROS => LogicalType::TIME_MICROS,\n+                parquet::ConvertedType::TIMESTAMP_MILLIS => LogicalType::TIMESTAMP_MILLIS,\n+                parquet::ConvertedType::TIMESTAMP_MICROS => LogicalType::TIMESTAMP_MICROS,\n+                parquet::ConvertedType::UINT_8 => LogicalType::UINT_8,\n+                parquet::ConvertedType::UINT_16 => LogicalType::UINT_16,\n+                parquet::ConvertedType::UINT_32 => LogicalType::UINT_32,\n+                parquet::ConvertedType::UINT_64 => LogicalType::UINT_64,\n+                parquet::ConvertedType::INT_8 => LogicalType::INT_8,\n+                parquet::ConvertedType::INT_16 => LogicalType::INT_16,\n+                parquet::ConvertedType::INT_32 => LogicalType::INT_32,\n+                parquet::ConvertedType::INT_64 => LogicalType::INT_64,\n+                parquet::ConvertedType::JSON => LogicalType::JSON,\n+                parquet::ConvertedType::BSON => LogicalType::BSON,\n+                parquet::ConvertedType::INTERVAL => LogicalType::INTERVAL,\n+            },\n+        }\n+    }\n+}\n+\n+impl convert::From<LogicalType> for Option<parquet::ConvertedType> {\n+    fn from(value: LogicalType) -> Self {\n+        match value {\n+            LogicalType::NONE => None,\n+            LogicalType::UTF8 => Some(parquet::ConvertedType::UTF8),\n+            LogicalType::MAP => Some(parquet::ConvertedType::MAP),\n+            LogicalType::MAP_KEY_VALUE => Some(parquet::ConvertedType::MAP_KEY_VALUE),\n+            LogicalType::LIST => Some(parquet::ConvertedType::LIST),\n+            LogicalType::ENUM => Some(parquet::ConvertedType::ENUM),\n+            LogicalType::DECIMAL => Some(parquet::ConvertedType::DECIMAL),\n+            LogicalType::DATE => Some(parquet::ConvertedType::DATE),\n+            LogicalType::TIME_MILLIS => Some(parquet::ConvertedType::TIME_MILLIS),\n+            LogicalType::TIME_MICROS => Some(parquet::ConvertedType::TIME_MICROS),\n+            LogicalType::TIMESTAMP_MILLIS => Some(parquet::ConvertedType::TIMESTAMP_MILLIS),\n+            LogicalType::TIMESTAMP_MICROS => Some(parquet::ConvertedType::TIMESTAMP_MICROS),\n+            LogicalType::UINT_8 => Some(parquet::ConvertedType::UINT_8),\n+            LogicalType::UINT_16 => Some(parquet::ConvertedType::UINT_16),\n+            LogicalType::UINT_32 => Some(parquet::ConvertedType::UINT_32),\n+            LogicalType::UINT_64 => Some(parquet::ConvertedType::UINT_64),\n+            LogicalType::INT_8 => Some(parquet::ConvertedType::INT_8),\n+            LogicalType::INT_16 => Some(parquet::ConvertedType::INT_16),\n+            LogicalType::INT_32 => Some(parquet::ConvertedType::INT_32),\n+            LogicalType::INT_64 => Some(parquet::ConvertedType::INT_64),\n+            LogicalType::JSON => Some(parquet::ConvertedType::JSON),\n+            LogicalType::BSON => Some(parquet::ConvertedType::BSON),\n+            LogicalType::INTERVAL => Some(parquet::ConvertedType::INTERVAL),\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::FieldRepetitionType <=> Repetition conversion\n+\n+impl convert::From<parquet::FieldRepetitionType> for Repetition {\n+    fn from(value: parquet::FieldRepetitionType) -> Self {\n+        match value {\n+            parquet::FieldRepetitionType::REQUIRED => Repetition::REQUIRED,\n+            parquet::FieldRepetitionType::OPTIONAL => Repetition::OPTIONAL,\n+            parquet::FieldRepetitionType::REPEATED => Repetition::REPEATED,\n+        }\n+    }\n+}\n+\n+impl convert::From<Repetition> for parquet::FieldRepetitionType {\n+    fn from(value: Repetition) -> Self {\n+        match value {\n+            Repetition::REQUIRED => parquet::FieldRepetitionType::REQUIRED,\n+            Repetition::OPTIONAL => parquet::FieldRepetitionType::OPTIONAL,\n+            Repetition::REPEATED => parquet::FieldRepetitionType::REPEATED,\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::Encoding <=> Encoding conversion\n+\n+impl convert::From<parquet::Encoding> for Encoding {\n+    fn from(value: parquet::Encoding) -> Self {\n+        match value {\n+            parquet::Encoding::PLAIN => Encoding::PLAIN,\n+            parquet::Encoding::PLAIN_DICTIONARY => Encoding::PLAIN_DICTIONARY,\n+            parquet::Encoding::RLE => Encoding::RLE,\n+            parquet::Encoding::BIT_PACKED => Encoding::BIT_PACKED,\n+            parquet::Encoding::DELTA_BINARY_PACKED => Encoding::DELTA_BINARY_PACKED,\n+            parquet::Encoding::DELTA_LENGTH_BYTE_ARRAY => Encoding::DELTA_LENGTH_BYTE_ARRAY,\n+            parquet::Encoding::DELTA_BYTE_ARRAY => Encoding::DELTA_BYTE_ARRAY,\n+            parquet::Encoding::RLE_DICTIONARY => Encoding::RLE_DICTIONARY,\n+        }\n+    }\n+}\n+\n+impl convert::From<Encoding> for parquet::Encoding {\n+    fn from(value: Encoding) -> Self {\n+        match value {\n+            Encoding::PLAIN => parquet::Encoding::PLAIN,\n+            Encoding::PLAIN_DICTIONARY => parquet::Encoding::PLAIN_DICTIONARY,\n+            Encoding::RLE => parquet::Encoding::RLE,\n+            Encoding::BIT_PACKED => parquet::Encoding::BIT_PACKED,\n+            Encoding::DELTA_BINARY_PACKED => parquet::Encoding::DELTA_BINARY_PACKED,\n+            Encoding::DELTA_LENGTH_BYTE_ARRAY => parquet::Encoding::DELTA_LENGTH_BYTE_ARRAY,\n+            Encoding::DELTA_BYTE_ARRAY => parquet::Encoding::DELTA_BYTE_ARRAY,\n+            Encoding::RLE_DICTIONARY => parquet::Encoding::RLE_DICTIONARY,\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::CompressionCodec <=> Compression conversion\n+\n+impl convert::From<parquet::CompressionCodec> for Compression {\n+    fn from(value: parquet::CompressionCodec) -> Self {\n+        match value {\n+            parquet::CompressionCodec::UNCOMPRESSED => Compression::UNCOMPRESSED,\n+            parquet::CompressionCodec::SNAPPY => Compression::SNAPPY,\n+            parquet::CompressionCodec::GZIP => Compression::GZIP,\n+            parquet::CompressionCodec::LZO => Compression::LZO,\n+            parquet::CompressionCodec::BROTLI => Compression::BROTLI,\n+            parquet::CompressionCodec::LZ4 => Compression::LZ4,\n+            parquet::CompressionCodec::ZSTD => Compression::ZSTD,\n+        }\n+    }\n+}\n+\n+impl convert::From<Compression> for parquet::CompressionCodec {\n+    fn from(value: Compression) -> Self {\n+        match value {\n+            Compression::UNCOMPRESSED => parquet::CompressionCodec::UNCOMPRESSED,\n+            Compression::SNAPPY => parquet::CompressionCodec::SNAPPY,\n+            Compression::GZIP => parquet::CompressionCodec::GZIP,\n+            Compression::LZO => parquet::CompressionCodec::LZO,\n+            Compression::BROTLI => parquet::CompressionCodec::BROTLI,\n+            Compression::LZ4 => parquet::CompressionCodec::LZ4,\n+            Compression::ZSTD => parquet::CompressionCodec::ZSTD,\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// parquet::PageType <=> PageType conversion\n+\n+impl convert::From<parquet::PageType> for PageType {\n+    fn from(value: parquet::PageType) -> Self {\n+        match value {\n+            parquet::PageType::DATA_PAGE => PageType::DATA_PAGE,\n+            parquet::PageType::INDEX_PAGE => PageType::INDEX_PAGE,\n+            parquet::PageType::DICTIONARY_PAGE => PageType::DICTIONARY_PAGE,\n+            parquet::PageType::DATA_PAGE_V2 => PageType::DATA_PAGE_V2,\n+        }\n+    }\n+}\n+\n+impl convert::From<PageType> for parquet::PageType {\n+    fn from(value: PageType) -> Self {\n+        match value {\n+            PageType::DATA_PAGE => parquet::PageType::DATA_PAGE,\n+            PageType::INDEX_PAGE => parquet::PageType::INDEX_PAGE,\n+            PageType::DICTIONARY_PAGE => parquet::PageType::DICTIONARY_PAGE,\n+            PageType::DATA_PAGE_V2 => parquet::PageType::DATA_PAGE_V2,\n+        }\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// String conversions for schema parsing.\n+\n+impl str::FromStr for Repetition {\n+    type Err = ParquetError;\n+\n+    fn from_str(s: &str) -> result::Result<Self, Self::Err> {\n+        match s {\n+            \"REQUIRED\" => Ok(Repetition::REQUIRED),\n+            \"OPTIONAL\" => Ok(Repetition::OPTIONAL),\n+            \"REPEATED\" => Ok(Repetition::REPEATED),\n+            other => Err(general_err!(\"Invalid repetition {}\", other)),\n+        }\n+    }\n+}\n+\n+impl str::FromStr for Type {\n+    type Err = ParquetError;\n+\n+    fn from_str(s: &str) -> result::Result<Self, Self::Err> {\n+        match s {\n+            \"BOOLEAN\" => Ok(Type::BOOLEAN),\n+            \"INT32\" => Ok(Type::INT32),\n+            \"INT64\" => Ok(Type::INT64),\n+            \"INT96\" => Ok(Type::INT96),\n+            \"FLOAT\" => Ok(Type::FLOAT),\n+            \"DOUBLE\" => Ok(Type::DOUBLE),\n+            \"BYTE_ARRAY\" | \"BINARY\" => Ok(Type::BYTE_ARRAY),\n+            \"FIXED_LEN_BYTE_ARRAY\" => Ok(Type::FIXED_LEN_BYTE_ARRAY),\n+            other => Err(general_err!(\"Invalid type {}\", other)),\n+        }\n+    }\n+}\n+\n+impl str::FromStr for LogicalType {\n+    type Err = ParquetError;\n+\n+    fn from_str(s: &str) -> result::Result<Self, Self::Err> {\n+        match s {\n+            \"NONE\" => Ok(LogicalType::NONE),\n+            \"UTF8\" => Ok(LogicalType::UTF8),\n+            \"MAP\" => Ok(LogicalType::MAP),\n+            \"MAP_KEY_VALUE\" => Ok(LogicalType::MAP_KEY_VALUE),\n+            \"LIST\" => Ok(LogicalType::LIST),\n+            \"ENUM\" => Ok(LogicalType::ENUM),\n+            \"DECIMAL\" => Ok(LogicalType::DECIMAL),\n+            \"DATE\" => Ok(LogicalType::DATE),\n+            \"TIME_MILLIS\" => Ok(LogicalType::TIME_MILLIS),\n+            \"TIME_MICROS\" => Ok(LogicalType::TIME_MICROS),\n+            \"TIMESTAMP_MILLIS\" => Ok(LogicalType::TIMESTAMP_MILLIS),\n+            \"TIMESTAMP_MICROS\" => Ok(LogicalType::TIMESTAMP_MICROS),\n+            \"UINT_8\" => Ok(LogicalType::UINT_8),\n+            \"UINT_16\" => Ok(LogicalType::UINT_16),\n+            \"UINT_32\" => Ok(LogicalType::UINT_32),\n+            \"UINT_64\" => Ok(LogicalType::UINT_64),\n+            \"INT_8\" => Ok(LogicalType::INT_8),\n+            \"INT_16\" => Ok(LogicalType::INT_16),\n+            \"INT_32\" => Ok(LogicalType::INT_32),\n+            \"INT_64\" => Ok(LogicalType::INT_64),\n+            \"JSON\" => Ok(LogicalType::JSON),\n+            \"BSON\" => Ok(LogicalType::BSON),\n+            \"INTERVAL\" => Ok(LogicalType::INTERVAL),\n+            other => Err(general_err!(\"Invalid logical type {}\", other)),\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_display_type() {\n+        assert_eq!(Type::BOOLEAN.to_string(), \"BOOLEAN\");\n+        assert_eq!(Type::INT32.to_string(), \"INT32\");\n+        assert_eq!(Type::INT64.to_string(), \"INT64\");\n+        assert_eq!(Type::INT96.to_string(), \"INT96\");\n+        assert_eq!(Type::FLOAT.to_string(), \"FLOAT\");\n+        assert_eq!(Type::DOUBLE.to_string(), \"DOUBLE\");\n+        assert_eq!(Type::BYTE_ARRAY.to_string(), \"BYTE_ARRAY\");\n+        assert_eq!(\n+            Type::FIXED_LEN_BYTE_ARRAY.to_string(),\n+            \"FIXED_LEN_BYTE_ARRAY\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_from_type() {\n+        assert_eq!(Type::from(parquet::Type::BOOLEAN), Type::BOOLEAN);\n+        assert_eq!(Type::from(parquet::Type::INT32), Type::INT32);\n+        assert_eq!(Type::from(parquet::Type::INT64), Type::INT64);\n+        assert_eq!(Type::from(parquet::Type::INT96), Type::INT96);\n+        assert_eq!(Type::from(parquet::Type::FLOAT), Type::FLOAT);\n+        assert_eq!(Type::from(parquet::Type::DOUBLE), Type::DOUBLE);\n+        assert_eq!(Type::from(parquet::Type::BYTE_ARRAY), Type::BYTE_ARRAY);\n+        assert_eq!(\n+            Type::from(parquet::Type::FIXED_LEN_BYTE_ARRAY),\n+            Type::FIXED_LEN_BYTE_ARRAY\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_type() {\n+        assert_eq!(parquet::Type::BOOLEAN, Type::BOOLEAN.into());\n+        assert_eq!(parquet::Type::INT32, Type::INT32.into());\n+        assert_eq!(parquet::Type::INT64, Type::INT64.into());\n+        assert_eq!(parquet::Type::INT96, Type::INT96.into());\n+        assert_eq!(parquet::Type::FLOAT, Type::FLOAT.into());\n+        assert_eq!(parquet::Type::DOUBLE, Type::DOUBLE.into());\n+        assert_eq!(parquet::Type::BYTE_ARRAY, Type::BYTE_ARRAY.into());\n+        assert_eq!(\n+            parquet::Type::FIXED_LEN_BYTE_ARRAY,\n+            Type::FIXED_LEN_BYTE_ARRAY.into()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_from_string_into_type() {\n+        assert_eq!(\n+            Type::BOOLEAN.to_string().parse::<Type>().unwrap(),\n+            Type::BOOLEAN\n+        );\n+        assert_eq!(\n+            Type::INT32.to_string().parse::<Type>().unwrap(),\n+            Type::INT32\n+        );\n+        assert_eq!(\n+            Type::INT64.to_string().parse::<Type>().unwrap(),\n+            Type::INT64\n+        );\n+        assert_eq!(\n+            Type::INT96.to_string().parse::<Type>().unwrap(),\n+            Type::INT96\n+        );\n+        assert_eq!(\n+            Type::FLOAT.to_string().parse::<Type>().unwrap(),\n+            Type::FLOAT\n+        );\n+        assert_eq!(\n+            Type::DOUBLE.to_string().parse::<Type>().unwrap(),\n+            Type::DOUBLE\n+        );\n+        assert_eq!(\n+            Type::BYTE_ARRAY.to_string().parse::<Type>().unwrap(),\n+            Type::BYTE_ARRAY\n+        );\n+        assert_eq!(\"BINARY\".parse::<Type>().unwrap(), Type::BYTE_ARRAY);\n+        assert_eq!(\n+            Type::FIXED_LEN_BYTE_ARRAY\n+                .to_string()\n+                .parse::<Type>()\n+                .unwrap(),\n+            Type::FIXED_LEN_BYTE_ARRAY\n+        );\n+    }\n+\n+    #[test]\n+    fn test_display_logical_type() {\n+        assert_eq!(LogicalType::NONE.to_string(), \"NONE\");\n+        assert_eq!(LogicalType::UTF8.to_string(), \"UTF8\");\n+        assert_eq!(LogicalType::MAP.to_string(), \"MAP\");\n+        assert_eq!(LogicalType::MAP_KEY_VALUE.to_string(), \"MAP_KEY_VALUE\");\n+        assert_eq!(LogicalType::LIST.to_string(), \"LIST\");\n+        assert_eq!(LogicalType::ENUM.to_string(), \"ENUM\");\n+        assert_eq!(LogicalType::DECIMAL.to_string(), \"DECIMAL\");\n+        assert_eq!(LogicalType::DATE.to_string(), \"DATE\");\n+        assert_eq!(LogicalType::TIME_MILLIS.to_string(), \"TIME_MILLIS\");\n+        assert_eq!(LogicalType::DATE.to_string(), \"DATE\");\n+        assert_eq!(LogicalType::TIME_MICROS.to_string(), \"TIME_MICROS\");\n+        assert_eq!(\n+            LogicalType::TIMESTAMP_MILLIS.to_string(),\n+            \"TIMESTAMP_MILLIS\"\n+        );\n+        assert_eq!(\n+            LogicalType::TIMESTAMP_MICROS.to_string(),\n+            \"TIMESTAMP_MICROS\"\n+        );\n+        assert_eq!(LogicalType::UINT_8.to_string(), \"UINT_8\");\n+        assert_eq!(LogicalType::UINT_16.to_string(), \"UINT_16\");\n+        assert_eq!(LogicalType::UINT_32.to_string(), \"UINT_32\");\n+        assert_eq!(LogicalType::UINT_64.to_string(), \"UINT_64\");\n+        assert_eq!(LogicalType::INT_8.to_string(), \"INT_8\");\n+        assert_eq!(LogicalType::INT_16.to_string(), \"INT_16\");\n+        assert_eq!(LogicalType::INT_32.to_string(), \"INT_32\");\n+        assert_eq!(LogicalType::INT_64.to_string(), \"INT_64\");\n+        assert_eq!(LogicalType::JSON.to_string(), \"JSON\");\n+        assert_eq!(LogicalType::BSON.to_string(), \"BSON\");\n+        assert_eq!(LogicalType::INTERVAL.to_string(), \"INTERVAL\");\n+    }\n+\n+    #[test]\n+    fn test_from_logical_type() {\n+        assert_eq!(LogicalType::from(None), LogicalType::NONE);\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::UTF8)),\n+            LogicalType::UTF8\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::MAP)),\n+            LogicalType::MAP\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::MAP_KEY_VALUE)),\n+            LogicalType::MAP_KEY_VALUE\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::LIST)),\n+            LogicalType::LIST\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::ENUM)),\n+            LogicalType::ENUM\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::DECIMAL)),\n+            LogicalType::DECIMAL\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::DATE)),\n+            LogicalType::DATE\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::TIME_MILLIS)),\n+            LogicalType::TIME_MILLIS\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::TIME_MICROS)),\n+            LogicalType::TIME_MICROS\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::TIMESTAMP_MILLIS)),\n+            LogicalType::TIMESTAMP_MILLIS\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::TIMESTAMP_MICROS)),\n+            LogicalType::TIMESTAMP_MICROS\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::UINT_8)),\n+            LogicalType::UINT_8\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::UINT_16)),\n+            LogicalType::UINT_16\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::UINT_32)),\n+            LogicalType::UINT_32\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::UINT_64)),\n+            LogicalType::UINT_64\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::INT_8)),\n+            LogicalType::INT_8\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::INT_16)),\n+            LogicalType::INT_16\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::INT_32)),\n+            LogicalType::INT_32\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::INT_64)),\n+            LogicalType::INT_64\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::JSON)),\n+            LogicalType::JSON\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::BSON)),\n+            LogicalType::BSON\n+        );\n+        assert_eq!(\n+            LogicalType::from(Some(parquet::ConvertedType::INTERVAL)),\n+            LogicalType::INTERVAL\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_logical_type() {\n+        let converted_type: Option<parquet::ConvertedType> = None;\n+        assert_eq!(converted_type, LogicalType::NONE.into());\n+        assert_eq!(Some(parquet::ConvertedType::UTF8), LogicalType::UTF8.into());\n+        assert_eq!(Some(parquet::ConvertedType::MAP), LogicalType::MAP.into());\n+        assert_eq!(\n+            Some(parquet::ConvertedType::MAP_KEY_VALUE),\n+            LogicalType::MAP_KEY_VALUE.into()\n+        );\n+        assert_eq!(Some(parquet::ConvertedType::LIST), LogicalType::LIST.into());\n+        assert_eq!(Some(parquet::ConvertedType::ENUM), LogicalType::ENUM.into());\n+        assert_eq!(\n+            Some(parquet::ConvertedType::DECIMAL),\n+            LogicalType::DECIMAL.into()\n+        );\n+        assert_eq!(Some(parquet::ConvertedType::DATE), LogicalType::DATE.into());\n+        assert_eq!(\n+            Some(parquet::ConvertedType::TIME_MILLIS),\n+            LogicalType::TIME_MILLIS.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::TIME_MICROS),\n+            LogicalType::TIME_MICROS.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::TIMESTAMP_MILLIS),\n+            LogicalType::TIMESTAMP_MILLIS.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::TIMESTAMP_MICROS),\n+            LogicalType::TIMESTAMP_MICROS.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::UINT_8),\n+            LogicalType::UINT_8.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::UINT_16),\n+            LogicalType::UINT_16.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::UINT_32),\n+            LogicalType::UINT_32.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::UINT_64),\n+            LogicalType::UINT_64.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::INT_8),\n+            LogicalType::INT_8.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::INT_16),\n+            LogicalType::INT_16.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::INT_32),\n+            LogicalType::INT_32.into()\n+        );\n+        assert_eq!(\n+            Some(parquet::ConvertedType::INT_64),\n+            LogicalType::INT_64.into()\n+        );\n+        assert_eq!(Some(parquet::ConvertedType::JSON), LogicalType::JSON.into());\n+        assert_eq!(Some(parquet::ConvertedType::BSON), LogicalType::BSON.into());\n+        assert_eq!(\n+            Some(parquet::ConvertedType::INTERVAL),\n+            LogicalType::INTERVAL.into()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_from_string_into_logical_type() {\n+        assert_eq!(\n+            LogicalType::NONE\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::NONE\n+        );\n+        assert_eq!(\n+            LogicalType::UTF8\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::UTF8\n+        );\n+        assert_eq!(\n+            LogicalType::MAP.to_string().parse::<LogicalType>().unwrap(),\n+            LogicalType::MAP\n+        );\n+        assert_eq!(\n+            LogicalType::MAP_KEY_VALUE\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::MAP_KEY_VALUE\n+        );\n+        assert_eq!(\n+            LogicalType::LIST\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::LIST\n+        );\n+        assert_eq!(\n+            LogicalType::ENUM\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::ENUM\n+        );\n+        assert_eq!(\n+            LogicalType::DECIMAL\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::DECIMAL\n+        );\n+        assert_eq!(\n+            LogicalType::DATE\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::DATE\n+        );\n+        assert_eq!(\n+            LogicalType::TIME_MILLIS\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::TIME_MILLIS\n+        );\n+        assert_eq!(\n+            LogicalType::TIME_MICROS\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::TIME_MICROS\n+        );\n+        assert_eq!(\n+            LogicalType::TIMESTAMP_MILLIS\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::TIMESTAMP_MILLIS\n+        );\n+        assert_eq!(\n+            LogicalType::TIMESTAMP_MICROS\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::TIMESTAMP_MICROS\n+        );\n+        assert_eq!(\n+            LogicalType::UINT_8\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::UINT_8\n+        );\n+        assert_eq!(\n+            LogicalType::UINT_16\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::UINT_16\n+        );\n+        assert_eq!(\n+            LogicalType::UINT_32\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::UINT_32\n+        );\n+        assert_eq!(\n+            LogicalType::UINT_64\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::UINT_64\n+        );\n+        assert_eq!(\n+            LogicalType::INT_8\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::INT_8\n+        );\n+        assert_eq!(\n+            LogicalType::INT_16\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::INT_16\n+        );\n+        assert_eq!(\n+            LogicalType::INT_32\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::INT_32\n+        );\n+        assert_eq!(\n+            LogicalType::INT_64\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::INT_64\n+        );\n+        assert_eq!(\n+            LogicalType::JSON\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::JSON\n+        );\n+        assert_eq!(\n+            LogicalType::BSON\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::BSON\n+        );\n+        assert_eq!(\n+            LogicalType::INTERVAL\n+                .to_string()\n+                .parse::<LogicalType>()\n+                .unwrap(),\n+            LogicalType::INTERVAL\n+        );\n+    }\n+\n+    #[test]\n+    fn test_display_repetition() {\n+        assert_eq!(Repetition::REQUIRED.to_string(), \"REQUIRED\");\n+        assert_eq!(Repetition::OPTIONAL.to_string(), \"OPTIONAL\");\n+        assert_eq!(Repetition::REPEATED.to_string(), \"REPEATED\");\n+    }\n+\n+    #[test]\n+    fn test_from_repetition() {\n+        assert_eq!(\n+            Repetition::from(parquet::FieldRepetitionType::REQUIRED),\n+            Repetition::REQUIRED\n+        );\n+        assert_eq!(\n+            Repetition::from(parquet::FieldRepetitionType::OPTIONAL),\n+            Repetition::OPTIONAL\n+        );\n+        assert_eq!(\n+            Repetition::from(parquet::FieldRepetitionType::REPEATED),\n+            Repetition::REPEATED\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_repetition() {\n+        assert_eq!(\n+            parquet::FieldRepetitionType::REQUIRED,\n+            Repetition::REQUIRED.into()\n+        );\n+        assert_eq!(\n+            parquet::FieldRepetitionType::OPTIONAL,\n+            Repetition::OPTIONAL.into()\n+        );\n+        assert_eq!(\n+            parquet::FieldRepetitionType::REPEATED,\n+            Repetition::REPEATED.into()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_from_string_into_repetition() {\n+        assert_eq!(\n+            Repetition::REQUIRED\n+                .to_string()\n+                .parse::<Repetition>()\n+                .unwrap(),\n+            Repetition::REQUIRED\n+        );\n+        assert_eq!(\n+            Repetition::OPTIONAL\n+                .to_string()\n+                .parse::<Repetition>()\n+                .unwrap(),\n+            Repetition::OPTIONAL\n+        );\n+        assert_eq!(\n+            Repetition::REPEATED\n+                .to_string()\n+                .parse::<Repetition>()\n+                .unwrap(),\n+            Repetition::REPEATED\n+        );\n+    }\n+\n+    #[test]\n+    fn test_display_encoding() {\n+        assert_eq!(Encoding::PLAIN.to_string(), \"PLAIN\");\n+        assert_eq!(Encoding::PLAIN_DICTIONARY.to_string(), \"PLAIN_DICTIONARY\");\n+        assert_eq!(Encoding::RLE.to_string(), \"RLE\");\n+        assert_eq!(Encoding::BIT_PACKED.to_string(), \"BIT_PACKED\");\n+        assert_eq!(\n+            Encoding::DELTA_BINARY_PACKED.to_string(),\n+            \"DELTA_BINARY_PACKED\"\n+        );\n+        assert_eq!(\n+            Encoding::DELTA_LENGTH_BYTE_ARRAY.to_string(),\n+            \"DELTA_LENGTH_BYTE_ARRAY\"\n+        );\n+        assert_eq!(Encoding::DELTA_BYTE_ARRAY.to_string(), \"DELTA_BYTE_ARRAY\");\n+        assert_eq!(Encoding::RLE_DICTIONARY.to_string(), \"RLE_DICTIONARY\");\n+    }\n+\n+    #[test]\n+    fn test_from_encoding() {\n+        assert_eq!(Encoding::from(parquet::Encoding::PLAIN), Encoding::PLAIN);\n+        assert_eq!(\n+            Encoding::from(parquet::Encoding::PLAIN_DICTIONARY),\n+            Encoding::PLAIN_DICTIONARY\n+        );\n+        assert_eq!(Encoding::from(parquet::Encoding::RLE), Encoding::RLE);\n+        assert_eq!(\n+            Encoding::from(parquet::Encoding::BIT_PACKED),\n+            Encoding::BIT_PACKED\n+        );\n+        assert_eq!(\n+            Encoding::from(parquet::Encoding::DELTA_BINARY_PACKED),\n+            Encoding::DELTA_BINARY_PACKED\n+        );\n+        assert_eq!(\n+            Encoding::from(parquet::Encoding::DELTA_LENGTH_BYTE_ARRAY),\n+            Encoding::DELTA_LENGTH_BYTE_ARRAY\n+        );\n+        assert_eq!(\n+            Encoding::from(parquet::Encoding::DELTA_BYTE_ARRAY),\n+            Encoding::DELTA_BYTE_ARRAY\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_encoding() {\n+        assert_eq!(parquet::Encoding::PLAIN, Encoding::PLAIN.into());\n+        assert_eq!(\n+            parquet::Encoding::PLAIN_DICTIONARY,\n+            Encoding::PLAIN_DICTIONARY.into()\n+        );\n+        assert_eq!(parquet::Encoding::RLE, Encoding::RLE.into());\n+        assert_eq!(parquet::Encoding::BIT_PACKED, Encoding::BIT_PACKED.into());\n+        assert_eq!(\n+            parquet::Encoding::DELTA_BINARY_PACKED,\n+            Encoding::DELTA_BINARY_PACKED.into()\n+        );\n+        assert_eq!(\n+            parquet::Encoding::DELTA_LENGTH_BYTE_ARRAY,\n+            Encoding::DELTA_LENGTH_BYTE_ARRAY.into()\n+        );\n+        assert_eq!(\n+            parquet::Encoding::DELTA_BYTE_ARRAY,\n+            Encoding::DELTA_BYTE_ARRAY.into()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_display_compression() {\n+        assert_eq!(Compression::UNCOMPRESSED.to_string(), \"UNCOMPRESSED\");\n+        assert_eq!(Compression::SNAPPY.to_string(), \"SNAPPY\");\n+        assert_eq!(Compression::GZIP.to_string(), \"GZIP\");\n+        assert_eq!(Compression::LZO.to_string(), \"LZO\");\n+        assert_eq!(Compression::BROTLI.to_string(), \"BROTLI\");\n+        assert_eq!(Compression::LZ4.to_string(), \"LZ4\");\n+        assert_eq!(Compression::ZSTD.to_string(), \"ZSTD\");\n+    }\n+\n+    #[test]\n+    fn test_from_compression() {\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::UNCOMPRESSED),\n+            Compression::UNCOMPRESSED\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::SNAPPY),\n+            Compression::SNAPPY\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::GZIP),\n+            Compression::GZIP\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::LZO),\n+            Compression::LZO\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::BROTLI),\n+            Compression::BROTLI\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::LZ4),\n+            Compression::LZ4\n+        );\n+        assert_eq!(\n+            Compression::from(parquet::CompressionCodec::ZSTD),\n+            Compression::ZSTD\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_compression() {\n+        assert_eq!(\n+            parquet::CompressionCodec::UNCOMPRESSED,\n+            Compression::UNCOMPRESSED.into()\n+        );\n+        assert_eq!(\n+            parquet::CompressionCodec::SNAPPY,\n+            Compression::SNAPPY.into()\n+        );\n+        assert_eq!(parquet::CompressionCodec::GZIP, Compression::GZIP.into());\n+        assert_eq!(parquet::CompressionCodec::LZO, Compression::LZO.into());\n+        assert_eq!(\n+            parquet::CompressionCodec::BROTLI,\n+            Compression::BROTLI.into()\n+        );\n+        assert_eq!(parquet::CompressionCodec::LZ4, Compression::LZ4.into());\n+        assert_eq!(parquet::CompressionCodec::ZSTD, Compression::ZSTD.into());\n+    }\n+\n+    #[test]\n+    fn test_display_page_type() {\n+        assert_eq!(PageType::DATA_PAGE.to_string(), \"DATA_PAGE\");\n+        assert_eq!(PageType::INDEX_PAGE.to_string(), \"INDEX_PAGE\");\n+        assert_eq!(PageType::DICTIONARY_PAGE.to_string(), \"DICTIONARY_PAGE\");\n+        assert_eq!(PageType::DATA_PAGE_V2.to_string(), \"DATA_PAGE_V2\");\n+    }\n+\n+    #[test]\n+    fn test_from_page_type() {\n+        assert_eq!(\n+            PageType::from(parquet::PageType::DATA_PAGE),\n+            PageType::DATA_PAGE\n+        );\n+        assert_eq!(\n+            PageType::from(parquet::PageType::INDEX_PAGE),\n+            PageType::INDEX_PAGE\n+        );\n+        assert_eq!(\n+            PageType::from(parquet::PageType::DICTIONARY_PAGE),\n+            PageType::DICTIONARY_PAGE\n+        );\n+        assert_eq!(\n+            PageType::from(parquet::PageType::DATA_PAGE_V2),\n+            PageType::DATA_PAGE_V2\n+        );\n+    }\n+\n+    #[test]\n+    fn test_into_page_type() {\n+        assert_eq!(parquet::PageType::DATA_PAGE, PageType::DATA_PAGE.into());\n+        assert_eq!(parquet::PageType::INDEX_PAGE, PageType::INDEX_PAGE.into());\n+        assert_eq!(\n+            parquet::PageType::DICTIONARY_PAGE,\n+            PageType::DICTIONARY_PAGE.into()\n+        );\n+        assert_eq!(\n+            parquet::PageType::DATA_PAGE_V2,\n+            PageType::DATA_PAGE_V2.into()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_display_sort_order() {\n+        assert_eq!(SortOrder::SIGNED.to_string(), \"SIGNED\");\n+        assert_eq!(SortOrder::UNSIGNED.to_string(), \"UNSIGNED\");\n+        assert_eq!(SortOrder::UNDEFINED.to_string(), \"UNDEFINED\");\n+    }\n+\n+    #[test]\n+    fn test_display_column_order() {\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::SIGNED).to_string(),\n+            \"TYPE_DEFINED_ORDER(SIGNED)\"\n+        );\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::UNSIGNED).to_string(),\n+            \"TYPE_DEFINED_ORDER(UNSIGNED)\"\n+        );\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::UNDEFINED).to_string(),\n+            \"TYPE_DEFINED_ORDER(UNDEFINED)\"\n+        );\n+        assert_eq!(ColumnOrder::UNDEFINED.to_string(), \"UNDEFINED\");\n+    }\n+\n+    #[test]\n+    fn test_column_order_get_sort_order() {\n+        // Helper to check the order in a list of values.\n+        // Only logical type is checked.\n+        fn check_sort_order(types: Vec<LogicalType>, expected_order: SortOrder) {\n+            for tpe in types {\n+                assert_eq!(\n+                    ColumnOrder::get_sort_order(tpe, Type::BYTE_ARRAY),\n+                    expected_order\n+                );\n+            }\n+        }\n+\n+        // Unsigned comparison (physical type does not matter)\n+        let unsigned = vec![\n+            LogicalType::UTF8,\n+            LogicalType::JSON,\n+            LogicalType::BSON,\n+            LogicalType::ENUM,\n+            LogicalType::UINT_8,\n+            LogicalType::UINT_16,\n+            LogicalType::UINT_32,\n+            LogicalType::UINT_64,\n+            LogicalType::INTERVAL,\n+        ];\n+        check_sort_order(unsigned, SortOrder::UNSIGNED);\n+\n+        // Signed comparison (physical type does not matter)\n+        let signed = vec![\n+            LogicalType::INT_8,\n+            LogicalType::INT_16,\n+            LogicalType::INT_32,\n+            LogicalType::INT_64,\n+            LogicalType::DECIMAL,\n+            LogicalType::DATE,\n+            LogicalType::TIME_MILLIS,\n+            LogicalType::TIME_MICROS,\n+            LogicalType::TIMESTAMP_MILLIS,\n+            LogicalType::TIMESTAMP_MICROS,\n+        ];\n+        check_sort_order(signed, SortOrder::SIGNED);\n+\n+        // Undefined comparison\n+        let undefined = vec![\n+            LogicalType::LIST,\n+            LogicalType::MAP,\n+            LogicalType::MAP_KEY_VALUE,\n+        ];\n+        check_sort_order(undefined, SortOrder::UNDEFINED);\n+\n+        // Check None logical type\n+        // This should return a sort order for byte array type.\n+        check_sort_order(vec![LogicalType::NONE], SortOrder::UNSIGNED);\n+    }\n+\n+    #[test]\n+    fn test_column_order_get_default_sort_order() {\n+        // Comparison based on physical type\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::BOOLEAN),\n+            SortOrder::UNSIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::INT32),\n+            SortOrder::SIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::INT64),\n+            SortOrder::SIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::INT96),\n+            SortOrder::UNDEFINED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::FLOAT),\n+            SortOrder::SIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::DOUBLE),\n+            SortOrder::SIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::BYTE_ARRAY),\n+            SortOrder::UNSIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::get_default_sort_order(Type::FIXED_LEN_BYTE_ARRAY),\n+            SortOrder::UNSIGNED\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_order_sort_order() {\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::SIGNED).sort_order(),\n+            SortOrder::SIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::UNSIGNED).sort_order(),\n+            SortOrder::UNSIGNED\n+        );\n+        assert_eq!(\n+            ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::UNDEFINED).sort_order(),\n+            SortOrder::UNDEFINED\n+        );\n+        assert_eq!(ColumnOrder::UNDEFINED.sort_order(), SortOrder::SIGNED);\n+    }\n+}\ndiff --git a/rust/src/parquet/column/mod.rs b/rust/src/parquet/column/mod.rs\nnew file mode 100644\nindex 0000000000..09c4bde51f\n--- /dev/null\n+++ b/rust/src/parquet/column/mod.rs\n@@ -0,0 +1,124 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Low level column reader and writer APIs.\n+//!\n+//! This API is designed for reading and writing column values, definition and repetition\n+//! levels directly.\n+//!\n+//! # Example of writing and reading data\n+//!\n+//! Data has the following format:\n+//! ```text\n+//! +---------------+\n+//! |         values|\n+//! +---------------+\n+//! |[1, 2]         |\n+//! |[3, null, null]|\n+//! +---------------+\n+//! ```\n+//!\n+//! The example uses column writer and reader APIs to write raw values, definition and\n+//! repetition levels and read them to verify write/read correctness.\n+//!\n+//! ```rust\n+//! use std::{fs, path::Path, rc::Rc};\n+//!\n+//! use arrow::parquet::{\n+//!     column::{reader::ColumnReader, writer::ColumnWriter},\n+//!     file::{\n+//!         properties::WriterProperties,\n+//!         reader::{FileReader, SerializedFileReader},\n+//!         writer::{FileWriter, SerializedFileWriter},\n+//!     },\n+//!     schema::parser::parse_message_type,\n+//! };\n+//!\n+//! let path = Path::new(\"target/debug/examples/column_sample.parquet\");\n+//!\n+//! // Writing data using column writer API.\n+//!\n+//! let message_type = \"\n+//!   message schema {\n+//!     optional group values (LIST) {\n+//!       repeated group list {\n+//!         optional INT32 element;\n+//!       }\n+//!     }\n+//!   }\n+//! \";\n+//! let schema = Rc::new(parse_message_type(message_type).unwrap());\n+//! let props = Rc::new(WriterProperties::builder().build());\n+//! let file = fs::File::create(path).unwrap();\n+//! let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+//! let mut row_group_writer = writer.next_row_group().unwrap();\n+//! while let Some(mut col_writer) = row_group_writer.next_column().unwrap() {\n+//!     match col_writer {\n+//!         // You can also use `get_typed_column_writer` method to extract typed writer.\n+//!         ColumnWriter::Int32ColumnWriter(ref mut typed_writer) => {\n+//!             typed_writer\n+//!                 .write_batch(&[1, 2, 3], Some(&[3, 3, 3, 2, 2]), Some(&[0, 1, 0, 1, 1]))\n+//!                 .unwrap();\n+//!         }\n+//!         _ => {}\n+//!     }\n+//!     row_group_writer.close_column(col_writer).unwrap();\n+//! }\n+//! writer.close_row_group(row_group_writer).unwrap();\n+//! writer.close().unwrap();\n+//!\n+//! // Reading data using column reader API.\n+//!\n+//! let file = fs::File::open(path).unwrap();\n+//! let reader = SerializedFileReader::new(file).unwrap();\n+//! let metadata = reader.metadata();\n+//!\n+//! let mut res = Ok((0, 0));\n+//! let mut values = vec![0; 8];\n+//! let mut def_levels = vec![0; 8];\n+//! let mut rep_levels = vec![0; 8];\n+//!\n+//! for i in 0..metadata.num_row_groups() {\n+//!     let row_group_reader = reader.get_row_group(i).unwrap();\n+//!     let row_group_metadata = metadata.row_group(i);\n+//!\n+//!     for j in 0..row_group_metadata.num_columns() {\n+//!         let mut column_reader = row_group_reader.get_column_reader(j).unwrap();\n+//!         match column_reader {\n+//!             // You can also use `get_typed_column_reader` method to extract typed reader.\n+//!             ColumnReader::Int32ColumnReader(ref mut typed_reader) => {\n+//!                 res = typed_reader.read_batch(\n+//!                     8, // batch size\n+//!                     Some(&mut def_levels),\n+//!                     Some(&mut rep_levels),\n+//!                     &mut values,\n+//!                 );\n+//!             }\n+//!             _ => {}\n+//!         }\n+//!     }\n+//! }\n+//!\n+//! assert_eq!(res, Ok((3, 5)));\n+//! assert_eq!(values, vec![1, 2, 3, 0, 0, 0, 0, 0]);\n+//! assert_eq!(def_levels, vec![3, 3, 3, 2, 2, 0, 0, 0]);\n+//! assert_eq!(rep_levels, vec![0, 1, 0, 1, 1, 0, 0, 0]);\n+//! ```\n+\n+pub mod page;\n+pub mod reader;\n+pub mod writer;\ndiff --git a/rust/src/parquet/column/page.rs b/rust/src/parquet/column/page.rs\nnew file mode 100644\nindex 0000000000..115037cba0\n--- /dev/null\n+++ b/rust/src/parquet/column/page.rs\n@@ -0,0 +1,296 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains Parquet Page definitions and page reader interface.\n+\n+use crate::parquet::basic::{Encoding, PageType};\n+use crate::parquet::errors::Result;\n+use crate::parquet::file::{metadata::ColumnChunkMetaData, statistics::Statistics};\n+use crate::parquet::util::memory::ByteBufferPtr;\n+\n+/// Parquet Page definition.\n+///\n+/// List of supported pages.\n+/// These are 1-to-1 mapped from the equivalent Thrift definitions, except `buf` which\n+/// used to store uncompressed bytes of the page.\n+pub enum Page {\n+    DataPage {\n+        buf: ByteBufferPtr,\n+        num_values: u32,\n+        encoding: Encoding,\n+        def_level_encoding: Encoding,\n+        rep_level_encoding: Encoding,\n+        statistics: Option<Statistics>,\n+    },\n+    DataPageV2 {\n+        buf: ByteBufferPtr,\n+        num_values: u32,\n+        encoding: Encoding,\n+        num_nulls: u32,\n+        num_rows: u32,\n+        def_levels_byte_len: u32,\n+        rep_levels_byte_len: u32,\n+        is_compressed: bool,\n+        statistics: Option<Statistics>,\n+    },\n+    DictionaryPage {\n+        buf: ByteBufferPtr,\n+        num_values: u32,\n+        encoding: Encoding,\n+        is_sorted: bool,\n+    },\n+}\n+\n+impl Page {\n+    /// Returns [`PageType`](`::basic::PageType`) for this page.\n+    pub fn page_type(&self) -> PageType {\n+        match self {\n+            &Page::DataPage { .. } => PageType::DATA_PAGE,\n+            &Page::DataPageV2 { .. } => PageType::DATA_PAGE_V2,\n+            &Page::DictionaryPage { .. } => PageType::DICTIONARY_PAGE,\n+        }\n+    }\n+\n+    /// Returns internal byte buffer reference for this page.\n+    pub fn buffer(&self) -> &ByteBufferPtr {\n+        match self {\n+            &Page::DataPage { ref buf, .. } => &buf,\n+            &Page::DataPageV2 { ref buf, .. } => &buf,\n+            &Page::DictionaryPage { ref buf, .. } => &buf,\n+        }\n+    }\n+\n+    /// Returns number of values in this page.\n+    pub fn num_values(&self) -> u32 {\n+        match self {\n+            &Page::DataPage { num_values, .. } => num_values,\n+            &Page::DataPageV2 { num_values, .. } => num_values,\n+            &Page::DictionaryPage { num_values, .. } => num_values,\n+        }\n+    }\n+\n+    /// Returns this page [`Encoding`](`::basic::Encoding`).\n+    pub fn encoding(&self) -> Encoding {\n+        match self {\n+            &Page::DataPage { encoding, .. } => encoding,\n+            &Page::DataPageV2 { encoding, .. } => encoding,\n+            &Page::DictionaryPage { encoding, .. } => encoding,\n+        }\n+    }\n+\n+    /// Returns optional [`Statistics`](`::file::metadata::Statistics`).\n+    pub fn statistics(&self) -> Option<&Statistics> {\n+        match self {\n+            &Page::DataPage { ref statistics, .. } => statistics.as_ref(),\n+            &Page::DataPageV2 { ref statistics, .. } => statistics.as_ref(),\n+            &Page::DictionaryPage { .. } => None,\n+        }\n+    }\n+}\n+\n+/// Helper struct to represent pages with potentially compressed buffer (data page v1) or\n+/// compressed and concatenated buffer (def levels + rep levels + compressed values for\n+/// data page v2).\n+///\n+/// The difference with `Page` is that `Page` buffer is always uncompressed.\n+pub struct CompressedPage {\n+    compressed_page: Page,\n+    uncompressed_size: usize,\n+}\n+\n+impl CompressedPage {\n+    /// Creates `CompressedPage` from a page with potentially compressed buffer and\n+    /// uncompressed size.\n+    pub fn new(compressed_page: Page, uncompressed_size: usize) -> Self {\n+        Self {\n+            compressed_page,\n+            uncompressed_size,\n+        }\n+    }\n+\n+    /// Returns page type.\n+    pub fn page_type(&self) -> PageType {\n+        self.compressed_page.page_type()\n+    }\n+\n+    /// Returns underlying page with potentially compressed buffer.\n+    pub fn compressed_page(&self) -> &Page {\n+        &self.compressed_page\n+    }\n+\n+    /// Returns uncompressed size in bytes.\n+    pub fn uncompressed_size(&self) -> usize {\n+        self.uncompressed_size\n+    }\n+\n+    /// Returns compressed size in bytes.\n+    ///\n+    /// Note that it is assumed that buffer is compressed, but it may not be. In this\n+    /// case compressed size will be equal to uncompressed size.\n+    pub fn compressed_size(&self) -> usize {\n+        self.compressed_page.buffer().len()\n+    }\n+\n+    /// Number of values in page.\n+    pub fn num_values(&self) -> u32 {\n+        self.compressed_page.num_values()\n+    }\n+\n+    /// Returns encoding for values in page.\n+    pub fn encoding(&self) -> Encoding {\n+        self.compressed_page.encoding()\n+    }\n+\n+    /// Returns slice of compressed buffer in the page.\n+    pub fn data(&self) -> &[u8] {\n+        self.compressed_page.buffer().data()\n+    }\n+}\n+\n+/// Contains page write metrics.\n+pub struct PageWriteSpec {\n+    pub page_type: PageType,\n+    pub uncompressed_size: usize,\n+    pub compressed_size: usize,\n+    pub num_values: u32,\n+    pub offset: u64,\n+    pub bytes_written: u64,\n+}\n+\n+impl PageWriteSpec {\n+    /// Creates new spec with default page write metrics.\n+    pub fn new() -> Self {\n+        Self {\n+            page_type: PageType::DATA_PAGE,\n+            uncompressed_size: 0,\n+            compressed_size: 0,\n+            num_values: 0,\n+            offset: 0,\n+            bytes_written: 0,\n+        }\n+    }\n+}\n+\n+/// API for reading pages from a column chunk.\n+/// This offers a iterator like API to get the next page.\n+pub trait PageReader {\n+    /// Gets the next page in the column chunk associated with this reader.\n+    /// Returns `None` if there are no pages left.\n+    fn get_next_page(&mut self) -> Result<Option<Page>>;\n+}\n+\n+/// API for writing pages in a column chunk.\n+///\n+/// It is reasonable to assume that all pages will be written in the correct order, e.g.\n+/// dictionary page followed by data pages, or a set of data pages, etc.\n+pub trait PageWriter {\n+    /// Writes a page into the output stream/sink.\n+    /// Returns `PageWriteSpec` that contains information about written page metrics,\n+    /// including number of bytes, size, number of values, offset, etc.\n+    ///\n+    /// This method is called for every compressed page we write into underlying buffer,\n+    /// either data page or dictionary page.\n+    fn write_page(&mut self, page: CompressedPage) -> Result<PageWriteSpec>;\n+\n+    /// Writes column chunk metadata into the output stream/sink.\n+    ///\n+    /// This method is called once before page writer is closed, normally when writes are\n+    /// finalised in column writer.\n+    fn write_metadata(&mut self, metadata: &ColumnChunkMetaData) -> Result<()>;\n+\n+    /// Closes resources and flushes underlying sink.\n+    /// Page writer should not be used after this method is called.\n+    fn close(&mut self) -> Result<()>;\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_page() {\n+        let data_page = Page::DataPage {\n+            buf: ByteBufferPtr::new(vec![0, 1, 2]),\n+            num_values: 10,\n+            encoding: Encoding::PLAIN,\n+            def_level_encoding: Encoding::RLE,\n+            rep_level_encoding: Encoding::RLE,\n+            statistics: Some(Statistics::int32(Some(1), Some(2), None, 1, true)),\n+        };\n+        assert_eq!(data_page.page_type(), PageType::DATA_PAGE);\n+        assert_eq!(data_page.buffer().data(), vec![0, 1, 2].as_slice());\n+        assert_eq!(data_page.num_values(), 10);\n+        assert_eq!(data_page.encoding(), Encoding::PLAIN);\n+        assert_eq!(\n+            data_page.statistics(),\n+            Some(&Statistics::int32(Some(1), Some(2), None, 1, true))\n+        );\n+\n+        let data_page_v2 = Page::DataPageV2 {\n+            buf: ByteBufferPtr::new(vec![0, 1, 2]),\n+            num_values: 10,\n+            encoding: Encoding::PLAIN,\n+            num_nulls: 5,\n+            num_rows: 20,\n+            def_levels_byte_len: 30,\n+            rep_levels_byte_len: 40,\n+            is_compressed: false,\n+            statistics: Some(Statistics::int32(Some(1), Some(2), None, 1, true)),\n+        };\n+        assert_eq!(data_page_v2.page_type(), PageType::DATA_PAGE_V2);\n+        assert_eq!(data_page_v2.buffer().data(), vec![0, 1, 2].as_slice());\n+        assert_eq!(data_page_v2.num_values(), 10);\n+        assert_eq!(data_page_v2.encoding(), Encoding::PLAIN);\n+        assert_eq!(\n+            data_page_v2.statistics(),\n+            Some(&Statistics::int32(Some(1), Some(2), None, 1, true))\n+        );\n+\n+        let dict_page = Page::DictionaryPage {\n+            buf: ByteBufferPtr::new(vec![0, 1, 2]),\n+            num_values: 10,\n+            encoding: Encoding::PLAIN,\n+            is_sorted: false,\n+        };\n+        assert_eq!(dict_page.page_type(), PageType::DICTIONARY_PAGE);\n+        assert_eq!(dict_page.buffer().data(), vec![0, 1, 2].as_slice());\n+        assert_eq!(dict_page.num_values(), 10);\n+        assert_eq!(dict_page.encoding(), Encoding::PLAIN);\n+        assert_eq!(dict_page.statistics(), None);\n+    }\n+\n+    #[test]\n+    fn test_compressed_page() {\n+        let data_page = Page::DataPage {\n+            buf: ByteBufferPtr::new(vec![0, 1, 2]),\n+            num_values: 10,\n+            encoding: Encoding::PLAIN,\n+            def_level_encoding: Encoding::RLE,\n+            rep_level_encoding: Encoding::RLE,\n+            statistics: Some(Statistics::int32(Some(1), Some(2), None, 1, true)),\n+        };\n+\n+        let cpage = CompressedPage::new(data_page, 5);\n+\n+        assert_eq!(cpage.page_type(), PageType::DATA_PAGE);\n+        assert_eq!(cpage.uncompressed_size(), 5);\n+        assert_eq!(cpage.compressed_size(), 3);\n+        assert_eq!(cpage.num_values(), 10);\n+        assert_eq!(cpage.encoding(), Encoding::PLAIN);\n+        assert_eq!(cpage.data(), &[0, 1, 2]);\n+    }\n+}\ndiff --git a/rust/src/parquet/column/reader.rs b/rust/src/parquet/column/reader.rs\nnew file mode 100644\nindex 0000000000..f3dde31ab9\n--- /dev/null\n+++ b/rust/src/parquet/column/reader.rs\n@@ -0,0 +1,1576 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains column reader API.\n+\n+use std::{\n+    cmp::{max, min},\n+    collections::HashMap,\n+    mem,\n+};\n+\n+use super::page::{Page, PageReader};\n+use crate::parquet::basic::*;\n+use crate::parquet::data_type::*;\n+use crate::parquet::encodings::{\n+    decoding::{get_decoder, Decoder, DictDecoder, PlainDecoder},\n+    levels::LevelDecoder,\n+};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::schema::types::ColumnDescPtr;\n+use crate::parquet::util::memory::ByteBufferPtr;\n+\n+/// Column reader for a Parquet type.\n+pub enum ColumnReader {\n+    BoolColumnReader(ColumnReaderImpl<BoolType>),\n+    Int32ColumnReader(ColumnReaderImpl<Int32Type>),\n+    Int64ColumnReader(ColumnReaderImpl<Int64Type>),\n+    Int96ColumnReader(ColumnReaderImpl<Int96Type>),\n+    FloatColumnReader(ColumnReaderImpl<FloatType>),\n+    DoubleColumnReader(ColumnReaderImpl<DoubleType>),\n+    ByteArrayColumnReader(ColumnReaderImpl<ByteArrayType>),\n+    FixedLenByteArrayColumnReader(ColumnReaderImpl<FixedLenByteArrayType>),\n+}\n+\n+/// Gets a specific column reader corresponding to column descriptor `col_descr`. The\n+/// column reader will read from pages in `col_page_reader`.\n+pub fn get_column_reader(\n+    col_descr: ColumnDescPtr,\n+    col_page_reader: Box<PageReader>,\n+) -> ColumnReader {\n+    match col_descr.physical_type() {\n+        Type::BOOLEAN => {\n+            ColumnReader::BoolColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::INT32 => {\n+            ColumnReader::Int32ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::INT64 => {\n+            ColumnReader::Int64ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::INT96 => {\n+            ColumnReader::Int96ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::FLOAT => {\n+            ColumnReader::FloatColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::DOUBLE => {\n+            ColumnReader::DoubleColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::BYTE_ARRAY => {\n+            ColumnReader::ByteArrayColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+        }\n+        Type::FIXED_LEN_BYTE_ARRAY => ColumnReader::FixedLenByteArrayColumnReader(\n+            ColumnReaderImpl::new(col_descr, col_page_reader),\n+        ),\n+    }\n+}\n+\n+/// Gets a typed column reader for the specific type `T`, by \"up-casting\" `col_reader` of\n+/// non-generic type to a generic column reader type `ColumnReaderImpl`.\n+///\n+/// NOTE: the caller MUST guarantee that the actual enum value for `col_reader` matches\n+/// the type `T`. Otherwise, disastrous consequence could happen.\n+pub fn get_typed_column_reader<T: DataType>(col_reader: ColumnReader) -> ColumnReaderImpl<T> {\n+    match col_reader {\n+        ColumnReader::BoolColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::Int32ColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::Int64ColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::Int96ColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::FloatColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::DoubleColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::ByteArrayColumnReader(r) => unsafe { mem::transmute(r) },\n+        ColumnReader::FixedLenByteArrayColumnReader(r) => unsafe { mem::transmute(r) },\n+    }\n+}\n+\n+/// Typed value reader for a particular primitive column.\n+pub struct ColumnReaderImpl<T: DataType> {\n+    descr: ColumnDescPtr,\n+    def_level_decoder: Option<LevelDecoder>,\n+    rep_level_decoder: Option<LevelDecoder>,\n+    page_reader: Box<PageReader>,\n+    current_encoding: Option<Encoding>,\n+\n+    // The total number of values stored in the data page.\n+    num_buffered_values: u32,\n+\n+    // The number of values from the current data page that has been decoded into memory\n+    // so far.\n+    num_decoded_values: u32,\n+\n+    // Cache of decoders for existing encodings\n+    decoders: HashMap<Encoding, Box<Decoder<T>>>,\n+}\n+\n+impl<T: DataType> ColumnReaderImpl<T> {\n+    /// Creates new column reader based on column descriptor and page reader.\n+    pub fn new(descr: ColumnDescPtr, page_reader: Box<PageReader>) -> Self {\n+        Self {\n+            descr,\n+            def_level_decoder: None,\n+            rep_level_decoder: None,\n+            page_reader,\n+            current_encoding: None,\n+            num_buffered_values: 0,\n+            num_decoded_values: 0,\n+            decoders: HashMap::new(),\n+        }\n+    }\n+\n+    /// Reads a batch of values of at most `batch_size`.\n+    ///\n+    /// This will try to read from the row group, and fills up at most `batch_size` values\n+    /// for `def_levels`, `rep_levels` and `values`. It will stop either when the row group\n+    /// is depleted or `batch_size` values has been read, or there is no space in the input\n+    /// slices (values/definition levels/repetition levels).\n+    ///\n+    /// Note that in case the field being read is not required, `values` could contain less\n+    /// values than `def_levels`. Also note that this will skip reading def / rep levels if\n+    /// the field is required / not repeated, respectively.\n+    ///\n+    /// If `def_levels` or `rep_levels` is `None`, this will also skip reading the\n+    /// respective levels. This is useful when the caller of this function knows in advance\n+    /// that the field is required and non-repeated, therefore can avoid allocating memory\n+    /// for the levels data. Note that if field has definition levels, but caller provides\n+    /// None, there might be inconsistency between levels/values (see comments below).\n+    ///\n+    /// Returns a tuple where the first element is the actual number of values read,\n+    /// and the second element is the actual number of levels read.\n+    #[inline]\n+    pub fn read_batch(\n+        &mut self,\n+        batch_size: usize,\n+        mut def_levels: Option<&mut [i16]>,\n+        mut rep_levels: Option<&mut [i16]>,\n+        values: &mut [T::T],\n+    ) -> Result<(usize, usize)> {\n+        let mut values_read = 0;\n+        let mut levels_read = 0;\n+\n+        // Compute the smallest batch size we can read based on provided slices\n+        let mut batch_size = min(batch_size, values.len());\n+        if let Some(ref levels) = def_levels {\n+            batch_size = min(batch_size, levels.len());\n+        }\n+        if let Some(ref levels) = rep_levels {\n+            batch_size = min(batch_size, levels.len());\n+        }\n+\n+        // Read exhaustively all pages until we read all batch_size values/levels\n+        // or there are no more values/levels to read.\n+        while max(values_read, levels_read) < batch_size {\n+            if !self.has_next()? {\n+                break;\n+            }\n+\n+            // Batch size for the current iteration\n+            let iter_batch_size = {\n+                // Compute approximate value based on values decoded so far\n+                let mut adjusted_size = min(\n+                    batch_size,\n+                    (self.num_buffered_values - self.num_decoded_values) as usize,\n+                );\n+\n+                // Adjust batch size by taking into account how much space is left in values\n+                // slice or levels slices (if available)\n+                adjusted_size = min(adjusted_size, values.len() - values_read);\n+                if let Some(ref levels) = def_levels {\n+                    adjusted_size = min(adjusted_size, levels.len() - levels_read);\n+                }\n+                if let Some(ref levels) = rep_levels {\n+                    adjusted_size = min(adjusted_size, levels.len() - levels_read);\n+                }\n+\n+                adjusted_size\n+            };\n+\n+            let mut values_to_read = 0;\n+            let mut num_def_levels = 0;\n+            let mut num_rep_levels = 0;\n+\n+            // If the field is required and non-repeated, there are no definition levels\n+            if self.descr.max_def_level() > 0 && def_levels.as_ref().is_some() {\n+                if let Some(ref mut levels) = def_levels {\n+                    num_def_levels = self\n+                        .read_def_levels(&mut levels[levels_read..levels_read + iter_batch_size])?;\n+                    for i in levels_read..levels_read + num_def_levels {\n+                        if levels[i] == self.descr.max_def_level() {\n+                            values_to_read += 1;\n+                        }\n+                    }\n+                }\n+            } else {\n+                // If max definition level == 0, then it is REQUIRED field, read all values.\n+                // If definition levels are not provided, we still read all values.\n+                values_to_read = iter_batch_size;\n+            }\n+\n+            if self.descr.max_rep_level() > 0 && rep_levels.is_some() {\n+                if let Some(ref mut levels) = rep_levels {\n+                    num_rep_levels = self\n+                        .read_rep_levels(&mut levels[levels_read..levels_read + iter_batch_size])?;\n+\n+                    // If definition levels are defined, check that rep levels == def levels\n+                    if def_levels.is_some() {\n+                        assert_eq!(\n+                            num_def_levels, num_rep_levels,\n+                            \"Number of decoded rep / def levels did not match\"\n+                        );\n+                    }\n+                }\n+            }\n+\n+            // At this point we have read values, definition and repetition levels.\n+            // If both definition and repetition levels are defined, their counts\n+            // should be equal. Values count is always less or equal to definition levels.\n+            //\n+            // Note that if field is not required, but no definition levels are provided,\n+            // we would read values of batch size and (if provided, of course) repetition\n+            // levels of batch size - [!] they will not be synced, because only definition\n+            // levels enforce number of non-null values to read.\n+\n+            let curr_values_read =\n+                self.read_values(&mut values[values_read..values_read + values_to_read])?;\n+\n+            // Update all \"return\" counters and internal state.\n+\n+            // This is to account for when def or rep levels are not provided\n+            let curr_levels_read = max(num_def_levels, num_rep_levels);\n+            self.num_decoded_values += max(curr_levels_read, curr_values_read) as u32;\n+            levels_read += curr_levels_read;\n+            values_read += curr_values_read;\n+        }\n+\n+        Ok((values_read, levels_read))\n+    }\n+\n+    /// Reads a new page and set up the decoders for levels, values or dictionary.\n+    /// Returns false if there's no page left.\n+    fn read_new_page(&mut self) -> Result<bool> {\n+        #[allow(while_true)]\n+        while true {\n+            match self.page_reader.get_next_page()? {\n+                // No more page to read\n+                None => return Ok(false),\n+                Some(current_page) => {\n+                    match current_page {\n+                        // 1. Dictionary page: configure dictionary for this page.\n+                        p @ Page::DictionaryPage { .. } => {\n+                            self.configure_dictionary(p)?;\n+                            continue;\n+                        }\n+                        // 2. Data page v1\n+                        Page::DataPage {\n+                            buf,\n+                            num_values,\n+                            encoding,\n+                            def_level_encoding,\n+                            rep_level_encoding,\n+                            statistics: _,\n+                        } => {\n+                            self.num_buffered_values = num_values;\n+                            self.num_decoded_values = 0;\n+\n+                            let mut buffer_ptr = buf;\n+\n+                            if self.descr.max_rep_level() > 0 {\n+                                let mut rep_decoder = LevelDecoder::v1(\n+                                    rep_level_encoding,\n+                                    self.descr.max_rep_level(),\n+                                );\n+                                let total_bytes = rep_decoder\n+                                    .set_data(self.num_buffered_values as usize, buffer_ptr.all());\n+                                buffer_ptr = buffer_ptr.start_from(total_bytes);\n+                                self.rep_level_decoder = Some(rep_decoder);\n+                            }\n+\n+                            if self.descr.max_def_level() > 0 {\n+                                let mut def_decoder = LevelDecoder::v1(\n+                                    def_level_encoding,\n+                                    self.descr.max_def_level(),\n+                                );\n+                                let total_bytes = def_decoder\n+                                    .set_data(self.num_buffered_values as usize, buffer_ptr.all());\n+                                buffer_ptr = buffer_ptr.start_from(total_bytes);\n+                                self.def_level_decoder = Some(def_decoder);\n+                            }\n+\n+                            // Data page v1 does not have offset, all content of buffer should be passed\n+                            self.set_current_page_encoding(\n+                                encoding,\n+                                &buffer_ptr,\n+                                0,\n+                                num_values as usize,\n+                            )?;\n+                            return Ok(true);\n+                        }\n+                        // 3. Data page v2\n+                        Page::DataPageV2 {\n+                            buf,\n+                            num_values,\n+                            encoding,\n+                            num_nulls: _,\n+                            num_rows: _,\n+                            def_levels_byte_len,\n+                            rep_levels_byte_len,\n+                            is_compressed: _,\n+                            statistics: _,\n+                        } => {\n+                            self.num_buffered_values = num_values;\n+                            self.num_decoded_values = 0;\n+\n+                            let mut offset = 0;\n+\n+                            // DataPage v2 only supports RLE encoding for repetition levels\n+                            if self.descr.max_rep_level() > 0 {\n+                                let mut rep_decoder = LevelDecoder::v2(self.descr.max_rep_level());\n+                                let bytes_read = rep_decoder.set_data_range(\n+                                    self.num_buffered_values as usize,\n+                                    &buf,\n+                                    offset,\n+                                    rep_levels_byte_len as usize,\n+                                );\n+                                offset += bytes_read;\n+                                self.rep_level_decoder = Some(rep_decoder);\n+                            }\n+\n+                            // DataPage v2 only supports RLE encoding for definition levels\n+                            if self.descr.max_def_level() > 0 {\n+                                let mut def_decoder = LevelDecoder::v2(self.descr.max_def_level());\n+                                let bytes_read = def_decoder.set_data_range(\n+                                    self.num_buffered_values as usize,\n+                                    &buf,\n+                                    offset,\n+                                    def_levels_byte_len as usize,\n+                                );\n+                                offset += bytes_read;\n+                                self.def_level_decoder = Some(def_decoder);\n+                            }\n+\n+                            self.set_current_page_encoding(\n+                                encoding,\n+                                &buf,\n+                                offset,\n+                                num_values as usize,\n+                            )?;\n+                            return Ok(true);\n+                        }\n+                    };\n+                }\n+            }\n+        }\n+\n+        Ok(true)\n+    }\n+\n+    /// Resolves and updates encoding and set decoder for the current page\n+    fn set_current_page_encoding(\n+        &mut self,\n+        mut encoding: Encoding,\n+        buffer_ptr: &ByteBufferPtr,\n+        offset: usize,\n+        len: usize,\n+    ) -> Result<()> {\n+        if encoding == Encoding::PLAIN_DICTIONARY {\n+            encoding = Encoding::RLE_DICTIONARY;\n+        }\n+\n+        let decoder = if encoding == Encoding::RLE_DICTIONARY {\n+            self.decoders\n+                .get_mut(&encoding)\n+                .expect(\"Decoder for dict should have been set\")\n+        } else {\n+            // Search cache for data page decoder\n+            if !self.decoders.contains_key(&encoding) {\n+                // Initialize decoder for this page\n+                let data_decoder = get_decoder::<T>(self.descr.clone(), encoding)?;\n+                self.decoders.insert(encoding, data_decoder);\n+            }\n+            self.decoders.get_mut(&encoding).unwrap()\n+        };\n+\n+        decoder.set_data(buffer_ptr.start_from(offset), len as usize)?;\n+        self.current_encoding = Some(encoding);\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn has_next(&mut self) -> Result<bool> {\n+        if self.num_buffered_values == 0 || self.num_buffered_values == self.num_decoded_values {\n+            // TODO: should we return false if read_new_page() = true and\n+            // num_buffered_values = 0?\n+            if !self.read_new_page()? {\n+                Ok(false)\n+            } else {\n+                Ok(self.num_buffered_values != 0)\n+            }\n+        } else {\n+            Ok(true)\n+        }\n+    }\n+\n+    #[inline]\n+    fn read_rep_levels(&mut self, buffer: &mut [i16]) -> Result<usize> {\n+        let level_decoder = self\n+            .rep_level_decoder\n+            .as_mut()\n+            .expect(\"rep_level_decoder be set\");\n+        level_decoder.get(buffer)\n+    }\n+\n+    #[inline]\n+    fn read_def_levels(&mut self, buffer: &mut [i16]) -> Result<usize> {\n+        let level_decoder = self\n+            .def_level_decoder\n+            .as_mut()\n+            .expect(\"def_level_decoder be set\");\n+        level_decoder.get(buffer)\n+    }\n+\n+    #[inline]\n+    fn read_values(&mut self, buffer: &mut [T::T]) -> Result<usize> {\n+        let encoding = self\n+            .current_encoding\n+            .expect(\"current_encoding should be set\");\n+        let current_decoder = self\n+            .decoders\n+            .get_mut(&encoding)\n+            .expect(format!(\"decoder for encoding {} should be set\", encoding).as_str());\n+        current_decoder.get(buffer)\n+    }\n+\n+    #[inline]\n+    fn configure_dictionary(&mut self, page: Page) -> Result<bool> {\n+        let mut encoding = page.encoding();\n+        if encoding == Encoding::PLAIN || encoding == Encoding::PLAIN_DICTIONARY {\n+            encoding = Encoding::RLE_DICTIONARY\n+        }\n+\n+        if self.decoders.contains_key(&encoding) {\n+            return Err(general_err!(\"Column cannot have more than one dictionary\"));\n+        }\n+\n+        if encoding == Encoding::RLE_DICTIONARY {\n+            let mut dictionary = PlainDecoder::<T>::new(self.descr.type_length());\n+            let num_values = page.num_values();\n+            dictionary.set_data(page.buffer().clone(), num_values as usize)?;\n+\n+            let mut decoder = DictDecoder::new();\n+            decoder.set_dict(Box::new(dictionary))?;\n+            self.decoders.insert(encoding, Box::new(decoder));\n+            Ok(true)\n+        } else {\n+            Err(nyi_err!(\n+                \"Invalid/Unsupported encoding type for dictionary: {}\",\n+                encoding\n+            ))\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use rand::distributions::range::SampleRange;\n+    use std::{collections::VecDeque, rc::Rc, vec::IntoIter};\n+\n+    use crate::parquet::basic::Type as PhysicalType;\n+    use crate::parquet::column::page::Page;\n+    use crate::parquet::encodings::{\n+        encoding::{get_encoder, DictEncoder, Encoder},\n+        levels::{max_buffer_size, LevelEncoder},\n+    };\n+    use crate::parquet::schema::types::{ColumnDescriptor, ColumnPath, Type as SchemaType};\n+    use crate::parquet::util::{\n+        memory::{ByteBufferPtr, MemTracker, MemTrackerPtr},\n+        test_common::random_numbers_range,\n+    };\n+\n+    const NUM_LEVELS: usize = 128;\n+    const NUM_PAGES: usize = 2;\n+    const MAX_DEF_LEVEL: i16 = 5;\n+    const MAX_REP_LEVEL: i16 = 5;\n+\n+    // Macro to generate test cases\n+    macro_rules! test {\n+        // branch for generating i32 cases\n+        ($test_func:ident, i32, $func:ident, $def_level:expr, $rep_level:expr,\n+     $num_pages:expr, $num_levels:expr, $batch_size:expr, $min:expr, $max:expr) => {\n+            test_internal!(\n+                $test_func,\n+                Int32Type,\n+                get_test_int32_type,\n+                $func,\n+                $def_level,\n+                $rep_level,\n+                $num_pages,\n+                $num_levels,\n+                $batch_size,\n+                $min,\n+                $max\n+            );\n+        };\n+        // branch for generating i64 cases\n+        ($test_func:ident, i64, $func:ident, $def_level:expr, $rep_level:expr,\n+     $num_pages:expr, $num_levels:expr, $batch_size:expr, $min:expr, $max:expr) => {\n+            test_internal!(\n+                $test_func,\n+                Int64Type,\n+                get_test_int64_type,\n+                $func,\n+                $def_level,\n+                $rep_level,\n+                $num_pages,\n+                $num_levels,\n+                $batch_size,\n+                $min,\n+                $max\n+            );\n+        };\n+    }\n+\n+    macro_rules! test_internal {\n+        ($test_func:ident, $ty:ident, $pty:ident, $func:ident, $def_level:expr,\n+     $rep_level:expr, $num_pages:expr, $num_levels:expr, $batch_size:expr,\n+     $min:expr, $max:expr) => {\n+            #[test]\n+            fn $test_func() {\n+                let desc = Rc::new(ColumnDescriptor::new(\n+                    Rc::new($pty()),\n+                    None,\n+                    $def_level,\n+                    $rep_level,\n+                    ColumnPath::new(Vec::new()),\n+                ));\n+                let mut tester = ColumnReaderTester::<$ty>::new();\n+                tester.$func(desc, $num_pages, $num_levels, $batch_size, $min, $max);\n+            }\n+        };\n+    }\n+\n+    test!(\n+        test_read_plain_v1_int32,\n+        i32,\n+        plain_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int32,\n+        i32,\n+        plain_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+\n+    test!(\n+        test_read_plain_v1_int32_uneven,\n+        i32,\n+        plain_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int32_uneven,\n+        i32,\n+        plain_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+\n+    test!(\n+        test_read_plain_v1_int32_multi_page,\n+        i32,\n+        plain_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int32_multi_page,\n+        i32,\n+        plain_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+\n+    // test cases when column descriptor has MAX_DEF_LEVEL = 0 and MAX_REP_LEVEL = 0\n+    test!(\n+        test_read_plain_v1_int32_required_non_repeated,\n+        i32,\n+        plain_v1,\n+        0,\n+        0,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int32_required_non_repeated,\n+        i32,\n+        plain_v2,\n+        0,\n+        0,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i32::MIN,\n+        ::std::i32::MAX\n+    );\n+\n+    test!(\n+        test_read_plain_v1_int64,\n+        i64,\n+        plain_v1,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int64,\n+        i64,\n+        plain_v2,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+\n+    test!(\n+        test_read_plain_v1_int64_uneven,\n+        i64,\n+        plain_v1,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int64_uneven,\n+        i64,\n+        plain_v2,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+\n+    test!(\n+        test_read_plain_v1_int64_multi_page,\n+        i64,\n+        plain_v1,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int64_multi_page,\n+        i64,\n+        plain_v2,\n+        1,\n+        1,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+\n+    // test cases when column descriptor has MAX_DEF_LEVEL = 0 and MAX_REP_LEVEL = 0\n+    test!(\n+        test_read_plain_v1_int64_required_non_repeated,\n+        i64,\n+        plain_v1,\n+        0,\n+        0,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+    test!(\n+        test_read_plain_v2_int64_required_non_repeated,\n+        i64,\n+        plain_v2,\n+        0,\n+        0,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        ::std::i64::MIN,\n+        ::std::i64::MAX\n+    );\n+\n+    test!(\n+        test_read_dict_v1_int32_small,\n+        i32,\n+        dict_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        2,\n+        2,\n+        16,\n+        0,\n+        3\n+    );\n+    test!(\n+        test_read_dict_v2_int32_small,\n+        i32,\n+        dict_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        2,\n+        2,\n+        16,\n+        0,\n+        3\n+    );\n+\n+    test!(\n+        test_read_dict_v1_int32,\n+        i32,\n+        dict_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        0,\n+        3\n+    );\n+    test!(\n+        test_read_dict_v2_int32,\n+        i32,\n+        dict_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        0,\n+        3\n+    );\n+\n+    test!(\n+        test_read_dict_v1_int32_uneven,\n+        i32,\n+        dict_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        0,\n+        3\n+    );\n+    test!(\n+        test_read_dict_v2_int32_uneven,\n+        i32,\n+        dict_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        17,\n+        0,\n+        3\n+    );\n+\n+    test!(\n+        test_read_dict_v1_int32_multi_page,\n+        i32,\n+        dict_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        0,\n+        3\n+    );\n+    test!(\n+        test_read_dict_v2_int32_multi_page,\n+        i32,\n+        dict_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        512,\n+        0,\n+        3\n+    );\n+\n+    test!(\n+        test_read_dict_v1_int64,\n+        i64,\n+        dict_v1,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        0,\n+        3\n+    );\n+    test!(\n+        test_read_dict_v2_int64,\n+        i64,\n+        dict_v2,\n+        MAX_DEF_LEVEL,\n+        MAX_REP_LEVEL,\n+        NUM_PAGES,\n+        NUM_LEVELS,\n+        16,\n+        0,\n+        3\n+    );\n+\n+    #[test]\n+    fn test_read_batch_values_only() {\n+        test_read_batch_int32(16, &mut vec![0; 10], None, None); // < batch_size\n+        test_read_batch_int32(16, &mut vec![0; 16], None, None); // == batch_size\n+        test_read_batch_int32(16, &mut vec![0; 51], None, None); // > batch_size\n+    }\n+\n+    #[test]\n+    fn test_read_batch_values_def_levels() {\n+        test_read_batch_int32(16, &mut vec![0; 10], Some(&mut vec![0; 10]), None);\n+        test_read_batch_int32(16, &mut vec![0; 16], Some(&mut vec![0; 16]), None);\n+        test_read_batch_int32(16, &mut vec![0; 51], Some(&mut vec![0; 51]), None);\n+    }\n+\n+    #[test]\n+    fn test_read_batch_values_rep_levels() {\n+        test_read_batch_int32(16, &mut vec![0; 10], None, Some(&mut vec![0; 10]));\n+        test_read_batch_int32(16, &mut vec![0; 16], None, Some(&mut vec![0; 16]));\n+        test_read_batch_int32(16, &mut vec![0; 51], None, Some(&mut vec![0; 51]));\n+    }\n+\n+    #[test]\n+    fn test_read_batch_different_buf_sizes() {\n+        test_read_batch_int32(\n+            16,\n+            &mut vec![0; 8],\n+            Some(&mut vec![0; 9]),\n+            Some(&mut vec![0; 7]),\n+        );\n+        test_read_batch_int32(\n+            16,\n+            &mut vec![0; 1],\n+            Some(&mut vec![0; 9]),\n+            Some(&mut vec![0; 3]),\n+        );\n+    }\n+\n+    #[test]\n+    fn test_read_batch_values_def_rep_levels() {\n+        test_read_batch_int32(\n+            128,\n+            &mut vec![0; 128],\n+            Some(&mut vec![0; 128]),\n+            Some(&mut vec![0; 128]),\n+        );\n+    }\n+\n+    #[test]\n+    fn test_read_batch_adjust_after_buffering_page() {\n+        // This test covers scenario when buffering new page results in setting number\n+        // of decoded values to 0, resulting on reading `batch_size` of values, but it is\n+        // larger than we can insert into slice (affects values and levels).\n+        //\n+        // Note: values are chosen to reproduce the issue.\n+        //\n+        let primitive_type = get_test_int32_type();\n+        let desc = Rc::new(ColumnDescriptor::new(\n+            Rc::new(primitive_type),\n+            None,\n+            1,\n+            1,\n+            ColumnPath::new(Vec::new()),\n+        ));\n+\n+        let num_pages = 2;\n+        let num_levels = 4;\n+        let batch_size = 5;\n+        let values = &mut vec![0; 7];\n+        let def_levels = &mut vec![0; 7];\n+        let rep_levels = &mut vec![0; 7];\n+\n+        let mut tester = ColumnReaderTester::<Int32Type>::new();\n+        tester.test_read_batch(\n+            desc,\n+            Encoding::RLE_DICTIONARY,\n+            num_pages,\n+            num_levels,\n+            batch_size,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            values,\n+            Some(def_levels),\n+            Some(rep_levels),\n+            false,\n+        );\n+    }\n+\n+    // ----------------------------------------------------------------------\n+    // Helper methods to make pages and test\n+    //\n+    // # Overview\n+    //\n+    // Most of the test functionality is implemented in `ColumnReaderTester`, which\n+    // provides some general data page test methods:\n+    // - `test_read_batch_general`\n+    // - `test_read_batch`\n+    //\n+    // There are also some high level wrappers that are part of `ColumnReaderTester`:\n+    // - `plain_v1` -> call `test_read_batch_general` with data page v1 and plain encoding\n+    // - `plain_v2` -> call `test_read_batch_general` with data page v2 and plain encoding\n+    // - `dict_v1` -> call `test_read_batch_general` with data page v1 + dictionary page\n+    // - `dict_v2` -> call `test_read_batch_general` with data page v2 + dictionary page\n+    //\n+    // And even higher level wrappers that simplify testing of almost the same test cases:\n+    // - `get_test_int32_type`, provides dummy schema type\n+    // - `get_test_int64_type`, provides dummy schema type\n+    // - `test_read_batch_int32`, wrapper for `read_batch` tests, since they are basically\n+    //   the same, just different def/rep levels and batch size.\n+    //\n+    // # Page assembly\n+    //\n+    // Page construction and generation of values, definition and repetition levels happens\n+    // in `make_pages` function.\n+    // All values are randomly generated based on provided min/max, levels are calculated\n+    // based on provided max level for column descriptor (which is basically either int32\n+    // or int64 type in tests) and `levels_per_page` variable.\n+    //\n+    // We use `DataPageBuilder` and its implementation `DataPageBuilderImpl` to actually\n+    // turn values, definition and repetition levels into data pages (either v1 or v2).\n+    //\n+    // Those data pages are then stored as part of `TestPageReader` (we just pass vector\n+    // of generated pages directly), which implements `PageReader` interface.\n+    //\n+    // # Comparison\n+    //\n+    // This allows us to pass test page reader into column reader, so we can test\n+    // functionality of column reader - see `test_read_batch`, where we create column\n+    // reader -> typed column reader, buffer values in `read_batch` method and compare\n+    // output with generated data.\n+\n+    // Returns dummy Parquet `Type` for primitive field, because most of our tests use\n+    // INT32 physical type.\n+    fn get_test_int32_type() -> SchemaType {\n+        SchemaType::primitive_type_builder(\"a\", PhysicalType::INT32)\n+            .with_repetition(Repetition::REQUIRED)\n+            .with_logical_type(LogicalType::INT_32)\n+            .with_length(-1)\n+            .build()\n+            .expect(\"build() should be OK\")\n+    }\n+\n+    // Returns dummy Parquet `Type` for INT64 physical type.\n+    fn get_test_int64_type() -> SchemaType {\n+        SchemaType::primitive_type_builder(\"a\", PhysicalType::INT64)\n+            .with_repetition(Repetition::REQUIRED)\n+            .with_logical_type(LogicalType::INT_64)\n+            .with_length(-1)\n+            .build()\n+            .expect(\"build() should be OK\")\n+    }\n+\n+    // Tests `read_batch()` functionality for INT32.\n+    //\n+    // This is a high level wrapper on `ColumnReaderTester` that allows us to specify some\n+    // boilerplate code for setting up definition/repetition levels and column descriptor.\n+    fn test_read_batch_int32(\n+        batch_size: usize,\n+        values: &mut [i32],\n+        def_levels: Option<&mut [i16]>,\n+        rep_levels: Option<&mut [i16]>,\n+    ) {\n+        let primitive_type = get_test_int32_type();\n+        // make field is required based on provided slices of levels\n+        let max_def_level = if def_levels.is_some() {\n+            MAX_DEF_LEVEL\n+        } else {\n+            0\n+        };\n+        let max_rep_level = if def_levels.is_some() {\n+            MAX_REP_LEVEL\n+        } else {\n+            0\n+        };\n+\n+        let desc = Rc::new(ColumnDescriptor::new(\n+            Rc::new(primitive_type),\n+            None,\n+            max_def_level,\n+            max_rep_level,\n+            ColumnPath::new(Vec::new()),\n+        ));\n+        let mut tester = ColumnReaderTester::<Int32Type>::new();\n+        tester.test_read_batch(\n+            desc,\n+            Encoding::RLE_DICTIONARY,\n+            NUM_PAGES,\n+            NUM_LEVELS,\n+            batch_size,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            values,\n+            def_levels,\n+            rep_levels,\n+            false,\n+        );\n+    }\n+\n+    struct ColumnReaderTester<T: DataType>\n+    where\n+        T::T: PartialOrd + SampleRange + Copy,\n+    {\n+        rep_levels: Vec<i16>,\n+        def_levels: Vec<i16>,\n+        values: Vec<T::T>,\n+    }\n+\n+    impl<T: DataType> ColumnReaderTester<T>\n+    where\n+        T::T: PartialOrd + SampleRange + Copy,\n+    {\n+        pub fn new() -> Self {\n+            Self {\n+                rep_levels: Vec::new(),\n+                def_levels: Vec::new(),\n+                values: Vec::new(),\n+            }\n+        }\n+\n+        // Method to generate and test data pages v1\n+        fn plain_v1(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+        ) {\n+            self.test_read_batch_general(\n+                desc,\n+                Encoding::PLAIN,\n+                num_pages,\n+                num_levels,\n+                batch_size,\n+                min,\n+                max,\n+                false,\n+            );\n+        }\n+\n+        // Method to generate and test data pages v2\n+        fn plain_v2(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+        ) {\n+            self.test_read_batch_general(\n+                desc,\n+                Encoding::PLAIN,\n+                num_pages,\n+                num_levels,\n+                batch_size,\n+                min,\n+                max,\n+                true,\n+            );\n+        }\n+\n+        // Method to generate and test dictionary page + data pages v1\n+        fn dict_v1(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+        ) {\n+            self.test_read_batch_general(\n+                desc,\n+                Encoding::RLE_DICTIONARY,\n+                num_pages,\n+                num_levels,\n+                batch_size,\n+                min,\n+                max,\n+                false,\n+            );\n+        }\n+\n+        // Method to generate and test dictionary page + data pages v2\n+        fn dict_v2(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+        ) {\n+            self.test_read_batch_general(\n+                desc,\n+                Encoding::RLE_DICTIONARY,\n+                num_pages,\n+                num_levels,\n+                batch_size,\n+                min,\n+                max,\n+                true,\n+            );\n+        }\n+\n+        // Helper function for the general case of `read_batch()` where `values`,\n+        // `def_levels` and `rep_levels` are always provided with enough space.\n+        fn test_read_batch_general(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            encoding: Encoding,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+            use_v2: bool,\n+        ) {\n+            let mut def_levels = vec![0; num_levels * num_pages];\n+            let mut rep_levels = vec![0; num_levels * num_pages];\n+            let mut values = vec![T::T::default(); num_levels * num_pages];\n+            self.test_read_batch(\n+                desc,\n+                encoding,\n+                num_pages,\n+                num_levels,\n+                batch_size,\n+                min,\n+                max,\n+                &mut values,\n+                Some(&mut def_levels),\n+                Some(&mut rep_levels),\n+                use_v2,\n+            );\n+        }\n+\n+        // Helper function to test `read_batch()` method with custom buffers for values,\n+        // definition and repetition levels.\n+        fn test_read_batch(\n+            &mut self,\n+            desc: ColumnDescPtr,\n+            encoding: Encoding,\n+            num_pages: usize,\n+            num_levels: usize,\n+            batch_size: usize,\n+            min: T::T,\n+            max: T::T,\n+            values: &mut [T::T],\n+            mut def_levels: Option<&mut [i16]>,\n+            mut rep_levels: Option<&mut [i16]>,\n+            use_v2: bool,\n+        ) {\n+            let mut pages = VecDeque::new();\n+            make_pages::<T>(\n+                desc.clone(),\n+                encoding,\n+                num_pages,\n+                num_levels,\n+                min,\n+                max,\n+                &mut self.def_levels,\n+                &mut self.rep_levels,\n+                &mut self.values,\n+                &mut pages,\n+                use_v2,\n+            );\n+            let max_def_level = desc.max_def_level();\n+            let page_reader = TestPageReader::new(Vec::from(pages));\n+            let column_reader: ColumnReader = get_column_reader(desc, Box::new(page_reader));\n+            let mut typed_column_reader = get_typed_column_reader::<T>(column_reader);\n+\n+            let mut curr_values_read = 0;\n+            let mut curr_levels_read = 0;\n+            let mut done = false;\n+            while !done {\n+                let actual_def_levels = match &mut def_levels {\n+                    Some(ref mut vec) => Some(&mut vec[curr_levels_read..]),\n+                    None => None,\n+                };\n+                let actual_rep_levels = match rep_levels {\n+                    Some(ref mut vec) => Some(&mut vec[curr_levels_read..]),\n+                    None => None,\n+                };\n+\n+                let (values_read, levels_read) = typed_column_reader\n+                    .read_batch(\n+                        batch_size,\n+                        actual_def_levels,\n+                        actual_rep_levels,\n+                        &mut values[curr_values_read..],\n+                    )\n+                    .expect(\"read_batch() should be OK\");\n+\n+                if values_read == 0 && levels_read == 0 {\n+                    done = true;\n+                }\n+\n+                curr_values_read += values_read;\n+                curr_levels_read += levels_read;\n+            }\n+\n+            assert!(\n+                values.len() >= curr_values_read,\n+                \"values.len() >= values_read\"\n+            );\n+            assert_eq!(\n+                &values[0..curr_values_read],\n+                &self.values[0..curr_values_read],\n+                \"values content doesn't match\"\n+            );\n+\n+            if let Some(ref levels) = def_levels {\n+                assert!(\n+                    levels.len() >= curr_levels_read,\n+                    \"def_levels.len() >= levels_read\"\n+                );\n+                assert_eq!(\n+                    &levels[0..curr_levels_read],\n+                    &self.def_levels[0..curr_levels_read],\n+                    \"definition levels content doesn't match\"\n+                );\n+            }\n+\n+            if let Some(ref levels) = rep_levels {\n+                assert!(\n+                    levels.len() >= curr_levels_read,\n+                    \"rep_levels.len() >= levels_read\"\n+                );\n+                assert_eq!(\n+                    &levels[0..curr_levels_read],\n+                    &self.rep_levels[0..curr_levels_read],\n+                    \"repetition levels content doesn't match\"\n+                );\n+            }\n+\n+            if def_levels.is_none() && rep_levels.is_none() {\n+                assert!(\n+                    curr_levels_read == 0,\n+                    \"expected to read 0 levels, found {}\",\n+                    curr_levels_read\n+                );\n+            } else if def_levels.is_some() && max_def_level > 0 {\n+                assert!(\n+                    curr_levels_read >= curr_values_read,\n+                    \"expected levels read to be greater than values read\"\n+                );\n+            }\n+        }\n+    }\n+\n+    struct TestPageReader {\n+        pages: IntoIter<Page>,\n+    }\n+\n+    impl TestPageReader {\n+        pub fn new(pages: Vec<Page>) -> Self {\n+            Self {\n+                pages: pages.into_iter(),\n+            }\n+        }\n+    }\n+\n+    impl PageReader for TestPageReader {\n+        fn get_next_page(&mut self) -> Result<Option<Page>> {\n+            Ok(self.pages.next())\n+        }\n+    }\n+\n+    // ----------------------------------------------------------------------\n+    // Utility functions for generating testing pages\n+\n+    trait DataPageBuilder {\n+        fn add_rep_levels(&mut self, max_level: i16, rep_levels: &[i16]);\n+        fn add_def_levels(&mut self, max_level: i16, def_levels: &[i16]);\n+        fn add_values<T: DataType>(&mut self, encoding: Encoding, values: &[T::T]);\n+        fn add_indices(&mut self, indices: ByteBufferPtr);\n+        fn consume(self) -> Page;\n+    }\n+\n+    /// A utility struct for building data pages (v1 or v2). Callers must call:\n+    ///   - add_rep_levels()\n+    ///   - add_def_levels()\n+    ///   - add_values() for normal data page / add_indices() for dictionary data page\n+    ///   - consume()\n+    /// in order to populate and obtain a data page.\n+    struct DataPageBuilderImpl {\n+        desc: ColumnDescPtr,\n+        encoding: Option<Encoding>,\n+        mem_tracker: MemTrackerPtr,\n+        num_values: u32,\n+        buffer: Vec<u8>,\n+        rep_levels_byte_len: u32,\n+        def_levels_byte_len: u32,\n+        datapage_v2: bool,\n+    }\n+\n+    impl DataPageBuilderImpl {\n+        // `num_values` is the number of non-null values to put in the data page.\n+        // `datapage_v2` flag is used to indicate if the generated data page should use V2\n+        // format or not.\n+        fn new(desc: ColumnDescPtr, num_values: u32, datapage_v2: bool) -> Self {\n+            DataPageBuilderImpl {\n+                desc,\n+                encoding: None,\n+                mem_tracker: Rc::new(MemTracker::new()),\n+                num_values,\n+                buffer: vec![],\n+                rep_levels_byte_len: 0,\n+                def_levels_byte_len: 0,\n+                datapage_v2,\n+            }\n+        }\n+\n+        // Adds levels to the buffer and return number of encoded bytes\n+        fn add_levels(&mut self, max_level: i16, levels: &[i16]) -> u32 {\n+            let size = max_buffer_size(Encoding::RLE, max_level, levels.len());\n+            let mut level_encoder = LevelEncoder::v1(Encoding::RLE, max_level, vec![0; size]);\n+            level_encoder.put(levels).expect(\"put() should be OK\");\n+            let encoded_levels = level_encoder.consume().expect(\"consume() should be OK\");\n+            // Actual encoded bytes (without length offset)\n+            let encoded_bytes = &encoded_levels[mem::size_of::<i32>()..];\n+            if self.datapage_v2 {\n+                // Level encoder always initializes with offset of i32, where it stores length of\n+                // encoded data; for data page v2 we explicitly store length, therefore we should\n+                // skip i32 bytes.\n+                self.buffer.extend_from_slice(encoded_bytes);\n+            } else {\n+                self.buffer.extend_from_slice(encoded_levels.as_slice());\n+            }\n+            encoded_bytes.len() as u32\n+        }\n+    }\n+\n+    impl DataPageBuilder for DataPageBuilderImpl {\n+        fn add_rep_levels(&mut self, max_levels: i16, rep_levels: &[i16]) {\n+            self.num_values = rep_levels.len() as u32;\n+            self.rep_levels_byte_len = self.add_levels(max_levels, rep_levels);\n+        }\n+\n+        fn add_def_levels(&mut self, max_levels: i16, def_levels: &[i16]) {\n+            assert!(\n+                self.num_values == def_levels.len() as u32,\n+                \"Must call `add_rep_levels() first!`\"\n+            );\n+\n+            self.def_levels_byte_len = self.add_levels(max_levels, def_levels);\n+        }\n+\n+        fn add_values<T: DataType>(&mut self, encoding: Encoding, values: &[T::T]) {\n+            assert!(\n+                self.num_values >= values.len() as u32,\n+                \"num_values: {}, values.len(): {}\",\n+                self.num_values,\n+                values.len()\n+            );\n+            self.encoding = Some(encoding);\n+            let mut encoder: Box<Encoder<T>> =\n+                get_encoder::<T>(self.desc.clone(), encoding, self.mem_tracker.clone())\n+                    .expect(\"get_encoder() should be OK\");\n+            encoder.put(values).expect(\"put() should be OK\");\n+            let encoded_values = encoder\n+                .flush_buffer()\n+                .expect(\"consume_buffer() should be OK\");\n+            self.buffer.extend_from_slice(encoded_values.data());\n+        }\n+\n+        fn add_indices(&mut self, indices: ByteBufferPtr) {\n+            self.encoding = Some(Encoding::RLE_DICTIONARY);\n+            self.buffer.extend_from_slice(indices.data());\n+        }\n+\n+        fn consume(self) -> Page {\n+            if self.datapage_v2 {\n+                Page::DataPageV2 {\n+                    buf: ByteBufferPtr::new(self.buffer),\n+                    num_values: self.num_values,\n+                    encoding: self.encoding.unwrap(),\n+                    num_nulls: 0, // set to dummy value - don't need this when reading data page\n+                    num_rows: self.num_values, // also don't need this when reading data page\n+                    def_levels_byte_len: self.def_levels_byte_len,\n+                    rep_levels_byte_len: self.rep_levels_byte_len,\n+                    is_compressed: false,\n+                    statistics: None, // set to None, we do not need statistics for tests\n+                }\n+            } else {\n+                Page::DataPage {\n+                    buf: ByteBufferPtr::new(self.buffer),\n+                    num_values: self.num_values,\n+                    encoding: self.encoding.unwrap(),\n+                    def_level_encoding: Encoding::RLE,\n+                    rep_level_encoding: Encoding::RLE,\n+                    statistics: None, // set to None, we do not need statistics for tests\n+                }\n+            }\n+        }\n+    }\n+\n+    fn make_pages<T: DataType>(\n+        desc: ColumnDescPtr,\n+        encoding: Encoding,\n+        num_pages: usize,\n+        levels_per_page: usize,\n+        min: T::T,\n+        max: T::T,\n+        def_levels: &mut Vec<i16>,\n+        rep_levels: &mut Vec<i16>,\n+        values: &mut Vec<T::T>,\n+        pages: &mut VecDeque<Page>,\n+        use_v2: bool,\n+    ) where\n+        T::T: PartialOrd + SampleRange + Copy,\n+    {\n+        let mut num_values = 0;\n+        let max_def_level = desc.max_def_level();\n+        let max_rep_level = desc.max_rep_level();\n+\n+        let mem_tracker = Rc::new(MemTracker::new());\n+        let mut dict_encoder = DictEncoder::<T>::new(desc.clone(), mem_tracker);\n+\n+        for i in 0..num_pages {\n+            let mut num_values_cur_page = 0;\n+            let level_range = i * levels_per_page..(i + 1) * levels_per_page;\n+\n+            if max_def_level > 0 {\n+                random_numbers_range(levels_per_page, 0, max_def_level + 1, def_levels);\n+                for dl in &def_levels[level_range.clone()] {\n+                    if *dl == max_def_level {\n+                        num_values_cur_page += 1;\n+                    }\n+                }\n+            } else {\n+                num_values_cur_page = levels_per_page;\n+            }\n+            if max_rep_level > 0 {\n+                random_numbers_range(levels_per_page, 0, max_rep_level + 1, rep_levels);\n+            }\n+            random_numbers_range(num_values_cur_page, min, max, values);\n+\n+            // Generate the current page\n+\n+            let mut pb = DataPageBuilderImpl::new(desc.clone(), num_values_cur_page as u32, use_v2);\n+            if max_rep_level > 0 {\n+                pb.add_rep_levels(max_rep_level, &rep_levels[level_range.clone()]);\n+            }\n+            if max_def_level > 0 {\n+                pb.add_def_levels(max_def_level, &def_levels[level_range]);\n+            }\n+\n+            let value_range = num_values..num_values + num_values_cur_page;\n+            match encoding {\n+                Encoding::PLAIN_DICTIONARY | Encoding::RLE_DICTIONARY => {\n+                    let _ = dict_encoder.put(&values[value_range.clone()]);\n+                    let indices = dict_encoder\n+                        .write_indices()\n+                        .expect(\"write_indices() should be OK\");\n+                    pb.add_indices(indices);\n+                }\n+                Encoding::PLAIN => {\n+                    pb.add_values::<T>(encoding, &values[value_range]);\n+                }\n+                enc @ _ => panic!(\"Unexpected encoding {}\", enc),\n+            }\n+\n+            let data_page = pb.consume();\n+            pages.push_back(data_page);\n+            num_values += num_values_cur_page;\n+        }\n+\n+        if encoding == Encoding::PLAIN_DICTIONARY || encoding == Encoding::RLE_DICTIONARY {\n+            let dict = dict_encoder\n+                .write_dict()\n+                .expect(\"write_dict() should be OK\");\n+            let dict_page = Page::DictionaryPage {\n+                buf: dict,\n+                num_values: dict_encoder.num_entries() as u32,\n+                encoding: Encoding::RLE_DICTIONARY,\n+                is_sorted: false,\n+            };\n+            pages.push_front(dict_page);\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/column/writer.rs b/rust/src/parquet/column/writer.rs\nnew file mode 100644\nindex 0000000000..4798d9ad17\n--- /dev/null\n+++ b/rust/src/parquet/column/writer.rs\n@@ -0,0 +1,1617 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains column writer API.\n+\n+use std::{cmp, collections::VecDeque, mem, rc::Rc};\n+\n+use crate::parquet::basic::{Compression, Encoding, PageType, Type};\n+use crate::parquet::column::page::{CompressedPage, Page, PageWriteSpec, PageWriter};\n+use crate::parquet::compression::{create_codec, Codec};\n+use crate::parquet::data_type::*;\n+use crate::parquet::encodings::{\n+    encoding::{get_encoder, DictEncoder, Encoder},\n+    levels::{max_buffer_size, LevelEncoder},\n+};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::file::{\n+    metadata::ColumnChunkMetaData,\n+    properties::{WriterProperties, WriterPropertiesPtr, WriterVersion},\n+};\n+use crate::parquet::schema::types::ColumnDescPtr;\n+use crate::parquet::util::memory::{ByteBufferPtr, MemTracker};\n+\n+/// Column writer for a Parquet type.\n+pub enum ColumnWriter {\n+    BoolColumnWriter(ColumnWriterImpl<BoolType>),\n+    Int32ColumnWriter(ColumnWriterImpl<Int32Type>),\n+    Int64ColumnWriter(ColumnWriterImpl<Int64Type>),\n+    Int96ColumnWriter(ColumnWriterImpl<Int96Type>),\n+    FloatColumnWriter(ColumnWriterImpl<FloatType>),\n+    DoubleColumnWriter(ColumnWriterImpl<DoubleType>),\n+    ByteArrayColumnWriter(ColumnWriterImpl<ByteArrayType>),\n+    FixedLenByteArrayColumnWriter(ColumnWriterImpl<FixedLenByteArrayType>),\n+}\n+\n+/// Gets a specific column writer corresponding to column descriptor `descr`.\n+pub fn get_column_writer(\n+    descr: ColumnDescPtr,\n+    props: WriterPropertiesPtr,\n+    page_writer: Box<PageWriter>,\n+) -> ColumnWriter {\n+    match descr.physical_type() {\n+        Type::BOOLEAN => {\n+            ColumnWriter::BoolColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::INT32 => {\n+            ColumnWriter::Int32ColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::INT64 => {\n+            ColumnWriter::Int64ColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::INT96 => {\n+            ColumnWriter::Int96ColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::FLOAT => {\n+            ColumnWriter::FloatColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::DOUBLE => {\n+            ColumnWriter::DoubleColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::BYTE_ARRAY => {\n+            ColumnWriter::ByteArrayColumnWriter(ColumnWriterImpl::new(descr, props, page_writer))\n+        }\n+        Type::FIXED_LEN_BYTE_ARRAY => ColumnWriter::FixedLenByteArrayColumnWriter(\n+            ColumnWriterImpl::new(descr, props, page_writer),\n+        ),\n+    }\n+}\n+\n+/// Gets a typed column writer for the specific type `T`, by \"up-casting\" `col_writer` of\n+/// non-generic type to a generic column writer type `ColumnWriterImpl`.\n+///\n+/// NOTE: the caller MUST guarantee that the actual enum value for `col_writer` matches\n+/// the type `T`. Otherwise, disastrous consequence could happen.\n+pub fn get_typed_column_writer<T: DataType>(col_writer: ColumnWriter) -> ColumnWriterImpl<T> {\n+    match col_writer {\n+        ColumnWriter::BoolColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::Int32ColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::Int64ColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::Int96ColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::FloatColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::DoubleColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::ByteArrayColumnWriter(r) => unsafe { mem::transmute(r) },\n+        ColumnWriter::FixedLenByteArrayColumnWriter(r) => unsafe { mem::transmute(r) },\n+    }\n+}\n+\n+/// Typed column writer for a primitive column.\n+pub struct ColumnWriterImpl<T: DataType> {\n+    // Column writer properties\n+    descr: ColumnDescPtr,\n+    props: WriterPropertiesPtr,\n+    page_writer: Box<PageWriter>,\n+    has_dictionary: bool,\n+    dict_encoder: Option<DictEncoder<T>>,\n+    encoder: Box<Encoder<T>>,\n+    codec: Compression,\n+    compressor: Option<Box<Codec>>,\n+    // Metrics per page\n+    num_buffered_values: u32,\n+    num_buffered_encoded_values: u32,\n+    num_buffered_rows: u32,\n+    // Metrics per column writer\n+    total_bytes_written: u64,\n+    total_rows_written: u64,\n+    total_uncompressed_size: u64,\n+    total_compressed_size: u64,\n+    total_num_values: u64,\n+    dictionary_page_offset: Option<u64>,\n+    data_page_offset: Option<u64>,\n+    // Reused buffers\n+    def_levels_sink: Vec<i16>,\n+    rep_levels_sink: Vec<i16>,\n+    data_pages: VecDeque<CompressedPage>,\n+}\n+\n+impl<T: DataType> ColumnWriterImpl<T> {\n+    pub fn new(\n+        descr: ColumnDescPtr,\n+        props: WriterPropertiesPtr,\n+        page_writer: Box<PageWriter>,\n+    ) -> Self {\n+        let codec = props.compression(descr.path());\n+        let compressor = create_codec(codec).unwrap();\n+\n+        // Optionally set dictionary encoder.\n+        let dict_encoder =\n+            if props.dictionary_enabled(descr.path()) && Self::has_dictionary_support(&props) {\n+                Some(DictEncoder::new(descr.clone(), Rc::new(MemTracker::new())))\n+            } else {\n+                None\n+            };\n+\n+        // Whether or not this column writer has a dictionary encoding.\n+        let has_dictionary = dict_encoder.is_some();\n+\n+        // Set either main encoder or fallback encoder.\n+        let fallback_encoder = get_encoder(\n+            descr.clone(),\n+            props\n+                .encoding(descr.path())\n+                .unwrap_or(Self::fallback_encoding(&props)),\n+            Rc::new(MemTracker::new()),\n+        )\n+        .unwrap();\n+\n+        Self {\n+            descr,\n+            props,\n+            page_writer,\n+            has_dictionary,\n+            dict_encoder,\n+            encoder: fallback_encoder,\n+            codec,\n+            compressor,\n+            num_buffered_values: 0,\n+            num_buffered_encoded_values: 0,\n+            num_buffered_rows: 0,\n+            total_bytes_written: 0,\n+            total_rows_written: 0,\n+            total_uncompressed_size: 0,\n+            total_compressed_size: 0,\n+            total_num_values: 0,\n+            dictionary_page_offset: None,\n+            data_page_offset: None,\n+            def_levels_sink: vec![],\n+            rep_levels_sink: vec![],\n+            data_pages: VecDeque::new(),\n+        }\n+    }\n+\n+    /// Writes batch of values, definition levels and repetition levels.\n+    /// Returns number of values processed (written).\n+    ///\n+    /// If definition and repetition levels are provided, we write fully those levels and\n+    /// select how many values to write (this number will be returned), since number of\n+    /// actual written values may be smaller than provided values.\n+    ///\n+    /// If only values are provided, then all values are written and the length of\n+    /// of the values buffer is returned.\n+    ///\n+    /// Definition and/or repetition levels can be omitted, if values are\n+    /// non-nullable and/or non-repeated.\n+    pub fn write_batch(\n+        &mut self,\n+        values: &[T::T],\n+        def_levels: Option<&[i16]>,\n+        rep_levels: Option<&[i16]>,\n+    ) -> Result<usize> {\n+        // We check for DataPage limits only after we have inserted the values. If a user\n+        // writes a large number of values, the DataPage size can be well above the limit.\n+        //\n+        // The purpose of this chunking is to bound this. Even if a user writes large number\n+        // of values, the chunking will ensure that we add data page at a reasonable pagesize\n+        // limit.\n+\n+        // TODO: find out why we don't account for size of levels when we estimate page size.\n+\n+        // Find out the minimal length to prevent index out of bound errors.\n+        let mut min_len = values.len();\n+        if let Some(levels) = def_levels {\n+            min_len = cmp::min(min_len, levels.len());\n+        }\n+        if let Some(levels) = rep_levels {\n+            min_len = cmp::min(min_len, levels.len());\n+        }\n+\n+        // Find out number of batches to process.\n+        let write_batch_size = self.props.write_batch_size();\n+        let num_batches = min_len / write_batch_size;\n+\n+        let mut values_offset = 0;\n+        let mut levels_offset = 0;\n+\n+        for _ in 0..num_batches {\n+            values_offset += self.write_mini_batch(\n+                &values[values_offset..values_offset + write_batch_size],\n+                def_levels.map(|lv| &lv[levels_offset..levels_offset + write_batch_size]),\n+                rep_levels.map(|lv| &lv[levels_offset..levels_offset + write_batch_size]),\n+            )?;\n+            levels_offset += write_batch_size;\n+        }\n+\n+        values_offset += self.write_mini_batch(\n+            &values[values_offset..],\n+            def_levels.map(|lv| &lv[levels_offset..]),\n+            rep_levels.map(|lv| &lv[levels_offset..]),\n+        )?;\n+\n+        // Return total number of values processed.\n+        Ok(values_offset)\n+    }\n+\n+    /// Returns total number of bytes written by this column writer so far.\n+    /// This value is also returned when column writer is closed.\n+    pub fn get_total_bytes_written(&self) -> u64 {\n+        self.total_bytes_written\n+    }\n+\n+    /// Returns total number of rows written by this column writer so far.\n+    /// This value is also returned when column writer is closed.\n+    pub fn get_total_rows_written(&self) -> u64 {\n+        self.total_rows_written\n+    }\n+\n+    /// Finalises writes and closes the column writer.\n+    /// Returns total bytes written, total rows written and column chunk metadata.\n+    pub fn close(mut self) -> Result<(u64, u64, ColumnChunkMetaData)> {\n+        if self.dict_encoder.is_some() {\n+            self.write_dictionary_page()?;\n+        }\n+        self.flush_data_pages()?;\n+        let metadata = self.write_column_metadata()?;\n+        self.dict_encoder = None;\n+        self.page_writer.close()?;\n+\n+        Ok((self.total_bytes_written, self.total_rows_written, metadata))\n+    }\n+\n+    /// Writes mini batch of values, definition and repetition levels.\n+    /// This allows fine-grained processing of values and maintaining a reasonable\n+    /// page size.\n+    fn write_mini_batch(\n+        &mut self,\n+        values: &[T::T],\n+        def_levels: Option<&[i16]>,\n+        rep_levels: Option<&[i16]>,\n+    ) -> Result<usize> {\n+        let num_values;\n+        let mut values_to_write = 0;\n+\n+        // Check if number of definition levels is the same as number of repetition levels.\n+        if def_levels.is_some() && rep_levels.is_some() {\n+            let def = def_levels.unwrap();\n+            let rep = rep_levels.unwrap();\n+            if def.len() != rep.len() {\n+                return Err(general_err!(\n+                    \"Inconsistent length of definition and repetition levels: {} != {}\",\n+                    def.len(),\n+                    rep.len()\n+                ));\n+            }\n+        }\n+\n+        // Process definition levels and determine how many values to write.\n+        if self.descr.max_def_level() > 0 {\n+            if def_levels.is_none() {\n+                return Err(general_err!(\n+                    \"Definition levels are required, because max definition level = {}\",\n+                    self.descr.max_def_level()\n+                ));\n+            }\n+\n+            let levels = def_levels.unwrap();\n+            num_values = levels.len();\n+            for &level in levels {\n+                values_to_write += (level == self.descr.max_def_level()) as usize;\n+            }\n+\n+            self.write_definition_levels(levels);\n+        } else {\n+            values_to_write = values.len();\n+            num_values = values_to_write;\n+        }\n+\n+        // Process repetition levels and determine how many rows we are about to process.\n+        if self.descr.max_rep_level() > 0 {\n+            // A row could contain more than one value.\n+            if rep_levels.is_none() {\n+                return Err(general_err!(\n+                    \"Repetition levels are required, because max repetition level = {}\",\n+                    self.descr.max_rep_level()\n+                ));\n+            }\n+\n+            // Count the occasions where we start a new row\n+            let levels = rep_levels.unwrap();\n+            for &level in levels {\n+                self.num_buffered_rows += (level == 0) as u32\n+            }\n+\n+            self.write_repetition_levels(levels);\n+        } else {\n+            // Each value is exactly one row.\n+            // Equals to the number of values, we count nulls as well.\n+            self.num_buffered_rows += num_values as u32;\n+        }\n+\n+        // Check that we have enough values to write.\n+        if values.len() < values_to_write {\n+            return Err(general_err!(\n+                \"Expected to write {} values, but have only {}\",\n+                values_to_write,\n+                values.len()\n+            ));\n+        }\n+\n+        // TODO: update page statistics\n+\n+        self.write_values(&values[0..values_to_write])?;\n+\n+        self.num_buffered_values += num_values as u32;\n+        self.num_buffered_encoded_values += values_to_write as u32;\n+\n+        if self.should_add_data_page() {\n+            self.add_data_page()?;\n+        }\n+\n+        if self.should_dict_fallback() {\n+            self.dict_fallback()?;\n+        }\n+\n+        Ok(values_to_write)\n+    }\n+\n+    #[inline]\n+    fn write_definition_levels(&mut self, def_levels: &[i16]) {\n+        self.def_levels_sink.extend_from_slice(def_levels);\n+    }\n+\n+    #[inline]\n+    fn write_repetition_levels(&mut self, rep_levels: &[i16]) {\n+        self.rep_levels_sink.extend_from_slice(rep_levels);\n+    }\n+\n+    #[inline]\n+    fn write_values(&mut self, values: &[T::T]) -> Result<()> {\n+        match self.dict_encoder {\n+            Some(ref mut encoder) => encoder.put(values),\n+            None => self.encoder.put(values),\n+        }\n+    }\n+\n+    /// Returns true if we need to fall back to non-dictionary encoding.\n+    ///\n+    /// We can only fall back if dictionary encoder is set and we have exceeded dictionary\n+    /// size.\n+    #[inline]\n+    fn should_dict_fallback(&self) -> bool {\n+        match self.dict_encoder {\n+            Some(ref encoder) => {\n+                encoder.dict_encoded_size() >= self.props.dictionary_pagesize_limit()\n+            }\n+            None => false,\n+        }\n+    }\n+\n+    /// Returns true if there is enough data for a data page, false otherwise.\n+    #[inline]\n+    fn should_add_data_page(&self) -> bool {\n+        self.encoder.estimated_data_encoded_size() >= self.props.data_pagesize_limit()\n+    }\n+\n+    /// Performs dictionary fallback.\n+    /// Prepares and writes dictionary and all data pages into page writer.\n+    fn dict_fallback(&mut self) -> Result<()> {\n+        // At this point we know that we need to fall back.\n+        self.write_dictionary_page()?;\n+        self.flush_data_pages()?;\n+        self.dict_encoder = None;\n+        Ok(())\n+    }\n+\n+    /// Adds data page.\n+    /// Data page is either buffered in case of dictionary encoding or written directly.\n+    fn add_data_page(&mut self) -> Result<()> {\n+        // Extract encoded values\n+        let value_bytes = match self.dict_encoder {\n+            Some(ref mut encoder) => encoder.write_indices()?,\n+            None => self.encoder.flush_buffer()?,\n+        };\n+\n+        // Select encoding based on current encoder and writer version (v1 or v2).\n+        let encoding = if self.dict_encoder.is_some() {\n+            self.props.dictionary_data_page_encoding()\n+        } else {\n+            self.encoder.encoding()\n+        };\n+\n+        let max_def_level = self.descr.max_def_level();\n+        let max_rep_level = self.descr.max_rep_level();\n+\n+        let compressed_page = match self.props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => {\n+                let mut buffer = vec![];\n+\n+                if max_rep_level > 0 {\n+                    buffer.extend_from_slice(\n+                        &self.encode_levels_v1(\n+                            Encoding::RLE,\n+                            &self.rep_levels_sink[..],\n+                            max_rep_level,\n+                        )?[..],\n+                    );\n+                }\n+\n+                if max_def_level > 0 {\n+                    buffer.extend_from_slice(\n+                        &self.encode_levels_v1(\n+                            Encoding::RLE,\n+                            &self.def_levels_sink[..],\n+                            max_def_level,\n+                        )?[..],\n+                    );\n+                }\n+\n+                buffer.extend_from_slice(value_bytes.data());\n+                let uncompressed_size = buffer.len();\n+\n+                if let Some(ref mut cmpr) = self.compressor {\n+                    let mut compressed_buf = Vec::with_capacity(value_bytes.data().len());\n+                    cmpr.compress(&buffer[..], &mut compressed_buf)?;\n+                    buffer = compressed_buf;\n+                }\n+\n+                let data_page = Page::DataPage {\n+                    buf: ByteBufferPtr::new(buffer),\n+                    num_values: self.num_buffered_values,\n+                    encoding,\n+                    def_level_encoding: Encoding::RLE,\n+                    rep_level_encoding: Encoding::RLE,\n+                    // TODO: process statistics\n+                    statistics: None,\n+                };\n+\n+                CompressedPage::new(data_page, uncompressed_size)\n+            }\n+            WriterVersion::PARQUET_2_0 => {\n+                let mut rep_levels_byte_len = 0;\n+                let mut def_levels_byte_len = 0;\n+                let mut buffer = vec![];\n+\n+                if max_rep_level > 0 {\n+                    let levels = self.encode_levels_v2(&self.rep_levels_sink[..], max_rep_level)?;\n+                    rep_levels_byte_len = levels.len();\n+                    buffer.extend_from_slice(&levels[..]);\n+                }\n+\n+                if max_def_level > 0 {\n+                    let levels = self.encode_levels_v2(&self.def_levels_sink[..], max_def_level)?;\n+                    def_levels_byte_len = levels.len();\n+                    buffer.extend_from_slice(&levels[..]);\n+                }\n+\n+                let uncompressed_size =\n+                    rep_levels_byte_len + def_levels_byte_len + value_bytes.len();\n+\n+                // Data Page v2 compresses values only.\n+                match self.compressor {\n+                    Some(ref mut cmpr) => {\n+                        let mut compressed_buf = Vec::with_capacity(value_bytes.data().len());\n+                        cmpr.compress(value_bytes.data(), &mut compressed_buf)?;\n+                        buffer.extend_from_slice(&compressed_buf[..]);\n+                    }\n+                    None => {\n+                        buffer.extend_from_slice(value_bytes.data());\n+                    }\n+                }\n+\n+                let data_page = Page::DataPageV2 {\n+                    buf: ByteBufferPtr::new(buffer),\n+                    num_values: self.num_buffered_values,\n+                    encoding,\n+                    num_nulls: self.num_buffered_values - self.num_buffered_encoded_values,\n+                    num_rows: self.num_buffered_rows,\n+                    def_levels_byte_len: def_levels_byte_len as u32,\n+                    rep_levels_byte_len: rep_levels_byte_len as u32,\n+                    is_compressed: self.compressor.is_some(),\n+                    // TODO: process statistics\n+                    statistics: None,\n+                };\n+\n+                CompressedPage::new(data_page, uncompressed_size)\n+            }\n+        };\n+\n+        // Check if we need to buffer data page or flush it to the sink directly.\n+        if self.dict_encoder.is_some() {\n+            self.data_pages.push_back(compressed_page);\n+        } else {\n+            self.write_data_page(compressed_page)?;\n+        }\n+\n+        // Update total number of rows.\n+        self.total_rows_written += self.num_buffered_rows as u64;\n+\n+        // Reset state.\n+        self.rep_levels_sink.clear();\n+        self.def_levels_sink.clear();\n+        self.num_buffered_values = 0;\n+        self.num_buffered_encoded_values = 0;\n+        self.num_buffered_rows = 0;\n+\n+        Ok(())\n+    }\n+\n+    /// Finalises any outstanding data pages and flushes buffered data pages from\n+    /// dictionary encoding into underlying sink.\n+    #[inline]\n+    fn flush_data_pages(&mut self) -> Result<()> {\n+        // Write all outstanding data to a new page.\n+        if self.num_buffered_values > 0 {\n+            self.add_data_page()?;\n+        }\n+\n+        while let Some(page) = self.data_pages.pop_front() {\n+            self.write_data_page(page)?;\n+        }\n+\n+        Ok(())\n+    }\n+\n+    /// Assembles and writes column chunk metadata.\n+    fn write_column_metadata(&mut self) -> Result<ColumnChunkMetaData> {\n+        let total_compressed_size = self.total_compressed_size as i64;\n+        let total_uncompressed_size = self.total_uncompressed_size as i64;\n+        let num_values = self.total_num_values as i64;\n+        let dict_page_offset = self.dictionary_page_offset.map(|v| v as i64);\n+        // If data page offset is not set, then no pages have been written\n+        let data_page_offset = self.data_page_offset.unwrap_or(0) as i64;\n+\n+        let file_offset;\n+        let mut encodings = Vec::new();\n+\n+        if self.has_dictionary {\n+            assert!(dict_page_offset.is_some(), \"Dictionary offset is not set\");\n+            file_offset = dict_page_offset.unwrap() + total_compressed_size;\n+            // NOTE: This should be in sync with writing dictionary pages.\n+            encodings.push(self.props.dictionary_page_encoding());\n+            encodings.push(self.props.dictionary_data_page_encoding());\n+            // Fallback to alternative encoding, add it to the list.\n+            if self.dict_encoder.is_none() {\n+                encodings.push(self.encoder.encoding());\n+            }\n+        } else {\n+            file_offset = data_page_offset + total_compressed_size;\n+            encodings.push(self.encoder.encoding());\n+        }\n+        // We use only RLE level encoding for data page v1 and data page v2.\n+        encodings.push(Encoding::RLE);\n+\n+        let metadata = ColumnChunkMetaData::builder(self.descr.clone())\n+            .set_compression(self.codec)\n+            .set_encodings(encodings)\n+            .set_file_offset(file_offset)\n+            .set_total_compressed_size(total_compressed_size)\n+            .set_total_uncompressed_size(total_uncompressed_size)\n+            .set_num_values(num_values)\n+            .set_data_page_offset(data_page_offset)\n+            .set_dictionary_page_offset(dict_page_offset)\n+            .build()?;\n+\n+        self.page_writer.write_metadata(&metadata)?;\n+\n+        Ok(metadata)\n+    }\n+\n+    /// Encodes definition or repetition levels for Data Page v1.\n+    #[inline]\n+    fn encode_levels_v1(\n+        &self,\n+        encoding: Encoding,\n+        levels: &[i16],\n+        max_level: i16,\n+    ) -> Result<Vec<u8>> {\n+        let size = max_buffer_size(encoding, max_level, levels.len());\n+        let mut encoder = LevelEncoder::v1(encoding, max_level, vec![0; size]);\n+        encoder.put(&levels)?;\n+        encoder.consume()\n+    }\n+\n+    /// Encodes definition or repetition levels for Data Page v2.\n+    /// Encoding is always RLE.\n+    #[inline]\n+    fn encode_levels_v2(&self, levels: &[i16], max_level: i16) -> Result<Vec<u8>> {\n+        let size = max_buffer_size(Encoding::RLE, max_level, levels.len());\n+        let mut encoder = LevelEncoder::v2(max_level, vec![0; size]);\n+        encoder.put(&levels)?;\n+        encoder.consume()\n+    }\n+\n+    /// Writes compressed data page into underlying sink and updates global metrics.\n+    #[inline]\n+    fn write_data_page(&mut self, page: CompressedPage) -> Result<()> {\n+        let page_spec = self.page_writer.write_page(page)?;\n+        self.update_metrics_for_page(page_spec);\n+        Ok(())\n+    }\n+\n+    /// Writes dictionary page into underlying sink.\n+    #[inline]\n+    fn write_dictionary_page(&mut self) -> Result<()> {\n+        if self.dict_encoder.is_none() {\n+            return Err(general_err!(\"Dictionary encoder is not set\"));\n+        }\n+\n+        let compressed_page = {\n+            let encoder = self.dict_encoder.as_ref().unwrap();\n+            let is_sorted = encoder.is_sorted();\n+            let num_values = encoder.num_entries();\n+            let mut values_buf = encoder.write_dict()?;\n+            let uncompressed_size = values_buf.len();\n+\n+            if let Some(ref mut cmpr) = self.compressor {\n+                let mut output_buf = Vec::with_capacity(uncompressed_size);\n+                cmpr.compress(values_buf.data(), &mut output_buf)?;\n+                values_buf = ByteBufferPtr::new(output_buf);\n+            }\n+\n+            let dict_page = Page::DictionaryPage {\n+                buf: values_buf,\n+                num_values: num_values as u32,\n+                encoding: self.props.dictionary_page_encoding(),\n+                is_sorted,\n+            };\n+            CompressedPage::new(dict_page, uncompressed_size)\n+        };\n+\n+        let page_spec = self.page_writer.write_page(compressed_page)?;\n+        self.update_metrics_for_page(page_spec);\n+        Ok(())\n+    }\n+\n+    /// Updates column writer metrics with each page metadata.\n+    #[inline]\n+    fn update_metrics_for_page(&mut self, page_spec: PageWriteSpec) {\n+        self.total_uncompressed_size += page_spec.uncompressed_size as u64;\n+        self.total_compressed_size += page_spec.compressed_size as u64;\n+        self.total_num_values += page_spec.num_values as u64;\n+        self.total_bytes_written += page_spec.bytes_written;\n+\n+        match page_spec.page_type {\n+            PageType::DATA_PAGE | PageType::DATA_PAGE_V2 => {\n+                if self.data_page_offset.is_none() {\n+                    self.data_page_offset = Some(page_spec.offset);\n+                }\n+            }\n+            PageType::DICTIONARY_PAGE => {\n+                assert!(\n+                    self.dictionary_page_offset.is_none(),\n+                    \"Dictionary offset is already set\"\n+                );\n+                self.dictionary_page_offset = Some(page_spec.offset);\n+            }\n+            _ => {}\n+        }\n+    }\n+\n+    /// Returns reference to the underlying page writer.\n+    /// This method is intended to use in tests only.\n+    fn get_page_writer_ref(&self) -> &Box<PageWriter> {\n+        &self.page_writer\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// Encoding support for column writer.\n+// This mirrors parquet-mr default encodings for writes. See:\n+// https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/values/factory/DefaultV1ValuesWriterFactory.java\n+// https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/column/values/factory/DefaultV2ValuesWriterFactory.java\n+\n+/// Trait to define default encoding for types, including whether or not the type\n+/// supports dictionary encoding.\n+trait EncodingWriteSupport {\n+    /// Returns encoding for a column when no other encoding is provided in writer\n+    /// properties.\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding;\n+\n+    /// Returns true if dictionary is supported for column writer, false otherwise.\n+    fn has_dictionary_support(props: &WriterProperties) -> bool;\n+}\n+\n+// Basic implementation, always falls back to PLAIN and supports dictionary.\n+impl<T: DataType> EncodingWriteSupport for ColumnWriterImpl<T> {\n+    default fn fallback_encoding(_props: &WriterProperties) -> Encoding {\n+        Encoding::PLAIN\n+    }\n+\n+    default fn has_dictionary_support(_props: &WriterProperties) -> bool {\n+        true\n+    }\n+}\n+\n+impl EncodingWriteSupport for ColumnWriterImpl<BoolType> {\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding {\n+        match props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => Encoding::PLAIN,\n+            WriterVersion::PARQUET_2_0 => Encoding::RLE,\n+        }\n+    }\n+\n+    // Boolean column does not support dictionary encoding and should fall back to\n+    // whatever fallback encoding is defined.\n+    fn has_dictionary_support(_props: &WriterProperties) -> bool {\n+        false\n+    }\n+}\n+\n+impl EncodingWriteSupport for ColumnWriterImpl<Int32Type> {\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding {\n+        match props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => Encoding::PLAIN,\n+            WriterVersion::PARQUET_2_0 => Encoding::DELTA_BINARY_PACKED,\n+        }\n+    }\n+}\n+\n+impl EncodingWriteSupport for ColumnWriterImpl<Int64Type> {\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding {\n+        match props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => Encoding::PLAIN,\n+            WriterVersion::PARQUET_2_0 => Encoding::DELTA_BINARY_PACKED,\n+        }\n+    }\n+}\n+\n+impl EncodingWriteSupport for ColumnWriterImpl<ByteArrayType> {\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding {\n+        match props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => Encoding::PLAIN,\n+            WriterVersion::PARQUET_2_0 => Encoding::DELTA_BYTE_ARRAY,\n+        }\n+    }\n+}\n+\n+impl EncodingWriteSupport for ColumnWriterImpl<FixedLenByteArrayType> {\n+    fn fallback_encoding(props: &WriterProperties) -> Encoding {\n+        match props.writer_version() {\n+            WriterVersion::PARQUET_1_0 => Encoding::PLAIN,\n+            WriterVersion::PARQUET_2_0 => Encoding::DELTA_BYTE_ARRAY,\n+        }\n+    }\n+\n+    fn has_dictionary_support(props: &WriterProperties) -> bool {\n+        match props.writer_version() {\n+            // Dictionary encoding was not enabled in PARQUET 1.0\n+            WriterVersion::PARQUET_1_0 => false,\n+            WriterVersion::PARQUET_2_0 => true,\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use std::error::Error;\n+\n+    use rand::distributions::range::SampleRange;\n+\n+    use crate::parquet::column::{\n+        page::PageReader,\n+        reader::{get_column_reader, get_typed_column_reader, ColumnReaderImpl},\n+    };\n+    use crate::parquet::file::{\n+        properties::WriterProperties, reader::SerializedPageReader, writer::SerializedPageWriter,\n+    };\n+    use crate::parquet::schema::types::{ColumnDescriptor, ColumnPath, Type as SchemaType};\n+    use crate::parquet::util::{\n+        io::{FileSink, FileSource},\n+        test_common::{get_temp_file, random_numbers_range},\n+    };\n+\n+    #[test]\n+    fn test_column_writer_inconsistent_def_rep_length() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 1, 1, props);\n+        let res = writer.write_batch(&[1, 2, 3, 4], Some(&[1, 1, 1]), Some(&[0, 0]));\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(\n+                err.description(),\n+                \"Inconsistent length of definition and repetition levels: 3 != 2\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_writer_invalid_def_levels() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 1, 0, props);\n+        let res = writer.write_batch(&[1, 2, 3, 4], None, None);\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(\n+                err.description(),\n+                \"Definition levels are required, because max definition level = 1\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_writer_invalid_rep_levels() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 0, 1, props);\n+        let res = writer.write_batch(&[1, 2, 3, 4], None, None);\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(\n+                err.description(),\n+                \"Repetition levels are required, because max repetition level = 1\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_writer_not_enough_values_to_write() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 1, 0, props);\n+        let res = writer.write_batch(&[1, 2], Some(&[1, 1, 1, 1]), None);\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(\n+                err.description(),\n+                \"Expected to write 4 values, but have only 2\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Dictionary offset is already set\")]\n+    fn test_column_writer_write_only_one_dictionary_page() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 0, 0, props);\n+        writer.write_batch(&[1, 2, 3, 4], None, None).unwrap();\n+        // First page should be correctly written.\n+        let res = writer.write_dictionary_page();\n+        assert!(res.is_ok());\n+        writer.write_dictionary_page().unwrap();\n+    }\n+\n+    #[test]\n+    fn test_column_writer_error_when_writing_disabled_dictionary() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(\n+            WriterProperties::builder()\n+                .set_dictionary_enabled(false)\n+                .build(),\n+        );\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 0, 0, props);\n+        writer.write_batch(&[1, 2, 3, 4], None, None).unwrap();\n+        let res = writer.write_dictionary_page();\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(err.description(), \"Dictionary encoder is not set\");\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_writer_boolean_type_does_not_support_dictionary() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(\n+            WriterProperties::builder()\n+                .set_dictionary_enabled(true)\n+                .build(),\n+        );\n+        let mut writer = get_test_column_writer::<BoolType>(page_writer, 0, 0, props);\n+        writer\n+            .write_batch(&[true, false, true, false], None, None)\n+            .unwrap();\n+\n+        let (bytes_written, rows_written, metadata) = writer.close().unwrap();\n+        // PlainEncoder uses bit writer to write boolean values, which all fit into 1 byte.\n+        assert_eq!(bytes_written, 1);\n+        assert_eq!(rows_written, 4);\n+        assert_eq!(metadata.encodings(), &vec![Encoding::PLAIN, Encoding::RLE]);\n+        assert_eq!(metadata.num_values(), 4); // just values\n+        assert_eq!(metadata.dictionary_page_offset(), None);\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_bool() {\n+        check_encoding_write_support::<BoolType>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[true, false],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<BoolType>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[true, false],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<BoolType>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[true, false],\n+            None,\n+            &[Encoding::RLE, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<BoolType>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[true, false],\n+            None,\n+            &[Encoding::RLE, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_int32() {\n+        check_encoding_write_support::<Int32Type>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[1, 2],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int32Type>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[1, 2],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int32Type>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[1, 2],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int32Type>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[1, 2],\n+            None,\n+            &[Encoding::DELTA_BINARY_PACKED, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_int64() {\n+        check_encoding_write_support::<Int64Type>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[1, 2],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int64Type>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[1, 2],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int64Type>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[1, 2],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int64Type>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[1, 2],\n+            None,\n+            &[Encoding::DELTA_BINARY_PACKED, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_int96() {\n+        check_encoding_write_support::<Int96Type>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[Int96::from(vec![1, 2, 3])],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int96Type>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[Int96::from(vec![1, 2, 3])],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int96Type>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[Int96::from(vec![1, 2, 3])],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<Int96Type>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[Int96::from(vec![1, 2, 3])],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_float() {\n+        check_encoding_write_support::<FloatType>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[1.0, 2.0],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FloatType>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[1.0, 2.0],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FloatType>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[1.0, 2.0],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FloatType>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[1.0, 2.0],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_double() {\n+        check_encoding_write_support::<DoubleType>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[1.0, 2.0],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<DoubleType>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[1.0, 2.0],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<DoubleType>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[1.0, 2.0],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<DoubleType>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[1.0, 2.0],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_byte_array() {\n+        check_encoding_write_support::<ByteArrayType>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[ByteArray::from(vec![1u8])],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<ByteArrayType>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[ByteArray::from(vec![1u8])],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<ByteArrayType>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[ByteArray::from(vec![1u8])],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<ByteArrayType>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[ByteArray::from(vec![1u8])],\n+            None,\n+            &[Encoding::DELTA_BYTE_ARRAY, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_default_encoding_support_fixed_len_byte_array() {\n+        check_encoding_write_support::<FixedLenByteArrayType>(\n+            WriterVersion::PARQUET_1_0,\n+            true,\n+            &[ByteArray::from(vec![1u8])],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FixedLenByteArrayType>(\n+            WriterVersion::PARQUET_1_0,\n+            false,\n+            &[ByteArray::from(vec![1u8])],\n+            None,\n+            &[Encoding::PLAIN, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FixedLenByteArrayType>(\n+            WriterVersion::PARQUET_2_0,\n+            true,\n+            &[ByteArray::from(vec![1u8])],\n+            Some(0),\n+            &[Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE],\n+        );\n+        check_encoding_write_support::<FixedLenByteArrayType>(\n+            WriterVersion::PARQUET_2_0,\n+            false,\n+            &[ByteArray::from(vec![1u8])],\n+            None,\n+            &[Encoding::DELTA_BYTE_ARRAY, Encoding::RLE],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_check_metadata() {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = get_test_column_writer::<Int32Type>(page_writer, 0, 0, props);\n+        writer.write_batch(&[1, 2, 3, 4], None, None).unwrap();\n+\n+        let (bytes_written, rows_written, metadata) = writer.close().unwrap();\n+        assert_eq!(bytes_written, 20);\n+        assert_eq!(rows_written, 4);\n+        assert_eq!(\n+            metadata.encodings(),\n+            &vec![Encoding::PLAIN, Encoding::RLE_DICTIONARY, Encoding::RLE]\n+        );\n+        assert_eq!(metadata.num_values(), 8); // dictionary + value indexes\n+        assert_eq!(metadata.compressed_size(), 20);\n+        assert_eq!(metadata.uncompressed_size(), 20);\n+        assert_eq!(metadata.data_page_offset(), 0);\n+        assert_eq!(metadata.dictionary_page_offset(), Some(0));\n+    }\n+\n+    #[test]\n+    fn test_column_writer_empty_column_roundtrip() {\n+        let props = WriterProperties::builder().build();\n+        column_roundtrip::<Int32Type>(\"test_col_writer_rnd_1\", props, &[], None, None);\n+    }\n+\n+    #[test]\n+    fn test_column_writer_non_nullable_values_roundtrip() {\n+        let props = WriterProperties::builder().build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_2\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            0,\n+            0,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_nullable_non_repeated_values_roundtrip() {\n+        let props = WriterProperties::builder().build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_column_writer_nullable_non_repeated_values_roundtrip\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            0,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_nullable_repeated_values_roundtrip() {\n+        let props = WriterProperties::builder().build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_3\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_dictionary_fallback_small_data_page() {\n+        let props = WriterProperties::builder()\n+            .set_dictionary_pagesize_limit(32)\n+            .set_data_pagesize_limit(32)\n+            .build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_4\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_small_write_batch_size() {\n+        for i in vec![1, 2, 5, 10, 11, 1023] {\n+            let props = WriterProperties::builder().set_write_batch_size(i).build();\n+\n+            column_roundtrip_random::<Int32Type>(\n+                \"test_col_writer_rnd_5\",\n+                props,\n+                1024,\n+                ::std::i32::MIN,\n+                ::std::i32::MAX,\n+                10,\n+                10,\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_writer_dictionary_disabled_v1() {\n+        let props = WriterProperties::builder()\n+            .set_writer_version(WriterVersion::PARQUET_1_0)\n+            .set_dictionary_enabled(false)\n+            .build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_6\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_dictionary_disabled_v2() {\n+        let props = WriterProperties::builder()\n+            .set_writer_version(WriterVersion::PARQUET_2_0)\n+            .set_dictionary_enabled(false)\n+            .build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_7\",\n+            props,\n+            1024,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_compression_v1() {\n+        let props = WriterProperties::builder()\n+            .set_writer_version(WriterVersion::PARQUET_1_0)\n+            .set_compression(Compression::SNAPPY)\n+            .build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_8\",\n+            props,\n+            2048,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_column_writer_compression_v2() {\n+        let props = WriterProperties::builder()\n+            .set_writer_version(WriterVersion::PARQUET_2_0)\n+            .set_compression(Compression::SNAPPY)\n+            .build();\n+        column_roundtrip_random::<Int32Type>(\n+            \"test_col_writer_rnd_9\",\n+            props,\n+            2048,\n+            ::std::i32::MIN,\n+            ::std::i32::MAX,\n+            10,\n+            10,\n+        );\n+    }\n+\n+    /// Performs write-read roundtrip with randomly generated values and levels.\n+    /// `max_size` is maximum number of values or levels (if `max_def_level` > 0) to write\n+    /// for a column.\n+    fn column_roundtrip_random<'a, T: DataType>(\n+        file_name: &'a str,\n+        props: WriterProperties,\n+        max_size: usize,\n+        min_value: T::T,\n+        max_value: T::T,\n+        max_def_level: i16,\n+        max_rep_level: i16,\n+    ) where\n+        T::T: PartialOrd + SampleRange + Copy,\n+    {\n+        let mut num_values: usize = 0;\n+\n+        let mut buf: Vec<i16> = Vec::new();\n+        let def_levels = if max_def_level > 0 {\n+            random_numbers_range(max_size, 0, max_def_level + 1, &mut buf);\n+            for &dl in &buf[..] {\n+                if dl == max_def_level {\n+                    num_values += 1;\n+                }\n+            }\n+            Some(&buf[..])\n+        } else {\n+            num_values = max_size;\n+            None\n+        };\n+\n+        let mut buf: Vec<i16> = Vec::new();\n+        let rep_levels = if max_rep_level > 0 {\n+            random_numbers_range(max_size, 0, max_rep_level + 1, &mut buf);\n+            Some(&buf[..])\n+        } else {\n+            None\n+        };\n+\n+        let mut values: Vec<T::T> = Vec::new();\n+        random_numbers_range(num_values, min_value, max_value, &mut values);\n+\n+        column_roundtrip::<T>(file_name, props, &values[..], def_levels, rep_levels);\n+    }\n+\n+    /// Performs write-read roundtrip and asserts written values and levels.\n+    fn column_roundtrip<'a, T: DataType>(\n+        file_name: &'a str,\n+        props: WriterProperties,\n+        values: &[T::T],\n+        def_levels: Option<&[i16]>,\n+        rep_levels: Option<&[i16]>,\n+    ) {\n+        let file = get_temp_file(file_name, &[]);\n+        let sink = FileSink::new(&file);\n+        let page_writer = Box::new(SerializedPageWriter::new(sink));\n+\n+        let max_def_level = match def_levels {\n+            Some(buf) => *buf.iter().max().unwrap_or(&0i16),\n+            None => 0i16,\n+        };\n+\n+        let max_rep_level = match rep_levels {\n+            Some(buf) => *buf.iter().max().unwrap_or(&0i16),\n+            None => 0i16,\n+        };\n+\n+        let mut max_batch_size = values.len();\n+        if let Some(levels) = def_levels {\n+            max_batch_size = cmp::max(max_batch_size, levels.len());\n+        }\n+        if let Some(levels) = rep_levels {\n+            max_batch_size = cmp::max(max_batch_size, levels.len());\n+        }\n+\n+        let mut writer =\n+            get_test_column_writer::<T>(page_writer, max_def_level, max_rep_level, Rc::new(props));\n+\n+        let values_written = writer.write_batch(values, def_levels, rep_levels).unwrap();\n+        assert_eq!(values_written, values.len());\n+        let (bytes_written, rows_written, column_metadata) = writer.close().unwrap();\n+\n+        let source = FileSource::new(&file, 0, bytes_written as usize);\n+        let page_reader = Box::new(\n+            SerializedPageReader::new(\n+                source,\n+                column_metadata.num_values(),\n+                column_metadata.compression(),\n+                T::get_physical_type(),\n+            )\n+            .unwrap(),\n+        );\n+        let reader = get_test_column_reader::<T>(page_reader, max_def_level, max_rep_level);\n+\n+        let mut actual_values = vec![T::T::default(); max_batch_size];\n+        let mut actual_def_levels = match def_levels {\n+            Some(_) => Some(vec![0i16; max_batch_size]),\n+            None => None,\n+        };\n+        let mut actual_rep_levels = match rep_levels {\n+            Some(_) => Some(vec![0i16; max_batch_size]),\n+            None => None,\n+        };\n+\n+        let (values_read, levels_read) = read_fully(\n+            reader,\n+            max_batch_size,\n+            actual_def_levels.as_mut(),\n+            actual_rep_levels.as_mut(),\n+            actual_values.as_mut_slice(),\n+        );\n+\n+        // Assert values, definition and repetition levels.\n+\n+        assert_eq!(&actual_values[..values_read], values);\n+        match actual_def_levels {\n+            Some(ref vec) => assert_eq!(Some(&vec[..levels_read]), def_levels),\n+            None => assert_eq!(None, def_levels),\n+        }\n+        match actual_rep_levels {\n+            Some(ref vec) => assert_eq!(Some(&vec[..levels_read]), rep_levels),\n+            None => assert_eq!(None, rep_levels),\n+        }\n+\n+        // Assert written rows.\n+\n+        if let Some(levels) = actual_rep_levels {\n+            let mut actual_rows_written = 0;\n+            for l in levels {\n+                if l == 0 {\n+                    actual_rows_written += 1;\n+                }\n+            }\n+            assert_eq!(actual_rows_written, rows_written);\n+        } else if actual_def_levels.is_some() {\n+            assert_eq!(levels_read as u64, rows_written);\n+        } else {\n+            assert_eq!(values_read as u64, rows_written);\n+        }\n+    }\n+\n+    /// Performs write of provided values and returns column metadata of those values.\n+    /// Used to test encoding support for column writer.\n+    fn column_write_and_get_metadata<T: DataType>(\n+        props: WriterProperties,\n+        values: &[T::T],\n+    ) -> ColumnChunkMetaData {\n+        let page_writer = get_test_page_writer();\n+        let props = Rc::new(props);\n+        let mut writer = get_test_column_writer::<T>(page_writer, 0, 0, props);\n+        writer.write_batch(values, None, None).unwrap();\n+        let (_, _, metadata) = writer.close().unwrap();\n+        metadata\n+    }\n+\n+    // Function to use in tests for EncodingWriteSupport. This checks that dictionary\n+    // offset and encodings to make sure that column writer uses provided by trait\n+    // encodings.\n+    fn check_encoding_write_support<T: DataType>(\n+        version: WriterVersion,\n+        dict_enabled: bool,\n+        data: &[T::T],\n+        dictionary_page_offset: Option<i64>,\n+        encodings: &[Encoding],\n+    ) {\n+        let props = WriterProperties::builder()\n+            .set_writer_version(version)\n+            .set_dictionary_enabled(dict_enabled)\n+            .build();\n+        let meta = column_write_and_get_metadata::<T>(props, data);\n+        assert_eq!(meta.dictionary_page_offset(), dictionary_page_offset);\n+        assert_eq!(meta.encodings(), &encodings);\n+    }\n+\n+    /// Reads one batch of data, considering that batch is large enough to capture all of\n+    /// the values and levels.\n+    fn read_fully<T: DataType>(\n+        mut reader: ColumnReaderImpl<T>,\n+        batch_size: usize,\n+        mut def_levels: Option<&mut Vec<i16>>,\n+        mut rep_levels: Option<&mut Vec<i16>>,\n+        values: &mut [T::T],\n+    ) -> (usize, usize) {\n+        let actual_def_levels = match &mut def_levels {\n+            Some(ref mut vec) => Some(&mut vec[..]),\n+            None => None,\n+        };\n+        let actual_rep_levels = match rep_levels {\n+            Some(ref mut vec) => Some(&mut vec[..]),\n+            None => None,\n+        };\n+        reader\n+            .read_batch(batch_size, actual_def_levels, actual_rep_levels, values)\n+            .unwrap()\n+    }\n+\n+    /// Returns column writer.\n+    fn get_test_column_writer<T: DataType>(\n+        page_writer: Box<PageWriter>,\n+        max_def_level: i16,\n+        max_rep_level: i16,\n+        props: WriterPropertiesPtr,\n+    ) -> ColumnWriterImpl<T> {\n+        let descr = Rc::new(get_test_column_descr::<T>(max_def_level, max_rep_level));\n+        let column_writer = get_column_writer(descr, props, page_writer);\n+        get_typed_column_writer::<T>(column_writer)\n+    }\n+\n+    /// Returns column reader.\n+    fn get_test_column_reader<T: DataType>(\n+        page_reader: Box<PageReader>,\n+        max_def_level: i16,\n+        max_rep_level: i16,\n+    ) -> ColumnReaderImpl<T> {\n+        let descr = Rc::new(get_test_column_descr::<T>(max_def_level, max_rep_level));\n+        let column_reader = get_column_reader(descr, page_reader);\n+        get_typed_column_reader::<T>(column_reader)\n+    }\n+\n+    /// Returns descriptor for primitive column.\n+    fn get_test_column_descr<T: DataType>(\n+        max_def_level: i16,\n+        max_rep_level: i16,\n+    ) -> ColumnDescriptor {\n+        let path = ColumnPath::from(\"col\");\n+        let tpe = SchemaType::primitive_type_builder(\"col\", T::get_physical_type())\n+            // length is set for \"encoding support\" tests for FIXED_LEN_BYTE_ARRAY type,\n+            // it should be no-op for other types\n+            .with_length(1)\n+            .build()\n+            .unwrap();\n+        ColumnDescriptor::new(Rc::new(tpe), None, max_def_level, max_rep_level, path)\n+    }\n+\n+    /// Returns page writer that collects pages without serializing them.\n+    fn get_test_page_writer() -> Box<PageWriter> {\n+        Box::new(TestPageWriter {})\n+    }\n+\n+    struct TestPageWriter {}\n+\n+    impl PageWriter for TestPageWriter {\n+        fn write_page(&mut self, page: CompressedPage) -> Result<PageWriteSpec> {\n+            let mut res = PageWriteSpec::new();\n+            res.page_type = page.page_type();\n+            res.uncompressed_size = page.uncompressed_size();\n+            res.compressed_size = page.compressed_size();\n+            res.num_values = page.num_values();\n+            res.offset = 0;\n+            res.bytes_written = page.data().len() as u64;\n+            Ok(res)\n+        }\n+\n+        fn write_metadata(&mut self, _metadata: &ColumnChunkMetaData) -> Result<()> {\n+            Ok(())\n+        }\n+\n+        fn close(&mut self) -> Result<()> {\n+            Ok(())\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/compression.rs b/rust/src/parquet/compression.rs\nnew file mode 100644\nindex 0000000000..3690cca032\n--- /dev/null\n+++ b/rust/src/parquet/compression.rs\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains codec interface and supported codec implementations.\n+//!\n+//! See [`Compression`](`::basic::Compression`) enum for all available compression\n+//! algorithms.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use arrow::parquet::{basic::Compression, compression::create_codec};\n+//!\n+//! let mut codec = match create_codec(Compression::SNAPPY) {\n+//!     Ok(Some(codec)) => codec,\n+//!     _ => panic!(),\n+//! };\n+//!\n+//! let data = vec![b'p', b'a', b'r', b'q', b'u', b'e', b't'];\n+//! let mut compressed = vec![];\n+//! codec.compress(&data[..], &mut compressed).unwrap();\n+//!\n+//! let mut output = vec![];\n+//! codec.decompress(&compressed[..], &mut output).unwrap();\n+//!\n+//! assert_eq!(output, data);\n+//! ```\n+\n+use std::io::{self, Read, Write};\n+\n+use brotli;\n+use flate2::{read, write, Compression};\n+use lz4;\n+use snap::{decompress_len, max_compress_len, Decoder, Encoder};\n+use zstd;\n+\n+use crate::parquet::basic::Compression as CodecType;\n+use crate::parquet::errors::{ParquetError, Result};\n+\n+/// Parquet compression codec interface.\n+pub trait Codec {\n+    /// Compresses data stored in slice `input_buf` and writes the compressed result\n+    /// to `output_buf`.\n+    /// Note that you'll need to call `clear()` before reusing the same `output_buf` across\n+    /// different `compress` calls.\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()>;\n+\n+    /// Decompresses data stored in slice `input_buf` and writes output to `output_buf`.\n+    /// Returns the total number of bytes written.\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize>;\n+}\n+\n+/// Given the compression type `codec`, returns a codec used to compress and decompress\n+/// bytes for the compression type.\n+/// This returns `None` if the codec type is `UNCOMPRESSED`.\n+pub fn create_codec(codec: CodecType) -> Result<Option<Box<Codec>>> {\n+    match codec {\n+        CodecType::BROTLI => Ok(Some(Box::new(BrotliCodec::new()))),\n+        CodecType::GZIP => Ok(Some(Box::new(GZipCodec::new()))),\n+        CodecType::SNAPPY => Ok(Some(Box::new(SnappyCodec::new()))),\n+        CodecType::LZ4 => Ok(Some(Box::new(LZ4Codec::new()))),\n+        CodecType::ZSTD => Ok(Some(Box::new(ZSTDCodec::new()))),\n+        CodecType::UNCOMPRESSED => Ok(None),\n+        _ => Err(nyi_err!(\"The codec type {} is not supported yet\", codec)),\n+    }\n+}\n+\n+/// Codec for Snappy compression format.\n+pub struct SnappyCodec {\n+    decoder: Decoder,\n+    encoder: Encoder,\n+}\n+\n+impl SnappyCodec {\n+    /// Creates new Snappy compression codec.\n+    fn new() -> Self {\n+        Self {\n+            decoder: Decoder::new(),\n+            encoder: Encoder::new(),\n+        }\n+    }\n+}\n+\n+impl Codec for SnappyCodec {\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize> {\n+        let len = decompress_len(input_buf)?;\n+        output_buf.resize(len, 0);\n+        self.decoder\n+            .decompress(input_buf, output_buf)\n+            .map_err(|e| e.into())\n+    }\n+\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()> {\n+        let required_len = max_compress_len(input_buf.len());\n+        if output_buf.len() < required_len {\n+            output_buf.resize(required_len, 0);\n+        }\n+        let n = self.encoder.compress(input_buf, &mut output_buf[..])?;\n+        output_buf.truncate(n);\n+        Ok(())\n+    }\n+}\n+\n+/// Codec for GZIP compression algorithm.\n+pub struct GZipCodec {}\n+\n+impl GZipCodec {\n+    /// Creates new GZIP compression codec.\n+    fn new() -> Self {\n+        Self {}\n+    }\n+}\n+\n+impl Codec for GZipCodec {\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize> {\n+        let mut decoder = read::GzDecoder::new(input_buf);\n+        decoder.read_to_end(output_buf).map_err(|e| e.into())\n+    }\n+\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()> {\n+        let mut encoder = write::GzEncoder::new(output_buf, Compression::default());\n+        encoder.write_all(input_buf)?;\n+        encoder.try_finish().map_err(|e| e.into())\n+    }\n+}\n+\n+const BROTLI_DEFAULT_BUFFER_SIZE: usize = 4096;\n+const BROTLI_DEFAULT_COMPRESSION_QUALITY: u32 = 1; // supported levels 0-9\n+const BROTLI_DEFAULT_LG_WINDOW_SIZE: u32 = 22; // recommended between 20-22\n+\n+/// Codec for Brotli compression algorithm.\n+pub struct BrotliCodec {}\n+\n+impl BrotliCodec {\n+    /// Creates new Brotli compression codec.\n+    fn new() -> Self {\n+        Self {}\n+    }\n+}\n+\n+impl Codec for BrotliCodec {\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize> {\n+        brotli::Decompressor::new(input_buf, BROTLI_DEFAULT_BUFFER_SIZE)\n+            .read_to_end(output_buf)\n+            .map_err(|e| e.into())\n+    }\n+\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()> {\n+        let mut encoder = brotli::CompressorWriter::new(\n+            output_buf,\n+            BROTLI_DEFAULT_BUFFER_SIZE,\n+            BROTLI_DEFAULT_COMPRESSION_QUALITY,\n+            BROTLI_DEFAULT_LG_WINDOW_SIZE,\n+        );\n+        encoder.write_all(&input_buf[..])?;\n+        encoder.flush().map_err(|e| e.into())\n+    }\n+}\n+\n+const LZ4_BUFFER_SIZE: usize = 4096;\n+\n+/// Codec for LZ4 compression algorithm.\n+pub struct LZ4Codec {}\n+\n+impl LZ4Codec {\n+    /// Creates new LZ4 compression codec.\n+    fn new() -> Self {\n+        Self {}\n+    }\n+}\n+\n+impl Codec for LZ4Codec {\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize> {\n+        let mut decoder = lz4::Decoder::new(input_buf)?;\n+        let mut buffer: [u8; LZ4_BUFFER_SIZE] = [0; LZ4_BUFFER_SIZE];\n+        let mut total_len = 0;\n+        loop {\n+            let len = decoder.read(&mut buffer)?;\n+            if len == 0 {\n+                break;\n+            }\n+            total_len += len;\n+            output_buf.write_all(&buffer[0..len])?;\n+        }\n+        Ok(total_len)\n+    }\n+\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()> {\n+        let mut encoder = lz4::EncoderBuilder::new().build(output_buf)?;\n+        let mut from = 0;\n+        loop {\n+            let to = ::std::cmp::min(from + LZ4_BUFFER_SIZE, input_buf.len());\n+            encoder.write_all(&input_buf[from..to])?;\n+            from += LZ4_BUFFER_SIZE;\n+            if from >= input_buf.len() {\n+                break;\n+            }\n+        }\n+        encoder.finish().1.map_err(|e| e.into())\n+    }\n+}\n+\n+/// Codec for Zstandard compression algorithm.\n+pub struct ZSTDCodec {}\n+\n+impl ZSTDCodec {\n+    /// Creates new Zstandard compression codec.\n+    fn new() -> Self {\n+        Self {}\n+    }\n+}\n+\n+/// Compression level (1-21) for ZSTD. Choose 1 here for better compression speed.\n+const ZSTD_COMPRESSION_LEVEL: i32 = 1;\n+\n+impl Codec for ZSTDCodec {\n+    fn decompress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<usize> {\n+        let mut decoder = zstd::Decoder::new(input_buf)?;\n+        match io::copy(&mut decoder, output_buf) {\n+            Ok(n) => Ok(n as usize),\n+            Err(e) => Err(e.into()),\n+        }\n+    }\n+\n+    fn compress(&mut self, input_buf: &[u8], output_buf: &mut Vec<u8>) -> Result<()> {\n+        let mut encoder = zstd::Encoder::new(output_buf, ZSTD_COMPRESSION_LEVEL)?;\n+        encoder.write_all(&input_buf[..])?;\n+        match encoder.finish() {\n+            Ok(_) => Ok(()),\n+            Err(e) => Err(e.into()),\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use crate::parquet::util::test_common::*;\n+\n+    fn test_roundtrip(c: CodecType, data: &Vec<u8>) {\n+        let mut c1 = create_codec(c).unwrap().unwrap();\n+        let mut c2 = create_codec(c).unwrap().unwrap();\n+\n+        // Compress with c1\n+        let mut compressed = Vec::new();\n+        let mut decompressed = Vec::new();\n+        c1.compress(data.as_slice(), &mut compressed)\n+            .expect(\"Error when compressing\");\n+\n+        // Decompress with c2\n+        let mut decompressed_size = c2\n+            .decompress(compressed.as_slice(), &mut decompressed)\n+            .expect(\"Error when decompressing\");\n+        assert_eq!(data.len(), decompressed_size);\n+        decompressed.truncate(decompressed_size);\n+        assert_eq!(*data, decompressed);\n+\n+        compressed.clear();\n+\n+        // Compress with c2\n+        c2.compress(data.as_slice(), &mut compressed)\n+            .expect(\"Error when compressing\");\n+\n+        // Decompress with c1\n+        decompressed_size = c1\n+            .decompress(compressed.as_slice(), &mut decompressed)\n+            .expect(\"Error when decompressing\");\n+        assert_eq!(data.len(), decompressed_size);\n+        decompressed.truncate(decompressed_size);\n+        assert_eq!(*data, decompressed);\n+    }\n+\n+    fn test_codec(c: CodecType) {\n+        let sizes = vec![100, 10000, 100000];\n+        for size in sizes {\n+            let mut data = random_bytes(size);\n+            test_roundtrip(c, &mut data);\n+        }\n+    }\n+\n+    #[test]\n+    fn test_codec_snappy() {\n+        test_codec(CodecType::SNAPPY);\n+    }\n+\n+    #[test]\n+    fn test_codec_gzip() {\n+        test_codec(CodecType::GZIP);\n+    }\n+\n+    #[test]\n+    fn test_codec_brotli() {\n+        test_codec(CodecType::BROTLI);\n+    }\n+\n+    #[test]\n+    fn test_codec_lz4() {\n+        test_codec(CodecType::LZ4);\n+    }\n+\n+    #[test]\n+    fn test_codec_zstd() {\n+        test_codec(CodecType::ZSTD);\n+    }\n+\n+}\ndiff --git a/rust/src/parquet/data_type.rs b/rust/src/parquet/data_type.rs\nnew file mode 100644\nindex 0000000000..26bdebd71b\n--- /dev/null\n+++ b/rust/src/parquet/data_type.rs\n@@ -0,0 +1,463 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Data types that connect Parquet physical types with their Rust-specific\n+//! representations.\n+\n+use std::mem;\n+\n+use byteorder::{BigEndian, ByteOrder};\n+\n+use crate::parquet::basic::Type;\n+use crate::parquet::util::memory::{ByteBuffer, ByteBufferPtr};\n+\n+/// Rust representation for logical type INT96, value is backed by an array of `u32`.\n+/// The type only takes 12 bytes, without extra padding.\n+#[derive(Clone, Debug)]\n+pub struct Int96 {\n+    value: Option<[u32; 3]>,\n+}\n+\n+impl Int96 {\n+    /// Creates new INT96 type struct with no data set.\n+    pub fn new() -> Self {\n+        Self { value: None }\n+    }\n+\n+    /// Returns underlying data as slice of [`u32`].\n+    pub fn data(&self) -> &[u32] {\n+        assert!(self.value.is_some());\n+        self.value.as_ref().unwrap()\n+    }\n+\n+    /// Sets data for this INT96 type.\n+    pub fn set_data(&mut self, elem0: u32, elem1: u32, elem2: u32) {\n+        self.value = Some([elem0, elem1, elem2]);\n+    }\n+}\n+\n+impl Default for Int96 {\n+    fn default() -> Self {\n+        Self { value: None }\n+    }\n+}\n+\n+impl PartialEq for Int96 {\n+    fn eq(&self, other: &Int96) -> bool {\n+        self.data() == other.data()\n+    }\n+}\n+\n+impl From<Vec<u32>> for Int96 {\n+    fn from(buf: Vec<u32>) -> Self {\n+        assert_eq!(buf.len(), 3);\n+        let mut result = Self::new();\n+        result.set_data(buf[0], buf[1], buf[2]);\n+        result\n+    }\n+}\n+\n+/// Rust representation for BYTE_ARRAY and FIXED_LEN_BYTE_ARRAY Parquet physical types.\n+/// Value is backed by a byte buffer.\n+#[derive(Clone, Debug)]\n+pub struct ByteArray {\n+    data: Option<ByteBufferPtr>,\n+}\n+\n+impl ByteArray {\n+    /// Creates new byte array with no data set.\n+    pub fn new() -> Self {\n+        ByteArray { data: None }\n+    }\n+\n+    /// Gets length of the underlying byte buffer.\n+    pub fn len(&self) -> usize {\n+        assert!(self.data.is_some());\n+        self.data.as_ref().unwrap().len()\n+    }\n+\n+    /// Returns slice of data.\n+    pub fn data(&self) -> &[u8] {\n+        assert!(self.data.is_some());\n+        self.data.as_ref().unwrap().as_ref()\n+    }\n+\n+    /// Set data from another byte buffer.\n+    pub fn set_data(&mut self, data: ByteBufferPtr) {\n+        self.data = Some(data);\n+    }\n+\n+    /// Returns `ByteArray` instance with slice of values for a data.\n+    pub fn slice(&self, start: usize, len: usize) -> Self {\n+        assert!(self.data.is_some());\n+        Self::from(self.data.as_ref().unwrap().range(start, len))\n+    }\n+}\n+\n+impl From<Vec<u8>> for ByteArray {\n+    fn from(buf: Vec<u8>) -> ByteArray {\n+        Self {\n+            data: Some(ByteBufferPtr::new(buf)),\n+        }\n+    }\n+}\n+\n+impl<'a> From<&'a str> for ByteArray {\n+    fn from(s: &'a str) -> ByteArray {\n+        let mut v = Vec::new();\n+        v.extend_from_slice(s.as_bytes());\n+        Self {\n+            data: Some(ByteBufferPtr::new(v)),\n+        }\n+    }\n+}\n+\n+impl From<ByteBufferPtr> for ByteArray {\n+    fn from(ptr: ByteBufferPtr) -> ByteArray {\n+        Self { data: Some(ptr) }\n+    }\n+}\n+\n+impl From<ByteBuffer> for ByteArray {\n+    fn from(mut buf: ByteBuffer) -> ByteArray {\n+        Self {\n+            data: Some(buf.consume()),\n+        }\n+    }\n+}\n+\n+impl Default for ByteArray {\n+    fn default() -> Self {\n+        ByteArray { data: None }\n+    }\n+}\n+\n+impl PartialEq for ByteArray {\n+    fn eq(&self, other: &ByteArray) -> bool {\n+        self.data() == other.data()\n+    }\n+}\n+\n+/// Rust representation for Decimal values.\n+///\n+/// This is not a representation of Parquet physical type, but rather a wrapper for\n+/// DECIMAL logical type, and serves as container for raw parts of decimal values:\n+/// unscaled value in bytes, precision and scale.\n+#[derive(Clone, Debug)]\n+pub enum Decimal {\n+    /// Decimal backed by `i32`.\n+    Int32 {\n+        value: [u8; 4],\n+        precision: i32,\n+        scale: i32,\n+    },\n+    /// Decimal backed by `i64`.\n+    Int64 {\n+        value: [u8; 8],\n+        precision: i32,\n+        scale: i32,\n+    },\n+    /// Decimal backed by byte array.\n+    Bytes {\n+        value: ByteArray,\n+        precision: i32,\n+        scale: i32,\n+    },\n+}\n+\n+impl Decimal {\n+    /// Creates new decimal value from `i32`.\n+    pub fn from_i32(value: i32, precision: i32, scale: i32) -> Self {\n+        let mut bytes = [0; 4];\n+        BigEndian::write_i32(&mut bytes, value);\n+        Decimal::Int32 {\n+            value: bytes,\n+            precision,\n+            scale,\n+        }\n+    }\n+\n+    /// Creates new decimal value from `i64`.\n+    pub fn from_i64(value: i64, precision: i32, scale: i32) -> Self {\n+        let mut bytes = [0; 8];\n+        BigEndian::write_i64(&mut bytes, value);\n+        Decimal::Int64 {\n+            value: bytes,\n+            precision,\n+            scale,\n+        }\n+    }\n+\n+    /// Creates new decimal value from `ByteArray`.\n+    pub fn from_bytes(value: ByteArray, precision: i32, scale: i32) -> Self {\n+        Decimal::Bytes {\n+            value,\n+            precision,\n+            scale,\n+        }\n+    }\n+\n+    /// Returns bytes of unscaled value.\n+    pub fn data(&self) -> &[u8] {\n+        match *self {\n+            Decimal::Int32 { ref value, .. } => value,\n+            Decimal::Int64 { ref value, .. } => value,\n+            Decimal::Bytes { ref value, .. } => value.data(),\n+        }\n+    }\n+\n+    /// Returns decimal precision.\n+    pub fn precision(&self) -> i32 {\n+        match *self {\n+            Decimal::Int32 { precision, .. } => precision,\n+            Decimal::Int64 { precision, .. } => precision,\n+            Decimal::Bytes { precision, .. } => precision,\n+        }\n+    }\n+\n+    /// Returns decimal scale.\n+    pub fn scale(&self) -> i32 {\n+        match *self {\n+            Decimal::Int32 { scale, .. } => scale,\n+            Decimal::Int64 { scale, .. } => scale,\n+            Decimal::Bytes { scale, .. } => scale,\n+        }\n+    }\n+}\n+\n+impl Default for Decimal {\n+    fn default() -> Self {\n+        Self::from_i32(0, 0, 0)\n+    }\n+}\n+\n+impl PartialEq for Decimal {\n+    fn eq(&self, other: &Decimal) -> bool {\n+        self.precision() == other.precision()\n+            && self.scale() == other.scale()\n+            && self.data() == other.data()\n+    }\n+}\n+\n+/// Converts an instance of data type to a slice of bytes as `u8`.\n+pub trait AsBytes {\n+    /// Returns slice of bytes for this data type.\n+    fn as_bytes(&self) -> &[u8];\n+}\n+\n+macro_rules! gen_as_bytes {\n+    ($source_ty:ident) => {\n+        impl AsBytes for $source_ty {\n+            fn as_bytes(&self) -> &[u8] {\n+                unsafe {\n+                    ::std::slice::from_raw_parts(\n+                        self as *const $source_ty as *const u8,\n+                        ::std::mem::size_of::<$source_ty>(),\n+                    )\n+                }\n+            }\n+        }\n+    };\n+}\n+\n+gen_as_bytes!(bool);\n+gen_as_bytes!(u8);\n+gen_as_bytes!(i32);\n+gen_as_bytes!(u32);\n+gen_as_bytes!(i64);\n+gen_as_bytes!(f32);\n+gen_as_bytes!(f64);\n+\n+impl AsBytes for Int96 {\n+    fn as_bytes(&self) -> &[u8] {\n+        unsafe { ::std::slice::from_raw_parts(self.data() as *const [u32] as *const u8, 12) }\n+    }\n+}\n+\n+impl AsBytes for ByteArray {\n+    fn as_bytes(&self) -> &[u8] {\n+        self.data()\n+    }\n+}\n+\n+impl AsBytes for Decimal {\n+    fn as_bytes(&self) -> &[u8] {\n+        self.data()\n+    }\n+}\n+\n+impl AsBytes for Vec<u8> {\n+    fn as_bytes(&self) -> &[u8] {\n+        self.as_slice()\n+    }\n+}\n+\n+impl<'a> AsBytes for &'a str {\n+    fn as_bytes(&self) -> &[u8] {\n+        (self as &str).as_bytes()\n+    }\n+}\n+\n+impl AsBytes for str {\n+    fn as_bytes(&self) -> &[u8] {\n+        (self as &str).as_bytes()\n+    }\n+}\n+\n+/// Contains the Parquet physical type information as well as the Rust primitive type\n+/// presentation.\n+pub trait DataType: 'static {\n+    type T: ::std::cmp::PartialEq\n+        + ::std::fmt::Debug\n+        + ::std::default::Default\n+        + ::std::clone::Clone\n+        + AsBytes;\n+\n+    /// Returns Parquet physical type.\n+    fn get_physical_type() -> Type;\n+\n+    /// Returns size in bytes for Rust representation of the physical type.\n+    fn get_type_size() -> usize;\n+}\n+\n+macro_rules! make_type {\n+    ($name:ident, $physical_ty:path, $native_ty:ty, $size:expr) => {\n+        pub struct $name {}\n+\n+        impl DataType for $name {\n+            type T = $native_ty;\n+\n+            fn get_physical_type() -> Type {\n+                $physical_ty\n+            }\n+\n+            fn get_type_size() -> usize {\n+                $size\n+            }\n+        }\n+    };\n+}\n+\n+/// Generate struct definitions for all physical types\n+\n+make_type!(BoolType, Type::BOOLEAN, bool, 1);\n+make_type!(Int32Type, Type::INT32, i32, 4);\n+make_type!(Int64Type, Type::INT64, i64, 8);\n+make_type!(Int96Type, Type::INT96, Int96, mem::size_of::<Int96>());\n+make_type!(FloatType, Type::FLOAT, f32, 4);\n+make_type!(DoubleType, Type::DOUBLE, f64, 8);\n+make_type!(\n+    ByteArrayType,\n+    Type::BYTE_ARRAY,\n+    ByteArray,\n+    mem::size_of::<ByteArray>()\n+);\n+make_type!(\n+    FixedLenByteArrayType,\n+    Type::FIXED_LEN_BYTE_ARRAY,\n+    ByteArray,\n+    mem::size_of::<ByteArray>()\n+);\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_as_bytes() {\n+        assert_eq!(false.as_bytes(), &[0]);\n+        assert_eq!(true.as_bytes(), &[1]);\n+        assert_eq!((7 as i32).as_bytes(), &[7, 0, 0, 0]);\n+        assert_eq!((555 as i32).as_bytes(), &[43, 2, 0, 0]);\n+        assert_eq!((555 as u32).as_bytes(), &[43, 2, 0, 0]);\n+        assert_eq!(i32::max_value().as_bytes(), &[255, 255, 255, 127]);\n+        assert_eq!(i32::min_value().as_bytes(), &[0, 0, 0, 128]);\n+        assert_eq!((7 as i64).as_bytes(), &[7, 0, 0, 0, 0, 0, 0, 0]);\n+        assert_eq!((555 as i64).as_bytes(), &[43, 2, 0, 0, 0, 0, 0, 0]);\n+        assert_eq!(\n+            (i64::max_value()).as_bytes(),\n+            &[255, 255, 255, 255, 255, 255, 255, 127]\n+        );\n+        assert_eq!((i64::min_value()).as_bytes(), &[0, 0, 0, 0, 0, 0, 0, 128]);\n+        assert_eq!((3.14 as f32).as_bytes(), &[195, 245, 72, 64]);\n+        assert_eq!(\n+            (3.14 as f64).as_bytes(),\n+            &[31, 133, 235, 81, 184, 30, 9, 64]\n+        );\n+        assert_eq!(\"hello\".as_bytes(), &[b'h', b'e', b'l', b'l', b'o']);\n+        assert_eq!(\n+            Vec::from(\"hello\".as_bytes()).as_bytes(),\n+            &[b'h', b'e', b'l', b'l', b'o']\n+        );\n+\n+        // Test Int96\n+        let i96 = Int96::from(vec![1, 2, 3]);\n+        assert_eq!(i96.as_bytes(), &[1, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0]);\n+\n+        // Test ByteArray\n+        let ba = ByteArray::from(vec![1, 2, 3]);\n+        assert_eq!(ba.as_bytes(), &[1, 2, 3]);\n+\n+        // Test Decimal\n+        let decimal = Decimal::from_i32(123, 5, 2);\n+        assert_eq!(decimal.as_bytes(), &[0, 0, 0, 123]);\n+        let decimal = Decimal::from_i64(123, 5, 2);\n+        assert_eq!(decimal.as_bytes(), &[0, 0, 0, 0, 0, 0, 0, 123]);\n+        let decimal = Decimal::from_bytes(ByteArray::from(vec![1, 2, 3]), 5, 2);\n+        assert_eq!(decimal.as_bytes(), &[1, 2, 3]);\n+    }\n+\n+    #[test]\n+    fn test_int96_from() {\n+        assert_eq!(\n+            Int96::from(vec![1, 12345, 1234567890]).data(),\n+            &[1, 12345, 1234567890]\n+        );\n+    }\n+\n+    #[test]\n+    fn test_byte_array_from() {\n+        assert_eq!(\n+            ByteArray::from(vec![b'A', b'B', b'C']).data(),\n+            &[b'A', b'B', b'C']\n+        );\n+        assert_eq!(ByteArray::from(\"ABC\").data(), &[b'A', b'B', b'C']);\n+        assert_eq!(\n+            ByteArray::from(ByteBufferPtr::new(vec![1u8, 2u8, 3u8, 4u8, 5u8])).data(),\n+            &[1u8, 2u8, 3u8, 4u8, 5u8]\n+        );\n+        let mut buf = ByteBuffer::new();\n+        buf.set_data(vec![6u8, 7u8, 8u8, 9u8, 10u8]);\n+        assert_eq!(ByteArray::from(buf).data(), &[6u8, 7u8, 8u8, 9u8, 10u8]);\n+    }\n+\n+    #[test]\n+    fn test_decimal_partial_eq() {\n+        assert_eq!(Decimal::default(), Decimal::from_i32(0, 0, 0));\n+        assert_eq!(Decimal::from_i32(222, 5, 2), Decimal::from_i32(222, 5, 2));\n+        assert_eq!(\n+            Decimal::from_bytes(ByteArray::from(vec![0, 0, 0, 3]), 5, 2),\n+            Decimal::from_i32(3, 5, 2)\n+        );\n+\n+        assert!(Decimal::from_i32(222, 5, 2) != Decimal::from_i32(111, 5, 2));\n+        assert!(Decimal::from_i32(222, 5, 2) != Decimal::from_i32(222, 6, 2));\n+        assert!(Decimal::from_i32(222, 5, 2) != Decimal::from_i32(222, 5, 3));\n+\n+        assert!(Decimal::from_i64(222, 5, 2) != Decimal::from_i32(222, 5, 2));\n+    }\n+}\ndiff --git a/rust/src/parquet/encodings/decoding.rs b/rust/src/parquet/encodings/decoding.rs\nnew file mode 100644\nindex 0000000000..c6a6fd49ee\n--- /dev/null\n+++ b/rust/src/parquet/encodings/decoding.rs\n@@ -0,0 +1,1403 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains all supported decoders for Parquet.\n+\n+use std::{cmp, marker::PhantomData, mem, slice::from_raw_parts_mut};\n+\n+use super::rle::RleDecoder;\n+\n+use byteorder::{ByteOrder, LittleEndian};\n+\n+use crate::parquet::basic::*;\n+use crate::parquet::data_type::*;\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::schema::types::ColumnDescPtr;\n+use crate::parquet::util::{\n+    bit_util::BitReader,\n+    memory::{ByteBuffer, ByteBufferPtr},\n+};\n+\n+// ----------------------------------------------------------------------\n+// Decoders\n+\n+/// A Parquet decoder for the data type `T`.\n+pub trait Decoder<T: DataType> {\n+    /// Sets the data to decode to be `data`, which should contain `num_values` of values\n+    /// to decode.\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()>;\n+\n+    /// Consumes values from this decoder and write the results to `buffer`. This will try\n+    /// to fill up `buffer`.\n+    ///\n+    /// Returns the actual number of values decoded, which should be equal to `buffer.len()`\n+    /// unless the remaining number of values is less than `buffer.len()`.\n+    fn get(&mut self, buffer: &mut [T::T]) -> Result<usize>;\n+\n+    /// Returns the number of values left in this decoder stream.\n+    fn values_left(&self) -> usize;\n+\n+    /// Returns the encoding for this decoder.\n+    fn encoding(&self) -> Encoding;\n+}\n+\n+/// Gets a decoder for the column descriptor `descr` and encoding type `encoding`.\n+///\n+/// NOTE: the primitive type in `descr` MUST match the data type `T`, otherwise\n+/// disastrous consequence could occur.\n+pub fn get_decoder<T: DataType>(\n+    descr: ColumnDescPtr,\n+    encoding: Encoding,\n+) -> Result<Box<Decoder<T>>> {\n+    let decoder: Box<Decoder<T>> = match encoding {\n+        Encoding::PLAIN => Box::new(PlainDecoder::new(descr.type_length())),\n+        Encoding::RLE_DICTIONARY | Encoding::PLAIN_DICTIONARY => {\n+            return Err(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            ));\n+        }\n+        Encoding::RLE => Box::new(RleValueDecoder::new()),\n+        Encoding::DELTA_BINARY_PACKED => Box::new(DeltaBitPackDecoder::new()),\n+        Encoding::DELTA_LENGTH_BYTE_ARRAY => Box::new(DeltaLengthByteArrayDecoder::new()),\n+        Encoding::DELTA_BYTE_ARRAY => Box::new(DeltaByteArrayDecoder::new()),\n+        e => return Err(nyi_err!(\"Encoding {} is not supported\", e)),\n+    };\n+    Ok(decoder)\n+}\n+\n+// ----------------------------------------------------------------------\n+// PLAIN Decoding\n+\n+/// Plain decoding that supports all types.\n+/// Values are encoded back to back. For native types, data is encoded as little endian.\n+/// Floating point types are encoded in IEEE.\n+/// See [`PlainDecoder`](`::encoding::PlainEncoder`) for more information.\n+pub struct PlainDecoder<T: DataType> {\n+    // The remaining number of values in the byte array\n+    num_values: usize,\n+\n+    // The current starting index in the byte array.\n+    start: usize,\n+\n+    // The length for the type `T`. Only used when `T` is `FixedLenByteArrayType`\n+    type_length: i32,\n+\n+    // The byte array to decode from. Not set if `T` is bool.\n+    data: Option<ByteBufferPtr>,\n+\n+    // Read `data` bit by bit. Only set if `T` is bool.\n+    bit_reader: Option<BitReader>,\n+\n+    // To allow `T` in the generic parameter for this struct. This doesn't take any space.\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> PlainDecoder<T> {\n+    /// Creates new plain decoder.\n+    pub fn new(type_length: i32) -> Self {\n+        PlainDecoder {\n+            data: None,\n+            bit_reader: None,\n+            type_length,\n+            num_values: 0,\n+            start: 0,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Decoder<T> for PlainDecoder<T> {\n+    #[inline]\n+    default fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        self.num_values = num_values;\n+        self.start = 0;\n+        self.data = Some(data);\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn values_left(&self) -> usize {\n+        self.num_values\n+    }\n+\n+    #[inline]\n+    fn encoding(&self) -> Encoding {\n+        Encoding::PLAIN\n+    }\n+\n+    #[inline]\n+    default fn get(&mut self, buffer: &mut [T::T]) -> Result<usize> {\n+        assert!(self.data.is_some());\n+\n+        let data = self.data.as_mut().unwrap();\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        let bytes_left = data.len() - self.start;\n+        let bytes_to_decode = mem::size_of::<T::T>() * num_values;\n+        if bytes_left < bytes_to_decode {\n+            return Err(eof_err!(\"Not enough bytes to decode\"));\n+        }\n+        let raw_buffer: &mut [u8] =\n+            unsafe { from_raw_parts_mut(buffer.as_ptr() as *mut u8, bytes_to_decode) };\n+        raw_buffer.copy_from_slice(data.range(self.start, bytes_to_decode).as_ref());\n+        self.start += bytes_to_decode;\n+        self.num_values -= num_values;\n+\n+        Ok(num_values)\n+    }\n+}\n+\n+impl Decoder<Int96Type> for PlainDecoder<Int96Type> {\n+    fn get(&mut self, buffer: &mut [Int96]) -> Result<usize> {\n+        assert!(self.data.is_some());\n+\n+        let data = self.data.as_ref().unwrap();\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        let bytes_left = data.len() - self.start;\n+        let bytes_to_decode = 12 * num_values;\n+        if bytes_left < bytes_to_decode {\n+            return Err(eof_err!(\"Not enough bytes to decode\"));\n+        }\n+\n+        let data_range = data.range(self.start, bytes_to_decode);\n+        let bytes: &[u8] = data_range.data();\n+        self.start += bytes_to_decode;\n+\n+        let mut pos = 0; // position in byte array\n+        for i in 0..num_values {\n+            let elem0 = LittleEndian::read_u32(&bytes[pos..pos + 4]);\n+            let elem1 = LittleEndian::read_u32(&bytes[pos + 4..pos + 8]);\n+            let elem2 = LittleEndian::read_u32(&bytes[pos + 8..pos + 12]);\n+            buffer[i].set_data(elem0, elem1, elem2);\n+            pos += 12;\n+        }\n+        self.num_values -= num_values;\n+\n+        Ok(num_values)\n+    }\n+}\n+\n+impl Decoder<BoolType> for PlainDecoder<BoolType> {\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        self.num_values = num_values;\n+        self.bit_reader = Some(BitReader::new(data));\n+        Ok(())\n+    }\n+\n+    fn get(&mut self, buffer: &mut [bool]) -> Result<usize> {\n+        assert!(self.bit_reader.is_some());\n+\n+        let bit_reader = self.bit_reader.as_mut().unwrap();\n+        let values_read = bit_reader.get_batch::<bool>(buffer, 1);\n+        self.num_values -= values_read;\n+\n+        Ok(values_read)\n+    }\n+}\n+\n+impl Decoder<ByteArrayType> for PlainDecoder<ByteArrayType> {\n+    fn get(&mut self, buffer: &mut [ByteArray]) -> Result<usize> {\n+        assert!(self.data.is_some());\n+\n+        let data = self.data.as_mut().unwrap();\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        for i in 0..num_values {\n+            let len: usize = read_num_bytes!(u32, 4, data.start_from(self.start).as_ref()) as usize;\n+            self.start += mem::size_of::<u32>();\n+            if data.len() < self.start + len {\n+                return Err(eof_err!(\"Not enough bytes to decode\"));\n+            }\n+            buffer[i].set_data(data.range(self.start, len));\n+            self.start += len;\n+        }\n+        self.num_values -= num_values;\n+\n+        Ok(num_values)\n+    }\n+}\n+\n+impl Decoder<FixedLenByteArrayType> for PlainDecoder<FixedLenByteArrayType> {\n+    fn get(&mut self, buffer: &mut [ByteArray]) -> Result<usize> {\n+        assert!(self.data.is_some());\n+        assert!(self.type_length > 0);\n+\n+        let data = self.data.as_mut().unwrap();\n+        let type_length = self.type_length as usize;\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        for i in 0..num_values {\n+            if data.len() < self.start + type_length {\n+                return Err(eof_err!(\"Not enough bytes to decode\"));\n+            }\n+            buffer[i].set_data(data.range(self.start, type_length));\n+            self.start += type_length;\n+        }\n+        self.num_values -= num_values;\n+\n+        Ok(num_values)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// RLE_DICTIONARY/PLAIN_DICTIONARY Decoding\n+\n+/// Dictionary decoder.\n+/// The dictionary encoding builds a dictionary of values encountered in a given column.\n+/// The dictionary is be stored in a dictionary page per column chunk.\n+/// See [`DictEncoder`](`::encoding::DictEncoder`) for more information.\n+pub struct DictDecoder<T: DataType> {\n+    // The dictionary, which maps ids to the values\n+    dictionary: Vec<T::T>,\n+\n+    // Whether `dictionary` has been initialized\n+    has_dictionary: bool,\n+\n+    // The decoder for the value ids\n+    rle_decoder: Option<RleDecoder>,\n+\n+    // Number of values left in the data stream\n+    num_values: usize,\n+}\n+\n+impl<T: DataType> DictDecoder<T> {\n+    /// Creates new dictionary decoder.\n+    pub fn new() -> Self {\n+        Self {\n+            dictionary: vec![],\n+            has_dictionary: false,\n+            rle_decoder: None,\n+            num_values: 0,\n+        }\n+    }\n+\n+    /// Decodes and sets values for dictionary using `decoder` decoder.\n+    pub fn set_dict(&mut self, mut decoder: Box<Decoder<T>>) -> Result<()> {\n+        let num_values = decoder.values_left();\n+        self.dictionary.resize(num_values, T::T::default());\n+        let _ = decoder.get(&mut self.dictionary)?;\n+        self.has_dictionary = true;\n+        Ok(())\n+    }\n+}\n+\n+impl<T: DataType> Decoder<T> for DictDecoder<T> {\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        // First byte in `data` is bit width\n+        let bit_width = data.as_ref()[0];\n+        let mut rle_decoder = RleDecoder::new(bit_width);\n+        rle_decoder.set_data(data.start_from(1));\n+        self.num_values = num_values;\n+        self.rle_decoder = Some(rle_decoder);\n+        Ok(())\n+    }\n+\n+    fn get(&mut self, buffer: &mut [T::T]) -> Result<usize> {\n+        assert!(self.rle_decoder.is_some());\n+        assert!(self.has_dictionary, \"Must call set_dict() first!\");\n+\n+        let rle = self.rle_decoder.as_mut().unwrap();\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        rle.get_batch_with_dict(&self.dictionary[..], buffer, num_values)\n+    }\n+\n+    /// Number of values left in this decoder stream\n+    fn values_left(&self) -> usize {\n+        self.num_values\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::RLE_DICTIONARY\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// RLE Decoding\n+\n+/// RLE/Bit-Packing hybrid decoding for values.\n+/// Currently is used only for data pages v2 and supports boolean types.\n+/// See [`RleValueEncoder`](`::encoding::RleValueEncoder`) for more information.\n+pub struct RleValueDecoder<T: DataType> {\n+    values_left: usize,\n+    decoder: Option<RleDecoder>,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> RleValueDecoder<T> {\n+    pub fn new() -> Self {\n+        Self {\n+            values_left: 0,\n+            decoder: None,\n+            _phantom: PhantomData,\n+        }\n+    }\n+\n+    #[inline]\n+    fn set_data_internal(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        // We still need to remove prefix of i32 from the stream.\n+        let i32_size = mem::size_of::<i32>();\n+        let data_size = read_num_bytes!(i32, i32_size, data.as_ref()) as usize;\n+        let rle_decoder = self\n+            .decoder\n+            .as_mut()\n+            .expect(\"RLE decoder is not initialized\");\n+        rle_decoder.set_data(data.range(i32_size, data_size));\n+        self.values_left = num_values;\n+        Ok(())\n+    }\n+}\n+\n+impl<T: DataType> Decoder<T> for RleValueDecoder<T> {\n+    #[inline]\n+    default fn set_data(&mut self, _data: ByteBufferPtr, _num_values: usize) -> Result<()> {\n+        panic!(\"RleValueDecoder only supports BoolType\");\n+    }\n+\n+    #[inline]\n+    fn values_left(&self) -> usize {\n+        self.values_left\n+    }\n+\n+    #[inline]\n+    fn encoding(&self) -> Encoding {\n+        Encoding::RLE\n+    }\n+\n+    #[inline]\n+    fn get(&mut self, buffer: &mut [T::T]) -> Result<usize> {\n+        let rle_decoder = self\n+            .decoder\n+            .as_mut()\n+            .expect(\"RLE decoder is not initialized\");\n+        let values_read = rle_decoder.get_batch(buffer)?;\n+        self.values_left -= values_read;\n+        Ok(values_read)\n+    }\n+}\n+\n+impl Decoder<BoolType> for RleValueDecoder<BoolType> {\n+    #[inline]\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        // Only support RLE value reader for boolean values with bit width of 1.\n+        self.decoder = Some(RleDecoder::new(1));\n+        self.set_data_internal(data, num_values)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_BINARY_PACKED Decoding\n+\n+/// Delta binary packed decoder.\n+/// Supports INT32 and INT64 types.\n+/// See [`DeltaBitPackEncoder`](`::encoding::DeltaBitPackEncoder`) for more information.\n+pub struct DeltaBitPackDecoder<T: DataType> {\n+    bit_reader: BitReader,\n+    initialized: bool,\n+\n+    // Header info\n+    num_values: usize,\n+    num_mini_blocks: i64,\n+    values_per_mini_block: usize,\n+    values_current_mini_block: usize,\n+    first_value: i64,\n+    first_value_read: bool,\n+\n+    // Per block info\n+    min_delta: i64,\n+    mini_block_idx: usize,\n+    delta_bit_width: u8,\n+    delta_bit_widths: ByteBuffer,\n+    deltas_in_mini_block: Vec<T::T>, // eagerly loaded deltas for a mini block\n+    use_batch: bool,\n+\n+    current_value: i64,\n+\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaBitPackDecoder<T> {\n+    /// Creates new delta bit packed decoder.\n+    pub fn new() -> Self {\n+        Self {\n+            bit_reader: BitReader::from(vec![]),\n+            initialized: false,\n+            num_values: 0,\n+            num_mini_blocks: 0,\n+            values_per_mini_block: 0,\n+            values_current_mini_block: 0,\n+            first_value: 0,\n+            first_value_read: false,\n+            min_delta: 0,\n+            mini_block_idx: 0,\n+            delta_bit_width: 0,\n+            delta_bit_widths: ByteBuffer::new(),\n+            deltas_in_mini_block: vec![],\n+            use_batch: mem::size_of::<T::T>() == 4,\n+            current_value: 0,\n+            _phantom: PhantomData,\n+        }\n+    }\n+\n+    /// Returns underlying bit reader offset.\n+    pub fn get_offset(&self) -> usize {\n+        assert!(self.initialized, \"Bit reader is not initialized\");\n+        self.bit_reader.get_byte_offset()\n+    }\n+\n+    /// Initializes new mini block.\n+    #[inline]\n+    fn init_block(&mut self) -> Result<()> {\n+        self.min_delta = self\n+            .bit_reader\n+            .get_zigzag_vlq_int()\n+            .ok_or(eof_err!(\"Not enough data to decode 'min_delta'\"))?;\n+\n+        let mut widths = vec![];\n+        for _ in 0..self.num_mini_blocks {\n+            let w = self\n+                .bit_reader\n+                .get_aligned::<u8>(1)\n+                .ok_or(eof_err!(\"Not enough data to decode 'width'\"))?;\n+            widths.push(w);\n+        }\n+\n+        self.delta_bit_widths.set_data(widths);\n+        self.mini_block_idx = 0;\n+        self.delta_bit_width = self.delta_bit_widths.data()[0];\n+        self.values_current_mini_block = self.values_per_mini_block;\n+        Ok(())\n+    }\n+\n+    /// Loads delta into mini block.\n+    #[inline]\n+    fn load_deltas_in_mini_block(&mut self) -> Result<()> {\n+        self.deltas_in_mini_block.clear();\n+        if self.use_batch {\n+            self.deltas_in_mini_block\n+                .resize(self.values_current_mini_block, T::T::default());\n+            let loaded = self.bit_reader.get_batch::<T::T>(\n+                &mut self.deltas_in_mini_block[..],\n+                self.delta_bit_width as usize,\n+            );\n+            assert!(loaded == self.values_current_mini_block);\n+        } else {\n+            for _ in 0..self.values_current_mini_block {\n+                // TODO: load one batch at a time similar to int32\n+                let delta = self\n+                    .bit_reader\n+                    .get_value::<T::T>(self.delta_bit_width as usize)\n+                    .ok_or(eof_err!(\"Not enough data to decode 'delta'\"))?;\n+                self.deltas_in_mini_block.push(delta);\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+}\n+\n+impl<T: DataType> Decoder<T> for DeltaBitPackDecoder<T> {\n+    // # of total values is derived from encoding\n+    #[inline]\n+    default fn set_data(&mut self, data: ByteBufferPtr, _: usize) -> Result<()> {\n+        self.bit_reader = BitReader::new(data);\n+        self.initialized = true;\n+\n+        let block_size = self\n+            .bit_reader\n+            .get_vlq_int()\n+            .ok_or(eof_err!(\"Not enough data to decode 'block_size'\"))?;\n+        self.num_mini_blocks = self\n+            .bit_reader\n+            .get_vlq_int()\n+            .ok_or(eof_err!(\"Not enough data to decode 'num_mini_blocks'\"))?;\n+        self.num_values =\n+            self.bit_reader\n+                .get_vlq_int()\n+                .ok_or(eof_err!(\"Not enough data to decode 'num_values'\"))? as usize;\n+        self.first_value = self\n+            .bit_reader\n+            .get_zigzag_vlq_int()\n+            .ok_or(eof_err!(\"Not enough data to decode 'first_value'\"))?;\n+\n+        // Reset decoding state\n+        self.first_value_read = false;\n+        self.mini_block_idx = 0;\n+        self.delta_bit_widths.clear();\n+        self.values_current_mini_block = 0;\n+\n+        self.values_per_mini_block = (block_size / self.num_mini_blocks) as usize;\n+        assert!(self.values_per_mini_block % 8 == 0);\n+\n+        Ok(())\n+    }\n+\n+    default fn get(&mut self, buffer: &mut [T::T]) -> Result<usize> {\n+        assert!(self.initialized, \"Bit reader is not initialized\");\n+\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        for i in 0..num_values {\n+            if !self.first_value_read {\n+                self.set_decoded_value(buffer, i, self.first_value);\n+                self.current_value = self.first_value;\n+                self.first_value_read = true;\n+                continue;\n+            }\n+\n+            if self.values_current_mini_block == 0 {\n+                self.mini_block_idx += 1;\n+                if self.mini_block_idx < self.delta_bit_widths.size() {\n+                    self.delta_bit_width = self.delta_bit_widths.data()[self.mini_block_idx];\n+                    self.values_current_mini_block = self.values_per_mini_block;\n+                } else {\n+                    self.init_block()?;\n+                }\n+                self.load_deltas_in_mini_block()?;\n+            }\n+\n+            // we decrement values in current mini block, so we need to invert index for delta\n+            let delta =\n+                self.get_delta(self.deltas_in_mini_block.len() - self.values_current_mini_block);\n+            // It is OK for deltas to contain \"overflowed\" values after encoding,\n+            // e.g. i64::MAX - i64::MIN, so we use `wrapping_add` to \"overflow\" again and\n+            // restore original value.\n+            self.current_value = self.current_value.wrapping_add(self.min_delta);\n+            self.current_value = self.current_value.wrapping_add(delta as i64);\n+            self.set_decoded_value(buffer, i, self.current_value);\n+            self.values_current_mini_block -= 1;\n+        }\n+\n+        self.num_values -= num_values;\n+        Ok(num_values)\n+    }\n+\n+    fn values_left(&self) -> usize {\n+        self.num_values\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_BINARY_PACKED\n+    }\n+}\n+\n+/// Helper trait to define specific conversions when decoding values\n+trait DeltaBitPackDecoderConversion<T: DataType> {\n+    /// Sets decoded value based on type `T`.\n+    #[inline]\n+    fn get_delta(&self, index: usize) -> i64;\n+\n+    #[inline]\n+    fn set_decoded_value(&self, buffer: &mut [T::T], index: usize, value: i64);\n+}\n+\n+impl<T: DataType> DeltaBitPackDecoderConversion<T> for DeltaBitPackDecoder<T> {\n+    #[inline]\n+    default fn get_delta(&self, _: usize) -> i64 {\n+        panic!(\"DeltaBitPackDecoder only supports Int32Type and Int64Type\")\n+    }\n+\n+    #[inline]\n+    default fn set_decoded_value(&self, _: &mut [T::T], _: usize, _: i64) {\n+        panic!(\"DeltaBitPackDecoder only supports Int32Type and Int64Type\")\n+    }\n+}\n+\n+impl DeltaBitPackDecoderConversion<Int32Type> for DeltaBitPackDecoder<Int32Type> {\n+    #[inline]\n+    fn get_delta(&self, index: usize) -> i64 {\n+        self.deltas_in_mini_block[index] as i64\n+    }\n+\n+    #[inline]\n+    fn set_decoded_value(&self, buffer: &mut [i32], index: usize, value: i64) {\n+        buffer[index] = value as i32;\n+    }\n+}\n+\n+impl DeltaBitPackDecoderConversion<Int64Type> for DeltaBitPackDecoder<Int64Type> {\n+    #[inline]\n+    fn get_delta(&self, index: usize) -> i64 {\n+        self.deltas_in_mini_block[index]\n+    }\n+\n+    #[inline]\n+    fn set_decoded_value(&self, buffer: &mut [i64], index: usize, value: i64) {\n+        buffer[index] = value;\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_LENGTH_BYTE_ARRAY Decoding\n+\n+/// Delta length byte array decoder.\n+/// Only applied to byte arrays to separate the length values and the data, the lengths\n+/// are encoded using DELTA_BINARY_PACKED encoding.\n+/// See [`DeltaLengthByteArrayEncoder`](`::encoding::DeltaLengthByteArrayEncoder`)\n+/// for more information.\n+pub struct DeltaLengthByteArrayDecoder<T: DataType> {\n+    // Lengths for each byte array in `data`\n+    // TODO: add memory tracker to this\n+    lengths: Vec<i32>,\n+\n+    // Current index into `lengths`\n+    current_idx: usize,\n+\n+    // Concatenated byte array data\n+    data: Option<ByteBufferPtr>,\n+\n+    // Offset into `data`, always point to the beginning of next byte array.\n+    offset: usize,\n+\n+    // Number of values left in this decoder stream\n+    num_values: usize,\n+\n+    // Placeholder to allow `T` as generic parameter\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaLengthByteArrayDecoder<T> {\n+    /// Creates new delta length byte array decoder.\n+    pub fn new() -> Self {\n+        Self {\n+            lengths: vec![],\n+            current_idx: 0,\n+            data: None,\n+            offset: 0,\n+            num_values: 0,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Decoder<T> for DeltaLengthByteArrayDecoder<T> {\n+    default fn set_data(&mut self, _: ByteBufferPtr, _: usize) -> Result<()> {\n+        Err(general_err!(\n+            \"DeltaLengthByteArrayDecoder only support ByteArrayType\"\n+        ))\n+    }\n+\n+    default fn get(&mut self, _: &mut [T::T]) -> Result<usize> {\n+        Err(general_err!(\n+            \"DeltaLengthByteArrayDecoder only support ByteArrayType\"\n+        ))\n+    }\n+\n+    fn values_left(&self) -> usize {\n+        self.num_values\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_LENGTH_BYTE_ARRAY\n+    }\n+}\n+\n+impl Decoder<ByteArrayType> for DeltaLengthByteArrayDecoder<ByteArrayType> {\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        let mut len_decoder = DeltaBitPackDecoder::<Int32Type>::new();\n+        len_decoder.set_data(data.all(), num_values)?;\n+        let num_lengths = len_decoder.values_left();\n+        self.lengths.resize(num_lengths, 0);\n+        len_decoder.get(&mut self.lengths[..])?;\n+\n+        self.data = Some(data.start_from(len_decoder.get_offset()));\n+        self.offset = 0;\n+        self.current_idx = 0;\n+        self.num_values = num_lengths;\n+        Ok(())\n+    }\n+\n+    fn get(&mut self, buffer: &mut [ByteArray]) -> Result<usize> {\n+        assert!(self.data.is_some());\n+\n+        let data = self.data.as_ref().unwrap();\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        for i in 0..num_values {\n+            let len = self.lengths[self.current_idx] as usize;\n+            buffer[i].set_data(data.range(self.offset, len));\n+            self.offset += len;\n+            self.current_idx += 1;\n+        }\n+\n+        self.num_values -= num_values;\n+        Ok(num_values)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_BYTE_ARRAY Decoding\n+\n+/// Delta byte array decoder.\n+/// Prefix lengths are encoded using `DELTA_BINARY_PACKED` encoding, Suffixes are stored\n+/// using `DELTA_LENGTH_BYTE_ARRAY` encoding.\n+/// See [`DeltaByteArrayEncoder`](`::encoding::DeltaByteArrayEncoder`) for more\n+/// information.\n+pub struct DeltaByteArrayDecoder<T: DataType> {\n+    // Prefix lengths for each byte array\n+    // TODO: add memory tracker to this\n+    prefix_lengths: Vec<i32>,\n+\n+    // The current index into `prefix_lengths`,\n+    current_idx: usize,\n+\n+    // Decoder for all suffixes, the # of which should be the same as\n+    // `prefix_lengths.len()`\n+    suffix_decoder: Option<DeltaLengthByteArrayDecoder<ByteArrayType>>,\n+\n+    // The last byte array, used to derive the current prefix\n+    previous_value: Vec<u8>,\n+\n+    // Number of values left\n+    num_values: usize,\n+\n+    // Placeholder to allow `T` as generic parameter\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaByteArrayDecoder<T> {\n+    /// Creates new delta byte array decoder.\n+    pub fn new() -> Self {\n+        Self {\n+            prefix_lengths: vec![],\n+            current_idx: 0,\n+            suffix_decoder: None,\n+            previous_value: vec![],\n+            num_values: 0,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<'m, T: DataType> Decoder<T> for DeltaByteArrayDecoder<T> {\n+    default fn set_data(&mut self, _: ByteBufferPtr, _: usize) -> Result<()> {\n+        Err(general_err!(\n+            \"DeltaByteArrayDecoder only supports ByteArrayType and FixedLenByteArrayType\"\n+        ))\n+    }\n+\n+    default fn get(&mut self, _: &mut [T::T]) -> Result<usize> {\n+        Err(general_err!(\n+            \"DeltaByteArrayDecoder only supports ByteArrayType and FixedLenByteArrayType\"\n+        ))\n+    }\n+\n+    fn values_left(&self) -> usize {\n+        self.num_values\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_BYTE_ARRAY\n+    }\n+}\n+\n+impl Decoder<ByteArrayType> for DeltaByteArrayDecoder<ByteArrayType> {\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        let mut prefix_len_decoder = DeltaBitPackDecoder::<Int32Type>::new();\n+        prefix_len_decoder.set_data(data.all(), num_values)?;\n+        let num_prefixes = prefix_len_decoder.values_left();\n+        self.prefix_lengths.resize(num_prefixes, 0);\n+        prefix_len_decoder.get(&mut self.prefix_lengths[..])?;\n+\n+        let mut suffix_decoder = DeltaLengthByteArrayDecoder::new();\n+        suffix_decoder.set_data(data.start_from(prefix_len_decoder.get_offset()), num_values)?;\n+        self.suffix_decoder = Some(suffix_decoder);\n+        self.num_values = num_prefixes;\n+        self.current_idx = 0;\n+        self.previous_value.clear();\n+        Ok(())\n+    }\n+\n+    fn get(&mut self, buffer: &mut [ByteArray]) -> Result<usize> {\n+        assert!(self.suffix_decoder.is_some());\n+\n+        let num_values = cmp::min(buffer.len(), self.num_values);\n+        let mut v: [ByteArray; 1] = [ByteArray::new(); 1];\n+        for i in 0..num_values {\n+            // Process suffix\n+            // TODO: this is awkward - maybe we should add a non-vectorized API?\n+            let suffix_decoder = self.suffix_decoder.as_mut().unwrap();\n+            suffix_decoder.get(&mut v[..])?;\n+            let suffix = v[0].data();\n+\n+            // Extract current prefix length, can be 0\n+            let prefix_len = self.prefix_lengths[self.current_idx] as usize;\n+\n+            // Concatenate prefix with suffix\n+            let mut result = Vec::new();\n+            result.extend_from_slice(&self.previous_value[0..prefix_len]);\n+            result.extend_from_slice(suffix);\n+\n+            let data = ByteBufferPtr::new(result.clone());\n+            buffer[i].set_data(data);\n+            self.previous_value = result;\n+            self.current_idx += 1;\n+        }\n+\n+        self.num_values -= num_values;\n+        Ok(num_values)\n+    }\n+}\n+\n+impl Decoder<FixedLenByteArrayType> for DeltaByteArrayDecoder<FixedLenByteArrayType> {\n+    fn set_data(&mut self, data: ByteBufferPtr, num_values: usize) -> Result<()> {\n+        let s: &mut DeltaByteArrayDecoder<ByteArrayType> = unsafe { mem::transmute(self) };\n+        s.set_data(data, num_values)\n+    }\n+\n+    fn get(&mut self, buffer: &mut [ByteArray]) -> Result<usize> {\n+        let s: &mut DeltaByteArrayDecoder<ByteArrayType> = unsafe { mem::transmute(self) };\n+        s.get(buffer)\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::{super::encoding::*, *};\n+\n+    use std::{mem, rc::Rc};\n+\n+    use crate::parquet::schema::types::{\n+        ColumnDescPtr, ColumnDescriptor, ColumnPath, Type as SchemaType,\n+    };\n+    use crate::parquet::util::{bit_util::set_array_bit, memory::MemTracker, test_common::RandGen};\n+\n+    #[test]\n+    fn test_get_decoders() {\n+        // supported encodings\n+        create_and_check_decoder::<Int32Type>(Encoding::PLAIN, None);\n+        create_and_check_decoder::<Int32Type>(Encoding::DELTA_BINARY_PACKED, None);\n+        create_and_check_decoder::<Int32Type>(Encoding::DELTA_LENGTH_BYTE_ARRAY, None);\n+        create_and_check_decoder::<Int32Type>(Encoding::DELTA_BYTE_ARRAY, None);\n+        create_and_check_decoder::<BoolType>(Encoding::RLE, None);\n+\n+        // error when initializing\n+        create_and_check_decoder::<Int32Type>(\n+            Encoding::RLE_DICTIONARY,\n+            Some(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            )),\n+        );\n+        create_and_check_decoder::<Int32Type>(\n+            Encoding::PLAIN_DICTIONARY,\n+            Some(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            )),\n+        );\n+\n+        // unsupported\n+        create_and_check_decoder::<Int32Type>(\n+            Encoding::BIT_PACKED,\n+            Some(nyi_err!(\"Encoding BIT_PACKED is not supported\")),\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_int32() {\n+        let data = vec![42, 18, 52];\n+        let data_bytes = Int32Type::to_byte_array(&data[..]);\n+        let mut buffer = vec![0; 3];\n+        test_plain_decode::<Int32Type>(\n+            ByteBufferPtr::new(data_bytes),\n+            3,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_int64() {\n+        let data = vec![42, 18, 52];\n+        let data_bytes = Int64Type::to_byte_array(&data[..]);\n+        let mut buffer = vec![0; 3];\n+        test_plain_decode::<Int64Type>(\n+            ByteBufferPtr::new(data_bytes),\n+            3,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_float() {\n+        let data = vec![3.14, 2.414, 12.51];\n+        let data_bytes = FloatType::to_byte_array(&data[..]);\n+        let mut buffer = vec![0.0; 3];\n+        test_plain_decode::<FloatType>(\n+            ByteBufferPtr::new(data_bytes),\n+            3,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_double() {\n+        let data = vec![3.14f64, 2.414f64, 12.51f64];\n+        let data_bytes = DoubleType::to_byte_array(&data[..]);\n+        let mut buffer = vec![0.0f64; 3];\n+        test_plain_decode::<DoubleType>(\n+            ByteBufferPtr::new(data_bytes),\n+            3,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_int96() {\n+        let mut data = vec![Int96::new(); 4];\n+        data[0].set_data(11, 22, 33);\n+        data[1].set_data(44, 55, 66);\n+        data[2].set_data(10, 20, 30);\n+        data[3].set_data(40, 50, 60);\n+        let data_bytes = Int96Type::to_byte_array(&data[..]);\n+        let mut buffer = vec![Int96::new(); 4];\n+        test_plain_decode::<Int96Type>(\n+            ByteBufferPtr::new(data_bytes),\n+            4,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_bool() {\n+        let data = vec![\n+            false, true, false, false, true, false, true, true, false, true,\n+        ];\n+        let data_bytes = BoolType::to_byte_array(&data[..]);\n+        let mut buffer = vec![false; 10];\n+        test_plain_decode::<BoolType>(\n+            ByteBufferPtr::new(data_bytes),\n+            10,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_byte_array() {\n+        let mut data = vec![ByteArray::new(); 2];\n+        data[0].set_data(ByteBufferPtr::new(String::from(\"hello\").into_bytes()));\n+        data[1].set_data(ByteBufferPtr::new(String::from(\"parquet\").into_bytes()));\n+        let data_bytes = ByteArrayType::to_byte_array(&data[..]);\n+        let mut buffer = vec![ByteArray::new(); 2];\n+        test_plain_decode::<ByteArrayType>(\n+            ByteBufferPtr::new(data_bytes),\n+            2,\n+            -1,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_plain_decode_fixed_len_byte_array() {\n+        let mut data = vec![ByteArray::default(); 3];\n+        data[0].set_data(ByteBufferPtr::new(String::from(\"bird\").into_bytes()));\n+        data[1].set_data(ByteBufferPtr::new(String::from(\"come\").into_bytes()));\n+        data[2].set_data(ByteBufferPtr::new(String::from(\"flow\").into_bytes()));\n+        let data_bytes = FixedLenByteArrayType::to_byte_array(&data[..]);\n+        let mut buffer = vec![ByteArray::default(); 3];\n+        test_plain_decode::<FixedLenByteArrayType>(\n+            ByteBufferPtr::new(data_bytes),\n+            3,\n+            4,\n+            &mut buffer[..],\n+            &data[..],\n+        );\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"RleValueEncoder only supports BoolType\")]\n+    fn test_rle_value_encode_int32_not_supported() {\n+        let mut encoder = RleValueEncoder::<Int32Type>::new();\n+        encoder.put(&vec![1, 2, 3, 4]).unwrap();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"RleValueDecoder only supports BoolType\")]\n+    fn test_rle_value_decode_int32_not_supported() {\n+        let mut decoder = RleValueDecoder::<Int32Type>::new();\n+        decoder\n+            .set_data(ByteBufferPtr::new(vec![5, 0, 0, 0]), 1)\n+            .unwrap();\n+    }\n+\n+    #[test]\n+    fn test_rle_value_decode_bool_decode() {\n+        // Test multiple 'put' calls on the same encoder\n+        let data = vec![\n+            BoolType::gen_vec(-1, 256),\n+            BoolType::gen_vec(-1, 257),\n+            BoolType::gen_vec(-1, 126),\n+        ];\n+        test_rle_value_decode::<BoolType>(data);\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Bit reader is not initialized\")]\n+    fn test_delta_bit_packed_not_initialized_offset() {\n+        // Fail if set_data() is not called before get_offset()\n+        let decoder = DeltaBitPackDecoder::<Int32Type>::new();\n+        decoder.get_offset();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Bit reader is not initialized\")]\n+    fn test_delta_bit_packed_not_initialized_get() {\n+        // Fail if set_data() is not called before get()\n+        let mut decoder = DeltaBitPackDecoder::<Int32Type>::new();\n+        let mut buffer = vec![];\n+        decoder.get(&mut buffer).unwrap();\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_empty() {\n+        let data = vec![vec![0; 0]];\n+        test_delta_bit_packed_decode::<Int32Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_repeat() {\n+        let block_data = vec![\n+            1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5,\n+            6, 7, 8,\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(vec![block_data]);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_uneven() {\n+        let block_data = vec![1, -2, 3, -4, 5, 6, 7, 8, 9, 10, 11];\n+        test_delta_bit_packed_decode::<Int32Type>(vec![block_data]);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_same_values() {\n+        let block_data = vec![\n+            127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127, 127,\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(vec![block_data]);\n+\n+        let block_data = vec![\n+            -127, -127, -127, -127, -127, -127, -127, -127, -127, -127, -127, -127, -127, -127,\n+            -127, -127,\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(vec![block_data]);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_min_max() {\n+        let block_data = vec![\n+            i32::min_value(),\n+            i32::max_value(),\n+            i32::min_value(),\n+            i32::max_value(),\n+            i32::min_value(),\n+            i32::max_value(),\n+            i32::min_value(),\n+            i32::max_value(),\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(vec![block_data]);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_multiple_blocks() {\n+        // Test multiple 'put' calls on the same encoder\n+        let data = vec![\n+            Int32Type::gen_vec(-1, 64),\n+            Int32Type::gen_vec(-1, 128),\n+            Int32Type::gen_vec(-1, 64),\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_data_across_blocks() {\n+        // Test multiple 'put' calls on the same encoder\n+        let data = vec![Int32Type::gen_vec(-1, 256), Int32Type::gen_vec(-1, 257)];\n+        test_delta_bit_packed_decode::<Int32Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int32_with_empty_blocks() {\n+        let data = vec![\n+            Int32Type::gen_vec(-1, 128),\n+            vec![0; 0],\n+            Int32Type::gen_vec(-1, 64),\n+        ];\n+        test_delta_bit_packed_decode::<Int32Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int64_empty() {\n+        let data = vec![vec![0; 0]];\n+        test_delta_bit_packed_decode::<Int64Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int64_min_max() {\n+        let block_data = vec![\n+            i64::min_value(),\n+            i64::max_value(),\n+            i64::min_value(),\n+            i64::max_value(),\n+            i64::min_value(),\n+            i64::max_value(),\n+            i64::min_value(),\n+            i64::max_value(),\n+        ];\n+        test_delta_bit_packed_decode::<Int64Type>(vec![block_data]);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_int64_multiple_blocks() {\n+        // Test multiple 'put' calls on the same encoder\n+        let data = vec![\n+            Int64Type::gen_vec(-1, 64),\n+            Int64Type::gen_vec(-1, 128),\n+            Int64Type::gen_vec(-1, 64),\n+        ];\n+        test_delta_bit_packed_decode::<Int64Type>(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_bit_packed_decoder_sample() {\n+        let data_bytes = vec![\n+            128, 1, 4, 3, 58, 28, 6, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n+            0, 0, 0, 0, 0, 0,\n+        ];\n+        let buffer = ByteBufferPtr::new(data_bytes);\n+        let mut decoder: DeltaBitPackDecoder<Int32Type> = DeltaBitPackDecoder::new();\n+        decoder.set_data(buffer, 3).unwrap();\n+        // check exact offsets, because when reading partial values we end up with\n+        // some data not being read from bit reader\n+        assert_eq!(decoder.get_offset(), 5);\n+        let mut result = vec![0, 0, 0];\n+        decoder.get(&mut result).unwrap();\n+        assert_eq!(decoder.get_offset(), 34);\n+        assert_eq!(result, vec![29, 43, 89]);\n+    }\n+\n+    #[test]\n+    fn test_delta_byte_array_same_arrays() {\n+        let data = vec![\n+            vec![ByteArray::from(vec![1, 2, 3, 4, 5, 6])],\n+            vec![\n+                ByteArray::from(vec![1, 2, 3, 4, 5, 6]),\n+                ByteArray::from(vec![1, 2, 3, 4, 5, 6]),\n+            ],\n+            vec![\n+                ByteArray::from(vec![1, 2, 3, 4, 5, 6]),\n+                ByteArray::from(vec![1, 2, 3, 4, 5, 6]),\n+            ],\n+        ];\n+        test_delta_byte_array_decode(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_byte_array_unique_arrays() {\n+        let data = vec![\n+            vec![ByteArray::from(vec![1])],\n+            vec![ByteArray::from(vec![2, 3]), ByteArray::from(vec![4, 5, 6])],\n+            vec![\n+                ByteArray::from(vec![7, 8]),\n+                ByteArray::from(vec![9, 0, 1, 2]),\n+            ],\n+        ];\n+        test_delta_byte_array_decode(data);\n+    }\n+\n+    #[test]\n+    fn test_delta_byte_array_single_array() {\n+        let data = vec![vec![ByteArray::from(vec![1, 2, 3, 4, 5, 6])]];\n+        test_delta_byte_array_decode(data);\n+    }\n+\n+    fn test_plain_decode<T: DataType>(\n+        data: ByteBufferPtr,\n+        num_values: usize,\n+        type_length: i32,\n+        buffer: &mut [T::T],\n+        expected: &[T::T],\n+    ) {\n+        let mut decoder: PlainDecoder<T> = PlainDecoder::new(type_length);\n+        let result = decoder.set_data(data, num_values);\n+        assert!(result.is_ok());\n+        let result = decoder.get(&mut buffer[..]);\n+        assert!(result.is_ok());\n+        assert_eq!(decoder.values_left(), 0);\n+        assert_eq!(buffer, expected);\n+    }\n+\n+    fn test_rle_value_decode<T: DataType>(data: Vec<Vec<T::T>>) {\n+        test_encode_decode::<T>(data, Encoding::RLE);\n+    }\n+\n+    fn test_delta_bit_packed_decode<T: DataType>(data: Vec<Vec<T::T>>) {\n+        test_encode_decode::<T>(data, Encoding::DELTA_BINARY_PACKED);\n+    }\n+\n+    fn test_delta_byte_array_decode(data: Vec<Vec<ByteArray>>) {\n+        test_encode_decode::<ByteArrayType>(data, Encoding::DELTA_BYTE_ARRAY);\n+    }\n+\n+    // Input data represents vector of data slices to write (test multiple `put()` calls)\n+    // For example,\n+    //   vec![vec![1, 2, 3]] invokes `put()` once and writes {1, 2, 3}\n+    //   vec![vec![1, 2], vec![3]] invokes `put()` twice and writes {1, 2, 3}\n+    fn test_encode_decode<T: DataType>(data: Vec<Vec<T::T>>, encoding: Encoding) {\n+        // Type length should not really matter for encode/decode test,\n+        // otherwise change it based on type\n+        let col_descr = create_test_col_desc_ptr(-1, T::get_physical_type());\n+\n+        // Encode data\n+        let mut encoder = get_encoder::<T>(col_descr.clone(), encoding, Rc::new(MemTracker::new()))\n+            .expect(\"get encoder\");\n+\n+        for v in &data[..] {\n+            encoder.put(&v[..]).expect(\"ok to encode\");\n+        }\n+        let bytes = encoder.flush_buffer().expect(\"ok to flush buffer\");\n+\n+        // Flatten expected data as contiguous array of values\n+        let expected: Vec<T::T> = data.iter().flat_map(|s| s.clone()).collect();\n+\n+        // Decode data and compare with original\n+        let mut decoder = get_decoder::<T>(col_descr.clone(), encoding).expect(\"get decoder\");\n+\n+        let mut result = vec![T::T::default(); expected.len()];\n+        decoder\n+            .set_data(bytes, expected.len())\n+            .expect(\"ok to set data\");\n+        let mut result_num_values = 0;\n+        while decoder.values_left() > 0 {\n+            result_num_values += decoder\n+                .get(&mut result[result_num_values..])\n+                .expect(\"ok to decode\");\n+        }\n+        assert_eq!(result_num_values, expected.len());\n+        assert_eq!(result, expected);\n+    }\n+\n+    fn create_and_check_decoder<T: DataType>(encoding: Encoding, err: Option<ParquetError>) {\n+        let descr = create_test_col_desc_ptr(-1, T::get_physical_type());\n+        let decoder = get_decoder::<T>(descr, encoding);\n+        match err {\n+            Some(parquet_error) => {\n+                assert!(decoder.is_err());\n+                assert_eq!(decoder.err().unwrap(), parquet_error);\n+            }\n+            None => {\n+                assert!(decoder.is_ok());\n+                assert_eq!(decoder.unwrap().encoding(), encoding);\n+            }\n+        }\n+    }\n+\n+    // Creates test column descriptor.\n+    fn create_test_col_desc_ptr(type_len: i32, t: Type) -> ColumnDescPtr {\n+        let ty = SchemaType::primitive_type_builder(\"t\", t)\n+            .with_length(type_len)\n+            .build()\n+            .unwrap();\n+        Rc::new(ColumnDescriptor::new(\n+            Rc::new(ty),\n+            None,\n+            0,\n+            0,\n+            ColumnPath::new(vec![]),\n+        ))\n+    }\n+\n+    fn usize_to_bytes(v: usize) -> [u8; 4] {\n+        unsafe { mem::transmute::<u32, [u8; 4]>(v as u32) }\n+    }\n+\n+    /// A util trait to convert slices of different types to byte arrays\n+    trait ToByteArray<T: DataType> {\n+        fn to_byte_array(data: &[T::T]) -> Vec<u8>;\n+    }\n+\n+    impl<T> ToByteArray<T> for T\n+    where\n+        T: DataType,\n+    {\n+        default fn to_byte_array(data: &[T::T]) -> Vec<u8> {\n+            let mut v = vec![];\n+            let type_len = ::std::mem::size_of::<T::T>();\n+            v.extend_from_slice(unsafe {\n+                ::std::slice::from_raw_parts(data.as_ptr() as *const u8, data.len() * type_len)\n+            });\n+            v\n+        }\n+    }\n+\n+    impl ToByteArray<BoolType> for BoolType {\n+        fn to_byte_array(data: &[bool]) -> Vec<u8> {\n+            let mut v = vec![];\n+            for i in 0..data.len() {\n+                if i % 8 == 0 {\n+                    v.push(0);\n+                }\n+                if data[i] {\n+                    set_array_bit(&mut v[..], i);\n+                }\n+            }\n+            v\n+        }\n+    }\n+\n+    impl ToByteArray<Int96Type> for Int96Type {\n+        fn to_byte_array(data: &[Int96]) -> Vec<u8> {\n+            let mut v = vec![];\n+            for d in data {\n+                unsafe {\n+                    let copy = ::std::slice::from_raw_parts(d.data().as_ptr() as *const u8, 12);\n+                    v.extend_from_slice(copy);\n+                };\n+            }\n+            v\n+        }\n+    }\n+\n+    impl ToByteArray<ByteArrayType> for ByteArrayType {\n+        fn to_byte_array(data: &[ByteArray]) -> Vec<u8> {\n+            let mut v = vec![];\n+            for d in data {\n+                let buf = d.data();\n+                let len = &usize_to_bytes(buf.len());\n+                v.extend_from_slice(len);\n+                v.extend(buf);\n+            }\n+            v\n+        }\n+    }\n+\n+    impl ToByteArray<FixedLenByteArrayType> for FixedLenByteArrayType {\n+        fn to_byte_array(data: &[ByteArray]) -> Vec<u8> {\n+            let mut v = vec![];\n+            for d in data {\n+                let buf = d.data();\n+                v.extend(buf);\n+            }\n+            v\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/encodings/encoding.rs b/rust/src/parquet/encodings/encoding.rs\nnew file mode 100644\nindex 0000000000..cecb03cb54\n--- /dev/null\n+++ b/rust/src/parquet/encodings/encoding.rs\n@@ -0,0 +1,1360 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains all supported encoders for Parquet.\n+\n+use std::{cmp, io::Write, marker::PhantomData, mem, slice};\n+\n+use crate::parquet::basic::*;\n+use crate::parquet::data_type::*;\n+use crate::parquet::encodings::rle::RleEncoder;\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::schema::types::ColumnDescPtr;\n+use crate::parquet::util::{\n+    bit_util::{log2, num_required_bits, BitWriter},\n+    hash_util,\n+    memory::{Buffer, ByteBuffer, ByteBufferPtr, MemTrackerPtr},\n+};\n+\n+// ----------------------------------------------------------------------\n+// Encoders\n+\n+/// An Parquet encoder for the data type `T`.\n+///\n+/// Currently this allocates internal buffers for the encoded values. After done putting\n+/// values, caller should call `flush_buffer()` to get an immutable buffer pointer.\n+pub trait Encoder<T: DataType> {\n+    /// Encodes data from `values`.\n+    fn put(&mut self, values: &[T::T]) -> Result<()>;\n+\n+    /// Returns the encoding type of this encoder.\n+    fn encoding(&self) -> Encoding;\n+\n+    /// Returns an estimate of the encoded data, in bytes.\n+    /// Method call must be O(1).\n+    fn estimated_data_encoded_size(&self) -> usize;\n+\n+    /// Flushes the underlying byte buffer that's being processed by this encoder, and\n+    /// return the immutable copy of it. This will also reset the internal state.\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr>;\n+}\n+\n+/// Gets a encoder for the particular data type `T` and encoding `encoding`. Memory usage\n+/// for the encoder instance is tracked by `mem_tracker`.\n+pub fn get_encoder<T: DataType>(\n+    desc: ColumnDescPtr,\n+    encoding: Encoding,\n+    mem_tracker: MemTrackerPtr,\n+) -> Result<Box<Encoder<T>>> {\n+    let encoder: Box<Encoder<T>> = match encoding {\n+        Encoding::PLAIN => Box::new(PlainEncoder::new(desc, mem_tracker, vec![])),\n+        Encoding::RLE_DICTIONARY | Encoding::PLAIN_DICTIONARY => {\n+            return Err(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            ));\n+        }\n+        Encoding::RLE => Box::new(RleValueEncoder::new()),\n+        Encoding::DELTA_BINARY_PACKED => Box::new(DeltaBitPackEncoder::new()),\n+        Encoding::DELTA_LENGTH_BYTE_ARRAY => Box::new(DeltaLengthByteArrayEncoder::new()),\n+        Encoding::DELTA_BYTE_ARRAY => Box::new(DeltaByteArrayEncoder::new()),\n+        e => return Err(nyi_err!(\"Encoding {} is not supported\", e)),\n+    };\n+    Ok(encoder)\n+}\n+\n+// ----------------------------------------------------------------------\n+// Plain encoding\n+\n+/// Plain encoding that supports all types.\n+/// Values are encoded back to back.\n+/// The plain encoding is used whenever a more efficient encoding can not be used.\n+/// It stores the data in the following format:\n+/// - BOOLEAN - 1 bit per value, 0 is false; 1 is true.\n+/// - INT32 - 4 bytes per value, stored as little-endian.\n+/// - INT64 - 8 bytes per value, stored as little-endian.\n+/// - FLOAT - 4 bytes per value, stored as IEEE little-endian.\n+/// - DOUBLE - 8 bytes per value, stored as IEEE little-endian.\n+/// - BYTE_ARRAY - 4 byte length stored as little endian, followed by bytes.\n+/// - FIXED_LEN_BYTE_ARRAY - just the bytes are stored.\n+pub struct PlainEncoder<T: DataType> {\n+    buffer: ByteBuffer,\n+    bit_writer: BitWriter,\n+    desc: ColumnDescPtr,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> PlainEncoder<T> {\n+    /// Creates new plain encoder.\n+    pub fn new(desc: ColumnDescPtr, mem_tracker: MemTrackerPtr, vec: Vec<u8>) -> Self {\n+        let mut byte_buffer = ByteBuffer::new().with_mem_tracker(mem_tracker);\n+        byte_buffer.set_data(vec);\n+        Self {\n+            buffer: byte_buffer,\n+            bit_writer: BitWriter::new(256),\n+            desc,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Encoder<T> for PlainEncoder<T> {\n+    default fn put(&mut self, values: &[T::T]) -> Result<()> {\n+        let bytes = unsafe {\n+            slice::from_raw_parts(\n+                values as *const [T::T] as *const u8,\n+                mem::size_of::<T::T>() * values.len(),\n+            )\n+        };\n+        self.buffer.write(bytes)?;\n+        Ok(())\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::PLAIN\n+    }\n+\n+    fn estimated_data_encoded_size(&self) -> usize {\n+        self.buffer.size() + self.bit_writer.bytes_written()\n+    }\n+\n+    #[inline]\n+    default fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        self.buffer.write(self.bit_writer.flush_buffer())?;\n+        self.buffer.flush()?;\n+        self.bit_writer.clear();\n+\n+        Ok(self.buffer.consume())\n+    }\n+}\n+\n+impl Encoder<BoolType> for PlainEncoder<BoolType> {\n+    fn put(&mut self, values: &[bool]) -> Result<()> {\n+        for v in values {\n+            self.bit_writer.put_value(*v as u64, 1);\n+        }\n+        Ok(())\n+    }\n+}\n+\n+impl Encoder<Int96Type> for PlainEncoder<Int96Type> {\n+    fn put(&mut self, values: &[Int96]) -> Result<()> {\n+        for v in values {\n+            self.buffer.write(v.as_bytes())?;\n+        }\n+        self.buffer.flush()?;\n+        Ok(())\n+    }\n+}\n+\n+impl Encoder<ByteArrayType> for PlainEncoder<ByteArrayType> {\n+    fn put(&mut self, values: &[ByteArray]) -> Result<()> {\n+        for v in values {\n+            self.buffer.write(&(v.len().to_le() as u32).as_bytes())?;\n+            self.buffer.write(v.data())?;\n+        }\n+        self.buffer.flush()?;\n+        Ok(())\n+    }\n+}\n+\n+impl Encoder<FixedLenByteArrayType> for PlainEncoder<FixedLenByteArrayType> {\n+    fn put(&mut self, values: &[ByteArray]) -> Result<()> {\n+        for v in values {\n+            self.buffer.write(v.data())?;\n+        }\n+        self.buffer.flush()?;\n+        Ok(())\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// Dictionary encoding\n+\n+const INITIAL_HASH_TABLE_SIZE: usize = 1024;\n+const MAX_HASH_LOAD: f32 = 0.7;\n+const HASH_SLOT_EMPTY: i32 = -1;\n+\n+/// Dictionary encoder.\n+/// The dictionary encoding builds a dictionary of values encountered in a given column.\n+/// The dictionary page is written first, before the data pages of the column chunk.\n+///\n+/// Dictionary page format: the entries in the dictionary - in dictionary order -\n+/// using the plain encoding.\n+///\n+/// Data page format: the bit width used to encode the entry ids stored as 1 byte\n+/// (max bit width = 32), followed by the values encoded using RLE/Bit packed described\n+/// above (with the given bit width).\n+pub struct DictEncoder<T: DataType> {\n+    // Descriptor for the column to be encoded.\n+    desc: ColumnDescPtr,\n+\n+    // Size of the table. **Must be** a power of 2.\n+    hash_table_size: usize,\n+\n+    // Store `hash_table_size` - 1, so that `j & mod_bitmask` is equivalent to\n+    // `j % hash_table_size`, but uses far fewer CPU cycles.\n+    mod_bitmask: u32,\n+\n+    // Stores indices which map (many-to-one) to the values in the `uniques` array.\n+    // Here we are using fix-sized array with linear probing.\n+    // A slot with `HASH_SLOT_EMPTY` indicates the slot is not currently occupied.\n+    hash_slots: Buffer<i32>,\n+\n+    // Indices that have not yet be written out by `write_indices()`.\n+    buffered_indices: Buffer<i32>,\n+\n+    // The unique observed values.\n+    uniques: Buffer<T::T>,\n+\n+    // Size in bytes needed to encode this dictionary.\n+    uniques_size_in_bytes: usize,\n+\n+    // Tracking memory usage for the various data structures in this struct.\n+    mem_tracker: MemTrackerPtr,\n+}\n+\n+impl<T: DataType> DictEncoder<T> {\n+    /// Creates new dictionary encoder.\n+    pub fn new(desc: ColumnDescPtr, mem_tracker: MemTrackerPtr) -> Self {\n+        let mut slots = Buffer::new().with_mem_tracker(mem_tracker.clone());\n+        slots.resize(INITIAL_HASH_TABLE_SIZE, -1);\n+        Self {\n+            desc,\n+            hash_table_size: INITIAL_HASH_TABLE_SIZE,\n+            mod_bitmask: (INITIAL_HASH_TABLE_SIZE - 1) as u32,\n+            hash_slots: slots,\n+            buffered_indices: Buffer::new().with_mem_tracker(mem_tracker.clone()),\n+            uniques: Buffer::new().with_mem_tracker(mem_tracker.clone()),\n+            uniques_size_in_bytes: 0,\n+            mem_tracker,\n+        }\n+    }\n+\n+    /// Returns true if dictionary entries are sorted, false otherwise.\n+    #[inline]\n+    pub fn is_sorted(&self) -> bool {\n+        // Sorting is not supported currently.\n+        false\n+    }\n+\n+    /// Returns number of unique values (keys) in the dictionary.\n+    pub fn num_entries(&self) -> usize {\n+        self.uniques.size()\n+    }\n+\n+    /// Returns size of unique values (keys) in the dictionary, in bytes.\n+    pub fn dict_encoded_size(&self) -> usize {\n+        self.uniques_size_in_bytes\n+    }\n+\n+    /// Writes out the dictionary values with PLAIN encoding in a byte buffer, and return\n+    /// the result.\n+    #[inline]\n+    pub fn write_dict(&self) -> Result<ByteBufferPtr> {\n+        let mut plain_encoder =\n+            PlainEncoder::<T>::new(self.desc.clone(), self.mem_tracker.clone(), vec![]);\n+        plain_encoder.put(self.uniques.data())?;\n+        plain_encoder.flush_buffer()\n+    }\n+\n+    /// Writes out the dictionary values with RLE encoding in a byte buffer, and return the\n+    /// result.\n+    #[inline]\n+    pub fn write_indices(&mut self) -> Result<ByteBufferPtr> {\n+        // TODO: the caller should allocate the buffer\n+        let buffer_len = self.estimated_data_encoded_size();\n+        let mut buffer: Vec<u8> = vec![0; buffer_len as usize];\n+        buffer[0] = self.bit_width() as u8;\n+        self.mem_tracker.alloc(buffer.capacity() as i64);\n+\n+        // Write bit width in the first byte\n+        buffer.write((self.bit_width() as u8).as_bytes())?;\n+        let mut encoder = RleEncoder::new_from_buf(self.bit_width(), buffer, 1);\n+        for index in self.buffered_indices.data() {\n+            if !encoder.put(*index as u64)? {\n+                return Err(general_err!(\"Encoder doesn't have enough space\"));\n+            }\n+        }\n+        self.buffered_indices.clear();\n+        Ok(ByteBufferPtr::new(encoder.consume()?))\n+    }\n+\n+    #[inline]\n+    fn put_one(&mut self, value: &T::T) -> Result<()> {\n+        let mut j = (hash_util::hash(value, 0) & self.mod_bitmask) as usize;\n+        let mut index = self.hash_slots[j];\n+\n+        while index != HASH_SLOT_EMPTY && self.uniques[index as usize] != *value {\n+            j += 1;\n+            if j == self.hash_table_size {\n+                j = 0;\n+            }\n+            index = self.hash_slots[j];\n+        }\n+\n+        if index == HASH_SLOT_EMPTY {\n+            index = self.uniques.size() as i32;\n+            self.hash_slots[j] = index;\n+            self.add_dict_key(value.clone());\n+\n+            if self.uniques.size() > (self.hash_table_size as f32 * MAX_HASH_LOAD) as usize {\n+                self.double_table_size();\n+            }\n+        }\n+\n+        self.buffered_indices.push(index);\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn add_dict_key(&mut self, value: T::T) {\n+        self.uniques_size_in_bytes += self.get_encoded_size(&value);\n+        self.uniques.push(value);\n+    }\n+\n+    #[inline]\n+    fn bit_width(&self) -> u8 {\n+        let num_entries = self.uniques.size();\n+        if num_entries == 0 {\n+            0\n+        } else if num_entries == 1 {\n+            1\n+        } else {\n+            log2(num_entries as u64) as u8\n+        }\n+    }\n+\n+    #[inline]\n+    fn double_table_size(&mut self) {\n+        let new_size = self.hash_table_size * 2;\n+        let mut new_hash_slots = Buffer::new().with_mem_tracker(self.mem_tracker.clone());\n+        new_hash_slots.resize(new_size, HASH_SLOT_EMPTY);\n+        for i in 0..self.hash_table_size {\n+            let index = self.hash_slots[i];\n+            if index == HASH_SLOT_EMPTY {\n+                continue;\n+            }\n+            let value = &self.uniques[index as usize];\n+            let mut j = (hash_util::hash(value, 0) & ((new_size - 1) as u32)) as usize;\n+            let mut slot = new_hash_slots[j];\n+            while slot != HASH_SLOT_EMPTY && self.uniques[slot as usize] != *value {\n+                j += 1;\n+                if j == new_size {\n+                    j = 0;\n+                }\n+                slot = new_hash_slots[j];\n+            }\n+\n+            new_hash_slots[j] = index;\n+        }\n+\n+        self.hash_table_size = new_size;\n+        self.mod_bitmask = (new_size - 1) as u32;\n+        mem::replace(&mut self.hash_slots, new_hash_slots);\n+    }\n+}\n+\n+impl<T: DataType> Encoder<T> for DictEncoder<T> {\n+    #[inline]\n+    fn put(&mut self, values: &[T::T]) -> Result<()> {\n+        for i in values {\n+            self.put_one(&i)?\n+        }\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn encoding(&self) -> Encoding {\n+        Encoding::PLAIN_DICTIONARY\n+    }\n+\n+    #[inline]\n+    fn estimated_data_encoded_size(&self) -> usize {\n+        let bit_width = self.bit_width();\n+        1 + RleEncoder::min_buffer_size(bit_width)\n+            + RleEncoder::max_buffer_size(bit_width, self.buffered_indices.size())\n+    }\n+\n+    #[inline]\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        self.write_indices()\n+    }\n+}\n+\n+/// Provides encoded size for a data type.\n+/// This is a workaround to calculate dictionary size in bytes.\n+trait DictEncodedSize<T: DataType> {\n+    #[inline]\n+    fn get_encoded_size(&self, value: &T::T) -> usize;\n+}\n+\n+impl<T: DataType> DictEncodedSize<T> for DictEncoder<T> {\n+    #[inline]\n+    default fn get_encoded_size(&self, _: &T::T) -> usize {\n+        mem::size_of::<T::T>()\n+    }\n+}\n+\n+impl DictEncodedSize<ByteArrayType> for DictEncoder<ByteArrayType> {\n+    #[inline]\n+    fn get_encoded_size(&self, value: &ByteArray) -> usize {\n+        mem::size_of::<u32>() + value.len()\n+    }\n+}\n+\n+impl DictEncodedSize<FixedLenByteArrayType> for DictEncoder<FixedLenByteArrayType> {\n+    #[inline]\n+    fn get_encoded_size(&self, _value: &ByteArray) -> usize {\n+        self.desc.type_length() as usize\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// RLE encoding\n+\n+const DEFAULT_RLE_BUFFER_LEN: usize = 1024;\n+\n+/// RLE/Bit-Packing hybrid encoding for values.\n+/// Currently is used only for data pages v2 and supports boolean types.\n+pub struct RleValueEncoder<T: DataType> {\n+    // Buffer with raw values that we collect,\n+    // when flushing buffer they are encoded using RLE encoder\n+    encoder: Option<RleEncoder>,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> RleValueEncoder<T> {\n+    /// Creates new rle value encoder.\n+    pub fn new() -> Self {\n+        Self {\n+            encoder: None,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Encoder<T> for RleValueEncoder<T> {\n+    #[inline]\n+    default fn put(&mut self, _values: &[T::T]) -> Result<()> {\n+        panic!(\"RleValueEncoder only supports BoolType\");\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::RLE\n+    }\n+\n+    #[inline]\n+    default fn estimated_data_encoded_size(&self) -> usize {\n+        match self.encoder {\n+            Some(ref enc) => enc.len(),\n+            None => 0,\n+        }\n+    }\n+\n+    #[inline]\n+    default fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        panic!(\"RleValueEncoder only supports BoolType\");\n+    }\n+}\n+\n+impl Encoder<BoolType> for RleValueEncoder<BoolType> {\n+    #[inline]\n+    default fn put(&mut self, values: &[bool]) -> Result<()> {\n+        if self.encoder.is_none() {\n+            self.encoder = Some(RleEncoder::new(1, DEFAULT_RLE_BUFFER_LEN));\n+        }\n+        let rle_encoder = self.encoder.as_mut().unwrap();\n+        for value in values {\n+            if !rle_encoder.put(*value as u64)? {\n+                return Err(general_err!(\"RLE buffer is full\"));\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        assert!(\n+            self.encoder.is_some(),\n+            \"RLE value encoder is not initialized\"\n+        );\n+        let rle_encoder = self.encoder.as_mut().unwrap();\n+\n+        // Flush all encoder buffers and raw values\n+        let encoded_data = {\n+            let buf = rle_encoder.flush_buffer()?;\n+\n+            // Note that buf does not have any offset, all data is encoded bytes\n+            let len = (buf.len() as i32).to_le();\n+            let len_bytes = len.as_bytes();\n+            let mut encoded_data = Vec::new();\n+            encoded_data.extend_from_slice(len_bytes);\n+            encoded_data.extend_from_slice(buf);\n+            encoded_data\n+        };\n+        // Reset rle encoder for the next batch\n+        rle_encoder.clear();\n+\n+        Ok(ByteBufferPtr::new(encoded_data))\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_BINARY_PACKED encoding\n+\n+const MAX_PAGE_HEADER_WRITER_SIZE: usize = 32;\n+const MAX_BIT_WRITER_SIZE: usize = 10 * 1024 * 1024;\n+const DEFAULT_BLOCK_SIZE: usize = 128;\n+const DEFAULT_NUM_MINI_BLOCKS: usize = 4;\n+\n+/// Delta bit packed encoder.\n+/// Consists of a header followed by blocks of delta encoded values binary packed.\n+///\n+/// Delta-binary-packing:\n+/// ```shell\n+///   [page-header] [block 1], [block 2], ... [block N]\n+/// ```\n+///\n+/// Each page header consists of:\n+/// ```shell\n+///   [block size] [number of miniblocks in a block] [total value count] [first value]\n+/// ```\n+///\n+/// Each block consists of:\n+/// ```shell\n+///   [min delta] [list of bitwidths of miniblocks] [miniblocks]\n+/// ```\n+///\n+/// Current implementation writes values in `put` method, multiple calls to `put` to\n+/// existing block or start new block if block size is exceeded. Calling `flush_buffer`\n+/// writes out all data and resets internal state, including page header.\n+///\n+/// Supports only INT32 and INT64.\n+pub struct DeltaBitPackEncoder<T: DataType> {\n+    page_header_writer: BitWriter,\n+    bit_writer: BitWriter,\n+    total_values: usize,\n+    first_value: i64,\n+    current_value: i64,\n+    block_size: usize,\n+    mini_block_size: usize,\n+    num_mini_blocks: usize,\n+    values_in_block: usize,\n+    deltas: Vec<i64>,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaBitPackEncoder<T> {\n+    /// Creates new delta bit packed encoder.\n+    pub fn new() -> Self {\n+        let block_size = DEFAULT_BLOCK_SIZE;\n+        let num_mini_blocks = DEFAULT_NUM_MINI_BLOCKS;\n+        let mini_block_size = block_size / num_mini_blocks;\n+        assert!(mini_block_size % 8 == 0);\n+        Self::assert_supported_type();\n+\n+        DeltaBitPackEncoder {\n+            page_header_writer: BitWriter::new(MAX_PAGE_HEADER_WRITER_SIZE),\n+            bit_writer: BitWriter::new(MAX_BIT_WRITER_SIZE),\n+            total_values: 0,\n+            first_value: 0,\n+            current_value: 0, // current value to keep adding deltas\n+            block_size,       // can write fewer values than block size for last block\n+            mini_block_size,\n+            num_mini_blocks,\n+            values_in_block: 0, // will be at most block_size\n+            deltas: vec![0; block_size],\n+            _phantom: PhantomData,\n+        }\n+    }\n+\n+    /// Writes page header for blocks, this method is invoked when we are done encoding\n+    /// values. It is also okay to encode when no values have been provided\n+    fn write_page_header(&mut self) {\n+        // We ignore the result of each 'put' operation, because MAX_PAGE_HEADER_WRITER_SIZE\n+        // is chosen to fit all header values and guarantees that writes will not fail.\n+\n+        // Write the size of each block\n+        self.page_header_writer.put_vlq_int(self.block_size as u64);\n+        // Write the number of mini blocks\n+        self.page_header_writer\n+            .put_vlq_int(self.num_mini_blocks as u64);\n+        // Write the number of all values (including non-encoded first value)\n+        self.page_header_writer\n+            .put_vlq_int(self.total_values as u64);\n+        // Write first value\n+        self.page_header_writer.put_zigzag_vlq_int(self.first_value);\n+    }\n+\n+    // Write current delta buffer (<= 'block size' values) into bit writer\n+    fn flush_block_values(&mut self) -> Result<()> {\n+        if self.values_in_block == 0 {\n+            return Ok(());\n+        }\n+\n+        let mut min_delta = i64::max_value();\n+        for i in 0..self.values_in_block {\n+            min_delta = cmp::min(min_delta, self.deltas[i]);\n+        }\n+\n+        // Write min delta\n+        self.bit_writer.put_zigzag_vlq_int(min_delta);\n+\n+        // Slice to store bit width for each mini block\n+        // apply unsafe allocation to avoid double mutable borrow\n+        let mini_block_widths: &mut [u8] = unsafe {\n+            let tmp_slice = self.bit_writer.get_next_byte_ptr(self.num_mini_blocks)?;\n+            slice::from_raw_parts_mut(tmp_slice.as_ptr() as *mut u8, self.num_mini_blocks)\n+        };\n+\n+        for i in 0..self.num_mini_blocks {\n+            // Find how many values we need to encode - either block size or whatever values\n+            // left\n+            let n = cmp::min(self.mini_block_size, self.values_in_block);\n+            if n == 0 {\n+                break;\n+            }\n+\n+            // Compute the max delta in current mini block\n+            let mut max_delta = i64::min_value();\n+            for j in 0..n {\n+                max_delta = cmp::max(max_delta, self.deltas[i * self.mini_block_size + j]);\n+            }\n+\n+            // Compute bit width to store (max_delta - min_delta)\n+            let bit_width = num_required_bits(self.subtract_u64(max_delta, min_delta));\n+            mini_block_widths[i] = bit_width as u8;\n+\n+            // Encode values in current mini block using min_delta and bit_width\n+            for j in 0..n {\n+                let packed_value =\n+                    self.subtract_u64(self.deltas[i * self.mini_block_size + j], min_delta);\n+                self.bit_writer.put_value(packed_value, bit_width);\n+            }\n+\n+            // Pad the last block (n < mini_block_size)\n+            for _ in n..self.mini_block_size {\n+                self.bit_writer.put_value(0, bit_width);\n+            }\n+\n+            self.values_in_block -= n;\n+        }\n+\n+        assert!(\n+            self.values_in_block == 0,\n+            \"Expected 0 values in block, found {}\",\n+            self.values_in_block\n+        );\n+        Ok(())\n+    }\n+}\n+\n+// Implementation is shared between Int32Type and Int64Type,\n+// see `DeltaBitPackEncoderConversion` below for specifics.\n+impl<T: DataType> Encoder<T> for DeltaBitPackEncoder<T> {\n+    fn put(&mut self, values: &[T::T]) -> Result<()> {\n+        if values.is_empty() {\n+            return Ok(());\n+        }\n+\n+        let mut idx;\n+        // Define values to encode, initialize state\n+        if self.total_values == 0 {\n+            self.first_value = self.as_i64(values, 0);\n+            self.current_value = self.first_value;\n+            idx = 1;\n+        } else {\n+            idx = 0;\n+        }\n+        // Add all values (including first value)\n+        self.total_values += values.len();\n+\n+        // Write block\n+        while idx < values.len() {\n+            let value = self.as_i64(values, idx);\n+            self.deltas[self.values_in_block] = self.subtract(value, self.current_value);\n+            self.current_value = value;\n+            idx += 1;\n+            self.values_in_block += 1;\n+            if self.values_in_block == self.block_size {\n+                self.flush_block_values()?;\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_BINARY_PACKED\n+    }\n+\n+    fn estimated_data_encoded_size(&self) -> usize {\n+        self.bit_writer.bytes_written()\n+    }\n+\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        // Write remaining values\n+        self.flush_block_values()?;\n+        // Write page header with total values\n+        self.write_page_header();\n+\n+        let mut buffer = ByteBuffer::new();\n+        buffer.write(self.page_header_writer.flush_buffer())?;\n+        buffer.write(self.bit_writer.flush_buffer())?;\n+        buffer.flush()?;\n+\n+        // Reset state\n+        self.page_header_writer.clear();\n+        self.bit_writer.clear();\n+        self.total_values = 0;\n+        self.first_value = 0;\n+        self.current_value = 0;\n+        self.values_in_block = 0;\n+\n+        Ok(buffer.consume())\n+    }\n+}\n+\n+/// Helper trait to define specific conversions and subtractions when computing deltas\n+trait DeltaBitPackEncoderConversion<T: DataType> {\n+    // Method should panic if type is not supported, otherwise no-op\n+    #[inline]\n+    fn assert_supported_type();\n+\n+    #[inline]\n+    fn as_i64(&self, values: &[T::T], index: usize) -> i64;\n+\n+    #[inline]\n+    fn subtract(&self, left: i64, right: i64) -> i64;\n+\n+    #[inline]\n+    fn subtract_u64(&self, left: i64, right: i64) -> u64;\n+}\n+\n+impl<T: DataType> DeltaBitPackEncoderConversion<T> for DeltaBitPackEncoder<T> {\n+    #[inline]\n+    default fn assert_supported_type() {\n+        panic!(\"DeltaBitPackDecoder only supports Int32Type and Int64Type\");\n+    }\n+\n+    #[inline]\n+    default fn as_i64(&self, _values: &[T::T], _index: usize) -> i64 {\n+        0\n+    }\n+\n+    #[inline]\n+    default fn subtract(&self, _left: i64, _right: i64) -> i64 {\n+        0\n+    }\n+\n+    #[inline]\n+    default fn subtract_u64(&self, _left: i64, _right: i64) -> u64 {\n+        0\n+    }\n+}\n+\n+impl DeltaBitPackEncoderConversion<Int32Type> for DeltaBitPackEncoder<Int32Type> {\n+    #[inline]\n+    fn assert_supported_type() {\n+        // no-op: supported type\n+    }\n+\n+    #[inline]\n+    fn as_i64(&self, values: &[i32], index: usize) -> i64 {\n+        values[index] as i64\n+    }\n+\n+    #[inline]\n+    fn subtract(&self, left: i64, right: i64) -> i64 {\n+        // It is okay for values to overflow, wrapping_sub wrapping around at the boundary\n+        (left as i32).wrapping_sub(right as i32) as i64\n+    }\n+\n+    #[inline]\n+    fn subtract_u64(&self, left: i64, right: i64) -> u64 {\n+        // Conversion of i32 -> u32 -> u64 is to avoid non-zero left most bytes in int\n+        // representation\n+        (left as i32).wrapping_sub(right as i32) as u32 as u64\n+    }\n+}\n+\n+impl DeltaBitPackEncoderConversion<Int64Type> for DeltaBitPackEncoder<Int64Type> {\n+    #[inline]\n+    fn assert_supported_type() {\n+        // no-op: supported type\n+    }\n+\n+    #[inline]\n+    fn as_i64(&self, values: &[i64], index: usize) -> i64 {\n+        values[index]\n+    }\n+\n+    #[inline]\n+    fn subtract(&self, left: i64, right: i64) -> i64 {\n+        // It is okay for values to overflow, wrapping_sub wrapping around at the boundary\n+        left.wrapping_sub(right)\n+    }\n+\n+    #[inline]\n+    fn subtract_u64(&self, left: i64, right: i64) -> u64 {\n+        left.wrapping_sub(right) as u64\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_LENGTH_BYTE_ARRAY encoding\n+\n+/// Encoding for byte arrays to separate the length values and the data.\n+/// The lengths are encoded using DELTA_BINARY_PACKED encoding, data is\n+/// stored as raw bytes.\n+pub struct DeltaLengthByteArrayEncoder<T: DataType> {\n+    // length encoder\n+    len_encoder: DeltaBitPackEncoder<Int32Type>,\n+    // byte array data\n+    data: Vec<ByteArray>,\n+    // data size in bytes of encoded values\n+    encoded_size: usize,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaLengthByteArrayEncoder<T> {\n+    /// Creates new delta length byte array encoder.\n+    pub fn new() -> Self {\n+        Self {\n+            len_encoder: DeltaBitPackEncoder::new(),\n+            data: vec![],\n+            encoded_size: 0,\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Encoder<T> for DeltaLengthByteArrayEncoder<T> {\n+    default fn put(&mut self, _values: &[T::T]) -> Result<()> {\n+        panic!(\"DeltaLengthByteArrayEncoder only supports ByteArrayType\");\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_LENGTH_BYTE_ARRAY\n+    }\n+\n+    fn estimated_data_encoded_size(&self) -> usize {\n+        self.len_encoder.estimated_data_encoded_size() + self.encoded_size\n+    }\n+\n+    default fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        panic!(\"DeltaLengthByteArrayEncoder only supports ByteArrayType\");\n+    }\n+}\n+\n+impl Encoder<ByteArrayType> for DeltaLengthByteArrayEncoder<ByteArrayType> {\n+    fn put(&mut self, values: &[ByteArray]) -> Result<()> {\n+        let lengths: Vec<i32> = values\n+            .iter()\n+            .map(|byte_array| byte_array.len() as i32)\n+            .collect();\n+        self.len_encoder.put(&lengths)?;\n+        for byte_array in values {\n+            self.encoded_size += byte_array.len();\n+            self.data.push(byte_array.clone());\n+        }\n+        Ok(())\n+    }\n+\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        let mut total_bytes = vec![];\n+        let lengths = self.len_encoder.flush_buffer()?;\n+        total_bytes.extend_from_slice(lengths.data());\n+        self.data.iter().for_each(|byte_array| {\n+            total_bytes.extend_from_slice(byte_array.data());\n+        });\n+        self.data.clear();\n+        self.encoded_size = 0;\n+        Ok(ByteBufferPtr::new(total_bytes))\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// DELTA_BYTE_ARRAY encoding\n+\n+/// Encoding for byte arrays, prefix lengths are encoded using DELTA_BINARY_PACKED\n+/// encoding, followed by suffixes with DELTA_LENGTH_BYTE_ARRAY encoding.\n+pub struct DeltaByteArrayEncoder<T: DataType> {\n+    prefix_len_encoder: DeltaBitPackEncoder<Int32Type>,\n+    suffix_writer: DeltaLengthByteArrayEncoder<T>,\n+    previous: Vec<u8>,\n+    _phantom: PhantomData<T>,\n+}\n+\n+impl<T: DataType> DeltaByteArrayEncoder<T> {\n+    /// Creates new delta byte array encoder.\n+    pub fn new() -> Self {\n+        Self {\n+            prefix_len_encoder: DeltaBitPackEncoder::<Int32Type>::new(),\n+            suffix_writer: DeltaLengthByteArrayEncoder::<T>::new(),\n+            previous: vec![],\n+            _phantom: PhantomData,\n+        }\n+    }\n+}\n+\n+impl<T: DataType> Encoder<T> for DeltaByteArrayEncoder<T> {\n+    default fn put(&mut self, _values: &[T::T]) -> Result<()> {\n+        panic!(\"DeltaByteArrayEncoder only supports ByteArrayType and FixedLenByteArrayType\");\n+    }\n+\n+    fn encoding(&self) -> Encoding {\n+        Encoding::DELTA_BYTE_ARRAY\n+    }\n+\n+    fn estimated_data_encoded_size(&self) -> usize {\n+        self.prefix_len_encoder.estimated_data_encoded_size()\n+            + self.suffix_writer.estimated_data_encoded_size()\n+    }\n+\n+    default fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        panic!(\"DeltaByteArrayEncoder only supports ByteArrayType and FixedLenByteArrayType\");\n+    }\n+}\n+\n+impl Encoder<ByteArrayType> for DeltaByteArrayEncoder<ByteArrayType> {\n+    fn put(&mut self, values: &[ByteArray]) -> Result<()> {\n+        let mut prefix_lengths: Vec<i32> = vec![];\n+        let mut suffixes: Vec<ByteArray> = vec![];\n+\n+        for byte_array in values {\n+            let current = byte_array.data();\n+            // Maximum prefix length that is shared between previous value and current value\n+            let prefix_len = cmp::min(self.previous.len(), current.len());\n+            let mut match_len = 0;\n+            while match_len < prefix_len && self.previous[match_len] == current[match_len] {\n+                match_len += 1;\n+            }\n+            prefix_lengths.push(match_len as i32);\n+            suffixes.push(byte_array.slice(match_len, byte_array.len() - match_len));\n+            // Update previous for the next prefix\n+            self.previous.clear();\n+            self.previous.extend_from_slice(current);\n+        }\n+        self.prefix_len_encoder.put(&prefix_lengths)?;\n+        self.suffix_writer.put(&suffixes)?;\n+        Ok(())\n+    }\n+\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        // TODO: investigate if we can merge lengths and suffixes\n+        // without copying data into new vector.\n+        let mut total_bytes = vec![];\n+        // Insert lengths ...\n+        let lengths = self.prefix_len_encoder.flush_buffer()?;\n+        total_bytes.extend_from_slice(lengths.data());\n+        // ... followed by suffixes\n+        let suffixes = self.suffix_writer.flush_buffer()?;\n+        total_bytes.extend_from_slice(suffixes.data());\n+\n+        self.previous.clear();\n+        Ok(ByteBufferPtr::new(total_bytes))\n+    }\n+}\n+\n+impl Encoder<FixedLenByteArrayType> for DeltaByteArrayEncoder<FixedLenByteArrayType> {\n+    fn put(&mut self, values: &[ByteArray]) -> Result<()> {\n+        let s: &mut DeltaByteArrayEncoder<ByteArrayType> = unsafe { mem::transmute(self) };\n+        s.put(values)\n+    }\n+\n+    fn flush_buffer(&mut self) -> Result<ByteBufferPtr> {\n+        let s: &mut DeltaByteArrayEncoder<ByteArrayType> = unsafe { mem::transmute(self) };\n+        s.flush_buffer()\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use std::rc::Rc;\n+\n+    use crate::parquet::decoding::{get_decoder, Decoder, DictDecoder, PlainDecoder};\n+    use crate::parquet::schema::types::{\n+        ColumnDescPtr, ColumnDescriptor, ColumnPath, Type as SchemaType,\n+    };\n+    use crate::parquet::util::{memory::MemTracker, test_common::RandGen};\n+\n+    const TEST_SET_SIZE: usize = 1024;\n+\n+    #[test]\n+    fn test_get_encoders() {\n+        // supported encodings\n+        create_and_check_encoder::<Int32Type>(Encoding::PLAIN, None);\n+        create_and_check_encoder::<Int32Type>(Encoding::DELTA_BINARY_PACKED, None);\n+        create_and_check_encoder::<Int32Type>(Encoding::DELTA_LENGTH_BYTE_ARRAY, None);\n+        create_and_check_encoder::<Int32Type>(Encoding::DELTA_BYTE_ARRAY, None);\n+        create_and_check_encoder::<BoolType>(Encoding::RLE, None);\n+\n+        // error when initializing\n+        create_and_check_encoder::<Int32Type>(\n+            Encoding::RLE_DICTIONARY,\n+            Some(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            )),\n+        );\n+        create_and_check_encoder::<Int32Type>(\n+            Encoding::PLAIN_DICTIONARY,\n+            Some(general_err!(\n+                \"Cannot initialize this encoding through this function\"\n+            )),\n+        );\n+\n+        // unsupported\n+        create_and_check_encoder::<Int32Type>(\n+            Encoding::BIT_PACKED,\n+            Some(nyi_err!(\"Encoding BIT_PACKED is not supported\")),\n+        );\n+    }\n+\n+    #[test]\n+    fn test_bool() {\n+        BoolType::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        BoolType::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+        BoolType::test(Encoding::RLE, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_i32() {\n+        Int32Type::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        Int32Type::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+        Int32Type::test(Encoding::DELTA_BINARY_PACKED, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_i64() {\n+        Int64Type::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        Int64Type::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+        Int64Type::test(Encoding::DELTA_BINARY_PACKED, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_i96() {\n+        Int96Type::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        Int96Type::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_float() {\n+        FloatType::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        FloatType::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_double() {\n+        DoubleType::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        DoubleType::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_byte_array() {\n+        ByteArrayType::test(Encoding::PLAIN, TEST_SET_SIZE, -1);\n+        ByteArrayType::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, -1);\n+        ByteArrayType::test(Encoding::DELTA_LENGTH_BYTE_ARRAY, TEST_SET_SIZE, -1);\n+        ByteArrayType::test(Encoding::DELTA_BYTE_ARRAY, TEST_SET_SIZE, -1);\n+    }\n+\n+    #[test]\n+    fn test_fixed_lenbyte_array() {\n+        FixedLenByteArrayType::test(Encoding::PLAIN, TEST_SET_SIZE, 100);\n+        FixedLenByteArrayType::test(Encoding::PLAIN_DICTIONARY, TEST_SET_SIZE, 100);\n+        FixedLenByteArrayType::test(Encoding::DELTA_BYTE_ARRAY, TEST_SET_SIZE, 100);\n+    }\n+\n+    #[test]\n+    fn test_dict_encoded_size() {\n+        fn run_test<T: DataType>(type_length: i32, values: &[T::T], expected_size: usize) {\n+            let mut encoder = create_test_dict_encoder::<T>(type_length);\n+            assert_eq!(encoder.dict_encoded_size(), 0);\n+            encoder.put(values).unwrap();\n+            assert_eq!(encoder.dict_encoded_size(), expected_size);\n+            // We do not reset encoded size of the dictionary keys after flush_buffer\n+            encoder.flush_buffer().unwrap();\n+            assert_eq!(encoder.dict_encoded_size(), expected_size);\n+        }\n+\n+        // Only 2 variations of values 1 byte each\n+        run_test::<BoolType>(-1, &[true, false, true, false, true], 2);\n+        run_test::<Int32Type>(-1, &[1i32, 2i32, 3i32, 4i32, 5i32], 20);\n+        run_test::<Int64Type>(-1, &[1i64, 2i64, 3i64, 4i64, 5i64], 40);\n+        run_test::<FloatType>(-1, &[1f32, 2f32, 3f32, 4f32, 5f32], 20);\n+        run_test::<DoubleType>(-1, &[1f64, 2f64, 3f64, 4f64, 5f64], 40);\n+        // Int96: len + reference\n+        run_test::<Int96Type>(\n+            -1,\n+            &[Int96::from(vec![1, 2, 3]), Int96::from(vec![2, 3, 4])],\n+            32,\n+        );\n+        run_test::<ByteArrayType>(-1, &[ByteArray::from(\"abcd\"), ByteArray::from(\"efj\")], 15);\n+        run_test::<FixedLenByteArrayType>(2, &[ByteArray::from(\"ab\"), ByteArray::from(\"bc\")], 4);\n+    }\n+\n+    #[test]\n+    fn test_estimated_data_encoded_size() {\n+        fn run_test<T: DataType>(\n+            encoding: Encoding,\n+            type_length: i32,\n+            values: &[T::T],\n+            initial_size: usize,\n+            max_size: usize,\n+            flush_size: usize,\n+        ) {\n+            let mut encoder = match encoding {\n+                Encoding::PLAIN_DICTIONARY | Encoding::RLE_DICTIONARY => {\n+                    Box::new(create_test_dict_encoder::<T>(type_length))\n+                }\n+                _ => create_test_encoder::<T>(type_length, encoding),\n+            };\n+            assert_eq!(encoder.estimated_data_encoded_size(), initial_size);\n+\n+            encoder.put(values).unwrap();\n+            assert_eq!(encoder.estimated_data_encoded_size(), max_size);\n+\n+            encoder.flush_buffer().unwrap();\n+            assert_eq!(encoder.estimated_data_encoded_size(), flush_size);\n+        }\n+\n+        // PLAIN\n+        run_test::<Int32Type>(Encoding::PLAIN, -1, &vec![123; 1024], 0, 4096, 0);\n+\n+        // DICTIONARY\n+        // NOTE: The final size is almost the same because the dictionary entries are\n+        // preserved after encoded values have been written.\n+        run_test::<Int32Type>(Encoding::RLE_DICTIONARY, -1, &vec![123, 1024], 11, 68, 66);\n+\n+        // DELTA_BINARY_PACKED\n+        run_test::<Int32Type>(\n+            Encoding::DELTA_BINARY_PACKED,\n+            -1,\n+            &vec![123; 1024],\n+            0,\n+            35,\n+            0,\n+        );\n+\n+        // RLE\n+        let mut values = vec![];\n+        values.extend_from_slice(&vec![true; 16]);\n+        values.extend_from_slice(&vec![false; 16]);\n+        run_test::<BoolType>(Encoding::RLE, -1, &values, 0, 2, 0);\n+\n+        // DELTA_LENGTH_BYTE_ARRAY\n+        run_test::<ByteArrayType>(\n+            Encoding::DELTA_LENGTH_BYTE_ARRAY,\n+            -1,\n+            &[ByteArray::from(\"ab\"), ByteArray::from(\"abc\")],\n+            0,\n+            5, // only value bytes, length encoder is not flushed yet\n+            0,\n+        );\n+\n+        // DELTA_BYTE_ARRAY\n+        run_test::<ByteArrayType>(\n+            Encoding::DELTA_BYTE_ARRAY,\n+            -1,\n+            &[ByteArray::from(\"ab\"), ByteArray::from(\"abc\")],\n+            0,\n+            3, // only suffix bytes, length encoder is not flushed yet\n+            0,\n+        );\n+    }\n+\n+    // See: https://github.com/sunchao/parquet-rs/issues/47\n+    #[test]\n+    fn test_issue_47() {\n+        let mut encoder = create_test_encoder::<ByteArrayType>(0, Encoding::DELTA_BYTE_ARRAY);\n+        let mut decoder = create_test_decoder::<ByteArrayType>(0, Encoding::DELTA_BYTE_ARRAY);\n+\n+        let mut input = vec![];\n+        input.push(ByteArray::from(\"aa\"));\n+        input.push(ByteArray::from(\"aaa\"));\n+        input.push(ByteArray::from(\"aa\"));\n+        input.push(ByteArray::from(\"aaa\"));\n+        let mut output = vec![ByteArray::default(); input.len()];\n+\n+        let mut result = put_and_get(&mut encoder, &mut decoder, &input[..2], &mut output[..2]);\n+        assert!(\n+            result.is_ok(),\n+            \"first put_and_get() failed with: {}\",\n+            result.unwrap_err()\n+        );\n+        result = put_and_get(&mut encoder, &mut decoder, &input[2..], &mut output[2..]);\n+        assert!(\n+            result.is_ok(),\n+            \"second put_and_get() failed with: {}\",\n+            result.unwrap_err()\n+        );\n+        assert_eq!(output, input);\n+    }\n+\n+    trait EncodingTester<T: DataType> {\n+        fn test(enc: Encoding, total: usize, type_length: i32) {\n+            let result = match enc {\n+                Encoding::PLAIN_DICTIONARY | Encoding::RLE_DICTIONARY => {\n+                    Self::test_dict_internal(total, type_length)\n+                }\n+                enc @ _ => Self::test_internal(enc, total, type_length),\n+            };\n+\n+            assert!(\n+                result.is_ok(),\n+                \"Expected result to be OK but got err:\\n {}\",\n+                result.unwrap_err()\n+            );\n+        }\n+\n+        fn test_internal(enc: Encoding, total: usize, type_length: i32) -> Result<()>;\n+\n+        fn test_dict_internal(total: usize, type_length: i32) -> Result<()>;\n+    }\n+\n+    impl<T: DataType> EncodingTester<T> for T {\n+        fn test_internal(enc: Encoding, total: usize, type_length: i32) -> Result<()> {\n+            let mut encoder = create_test_encoder::<T>(type_length, enc);\n+            let mut decoder = create_test_decoder::<T>(type_length, enc);\n+            let mut values = <T as RandGen<T>>::gen_vec(type_length, total);\n+            let mut result_data = vec![T::T::default(); total];\n+\n+            let mut actual_total = put_and_get(\n+                &mut encoder,\n+                &mut decoder,\n+                &values[..],\n+                &mut result_data[..],\n+            )?;\n+            assert_eq!(actual_total, total);\n+            assert_eq!(result_data, values);\n+\n+            // Encode more data after flush and test with decoder\n+\n+            values = <T as RandGen<T>>::gen_vec(type_length, total);\n+            actual_total = put_and_get(\n+                &mut encoder,\n+                &mut decoder,\n+                &values[..],\n+                &mut result_data[..],\n+            )?;\n+            assert_eq!(actual_total, total);\n+            assert_eq!(result_data, values);\n+\n+            Ok(())\n+        }\n+\n+        fn test_dict_internal(total: usize, type_length: i32) -> Result<()> {\n+            let mut encoder = create_test_dict_encoder::<T>(type_length);\n+            let mut values = <T as RandGen<T>>::gen_vec(type_length, total);\n+            encoder.put(&values[..])?;\n+\n+            let mut data = encoder.flush_buffer()?;\n+            let mut decoder = create_test_dict_decoder::<T>();\n+            let mut dict_decoder = PlainDecoder::<T>::new(type_length);\n+            dict_decoder.set_data(encoder.write_dict()?, encoder.num_entries())?;\n+            decoder.set_dict(Box::new(dict_decoder))?;\n+            let mut result_data = vec![T::T::default(); total];\n+            decoder.set_data(data, total)?;\n+            let mut actual_total = decoder.get(&mut result_data)?;\n+\n+            assert_eq!(actual_total, total);\n+            assert_eq!(result_data, values);\n+\n+            // Encode more data after flush and test with decoder\n+\n+            values = <T as RandGen<T>>::gen_vec(type_length, total);\n+            encoder.put(&values[..])?;\n+            data = encoder.flush_buffer()?;\n+\n+            let mut dict_decoder = PlainDecoder::<T>::new(type_length);\n+            dict_decoder.set_data(encoder.write_dict()?, encoder.num_entries())?;\n+            decoder.set_dict(Box::new(dict_decoder))?;\n+            decoder.set_data(data, total)?;\n+            actual_total = decoder.get(&mut result_data)?;\n+\n+            assert_eq!(actual_total, total);\n+            assert_eq!(result_data, values);\n+\n+            Ok(())\n+        }\n+    }\n+\n+    fn put_and_get<T: DataType>(\n+        encoder: &mut Box<Encoder<T>>,\n+        decoder: &mut Box<Decoder<T>>,\n+        input: &[T::T],\n+        output: &mut [T::T],\n+    ) -> Result<usize> {\n+        encoder.put(input)?;\n+        let data = encoder.flush_buffer()?;\n+        decoder.set_data(data, input.len())?;\n+        decoder.get(output)\n+    }\n+\n+    fn create_and_check_encoder<T: DataType>(encoding: Encoding, err: Option<ParquetError>) {\n+        let descr = create_test_col_desc_ptr(-1, T::get_physical_type());\n+        let mem_tracker = Rc::new(MemTracker::new());\n+        let encoder = get_encoder::<T>(descr, encoding, mem_tracker);\n+        match err {\n+            Some(parquet_error) => {\n+                assert!(encoder.is_err());\n+                assert_eq!(encoder.err().unwrap(), parquet_error);\n+            }\n+            None => {\n+                assert!(encoder.is_ok());\n+                assert_eq!(encoder.unwrap().encoding(), encoding);\n+            }\n+        }\n+    }\n+\n+    // Creates test column descriptor.\n+    fn create_test_col_desc_ptr(type_len: i32, t: Type) -> ColumnDescPtr {\n+        let ty = SchemaType::primitive_type_builder(\"t\", t)\n+            .with_length(type_len)\n+            .build()\n+            .unwrap();\n+        Rc::new(ColumnDescriptor::new(\n+            Rc::new(ty),\n+            None,\n+            0,\n+            0,\n+            ColumnPath::new(vec![]),\n+        ))\n+    }\n+\n+    fn create_test_encoder<T: DataType>(type_len: i32, enc: Encoding) -> Box<Encoder<T>> {\n+        let desc = create_test_col_desc_ptr(type_len, T::get_physical_type());\n+        let mem_tracker = Rc::new(MemTracker::new());\n+        get_encoder(desc, enc, mem_tracker).unwrap()\n+    }\n+\n+    fn create_test_decoder<T: DataType>(type_len: i32, enc: Encoding) -> Box<Decoder<T>> {\n+        let desc = create_test_col_desc_ptr(type_len, T::get_physical_type());\n+        get_decoder(desc, enc).unwrap()\n+    }\n+\n+    fn create_test_dict_encoder<T: DataType>(type_len: i32) -> DictEncoder<T> {\n+        let desc = create_test_col_desc_ptr(type_len, T::get_physical_type());\n+        let mem_tracker = Rc::new(MemTracker::new());\n+        DictEncoder::<T>::new(desc, mem_tracker)\n+    }\n+\n+    fn create_test_dict_decoder<T: DataType>() -> DictDecoder<T> {\n+        DictDecoder::<T>::new()\n+    }\n+}\ndiff --git a/rust/src/parquet/encodings/levels.rs b/rust/src/parquet/encodings/levels.rs\nnew file mode 100644\nindex 0000000000..ec65198ce5\n--- /dev/null\n+++ b/rust/src/parquet/encodings/levels.rs\n@@ -0,0 +1,529 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+use std::{cmp, mem};\n+\n+use super::rle::{RleDecoder, RleEncoder};\n+\n+use crate::parquet::basic::Encoding;\n+use crate::parquet::data_type::AsBytes;\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::util::{\n+    bit_util::{ceil, log2, BitReader, BitWriter},\n+    memory::ByteBufferPtr,\n+};\n+\n+/// Computes max buffer size for level encoder/decoder based on encoding, max\n+/// repetition/definition level and number of total buffered values (includes null\n+/// values).\n+#[inline]\n+pub fn max_buffer_size(encoding: Encoding, max_level: i16, num_buffered_values: usize) -> usize {\n+    let bit_width = log2(max_level as u64 + 1) as u8;\n+    match encoding {\n+        Encoding::RLE => {\n+            RleEncoder::max_buffer_size(bit_width, num_buffered_values)\n+                + RleEncoder::min_buffer_size(bit_width)\n+        }\n+        Encoding::BIT_PACKED => ceil((num_buffered_values * bit_width as usize) as i64, 8) as usize,\n+        _ => panic!(\"Unsupported encoding type {}\", encoding),\n+    }\n+}\n+\n+/// Encoder for definition/repetition levels.\n+/// Currently only supports RLE and BIT_PACKED (dev/null) encoding, including v2.\n+pub enum LevelEncoder {\n+    RLE(RleEncoder),\n+    RLE_V2(RleEncoder),\n+    BIT_PACKED(u8, BitWriter),\n+}\n+\n+impl LevelEncoder {\n+    /// Creates new level encoder based on encoding, max level and underlying byte buffer.\n+    /// For bit packed encoding it is assumed that buffer is already allocated with\n+    /// `levels::max_buffer_size` method.\n+    ///\n+    /// Used to encode levels for Data Page v1.\n+    ///\n+    /// Panics, if encoding is not supported.\n+    pub fn v1(encoding: Encoding, max_level: i16, byte_buffer: Vec<u8>) -> Self {\n+        let bit_width = log2(max_level as u64 + 1) as u8;\n+        match encoding {\n+            Encoding::RLE => LevelEncoder::RLE(RleEncoder::new_from_buf(\n+                bit_width,\n+                byte_buffer,\n+                mem::size_of::<i32>(),\n+            )),\n+            Encoding::BIT_PACKED => {\n+                // Here we set full byte buffer without adjusting for num_buffered_values,\n+                // because byte buffer will already be allocated with size from\n+                // `max_buffer_size()` method.\n+                LevelEncoder::BIT_PACKED(bit_width, BitWriter::new_from_buf(byte_buffer, 0))\n+            }\n+            _ => panic!(\"Unsupported encoding type {}\", encoding),\n+        }\n+    }\n+\n+    /// Creates new level encoder based on RLE encoding. Used to encode Data Page v2\n+    /// repetition and definition levels.\n+    pub fn v2(max_level: i16, byte_buffer: Vec<u8>) -> Self {\n+        let bit_width = log2(max_level as u64 + 1) as u8;\n+        LevelEncoder::RLE_V2(RleEncoder::new_from_buf(bit_width, byte_buffer, 0))\n+    }\n+\n+    /// Put/encode levels vector into this level encoder.\n+    /// Returns number of encoded values that are less than or equal to length of the input\n+    /// buffer.\n+    ///\n+    /// RLE and BIT_PACKED level encoders return Err() when internal buffer overflows or\n+    /// flush fails.\n+    #[inline]\n+    pub fn put(&mut self, buffer: &[i16]) -> Result<usize> {\n+        let mut num_encoded = 0;\n+        match *self {\n+            LevelEncoder::RLE(ref mut encoder) | LevelEncoder::RLE_V2(ref mut encoder) => {\n+                for value in buffer {\n+                    if !encoder.put(*value as u64)? {\n+                        return Err(general_err!(\"RLE buffer is full\"));\n+                    }\n+                    num_encoded += 1;\n+                }\n+                encoder.flush()?;\n+            }\n+            LevelEncoder::BIT_PACKED(bit_width, ref mut encoder) => {\n+                for value in buffer {\n+                    if !encoder.put_value(*value as u64, bit_width as usize) {\n+                        return Err(general_err!(\"Not enough bytes left\"));\n+                    }\n+                    num_encoded += 1;\n+                }\n+                encoder.flush();\n+            }\n+        }\n+        Ok(num_encoded)\n+    }\n+\n+    /// Finalizes level encoder, flush all intermediate buffers and return resulting\n+    /// encoded buffer. Returned buffer is already truncated to encoded bytes only.\n+    #[inline]\n+    pub fn consume(self) -> Result<Vec<u8>> {\n+        match self {\n+            LevelEncoder::RLE(encoder) => {\n+                let mut encoded_data = encoder.consume()?;\n+                // Account for the buffer offset\n+                let encoded_len = encoded_data.len() - mem::size_of::<i32>();\n+                let len = (encoded_len as i32).to_le();\n+                let len_bytes = len.as_bytes();\n+                encoded_data[0..len_bytes.len()].copy_from_slice(len_bytes);\n+                Ok(encoded_data)\n+            }\n+            LevelEncoder::RLE_V2(encoder) => encoder.consume(),\n+            LevelEncoder::BIT_PACKED(_, encoder) => Ok(encoder.consume()),\n+        }\n+    }\n+}\n+\n+/// Decoder for definition/repetition levels.\n+/// Currently only supports RLE and BIT_PACKED encoding for Data Page v1 and\n+/// RLE for Data Page v2.\n+pub enum LevelDecoder {\n+    RLE(Option<usize>, RleDecoder),\n+    RLE_V2(Option<usize>, RleDecoder),\n+    BIT_PACKED(Option<usize>, u8, BitReader),\n+}\n+\n+impl LevelDecoder {\n+    /// Creates new level decoder based on encoding and max definition/repetition level.\n+    /// This method only initializes level decoder, `set_data` method must be called\n+    /// before reading any value.\n+    ///\n+    /// Used to encode levels for Data Page v1.\n+    ///\n+    /// Panics if encoding is not supported\n+    pub fn v1(encoding: Encoding, max_level: i16) -> Self {\n+        let bit_width = log2(max_level as u64 + 1) as u8;\n+        match encoding {\n+            Encoding::RLE => LevelDecoder::RLE(None, RleDecoder::new(bit_width)),\n+            Encoding::BIT_PACKED => {\n+                LevelDecoder::BIT_PACKED(None, bit_width, BitReader::from(Vec::new()))\n+            }\n+            _ => panic!(\"Unsupported encoding type {}\", encoding),\n+        }\n+    }\n+\n+    /// Creates new level decoder based on RLE encoding.\n+    /// Used to decode Data Page v2 repetition and definition levels.\n+    ///\n+    /// To set data for this decoder, use `set_data_range` method.\n+    pub fn v2(max_level: i16) -> Self {\n+        let bit_width = log2(max_level as u64 + 1) as u8;\n+        LevelDecoder::RLE_V2(None, RleDecoder::new(bit_width))\n+    }\n+\n+    /// Sets data for this level decoder, and returns total number of bytes set.\n+    /// This is used for Data Page v1 levels.\n+    ///\n+    /// `data` is encoded data as byte buffer, `num_buffered_values` represents total\n+    /// number of values that is expected.\n+    ///\n+    /// Both RLE and BIT_PACKED level decoders set `num_buffered_values` as total number of\n+    /// values that they can return and track num values.\n+    #[inline]\n+    pub fn set_data(&mut self, num_buffered_values: usize, data: ByteBufferPtr) -> usize {\n+        match *self {\n+            LevelDecoder::RLE(ref mut num_values, ref mut decoder) => {\n+                *num_values = Some(num_buffered_values);\n+                let i32_size = mem::size_of::<i32>();\n+                let data_size = read_num_bytes!(i32, i32_size, data.as_ref()) as usize;\n+                decoder.set_data(data.range(i32_size, data_size));\n+                i32_size + data_size\n+            }\n+            LevelDecoder::BIT_PACKED(ref mut num_values, bit_width, ref mut decoder) => {\n+                *num_values = Some(num_buffered_values);\n+                // Set appropriate number of bytes: if max size is larger than buffer - set full\n+                // buffer\n+                let num_bytes = ceil((num_buffered_values * bit_width as usize) as i64, 8);\n+                let data_size = cmp::min(num_bytes as usize, data.len());\n+                decoder.reset(data.range(data.start(), data_size));\n+                data_size\n+            }\n+            _ => panic!(),\n+        }\n+    }\n+\n+    /// Sets byte array explicitly when start position `start` and length `len` are known\n+    /// in advance. Only supported by RLE level decoder and used for Data Page v2 levels.\n+    /// Returns number of total bytes set for this decoder (len).\n+    #[inline]\n+    pub fn set_data_range(\n+        &mut self,\n+        num_buffered_values: usize,\n+        data: &ByteBufferPtr,\n+        start: usize,\n+        len: usize,\n+    ) -> usize {\n+        match *self {\n+            LevelDecoder::RLE_V2(ref mut num_values, ref mut decoder) => {\n+                decoder.set_data(data.range(start, len));\n+                *num_values = Some(num_buffered_values);\n+                len\n+            }\n+            _ => panic!(\"set_data_range() method is only supported by RLE v2 encoding type\"),\n+        }\n+    }\n+\n+    /// Returns true if data is set for decoder, false otherwise.\n+    #[inline]\n+    pub fn is_data_set(&self) -> bool {\n+        match self {\n+            LevelDecoder::RLE(ref num_values, _) => num_values.is_some(),\n+            LevelDecoder::RLE_V2(ref num_values, _) => num_values.is_some(),\n+            LevelDecoder::BIT_PACKED(ref num_values, ..) => num_values.is_some(),\n+        }\n+    }\n+\n+    /// Decodes values and puts them into `buffer`.\n+    /// Returns number of values that were successfully decoded (less than or equal to\n+    /// buffer length).\n+    #[inline]\n+    pub fn get(&mut self, buffer: &mut [i16]) -> Result<usize> {\n+        assert!(self.is_data_set(), \"No data set for decoding\");\n+        match *self {\n+            LevelDecoder::RLE(ref mut num_values, ref mut decoder)\n+            | LevelDecoder::RLE_V2(ref mut num_values, ref mut decoder) => {\n+                // Max length we can read\n+                let len = cmp::min(num_values.unwrap(), buffer.len());\n+                let values_read = decoder.get_batch::<i16>(&mut buffer[0..len])?;\n+                *num_values = num_values.map(|len| len - values_read);\n+                Ok(values_read)\n+            }\n+            LevelDecoder::BIT_PACKED(ref mut num_values, bit_width, ref mut decoder) => {\n+                // When extracting values from bit reader, it might return more values than left\n+                // because of padding to a full byte, we use num_values to track precise number\n+                // of values.\n+                let len = cmp::min(num_values.unwrap(), buffer.len());\n+                let values_read = decoder.get_batch::<i16>(&mut buffer[..len], bit_width as usize);\n+                *num_values = num_values.map(|len| len - values_read);\n+                Ok(values_read)\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use crate::parquet::util::test_common::random_numbers_range;\n+\n+    fn test_internal_roundtrip(enc: Encoding, levels: &[i16], max_level: i16, v2: bool) {\n+        let size = max_buffer_size(enc, max_level, levels.len());\n+        let mut encoder = if v2 {\n+            LevelEncoder::v2(max_level, vec![0; size])\n+        } else {\n+            LevelEncoder::v1(enc, max_level, vec![0; size])\n+        };\n+        encoder.put(&levels).expect(\"put() should be OK\");\n+        let encoded_levels = encoder.consume().expect(\"consume() should be OK\");\n+\n+        let byte_buf = ByteBufferPtr::new(encoded_levels);\n+        let mut decoder;\n+        if v2 {\n+            decoder = LevelDecoder::v2(max_level);\n+            decoder.set_data_range(levels.len(), &byte_buf, 0, byte_buf.len());\n+        } else {\n+            decoder = LevelDecoder::v1(enc, max_level);\n+            decoder.set_data(levels.len(), byte_buf);\n+        };\n+\n+        let mut buffer = vec![0; levels.len()];\n+        let num_decoded = decoder.get(&mut buffer).expect(\"get() should be OK\");\n+        assert_eq!(num_decoded, levels.len());\n+        assert_eq!(buffer, levels);\n+    }\n+\n+    // Performs incremental read until all bytes are read\n+    fn test_internal_roundtrip_incremental(\n+        enc: Encoding,\n+        levels: &[i16],\n+        max_level: i16,\n+        v2: bool,\n+    ) {\n+        let size = max_buffer_size(enc, max_level, levels.len());\n+        let mut encoder = if v2 {\n+            LevelEncoder::v2(max_level, vec![0; size])\n+        } else {\n+            LevelEncoder::v1(enc, max_level, vec![0; size])\n+        };\n+        encoder.put(&levels).expect(\"put() should be OK\");\n+        let encoded_levels = encoder.consume().expect(\"consume() should be OK\");\n+\n+        let byte_buf = ByteBufferPtr::new(encoded_levels);\n+        let mut decoder;\n+        if v2 {\n+            decoder = LevelDecoder::v2(max_level);\n+            decoder.set_data_range(levels.len(), &byte_buf, 0, byte_buf.len());\n+        } else {\n+            decoder = LevelDecoder::v1(enc, max_level);\n+            decoder.set_data(levels.len(), byte_buf);\n+        }\n+\n+        let mut buffer = vec![0; levels.len() * 2];\n+        let mut total_decoded = 0;\n+        let mut safe_stop = levels.len() * 2; // still terminate in case of issues in the code\n+        while safe_stop > 0 {\n+            safe_stop -= 1;\n+            let num_decoded = decoder\n+                .get(&mut buffer[total_decoded..total_decoded + 1])\n+                .expect(\"get() should be OK\");\n+            if num_decoded == 0 {\n+                break;\n+            }\n+            total_decoded += num_decoded;\n+        }\n+        assert!(\n+            safe_stop > 0,\n+            \"Failed to read values incrementally, reached safe stop\"\n+        );\n+        assert_eq!(total_decoded, levels.len());\n+        assert_eq!(&buffer[0..levels.len()], levels);\n+    }\n+\n+    // Tests encoding/decoding of values when output buffer is larger than number of\n+    // encoded values\n+    fn test_internal_roundtrip_underflow(enc: Encoding, levels: &[i16], max_level: i16, v2: bool) {\n+        let size = max_buffer_size(enc, max_level, levels.len());\n+        let mut encoder = if v2 {\n+            LevelEncoder::v2(max_level, vec![0; size])\n+        } else {\n+            LevelEncoder::v1(enc, max_level, vec![0; size])\n+        };\n+        // Encode only one value\n+        let num_encoded = encoder.put(&levels[0..1]).expect(\"put() should be OK\");\n+        let encoded_levels = encoder.consume().expect(\"consume() should be OK\");\n+        assert_eq!(num_encoded, 1);\n+\n+        let byte_buf = ByteBufferPtr::new(encoded_levels);\n+        let mut decoder;\n+        // Set one encoded value as `num_buffered_values`\n+        if v2 {\n+            decoder = LevelDecoder::v2(max_level);\n+            decoder.set_data_range(1, &byte_buf, 0, byte_buf.len());\n+        } else {\n+            decoder = LevelDecoder::v1(enc, max_level);\n+            decoder.set_data(1, byte_buf);\n+        }\n+\n+        let mut buffer = vec![0; levels.len()];\n+        let num_decoded = decoder.get(&mut buffer).expect(\"get() should be OK\");\n+        assert_eq!(num_decoded, num_encoded);\n+        assert_eq!(buffer[0..num_decoded], levels[0..num_decoded]);\n+    }\n+\n+    // Tests when encoded values are larger than encoder's buffer\n+    fn test_internal_roundtrip_overflow(enc: Encoding, levels: &[i16], max_level: i16, v2: bool) {\n+        let size = max_buffer_size(enc, max_level, levels.len());\n+        let mut encoder = if v2 {\n+            LevelEncoder::v2(max_level, vec![0; size])\n+        } else {\n+            LevelEncoder::v1(enc, max_level, vec![0; size])\n+        };\n+        let mut found_err = false;\n+        // Insert a large number of values, so we run out of space\n+        for _ in 0..100 {\n+            match encoder.put(&levels) {\n+                Err(err) => {\n+                    assert!(format!(\"{}\", err).contains(\"Not enough bytes left\"));\n+                    found_err = true;\n+                    break;\n+                }\n+                Ok(_) => {}\n+            }\n+        }\n+        if !found_err {\n+            panic!(\"Failed test: no buffer overflow\");\n+        }\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_one() {\n+        let levels = vec![0, 1, 1, 1, 1, 0, 0, 0, 0, 1];\n+        let max_level = 1;\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip() {\n+        let levels = vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9];\n+        let max_level = 10;\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_incremental() {\n+        let levels = vec![0, 1, 2, 3, 4, 5, 6, 7, 8, 9];\n+        let max_level = 10;\n+        test_internal_roundtrip_incremental(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip_incremental(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip_incremental(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_all_zeros() {\n+        let levels = vec![0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\n+        let max_level = 1;\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_random() {\n+        // This test is mainly for bit packed level encoder/decoder\n+        let mut levels = Vec::new();\n+        let max_level = 5;\n+        random_numbers_range::<i16>(120, 0, max_level, &mut levels);\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_underflow() {\n+        let levels = vec![1, 1, 2, 3, 2, 1, 1, 2, 3, 1];\n+        let max_level = 3;\n+        test_internal_roundtrip_underflow(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip_underflow(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip_underflow(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_roundtrip_overflow() {\n+        let levels = vec![1, 1, 2, 3, 2, 1, 1, 2, 3, 1];\n+        let max_level = 3;\n+        test_internal_roundtrip_overflow(Encoding::RLE, &levels, max_level, false);\n+        test_internal_roundtrip_overflow(Encoding::BIT_PACKED, &levels, max_level, false);\n+        test_internal_roundtrip_overflow(Encoding::RLE, &levels, max_level, true);\n+    }\n+\n+    #[test]\n+    fn test_rle_decoder_set_data_range() {\n+        // Buffer containing both repetition and definition levels\n+        let buffer = ByteBufferPtr::new(vec![5, 198, 2, 5, 42, 168, 10, 0, 2, 3, 36, 73]);\n+\n+        let max_rep_level = 1;\n+        let mut decoder = LevelDecoder::v2(max_rep_level);\n+        assert_eq!(decoder.set_data_range(10, &buffer, 0, 3), 3);\n+        let mut result = vec![0; 10];\n+        let num_decoded = decoder.get(&mut result).expect(\"get() should be OK\");\n+        assert_eq!(num_decoded, 10);\n+        assert_eq!(result, vec![0, 1, 1, 0, 0, 0, 1, 1, 0, 1]);\n+\n+        let max_def_level = 2;\n+        let mut decoder = LevelDecoder::v2(max_def_level);\n+        assert_eq!(decoder.set_data_range(10, &buffer, 3, 5), 5);\n+        let mut result = vec![0; 10];\n+        let num_decoded = decoder.get(&mut result).expect(\"get() should be OK\");\n+        assert_eq!(num_decoded, 10);\n+        assert_eq!(result, vec![2, 2, 2, 0, 0, 2, 2, 2, 2, 2]);\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"set_data_range() method is only supported by RLE v2 encoding type\")]\n+    fn test_bit_packed_decoder_set_data_range() {\n+        // Buffer containing both repetition and definition levels\n+        let buffer = ByteBufferPtr::new(vec![1, 2, 3, 4, 5]);\n+        let max_level = 1;\n+        let mut decoder = LevelDecoder::v1(Encoding::BIT_PACKED, max_level);\n+        decoder.set_data_range(10, &buffer, 0, 3);\n+    }\n+\n+    #[test]\n+    fn test_bit_packed_decoder_set_data() {\n+        // Test the maximum size that is assigned based on number of values and buffer length\n+        let buffer = ByteBufferPtr::new(vec![1, 2, 3, 4, 5]);\n+        let max_level = 1;\n+        let mut decoder = LevelDecoder::v1(Encoding::BIT_PACKED, max_level);\n+        // This should reset to entire buffer\n+        assert_eq!(decoder.set_data(1024, buffer.all()), buffer.len());\n+        // This should set smallest num bytes\n+        assert_eq!(decoder.set_data(3, buffer.all()), 1);\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"No data set for decoding\")]\n+    fn test_rle_level_decoder_get_no_set_data() {\n+        // `get()` normally panics because bit_reader is not set for RLE decoding\n+        // we have explicit check now in set_data\n+        let max_rep_level = 2;\n+        let mut decoder = LevelDecoder::v1(Encoding::RLE, max_rep_level);\n+        let mut buffer = vec![0; 16];\n+        decoder.get(&mut buffer).unwrap();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"No data set for decoding\")]\n+    fn test_bit_packed_level_decoder_get_no_set_data() {\n+        let max_rep_level = 2;\n+        let mut decoder = LevelDecoder::v1(Encoding::BIT_PACKED, max_rep_level);\n+        let mut buffer = vec![0; 16];\n+        decoder.get(&mut buffer).unwrap();\n+    }\n+}\ndiff --git a/rust/src/parquet/encodings/mod.rs b/rust/src/parquet/encodings/mod.rs\nnew file mode 100644\nindex 0000000000..33b1e233d8\n--- /dev/null\n+++ b/rust/src/parquet/encodings/mod.rs\n@@ -0,0 +1,21 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+pub mod decoding;\n+pub mod encoding;\n+pub mod levels;\n+mod rle;\ndiff --git a/rust/src/parquet/encodings/rle.rs b/rust/src/parquet/encodings/rle.rs\nnew file mode 100644\nindex 0000000000..5b56c2a250\n--- /dev/null\n+++ b/rust/src/parquet/encodings/rle.rs\n@@ -0,0 +1,839 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+use std::{\n+    cmp,\n+    mem::{size_of, transmute_copy},\n+};\n+\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::util::{\n+    bit_util::{self, BitReader, BitWriter},\n+    memory::ByteBufferPtr,\n+};\n+\n+/// Rle/Bit-Packing Hybrid Encoding\n+/// The grammar for this encoding looks like the following (copied verbatim\n+/// from https://github.com/Parquet/parquet-format/blob/master/Encodings.md):\n+///\n+/// rle-bit-packed-hybrid: <length> <encoded-data>\n+/// length := length of the <encoded-data> in bytes stored as 4 bytes little endian\n+/// encoded-data := <run>*\n+/// run := <bit-packed-run> | <rle-run>\n+/// bit-packed-run := <bit-packed-header> <bit-packed-values>\n+/// bit-packed-header := varint-encode(<bit-pack-count> << 1 | 1)\n+/// we always bit-pack a multiple of 8 values at a time, so we only store the number of\n+/// values / 8\n+/// bit-pack-count := (number of values in this run) / 8\n+/// bit-packed-values := *see 1 below*\n+/// rle-run := <rle-header> <repeated-value>\n+/// rle-header := varint-encode( (number of times repeated) << 1)\n+/// repeated-value := value that is repeated, using a fixed-width of\n+/// round-up-to-next-byte(bit-width)\n+\n+/// Maximum groups per bit-packed run. Current value is 64.\n+const MAX_GROUPS_PER_BIT_PACKED_RUN: usize = 1 << 6;\n+const MAX_VALUES_PER_BIT_PACKED_RUN: usize = MAX_GROUPS_PER_BIT_PACKED_RUN * 8;\n+const MAX_WRITER_BUF_SIZE: usize = 1 << 10;\n+\n+/// A RLE/Bit-Packing hybrid encoder.\n+// TODO: tracking memory usage\n+pub struct RleEncoder {\n+    // Number of bits needed to encode the value. Must be in the range of [0, 64].\n+    bit_width: u8,\n+\n+    // Underlying writer which holds an internal buffer.\n+    bit_writer: BitWriter,\n+\n+    // If this is true, the buffer is full and subsequent `put()` calls will fail.\n+    buffer_full: bool,\n+\n+    // The maximum byte size a single run can take.\n+    max_run_byte_size: usize,\n+\n+    // Buffered values for bit-packed runs.\n+    buffered_values: [u64; 8],\n+\n+    // Number of current buffered values. Must be less than 8.\n+    num_buffered_values: usize,\n+\n+    // The current (also last) value that was written and the count of how many\n+    // times in a row that value has been seen.\n+    current_value: u64,\n+\n+    // The number of repetitions for `current_value`. If this gets too high we'd\n+    // switch to use RLE encoding.\n+    repeat_count: usize,\n+\n+    // Number of bit-packed values in the current run. This doesn't include values\n+    // in `buffered_values`.\n+    bit_packed_count: usize,\n+\n+    // The position of the indicator byte in the `bit_writer`.\n+    indicator_byte_pos: i64,\n+}\n+\n+impl RleEncoder {\n+    pub fn new(bit_width: u8, buffer_len: usize) -> Self {\n+        let buffer = vec![0; buffer_len];\n+        RleEncoder::new_from_buf(bit_width, buffer, 0)\n+    }\n+\n+    /// Initialize the encoder from existing `buffer` and the starting offset `start`.\n+    pub fn new_from_buf(bit_width: u8, buffer: Vec<u8>, start: usize) -> Self {\n+        assert!(bit_width <= 64, \"bit_width ({}) out of range.\", bit_width);\n+        let max_run_byte_size = RleEncoder::min_buffer_size(bit_width);\n+        assert!(\n+            buffer.len() >= max_run_byte_size,\n+            \"buffer length {} must be greater than {}\",\n+            buffer.len(),\n+            max_run_byte_size\n+        );\n+        let bit_writer = BitWriter::new_from_buf(buffer, start);\n+        RleEncoder {\n+            bit_width,\n+            bit_writer,\n+            buffer_full: false,\n+            max_run_byte_size,\n+            buffered_values: [0; 8],\n+            num_buffered_values: 0,\n+            current_value: 0,\n+            repeat_count: 0,\n+            bit_packed_count: 0,\n+            indicator_byte_pos: -1,\n+        }\n+    }\n+\n+    /// Returns the minimum buffer size needed to use the encoder for `bit_width`.\n+    /// This is the maximum length of a single run for `bit_width`.\n+    pub fn min_buffer_size(bit_width: u8) -> usize {\n+        let max_bit_packed_run_size = 1 + bit_util::ceil(\n+            (MAX_VALUES_PER_BIT_PACKED_RUN * bit_width as usize) as i64,\n+            8,\n+        );\n+        let max_rle_run_size =\n+            bit_util::MAX_VLQ_BYTE_LEN + bit_util::ceil(bit_width as i64, 8) as usize;\n+        ::std::cmp::max(max_bit_packed_run_size as usize, max_rle_run_size)\n+    }\n+\n+    /// Returns the maximum buffer size takes to encode `num_values` values with\n+    /// `bit_width`.\n+    pub fn max_buffer_size(bit_width: u8, num_values: usize) -> usize {\n+        // First the maximum size for bit-packed run\n+        let bytes_per_run = bit_width;\n+        let num_runs = bit_util::ceil(num_values as i64, 8) as usize;\n+        let bit_packed_max_size = num_runs + num_runs * bytes_per_run as usize;\n+\n+        // Second the maximum size for RLE run\n+        let min_rle_run_size = 1 + bit_util::ceil(bit_width as i64, 8) as usize;\n+        let rle_max_size = bit_util::ceil(num_values as i64, 8) as usize * min_rle_run_size;\n+        ::std::cmp::max(bit_packed_max_size, rle_max_size) as usize\n+    }\n+\n+    /// Encodes `value`, which must be representable with `bit_width` bits.\n+    /// Returns true if the value fits in buffer, false if it doesn't, or\n+    /// error if something is wrong.\n+    #[inline]\n+    pub fn put(&mut self, value: u64) -> Result<bool> {\n+        // This function buffers 8 values at a time. After seeing 8 values, it\n+        // decides whether the current run should be encoded in bit-packed or RLE.\n+        if self.buffer_full {\n+            // The value cannot fit in the current buffer.\n+            return Ok(false);\n+        }\n+        if self.current_value == value {\n+            self.repeat_count += 1;\n+            if self.repeat_count > 8 {\n+                // A continuation of last value. No need to buffer.\n+                return Ok(true);\n+            }\n+        } else {\n+            if self.repeat_count >= 8 {\n+                // The current RLE run has ended and we've gathered enough. Flush first.\n+                assert_eq!(self.bit_packed_count, 0);\n+                self.flush_rle_run()?;\n+            }\n+            self.repeat_count = 1;\n+            self.current_value = value;\n+        }\n+\n+        self.buffered_values[self.num_buffered_values] = value;\n+        self.num_buffered_values += 1;\n+        if self.num_buffered_values == 8 {\n+            // Buffered values are full. Flush them.\n+            assert_eq!(self.bit_packed_count % 8, 0);\n+            self.flush_buffered_values()?;\n+        }\n+\n+        Ok(true)\n+    }\n+\n+    #[inline]\n+    pub fn buffer(&self) -> &[u8] {\n+        self.bit_writer.buffer()\n+    }\n+\n+    #[inline]\n+    pub fn len(&self) -> usize {\n+        self.bit_writer.bytes_written()\n+    }\n+\n+    #[inline]\n+    pub fn consume(mut self) -> Result<Vec<u8>> {\n+        self.flush()?;\n+        Ok(self.bit_writer.consume())\n+    }\n+\n+    /// Borrow equivalent of the `consume` method.\n+    /// Call `clear()` after invoking this method.\n+    #[inline]\n+    pub fn flush_buffer(&mut self) -> Result<&[u8]> {\n+        self.flush()?;\n+        Ok(self.bit_writer.flush_buffer())\n+    }\n+\n+    /// Clears the internal state so this encoder can be reused (e.g., after becoming full).\n+    #[inline]\n+    pub fn clear(&mut self) {\n+        self.bit_writer.clear();\n+        self.buffer_full = false;\n+        self.num_buffered_values = 0;\n+        self.current_value = 0;\n+        self.repeat_count = 0;\n+        self.bit_packed_count = 0;\n+        self.indicator_byte_pos = -1;\n+    }\n+\n+    /// Flushes all remaining values and return the final byte buffer maintained by the\n+    /// internal writer.\n+    #[inline]\n+    pub fn flush(&mut self) -> Result<()> {\n+        if self.bit_packed_count > 0 || self.repeat_count > 0 || self.num_buffered_values > 0 {\n+            let all_repeat = self.bit_packed_count == 0\n+                && (self.repeat_count == self.num_buffered_values || self.num_buffered_values == 0);\n+            if self.repeat_count > 0 && all_repeat {\n+                self.flush_rle_run()?;\n+            } else {\n+                // Buffer the last group of bit-packed values to 8 by padding with 0s.\n+                if self.num_buffered_values > 0 {\n+                    while self.num_buffered_values < 8 {\n+                        self.buffered_values[self.num_buffered_values] = 0;\n+                        self.num_buffered_values += 1;\n+                    }\n+                }\n+                self.bit_packed_count += self.num_buffered_values;\n+                self.flush_bit_packed_run(true)?;\n+                self.repeat_count = 0;\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn flush_rle_run(&mut self) -> Result<()> {\n+        assert!(self.repeat_count > 0);\n+        let indicator_value = self.repeat_count << 1 | 0;\n+        let mut result = self.bit_writer.put_vlq_int(indicator_value as u64);\n+        result &= self.bit_writer.put_aligned(\n+            self.current_value,\n+            bit_util::ceil(self.bit_width as i64, 8) as usize,\n+        );\n+        if !result {\n+            return Err(general_err!(\"Failed to write RLE run\"));\n+        }\n+        self.num_buffered_values = 0;\n+        self.repeat_count = 0;\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn flush_bit_packed_run(&mut self, update_indicator_byte: bool) -> Result<()> {\n+        if self.indicator_byte_pos < 0 {\n+            self.indicator_byte_pos = self.bit_writer.skip(1)? as i64;\n+        }\n+\n+        // Write all buffered values as bit-packed literals\n+        for i in 0..self.num_buffered_values {\n+            let _ = self\n+                .bit_writer\n+                .put_value(self.buffered_values[i], self.bit_width as usize);\n+        }\n+        self.num_buffered_values = 0;\n+        if update_indicator_byte {\n+            // Write the indicator byte to the reserved position in `bit_writer`\n+            let num_groups = self.bit_packed_count / 8;\n+            let indicator_byte = ((num_groups << 1) | 1) as u8;\n+            if !self.bit_writer.put_aligned_offset(\n+                indicator_byte,\n+                1,\n+                self.indicator_byte_pos as usize,\n+            ) {\n+                return Err(general_err!(\"Not enough space to write indicator byte\"));\n+            }\n+            self.indicator_byte_pos = -1;\n+            self.bit_packed_count = 0;\n+        }\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn flush_buffered_values(&mut self) -> Result<()> {\n+        if self.repeat_count >= 8 {\n+            self.num_buffered_values = 0;\n+            if self.bit_packed_count > 0 {\n+                // In this case we choose RLE encoding. Flush the current buffered values\n+                // as bit-packed encoding.\n+                assert_eq!(self.bit_packed_count % 8, 0);\n+                self.flush_bit_packed_run(true)?\n+            }\n+            return Ok(());\n+        }\n+\n+        self.bit_packed_count += self.num_buffered_values;\n+        let num_groups = self.bit_packed_count / 8;\n+        if num_groups + 1 >= MAX_GROUPS_PER_BIT_PACKED_RUN {\n+            // We've reached the maximum value that can be hold in a single bit-packed run.\n+            assert!(self.indicator_byte_pos >= 0);\n+            self.flush_bit_packed_run(true)?;\n+        } else {\n+            self.flush_bit_packed_run(false)?;\n+        }\n+        self.repeat_count = 0;\n+        Ok(())\n+    }\n+}\n+\n+/// A RLE/Bit-Packing hybrid decoder.\n+pub struct RleDecoder {\n+    // Number of bits used to encode the value. Must be between [0, 64].\n+    bit_width: u8,\n+\n+    // Bit reader loaded with input buffer.\n+    bit_reader: Option<BitReader>,\n+\n+    // Buffer used when `bit_reader` is not `None`, for batch reading.\n+    index_buf: Option<[i32; 1024]>,\n+\n+    // The remaining number of values in RLE for this run\n+    rle_left: u32,\n+\n+    // The remaining number of values in Bit-Packing for this run\n+    bit_packed_left: u32,\n+\n+    // The current value for the case of RLE mode\n+    current_value: Option<u64>,\n+}\n+\n+impl RleDecoder {\n+    pub fn new(bit_width: u8) -> Self {\n+        RleDecoder {\n+            bit_width,\n+            rle_left: 0,\n+            bit_packed_left: 0,\n+            bit_reader: None,\n+            index_buf: None,\n+            current_value: None,\n+        }\n+    }\n+\n+    pub fn set_data(&mut self, data: ByteBufferPtr) {\n+        if let Some(ref mut bit_reader) = self.bit_reader {\n+            bit_reader.reset(data);\n+        } else {\n+            self.bit_reader = Some(BitReader::new(data));\n+            self.index_buf = Some([0; 1024]);\n+        }\n+\n+        let _ = self.reload();\n+    }\n+\n+    #[inline]\n+    pub fn get<T: Default>(&mut self) -> Result<Option<T>> {\n+        assert!(size_of::<T>() <= 8);\n+\n+        while self.rle_left <= 0 && self.bit_packed_left <= 0 {\n+            if !self.reload() {\n+                return Ok(None);\n+            }\n+        }\n+\n+        let value = if self.rle_left > 0 {\n+            let rle_value = unsafe {\n+                transmute_copy::<u64, T>(\n+                    self.current_value\n+                        .as_mut()\n+                        .expect(\"current_value should be Some\"),\n+                )\n+            };\n+            self.rle_left -= 1;\n+            rle_value\n+        } else {\n+            // self.bit_packed_left > 0\n+            let bit_reader = self.bit_reader.as_mut().expect(\"bit_reader should be Some\");\n+            let bit_packed_value = bit_reader\n+                .get_value(self.bit_width as usize)\n+                .ok_or(eof_err!(\"Not enough data for 'bit_packed_value'\"))?;\n+            self.bit_packed_left -= 1;\n+            bit_packed_value\n+        };\n+\n+        Ok(Some(value))\n+    }\n+\n+    #[inline]\n+    pub fn get_batch<T: Default>(&mut self, buffer: &mut [T]) -> Result<usize> {\n+        assert!(self.bit_reader.is_some());\n+        assert!(size_of::<T>() <= 8);\n+\n+        let mut values_read = 0;\n+        while values_read < buffer.len() {\n+            if self.rle_left > 0 {\n+                assert!(self.current_value.is_some());\n+                let num_values = cmp::min(buffer.len() - values_read, self.rle_left as usize);\n+                for i in 0..num_values {\n+                    let repeated_value =\n+                        unsafe { transmute_copy::<u64, T>(self.current_value.as_mut().unwrap()) };\n+                    buffer[values_read + i] = repeated_value;\n+                }\n+                self.rle_left -= num_values as u32;\n+                values_read += num_values;\n+            } else if self.bit_packed_left > 0 {\n+                assert!(self.bit_reader.is_some());\n+                let mut num_values =\n+                    cmp::min(buffer.len() - values_read, self.bit_packed_left as usize);\n+                if let Some(ref mut bit_reader) = self.bit_reader {\n+                    num_values = bit_reader.get_batch::<T>(\n+                        &mut buffer[values_read..values_read + num_values],\n+                        self.bit_width as usize,\n+                    );\n+                    self.bit_packed_left -= num_values as u32;\n+                    values_read += num_values;\n+                }\n+            } else {\n+                if !self.reload() {\n+                    break;\n+                }\n+            }\n+        }\n+\n+        Ok(values_read)\n+    }\n+\n+    #[inline]\n+    pub fn get_batch_with_dict<T>(\n+        &mut self,\n+        dict: &[T],\n+        buffer: &mut [T],\n+        max_values: usize,\n+    ) -> Result<usize>\n+    where\n+        T: Default + Clone,\n+    {\n+        assert!(buffer.len() >= max_values);\n+\n+        let mut values_read = 0;\n+        while values_read < max_values {\n+            if self.rle_left > 0 {\n+                assert!(self.current_value.is_some());\n+                let num_values = cmp::min(max_values - values_read, self.rle_left as usize);\n+                let dict_idx = self.current_value.unwrap() as usize;\n+                for i in 0..num_values {\n+                    buffer[values_read + i] = dict[dict_idx].clone();\n+                }\n+                self.rle_left -= num_values as u32;\n+                values_read += num_values;\n+            } else if self.bit_packed_left > 0 {\n+                assert!(self.bit_reader.is_some());\n+                let mut num_values =\n+                    cmp::min(max_values - values_read, self.bit_packed_left as usize);\n+                if let Some(ref mut bit_reader) = self.bit_reader {\n+                    let mut index_buf = self.index_buf.unwrap();\n+                    num_values = cmp::min(num_values, index_buf.len());\n+                    loop {\n+                        num_values = bit_reader.get_batch::<i32>(\n+                            &mut index_buf[..num_values],\n+                            self.bit_width as usize,\n+                        );\n+                        for i in 0..num_values {\n+                            buffer[values_read + i] = dict[index_buf[i] as usize].clone();\n+                        }\n+                        self.bit_packed_left -= num_values as u32;\n+                        values_read += num_values;\n+                        if num_values < index_buf.len() {\n+                            break;\n+                        }\n+                    }\n+                }\n+            } else {\n+                if !self.reload() {\n+                    break;\n+                }\n+            }\n+        }\n+\n+        Ok(values_read)\n+    }\n+\n+    #[inline]\n+    fn reload(&mut self) -> bool {\n+        assert!(self.bit_reader.is_some());\n+        if let Some(ref mut bit_reader) = self.bit_reader {\n+            if let Some(indicator_value) = bit_reader.get_vlq_int() {\n+                if indicator_value & 1 == 1 {\n+                    self.bit_packed_left = ((indicator_value >> 1) * 8) as u32;\n+                } else {\n+                    self.rle_left = (indicator_value >> 1) as u32;\n+                    let value_width = bit_util::ceil(self.bit_width as i64, 8);\n+                    self.current_value = bit_reader.get_aligned::<u64>(value_width as usize);\n+                    assert!(self.current_value.is_some());\n+                }\n+                return true;\n+            } else {\n+                return false;\n+            }\n+        }\n+        return false;\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use rand::{\n+        self,\n+        distributions::{Distribution, Standard},\n+        thread_rng, Rng, SeedableRng,\n+    };\n+\n+    use crate::parquet::util::memory::ByteBufferPtr;\n+\n+    const MAX_WIDTH: usize = 32;\n+\n+    #[test]\n+    fn test_rle_decode_int32() {\n+        // Test data: 0-7 with bit width 3\n+        // 00000011 10001000 11000110 11111010\n+        let data = ByteBufferPtr::new(vec![0x03, 0x88, 0xC6, 0xFA]);\n+        let mut decoder: RleDecoder = RleDecoder::new(3);\n+        decoder.set_data(data);\n+        let mut buffer = vec![0; 8];\n+        let expected = vec![0, 1, 2, 3, 4, 5, 6, 7];\n+        let result = decoder.get_batch::<i32>(&mut buffer);\n+        assert!(result.is_ok());\n+        assert_eq!(buffer, expected);\n+    }\n+\n+    #[test]\n+    fn test_rle_consume_flush_buffer() {\n+        let data = vec![1, 1, 1, 2, 2, 3, 3, 3];\n+        let mut encoder1 = RleEncoder::new(3, 256);\n+        let mut encoder2 = RleEncoder::new(3, 256);\n+        for value in data {\n+            encoder1.put(value as u64).unwrap();\n+            encoder2.put(value as u64).unwrap();\n+        }\n+        let res1 = encoder1.flush_buffer().unwrap();\n+        let res2 = encoder2.consume().unwrap();\n+        assert_eq!(res1, &res2[..]);\n+    }\n+\n+    #[test]\n+    fn test_rle_decode_bool() {\n+        // RLE test data: 50 1s followed by 50 0s\n+        // 01100100 00000001 01100100 00000000\n+        let data1 = ByteBufferPtr::new(vec![0x64, 0x01, 0x64, 0x00]);\n+\n+        // Bit-packing test data: alternating 1s and 0s, 100 total\n+        // 100 / 8 = 13 groups\n+        // 00011011 10101010 ... 00001010\n+        let data2 = ByteBufferPtr::new(vec![\n+            0x1B, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0xAA, 0x0A,\n+        ]);\n+\n+        let mut decoder: RleDecoder = RleDecoder::new(1);\n+        decoder.set_data(data1);\n+        let mut buffer = vec![false; 100];\n+        let mut expected = vec![];\n+        for i in 0..100 {\n+            if i < 50 {\n+                expected.push(true);\n+            } else {\n+                expected.push(false);\n+            }\n+        }\n+        let result = decoder.get_batch::<bool>(&mut buffer);\n+        assert!(result.is_ok());\n+        assert_eq!(buffer, expected);\n+\n+        decoder.set_data(data2);\n+        let mut buffer = vec![false; 100];\n+        let mut expected = vec![];\n+        for i in 0..100 {\n+            if i % 2 == 0 {\n+                expected.push(false);\n+            } else {\n+                expected.push(true);\n+            }\n+        }\n+        let result = decoder.get_batch::<bool>(&mut buffer);\n+        assert!(result.is_ok());\n+        assert_eq!(buffer, expected);\n+    }\n+\n+    #[test]\n+    fn test_rle_decode_with_dict_int32() {\n+        // Test RLE encoding: 3 0s followed by 4 1s followed by 5 2s\n+        // 00000110 00000000 00001000 00000001 00001010 00000010\n+        let dict = vec![10, 20, 30];\n+        let data = ByteBufferPtr::new(vec![0x06, 0x00, 0x08, 0x01, 0x0A, 0x02]);\n+        let mut decoder: RleDecoder = RleDecoder::new(3);\n+        decoder.set_data(data);\n+        let mut buffer = vec![0; 12];\n+        let expected = vec![10, 10, 10, 20, 20, 20, 20, 30, 30, 30, 30, 30];\n+        let result = decoder.get_batch_with_dict::<i32>(&dict, &mut buffer, 12);\n+        assert!(result.is_ok());\n+        assert_eq!(buffer, expected);\n+\n+        // Test bit-pack encoding: 345345345455 (2 groups: 8 and 4)\n+        // 011 100 101 011 100 101 011 100 101 100 101 101\n+        // 00000011 01100011 11000111 10001110 00000011 01100101 00001011\n+        let dict = vec![\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"fff\"];\n+        let data = ByteBufferPtr::new(vec![0x03, 0x63, 0xC7, 0x8E, 0x03, 0x65, 0x0B]);\n+        let mut decoder: RleDecoder = RleDecoder::new(3);\n+        decoder.set_data(data);\n+        let mut buffer = vec![\"\"; 12];\n+        let expected = vec![\n+            \"ddd\", \"eee\", \"fff\", \"ddd\", \"eee\", \"fff\", \"ddd\", \"eee\", \"fff\", \"eee\", \"fff\", \"fff\",\n+        ];\n+        let result =\n+            decoder.get_batch_with_dict::<&str>(dict.as_slice(), buffer.as_mut_slice(), 12);\n+        assert!(result.is_ok());\n+        assert_eq!(buffer, expected);\n+    }\n+\n+    fn validate_rle(\n+        values: &[i64],\n+        bit_width: u8,\n+        expected_encoding: Option<&[u8]>,\n+        expected_len: i32,\n+    ) {\n+        let buffer_len = 64 * 1024;\n+        let mut encoder = RleEncoder::new(bit_width, buffer_len);\n+        for v in values {\n+            let result = encoder.put(*v as u64);\n+            assert!(result.is_ok());\n+        }\n+        let buffer = ByteBufferPtr::new(encoder.consume().expect(\"Expect consume() OK\"));\n+        if expected_len != -1 {\n+            assert_eq!(buffer.len(), expected_len as usize);\n+        }\n+        match expected_encoding {\n+            Some(b) => assert_eq!(buffer.as_ref(), b),\n+            _ => (),\n+        }\n+\n+        // Verify read\n+        let mut decoder = RleDecoder::new(bit_width);\n+        decoder.set_data(buffer.all());\n+        for v in values {\n+            let val: i64 = decoder\n+                .get()\n+                .expect(\"get() should be OK\")\n+                .expect(\"get() should return more value\");\n+            assert_eq!(val, *v);\n+        }\n+\n+        // Verify batch read\n+        decoder.set_data(buffer);\n+        let mut values_read: Vec<i64> = vec![0; values.len()];\n+        decoder\n+            .get_batch(&mut values_read[..])\n+            .expect(\"get_batch() should be OK\");\n+        assert_eq!(&values_read[..], values);\n+    }\n+\n+    #[test]\n+    fn test_rle_specific_sequences() {\n+        let mut expected_buffer = Vec::new();\n+        let mut values = Vec::new();\n+        for _ in 0..50 {\n+            values.push(0);\n+        }\n+        for _ in 0..50 {\n+            values.push(1);\n+        }\n+        expected_buffer.push(50 << 1);\n+        expected_buffer.push(0);\n+        expected_buffer.push(50 << 1);\n+        expected_buffer.push(1);\n+\n+        for width in 1..9 {\n+            validate_rle(&values[..], width, Some(&expected_buffer[..]), 4);\n+        }\n+        for width in 9..MAX_WIDTH + 1 {\n+            validate_rle(\n+                &values[..],\n+                width as u8,\n+                None,\n+                2 * (1 + bit_util::ceil(width as i64, 8) as i32),\n+            );\n+        }\n+\n+        // Test 100 0's and 1's alternating\n+        values.clear();\n+        expected_buffer.clear();\n+        for i in 0..101 {\n+            values.push(i % 2);\n+        }\n+        let num_groups = bit_util::ceil(100, 8) as u8;\n+        expected_buffer.push(((num_groups << 1) as u8) | 1);\n+        for _ in 1..(100 / 8) + 1 {\n+            expected_buffer.push(0b10101010);\n+        }\n+        // For the last 4 0 and 1's, padded with 0.\n+        expected_buffer.push(0b00001010);\n+        validate_rle(\n+            &values,\n+            1,\n+            Some(&expected_buffer[..]),\n+            1 + num_groups as i32,\n+        );\n+        for width in 2..MAX_WIDTH + 1 {\n+            let num_values = bit_util::ceil(100, 8) * 8;\n+            validate_rle(\n+                &values,\n+                width as u8,\n+                None,\n+                1 + bit_util::ceil(width as i64 * num_values, 8) as i32,\n+            );\n+        }\n+    }\n+\n+    // `validate_rle` on `num_vals` with width `bit_width`. If `value` is -1, that value\n+    // is used, otherwise alternating values are used.\n+    fn test_rle_values(bit_width: usize, num_vals: usize, value: i32) {\n+        let mod_val = if bit_width == 64 {\n+            1\n+        } else {\n+            1u64 << bit_width\n+        };\n+        let mut values: Vec<i64> = vec![];\n+        for v in 0..num_vals {\n+            let val = if value == -1 {\n+                v as i64 % mod_val as i64\n+            } else {\n+                value as i64\n+            };\n+            values.push(val);\n+        }\n+        validate_rle(&values, bit_width as u8, None, -1);\n+    }\n+\n+    #[test]\n+    fn test_values() {\n+        for width in 1..MAX_WIDTH + 1 {\n+            test_rle_values(width, 1, -1);\n+            test_rle_values(width, 1024, -1);\n+            test_rle_values(width, 1024, 0);\n+            test_rle_values(width, 1024, 1);\n+        }\n+    }\n+\n+    #[test]\n+    fn test_rle_specific_roundtrip() {\n+        let bit_width = 1;\n+        let buffer_len = RleEncoder::min_buffer_size(bit_width);\n+        let values: Vec<i16> = vec![0, 1, 1, 1, 1, 0, 0, 0, 0, 1];\n+        let mut encoder = RleEncoder::new(bit_width, buffer_len);\n+        for v in &values {\n+            assert!(encoder.put(*v as u64).expect(\"put() should be OK\"));\n+        }\n+        let buffer = encoder.consume().expect(\"consume() should be OK\");\n+        let mut decoder = RleDecoder::new(bit_width);\n+        decoder.set_data(ByteBufferPtr::new(buffer));\n+        let mut actual_values: Vec<i16> = vec![0; values.len()];\n+        decoder\n+            .get_batch(&mut actual_values)\n+            .expect(\"get_batch() should be OK\");\n+        assert_eq!(actual_values, values);\n+    }\n+\n+    fn test_round_trip(values: &[i32], bit_width: u8) {\n+        let buffer_len = 64 * 1024;\n+        let mut encoder = RleEncoder::new(bit_width, buffer_len);\n+        for v in values {\n+            let result = encoder.put(*v as u64).expect(\"put() should be OK\");\n+            assert!(result, \"put() should not return false\");\n+        }\n+\n+        let buffer = ByteBufferPtr::new(encoder.consume().expect(\"consume() should be OK\"));\n+\n+        // Verify read\n+        let mut decoder = RleDecoder::new(bit_width);\n+        decoder.set_data(buffer.all());\n+        for v in values {\n+            let val = decoder\n+                .get::<i32>()\n+                .expect(\"get() should be OK\")\n+                .expect(\"get() should return value\");\n+            assert_eq!(val, *v);\n+        }\n+\n+        // Verify batch read\n+        let mut decoder = RleDecoder::new(bit_width);\n+        decoder.set_data(buffer);\n+        let mut values_read: Vec<i32> = vec![0; values.len()];\n+        decoder\n+            .get_batch(&mut values_read[..])\n+            .expect(\"get_batch() should be OK\");\n+        assert_eq!(&values_read[..], values);\n+    }\n+\n+    #[test]\n+    fn test_random() {\n+        let seed_len = 32;\n+        let niters = 50;\n+        let ngroups = 1000;\n+        let max_group_size = 15;\n+        let mut values = vec![];\n+\n+        for _ in 0..niters {\n+            values.clear();\n+            let mut rng = thread_rng();\n+            let seed_vec: Vec<u8> = Standard.sample_iter(&mut rng).take(seed_len).collect();\n+            let mut seed = [0u8; 32];\n+            seed.copy_from_slice(&seed_vec[0..seed_len]);\n+            let mut gen = rand::StdRng::from_seed(seed);\n+\n+            let mut parity = false;\n+            for _ in 0..ngroups {\n+                let mut group_size = gen.gen_range::<u32>(1, 20);\n+                if group_size > max_group_size {\n+                    group_size = 1;\n+                }\n+                for _ in 0..group_size {\n+                    values.push(parity as i32);\n+                }\n+                parity = !parity;\n+            }\n+            let bit_width = bit_util::num_required_bits(values.len() as u64);\n+            assert!(bit_width < 64);\n+            test_round_trip(&values[..], bit_width as u8);\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/errors.rs b/rust/src/parquet/errors.rs\nnew file mode 100644\nindex 0000000000..a5532c1eb6\n--- /dev/null\n+++ b/rust/src/parquet/errors.rs\n@@ -0,0 +1,87 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Common Parquet errors and macros.\n+\n+use std::{cell, convert, io, result};\n+\n+use quick_error::quick_error;\n+use snap;\n+use thrift;\n+\n+quick_error! {\n+  /// Set of errors that can be produced during different operations in Parquet.\n+  #[derive(Debug, PartialEq)]\n+  pub enum ParquetError {\n+      /// General Parquet error.\n+      /// Returned when code violates normal workflow of working with Parquet files.\n+      General(message: String) {\n+          display(\"Parquet error: {}\", message)\n+              description(message)\n+              from(e: io::Error) -> (format!(\"underlying IO error: {}\", e))\n+              from(e: snap::Error) -> (format!(\"underlying snap error: {}\", e))\n+              from(e: thrift::Error) -> (format!(\"underlying Thrift error: {}\", e))\n+              from(e: cell::BorrowMutError) -> (format!(\"underlying borrow error: {}\", e))\n+      }\n+      /// \"Not yet implemented\" Parquet error.\n+      /// Returned when functionality is not yet available.\n+      NYI(message: String) {\n+          display(\"NYI: {}\", message)\n+              description(message)\n+      }\n+      /// \"End of file\" Parquet error.\n+      /// Returned when IO related failures occur, e.g. when there are not enough bytes to\n+      /// decode.\n+      EOF(message: String) {\n+          display(\"EOF: {}\", message)\n+              description(message)\n+      }\n+  }\n+}\n+\n+/// A specialized `Result` for Parquet errors.\n+pub type Result<T> = result::Result<T, ParquetError>;\n+\n+// ----------------------------------------------------------------------\n+// Conversion from `ParquetError` to other types of `Error`s\n+\n+impl convert::From<ParquetError> for io::Error {\n+    fn from(e: ParquetError) -> Self {\n+        io::Error::new(io::ErrorKind::Other, e)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// Convenient macros for different errors\n+\n+macro_rules! general_err {\n+    ($fmt:expr) => (ParquetError::General($fmt.to_owned()));\n+    ($fmt:expr, $($args:expr),*) => (ParquetError::General(format!($fmt, $($args),*)));\n+    ($e:expr, $fmt:expr) => (ParquetError::General($fmt.to_owned(), $e));\n+    ($e:ident, $fmt:expr, $($args:tt),*) => (\n+        ParquetError::General(&format!($fmt, $($args),*), $e));\n+}\n+\n+macro_rules! nyi_err {\n+    ($fmt:expr) => (ParquetError::NYI($fmt.to_owned()));\n+    ($fmt:expr, $($args:expr),*) => (ParquetError::NYI(format!($fmt, $($args),*)));\n+}\n+\n+macro_rules! eof_err {\n+    ($fmt:expr) => (ParquetError::EOF($fmt.to_owned()));\n+    ($fmt:expr, $($args:expr),*) => (ParquetError::EOF(format!($fmt, $($args),*)));\n+}\ndiff --git a/rust/src/parquet/file/metadata.rs b/rust/src/parquet/file/metadata.rs\nnew file mode 100644\nindex 0000000000..7f2442506f\n--- /dev/null\n+++ b/rust/src/parquet/file/metadata.rs\n@@ -0,0 +1,736 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains information about available Parquet metadata.\n+//!\n+//! The hierarchy of metadata is as follows:\n+//!\n+//! [`ParquetMetaData`](struct.ParquetMetaData.html) contains\n+//! [`FileMetaData`](struct.FileMetaData.html) and zero or more\n+//! [`RowGroupMetaData`](struct.RowGroupMetaData.html) for each row group.\n+//!\n+//! [`FileMetaData`](struct.FileMetaData.html) includes file version, application specific\n+//! metadata.\n+//!\n+//! Each [`RowGroupMetaData`](struct.RowGroupMetaData.html) contains information about row\n+//! group and one or more [`ColumnChunkMetaData`](struct.ColumnChunkMetaData.html) for\n+//! each column chunk.\n+//!\n+//! [`ColumnChunkMetaData`](struct.ColumnChunkMetaData.html) has information about column\n+//! chunk (primitive leaf column), including encoding/compression, number of values, etc.\n+\n+use std::rc::Rc;\n+\n+use parquet_format::{ColumnChunk, ColumnMetaData, RowGroup};\n+\n+use crate::parquet::basic::{ColumnOrder, Compression, Encoding, Type};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::file::statistics::{self, Statistics};\n+use crate::parquet::schema::types::{\n+    ColumnDescPtr, ColumnDescriptor, ColumnPath, SchemaDescPtr, SchemaDescriptor,\n+    Type as SchemaType, TypePtr,\n+};\n+\n+/// Reference counted pointer for [`ParquetMetaData`].\n+pub type ParquetMetaDataPtr = Rc<ParquetMetaData>;\n+\n+/// Global Parquet metadata.\n+pub struct ParquetMetaData {\n+    file_metadata: FileMetaDataPtr,\n+    row_groups: Vec<RowGroupMetaDataPtr>,\n+}\n+\n+impl ParquetMetaData {\n+    /// Creates Parquet metadata from file metadata and a list of row group metadata `Rc`s\n+    /// for each available row group.\n+    pub fn new(file_metadata: FileMetaData, row_group_ptrs: Vec<RowGroupMetaDataPtr>) -> Self {\n+        ParquetMetaData {\n+            file_metadata: Rc::new(file_metadata),\n+            row_groups: row_group_ptrs,\n+        }\n+    }\n+\n+    /// Returns file metadata as reference counted clone.\n+    pub fn file_metadata(&self) -> FileMetaDataPtr {\n+        self.file_metadata.clone()\n+    }\n+\n+    /// Returns number of row groups in this file.\n+    pub fn num_row_groups(&self) -> usize {\n+        self.row_groups.len()\n+    }\n+\n+    /// Returns row group metadata for `i`th position.\n+    /// Position should be less than number of row groups `num_row_groups`.\n+    pub fn row_group(&self, i: usize) -> RowGroupMetaDataPtr {\n+        self.row_groups[i].clone()\n+    }\n+\n+    /// Returns slice of row group reference counted pointers in this file.\n+    pub fn row_groups(&self) -> &[RowGroupMetaDataPtr] {\n+        &self.row_groups.as_slice()\n+    }\n+}\n+\n+/// Reference counted pointer for [`FileMetaData`].\n+pub type FileMetaDataPtr = Rc<FileMetaData>;\n+\n+/// Metadata for a Parquet file.\n+pub struct FileMetaData {\n+    version: i32,\n+    num_rows: i64,\n+    created_by: Option<String>,\n+    schema: TypePtr,\n+    schema_descr: SchemaDescPtr,\n+    column_orders: Option<Vec<ColumnOrder>>,\n+}\n+\n+impl FileMetaData {\n+    /// Creates new file metadata.\n+    pub fn new(\n+        version: i32,\n+        num_rows: i64,\n+        created_by: Option<String>,\n+        schema: TypePtr,\n+        schema_descr: SchemaDescPtr,\n+        column_orders: Option<Vec<ColumnOrder>>,\n+    ) -> Self {\n+        FileMetaData {\n+            version,\n+            num_rows,\n+            created_by,\n+            schema,\n+            schema_descr,\n+            column_orders,\n+        }\n+    }\n+\n+    /// Returns version of this file.\n+    pub fn version(&self) -> i32 {\n+        self.version\n+    }\n+\n+    /// Returns number of rows in the file.\n+    pub fn num_rows(&self) -> i64 {\n+        self.num_rows\n+    }\n+\n+    /// String message for application that wrote this file.\n+    ///\n+    /// This should have the following format:\n+    /// `<application> version <application version> (build <application build hash>)`.\n+    ///\n+    /// ```shell\n+    /// parquet-mr version 1.8.0 (build 0fda28af84b9746396014ad6a415b90592a98b3b)\n+    /// ```\n+    pub fn created_by(&self) -> &Option<String> {\n+        &self.created_by\n+    }\n+\n+    /// Returns Parquet ['Type`] that describes schema in this file.\n+    pub fn schema(&self) -> &SchemaType {\n+        self.schema.as_ref()\n+    }\n+\n+    /// Returns a reference to schema descriptor.\n+    pub fn schema_descr(&self) -> &SchemaDescriptor {\n+        &self.schema_descr\n+    }\n+\n+    /// Returns reference counted clone for schema descriptor.\n+    pub fn schema_descr_ptr(&self) -> SchemaDescPtr {\n+        self.schema_descr.clone()\n+    }\n+\n+    /// Column (sort) order used for `min` and `max` values of each column in this file.\n+    ///\n+    /// Each column order corresponds to one column, determined by its position in the list,\n+    /// matching the position of the column in the schema.\n+    ///\n+    /// When `None` is returned, there are no column orders available, and each column\n+    /// should be assumed to have undefined (legacy) column order.\n+    pub fn column_orders(&self) -> Option<&Vec<ColumnOrder>> {\n+        self.column_orders.as_ref()\n+    }\n+\n+    /// Returns column order for `i`th column in this file.\n+    /// If column orders are not available, returns undefined (legacy) column order.\n+    pub fn column_order(&self, i: usize) -> ColumnOrder {\n+        self.column_orders\n+            .as_ref()\n+            .map(|data| data[i])\n+            .unwrap_or(ColumnOrder::UNDEFINED)\n+    }\n+}\n+\n+/// Reference counted pointer for [`RowGroupMetaData`].\n+pub type RowGroupMetaDataPtr = Rc<RowGroupMetaData>;\n+\n+/// Metadata for a row group.\n+pub struct RowGroupMetaData {\n+    columns: Vec<ColumnChunkMetaDataPtr>,\n+    num_rows: i64,\n+    total_byte_size: i64,\n+    schema_descr: SchemaDescPtr,\n+}\n+\n+impl RowGroupMetaData {\n+    /// Returns builer for row group metadata.\n+    pub fn builder(schema_descr: SchemaDescPtr) -> RowGroupMetaDataBuilder {\n+        RowGroupMetaDataBuilder::new(schema_descr)\n+    }\n+\n+    /// Number of columns in this row group.\n+    pub fn num_columns(&self) -> usize {\n+        self.columns.len()\n+    }\n+\n+    /// Returns column chunk metadata for `i`th column.\n+    pub fn column(&self, i: usize) -> &ColumnChunkMetaData {\n+        &self.columns[i]\n+    }\n+\n+    /// Returns slice of column chunk metadata [`Rc`] pointers.\n+    pub fn columns(&self) -> &[ColumnChunkMetaDataPtr] {\n+        &self.columns\n+    }\n+\n+    /// Number of rows in this row group.\n+    pub fn num_rows(&self) -> i64 {\n+        self.num_rows\n+    }\n+\n+    /// Total byte size of all uncompressed column data in this row group.\n+    pub fn total_byte_size(&self) -> i64 {\n+        self.total_byte_size\n+    }\n+\n+    /// Returns reference to a schema descriptor.\n+    pub fn schema_descr(&self) -> &SchemaDescriptor {\n+        self.schema_descr.as_ref()\n+    }\n+\n+    /// Returns reference counted clone of schema descriptor.\n+    pub fn schema_descr_ptr(&self) -> SchemaDescPtr {\n+        self.schema_descr.clone()\n+    }\n+\n+    /// Method to convert from Thrift.\n+    pub fn from_thrift(schema_descr: SchemaDescPtr, mut rg: RowGroup) -> Result<RowGroupMetaData> {\n+        assert_eq!(schema_descr.num_columns(), rg.columns.len());\n+        let total_byte_size = rg.total_byte_size;\n+        let num_rows = rg.num_rows;\n+        let mut columns = vec![];\n+        for (c, d) in rg.columns.drain(0..).zip(schema_descr.columns()) {\n+            let cc = ColumnChunkMetaData::from_thrift(d.clone(), c)?;\n+            columns.push(Rc::new(cc));\n+        }\n+        Ok(RowGroupMetaData {\n+            columns,\n+            num_rows,\n+            total_byte_size,\n+            schema_descr,\n+        })\n+    }\n+\n+    /// Method to convert to Thrift.\n+    pub fn to_thrift(&self) -> RowGroup {\n+        RowGroup {\n+            columns: self.columns().into_iter().map(|v| v.to_thrift()).collect(),\n+            total_byte_size: self.total_byte_size,\n+            num_rows: self.num_rows,\n+            sorting_columns: None,\n+        }\n+    }\n+}\n+\n+/// Builder for row group metadata.\n+pub struct RowGroupMetaDataBuilder {\n+    columns: Vec<ColumnChunkMetaDataPtr>,\n+    schema_descr: SchemaDescPtr,\n+    num_rows: i64,\n+    total_byte_size: i64,\n+}\n+\n+impl RowGroupMetaDataBuilder {\n+    /// Creates new builder from schema descriptor.\n+    fn new(schema_descr: SchemaDescPtr) -> Self {\n+        Self {\n+            columns: Vec::with_capacity(schema_descr.num_columns()),\n+            schema_descr,\n+            num_rows: 0,\n+            total_byte_size: 0,\n+        }\n+    }\n+\n+    /// Sets number of rows in this row group.\n+    pub fn set_num_rows(mut self, value: i64) -> Self {\n+        self.num_rows = value;\n+        self\n+    }\n+\n+    /// Sets total size in bytes for this row group.\n+    pub fn set_total_byte_size(mut self, value: i64) -> Self {\n+        self.total_byte_size = value;\n+        self\n+    }\n+\n+    /// Sets column metadata for this row group.\n+    pub fn set_column_metadata(mut self, value: Vec<ColumnChunkMetaDataPtr>) -> Self {\n+        self.columns = value;\n+        self\n+    }\n+\n+    /// Builds row group metadata.\n+    pub fn build(self) -> Result<RowGroupMetaData> {\n+        if self.schema_descr.num_columns() != self.columns.len() {\n+            return Err(general_err!(\n+                \"Column length mismatch: {} != {}\",\n+                self.schema_descr.num_columns(),\n+                self.columns.len()\n+            ));\n+        }\n+\n+        Ok(RowGroupMetaData {\n+            columns: self.columns,\n+            num_rows: self.num_rows,\n+            total_byte_size: self.total_byte_size,\n+            schema_descr: self.schema_descr,\n+        })\n+    }\n+}\n+\n+/// Reference counted pointer for [`ColumnChunkMetaData`].\n+pub type ColumnChunkMetaDataPtr = Rc<ColumnChunkMetaData>;\n+\n+/// Metadata for a column chunk.\n+pub struct ColumnChunkMetaData {\n+    column_type: Type,\n+    column_path: ColumnPath,\n+    column_descr: ColumnDescPtr,\n+    encodings: Vec<Encoding>,\n+    file_path: Option<String>,\n+    file_offset: i64,\n+    num_values: i64,\n+    compression: Compression,\n+    total_compressed_size: i64,\n+    total_uncompressed_size: i64,\n+    data_page_offset: i64,\n+    index_page_offset: Option<i64>,\n+    dictionary_page_offset: Option<i64>,\n+    statistics: Option<Statistics>,\n+}\n+\n+/// Represents common operations for a column chunk.\n+impl ColumnChunkMetaData {\n+    /// Returns builder for column chunk metadata.\n+    pub fn builder(column_descr: ColumnDescPtr) -> ColumnChunkMetaDataBuilder {\n+        ColumnChunkMetaDataBuilder::new(column_descr)\n+    }\n+\n+    /// File where the column chunk is stored.\n+    ///\n+    /// If not set, assumed to belong to the same file as the metadata.\n+    /// This path is relative to the current file.\n+    pub fn file_path(&self) -> Option<&String> {\n+        self.file_path.as_ref()\n+    }\n+\n+    /// Byte offset in `file_path()`.\n+    pub fn file_offset(&self) -> i64 {\n+        self.file_offset\n+    }\n+\n+    /// Type of this column. Must be primitive.\n+    pub fn column_type(&self) -> Type {\n+        self.column_type\n+    }\n+\n+    /// Path (or identifier) of this column.\n+    pub fn column_path(&self) -> &ColumnPath {\n+        &self.column_path\n+    }\n+\n+    /// Descriptor for this column.\n+    pub fn column_descr(&self) -> &ColumnDescriptor {\n+        self.column_descr.as_ref()\n+    }\n+\n+    /// Reference counted clone of descriptor for this column.\n+    pub fn column_descr_ptr(&self) -> ColumnDescPtr {\n+        self.column_descr.clone()\n+    }\n+\n+    /// All encodings used for this column.\n+    pub fn encodings(&self) -> &Vec<Encoding> {\n+        &self.encodings\n+    }\n+\n+    /// Total number of values in this column chunk.\n+    pub fn num_values(&self) -> i64 {\n+        self.num_values\n+    }\n+\n+    /// Compression for this column.\n+    pub fn compression(&self) -> Compression {\n+        self.compression\n+    }\n+\n+    /// Returns the total compressed data size of this column chunk.\n+    pub fn compressed_size(&self) -> i64 {\n+        self.total_compressed_size\n+    }\n+\n+    /// Returns the total uncompressed data size of this column chunk.\n+    pub fn uncompressed_size(&self) -> i64 {\n+        self.total_uncompressed_size\n+    }\n+\n+    /// Returns the offset for the column data.\n+    pub fn data_page_offset(&self) -> i64 {\n+        self.data_page_offset\n+    }\n+\n+    /// Returns `true` if this column chunk contains a index page, `false` otherwise.\n+    pub fn has_index_page(&self) -> bool {\n+        self.index_page_offset.is_some()\n+    }\n+\n+    /// Returns the offset for the index page.\n+    pub fn index_page_offset(&self) -> Option<i64> {\n+        self.index_page_offset\n+    }\n+\n+    /// Returns `true` if this column chunk contains a dictionary page, `false` otherwise.\n+    pub fn has_dictionary_page(&self) -> bool {\n+        self.dictionary_page_offset.is_some()\n+    }\n+\n+    /// Returns the offset for the dictionary page, if any.\n+    pub fn dictionary_page_offset(&self) -> Option<i64> {\n+        self.dictionary_page_offset\n+    }\n+\n+    /// Returns statistics that are set for this column chunk,\n+    /// or `None` if no statistics are available.\n+    pub fn statistics(&self) -> Option<&Statistics> {\n+        self.statistics.as_ref()\n+    }\n+\n+    /// Method to convert from Thrift.\n+    pub fn from_thrift(column_descr: ColumnDescPtr, cc: ColumnChunk) -> Result<Self> {\n+        if cc.meta_data.is_none() {\n+            return Err(general_err!(\"Expected to have column metadata\"));\n+        }\n+        let mut col_metadata: ColumnMetaData = cc.meta_data.unwrap();\n+        let column_type = Type::from(col_metadata.type_);\n+        let column_path = ColumnPath::new(col_metadata.path_in_schema);\n+        let encodings = col_metadata\n+            .encodings\n+            .drain(0..)\n+            .map(Encoding::from)\n+            .collect();\n+        let compression = Compression::from(col_metadata.codec);\n+        let file_path = cc.file_path;\n+        let file_offset = cc.file_offset;\n+        let num_values = col_metadata.num_values;\n+        let total_compressed_size = col_metadata.total_compressed_size;\n+        let total_uncompressed_size = col_metadata.total_uncompressed_size;\n+        let data_page_offset = col_metadata.data_page_offset;\n+        let index_page_offset = col_metadata.index_page_offset;\n+        let dictionary_page_offset = col_metadata.dictionary_page_offset;\n+        let statistics = statistics::from_thrift(column_type, col_metadata.statistics);\n+        let result = ColumnChunkMetaData {\n+            column_type,\n+            column_path,\n+            column_descr,\n+            encodings,\n+            file_path,\n+            file_offset,\n+            num_values,\n+            compression,\n+            total_compressed_size,\n+            total_uncompressed_size,\n+            data_page_offset,\n+            index_page_offset,\n+            dictionary_page_offset,\n+            statistics,\n+        };\n+        Ok(result)\n+    }\n+\n+    /// Method to convert to Thrift.\n+    pub fn to_thrift(&self) -> ColumnChunk {\n+        let column_metadata = ColumnMetaData {\n+            type_: self.column_type.into(),\n+            encodings: self.encodings().into_iter().map(|&v| v.into()).collect(),\n+            path_in_schema: Vec::from(self.column_path.as_ref()),\n+            codec: self.compression.into(),\n+            num_values: self.num_values,\n+            total_uncompressed_size: self.total_uncompressed_size,\n+            total_compressed_size: self.total_compressed_size,\n+            key_value_metadata: None,\n+            data_page_offset: self.data_page_offset,\n+            index_page_offset: self.index_page_offset,\n+            dictionary_page_offset: self.dictionary_page_offset,\n+            statistics: statistics::to_thrift(self.statistics.as_ref()),\n+            encoding_stats: None,\n+        };\n+\n+        ColumnChunk {\n+            file_path: self.file_path().map(|v| v.clone()),\n+            file_offset: self.file_offset,\n+            meta_data: Some(column_metadata),\n+            offset_index_offset: None,\n+            offset_index_length: None,\n+            column_index_offset: None,\n+            column_index_length: None,\n+        }\n+    }\n+}\n+\n+/// Builder for column chunk metadata.\n+pub struct ColumnChunkMetaDataBuilder {\n+    column_descr: ColumnDescPtr,\n+    encodings: Vec<Encoding>,\n+    file_path: Option<String>,\n+    file_offset: i64,\n+    num_values: i64,\n+    compression: Compression,\n+    total_compressed_size: i64,\n+    total_uncompressed_size: i64,\n+    data_page_offset: i64,\n+    index_page_offset: Option<i64>,\n+    dictionary_page_offset: Option<i64>,\n+    statistics: Option<Statistics>,\n+}\n+\n+impl ColumnChunkMetaDataBuilder {\n+    /// Creates new column chunk metadata builder.\n+    fn new(column_descr: ColumnDescPtr) -> Self {\n+        Self {\n+            column_descr,\n+            encodings: Vec::new(),\n+            file_path: None,\n+            file_offset: 0,\n+            num_values: 0,\n+            compression: Compression::UNCOMPRESSED,\n+            total_compressed_size: 0,\n+            total_uncompressed_size: 0,\n+            data_page_offset: 0,\n+            index_page_offset: None,\n+            dictionary_page_offset: None,\n+            statistics: None,\n+        }\n+    }\n+\n+    /// Sets list of encodings for this column chunk.\n+    pub fn set_encodings(mut self, encodings: Vec<Encoding>) -> Self {\n+        self.encodings = encodings;\n+        self\n+    }\n+\n+    /// Sets optional file path for this column chunk.\n+    pub fn set_file_path(mut self, value: String) -> Self {\n+        self.file_path = Some(value);\n+        self\n+    }\n+\n+    /// Sets file offset in bytes.\n+    pub fn set_file_offset(mut self, value: i64) -> Self {\n+        self.file_offset = value;\n+        self\n+    }\n+\n+    /// Sets number of values.\n+    pub fn set_num_values(mut self, value: i64) -> Self {\n+        self.num_values = value;\n+        self\n+    }\n+\n+    /// Sets compression.\n+    pub fn set_compression(mut self, value: Compression) -> Self {\n+        self.compression = value;\n+        self\n+    }\n+\n+    /// Sets total compressed size in bytes.\n+    pub fn set_total_compressed_size(mut self, value: i64) -> Self {\n+        self.total_compressed_size = value;\n+        self\n+    }\n+\n+    /// Sets total uncompressed size in bytes.\n+    pub fn set_total_uncompressed_size(mut self, value: i64) -> Self {\n+        self.total_uncompressed_size = value;\n+        self\n+    }\n+\n+    /// Sets data page offset in bytes.\n+    pub fn set_data_page_offset(mut self, value: i64) -> Self {\n+        self.data_page_offset = value;\n+        self\n+    }\n+\n+    /// Sets optional dictionary page ofset in bytes.\n+    pub fn set_dictionary_page_offset(mut self, value: Option<i64>) -> Self {\n+        self.dictionary_page_offset = value;\n+        self\n+    }\n+\n+    /// Sets optional index page offset in bytes.\n+    pub fn set_index_page_offset(mut self, value: Option<i64>) -> Self {\n+        self.index_page_offset = value;\n+        self\n+    }\n+\n+    /// Sets statistics for this column chunk.\n+    pub fn set_statistics(mut self, value: Statistics) -> Self {\n+        self.statistics = Some(value);\n+        self\n+    }\n+\n+    /// Builds column chunk metadata.\n+    pub fn build(self) -> Result<ColumnChunkMetaData> {\n+        Ok(ColumnChunkMetaData {\n+            column_type: self.column_descr.physical_type(),\n+            column_path: self.column_descr.path().clone(),\n+            column_descr: self.column_descr,\n+            encodings: self.encodings,\n+            file_path: self.file_path,\n+            file_offset: self.file_offset,\n+            num_values: self.num_values,\n+            compression: self.compression,\n+            total_compressed_size: self.total_compressed_size,\n+            total_uncompressed_size: self.total_uncompressed_size,\n+            data_page_offset: self.data_page_offset,\n+            index_page_offset: self.index_page_offset,\n+            dictionary_page_offset: self.dictionary_page_offset,\n+            statistics: self.statistics,\n+        })\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_row_group_metadata_thrift_conversion() {\n+        let schema_descr = get_test_schema_descr();\n+\n+        let mut columns = vec![];\n+        for ptr in schema_descr.columns() {\n+            let column = ColumnChunkMetaData::builder(ptr.clone()).build().unwrap();\n+            columns.push(Rc::new(column));\n+        }\n+        let row_group_meta = RowGroupMetaData::builder(schema_descr.clone())\n+            .set_num_rows(1000)\n+            .set_total_byte_size(2000)\n+            .set_column_metadata(columns)\n+            .build()\n+            .unwrap();\n+\n+        let row_group_exp = row_group_meta.to_thrift();\n+        let row_group_res =\n+            RowGroupMetaData::from_thrift(schema_descr.clone(), row_group_exp.clone())\n+                .unwrap()\n+                .to_thrift();\n+\n+        assert_eq!(row_group_res, row_group_exp);\n+    }\n+\n+    #[test]\n+    fn test_row_group_metadata_thrift_conversion_empty() {\n+        let schema_descr = get_test_schema_descr();\n+\n+        let row_group_meta = RowGroupMetaData::builder(schema_descr.clone()).build();\n+\n+        assert!(row_group_meta.is_err());\n+        if let Err(e) = row_group_meta {\n+            assert_eq!(\n+                e.to_string(),\n+                \"Parquet error: Column length mismatch: 2 != 0\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_column_chunk_metadata_thrift_conversion() {\n+        let column_descr = get_test_schema_descr().column(0);\n+\n+        let col_metadata = ColumnChunkMetaData::builder(column_descr.clone())\n+            .set_encodings(vec![Encoding::PLAIN, Encoding::RLE])\n+            .set_file_path(\"file_path\".to_owned())\n+            .set_file_offset(100)\n+            .set_num_values(1000)\n+            .set_compression(Compression::SNAPPY)\n+            .set_total_compressed_size(2000)\n+            .set_total_uncompressed_size(3000)\n+            .set_data_page_offset(4000)\n+            .set_dictionary_page_offset(Some(5000))\n+            .build()\n+            .unwrap();\n+\n+        let col_chunk_exp = col_metadata.to_thrift();\n+\n+        let col_chunk_res =\n+            ColumnChunkMetaData::from_thrift(column_descr.clone(), col_chunk_exp.clone())\n+                .unwrap()\n+                .to_thrift();\n+\n+        assert_eq!(col_chunk_res, col_chunk_exp);\n+    }\n+\n+    #[test]\n+    fn test_column_chunk_metadata_thrift_conversion_empty() {\n+        let column_descr = get_test_schema_descr().column(0);\n+\n+        let col_metadata = ColumnChunkMetaData::builder(column_descr.clone())\n+            .build()\n+            .unwrap();\n+\n+        let col_chunk_exp = col_metadata.to_thrift();\n+        let col_chunk_res =\n+            ColumnChunkMetaData::from_thrift(column_descr.clone(), col_chunk_exp.clone())\n+                .unwrap()\n+                .to_thrift();\n+\n+        assert_eq!(col_chunk_res, col_chunk_exp);\n+    }\n+\n+    /// Returns sample schema descriptor so we can create column metadata.\n+    fn get_test_schema_descr() -> SchemaDescPtr {\n+        let schema = SchemaType::group_type_builder(\"schema\")\n+            .with_fields(&mut vec![\n+                Rc::new(\n+                    SchemaType::primitive_type_builder(\"a\", Type::INT32)\n+                        .build()\n+                        .unwrap(),\n+                ),\n+                Rc::new(\n+                    SchemaType::primitive_type_builder(\"b\", Type::INT32)\n+                        .build()\n+                        .unwrap(),\n+                ),\n+            ])\n+            .build()\n+            .unwrap();\n+\n+        Rc::new(SchemaDescriptor::new(Rc::new(schema)))\n+    }\n+}\ndiff --git a/rust/src/parquet/file/mod.rs b/rust/src/parquet/file/mod.rs\nnew file mode 100644\nindex 0000000000..ebaebbad0b\n--- /dev/null\n+++ b/rust/src/parquet/file/mod.rs\n@@ -0,0 +1,88 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Main entrypoint for working with Parquet API.\n+//!\n+//! Provides access to file and row group readers and writers, record API, metadata, etc.\n+//!\n+//! See [`reader::SerializedFileReader`](reader/struct.SerializedFileReader.html) or\n+//! [`writer::SerializedFileWriter`](writer/struct.SerializedFileWriter.html) for a\n+//! starting reference, [`metadata::ParquetMetaData`](metadata/index.html) for file\n+//! metadata, and [`statistics`](statistics/index.html) for working with statistics.\n+//!\n+//! # Example of writing a new file\n+//!\n+//! ```rust\n+//! use std::{fs, path::Path, rc::Rc};\n+//!\n+//! use arrow::parquet::{\n+//!     file::{\n+//!         properties::WriterProperties,\n+//!         writer::{FileWriter, SerializedFileWriter},\n+//!     },\n+//!     schema::parser::parse_message_type,\n+//! };\n+//!\n+//! let path = Path::new(\"target/debug/examples/sample.parquet\");\n+//!\n+//! let message_type = \"\n+//!   message schema {\n+//!     REQUIRED INT32 b;\n+//!   }\n+//! \";\n+//! let schema = Rc::new(parse_message_type(message_type).unwrap());\n+//! let props = Rc::new(WriterProperties::builder().build());\n+//! let file = fs::File::create(&path).unwrap();\n+//! let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+//! let mut row_group_writer = writer.next_row_group().unwrap();\n+//! while let Some(mut col_writer) = row_group_writer.next_column().unwrap() {\n+//!     // ... write values to a column writer\n+//!     row_group_writer.close_column(col_writer).unwrap();\n+//! }\n+//! writer.close_row_group(row_group_writer).unwrap();\n+//! writer.close().unwrap();\n+//!\n+//! let bytes = fs::read(&path).unwrap();\n+//! assert_eq!(&bytes[0..4], &[b'P', b'A', b'R', b'1']);\n+//! ```\n+//! # Example of reading an existing file\n+//!\n+//! ```rust\n+//! use arrow::parquet::file::reader::{FileReader, SerializedFileReader};\n+//! use std::{fs::File, path::Path};\n+//!\n+//! let path = Path::new(\"target/debug/examples/sample.parquet\");\n+//! if let Ok(file) = File::open(&path) {\n+//!     let file = File::open(&path).unwrap();\n+//!     let reader = SerializedFileReader::new(file).unwrap();\n+//!\n+//!     let parquet_metadata = reader.metadata();\n+//!     assert_eq!(parquet_metadata.num_row_groups(), 1);\n+//!\n+//!     let row_group_reader = reader.get_row_group(0).unwrap();\n+//!     assert_eq!(row_group_reader.num_columns(), 1);\n+//! }\n+//! ```\n+\n+pub mod metadata;\n+pub mod properties;\n+pub mod reader;\n+pub mod statistics;\n+pub mod writer;\n+\n+const FOOTER_SIZE: usize = 8;\n+const PARQUET_MAGIC: [u8; 4] = [b'P', b'A', b'R', b'1'];\ndiff --git a/rust/src/parquet/file/properties.rs b/rust/src/parquet/file/properties.rs\nnew file mode 100644\nindex 0000000000..911ec55733\n--- /dev/null\n+++ b/rust/src/parquet/file/properties.rs\n@@ -0,0 +1,648 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Writer properties.\n+//!\n+//! # Usage\n+//!\n+//! ```rust\n+//! use arrow::parquet::{\n+//!     basic::{Compression, Encoding},\n+//!     file::properties::*,\n+//!     schema::types::ColumnPath,\n+//! };\n+//!\n+//! // Create properties with default configuration.\n+//! let props = WriterProperties::builder().build();\n+//!\n+//! // Use properties builder to set certain options and assemble the configuration.\n+//! let props = WriterProperties::builder()\n+//!     .set_writer_version(WriterVersion::PARQUET_1_0)\n+//!     .set_encoding(Encoding::PLAIN)\n+//!     .set_column_encoding(ColumnPath::from(\"col1\"), Encoding::DELTA_BINARY_PACKED)\n+//!     .set_compression(Compression::SNAPPY)\n+//!     .build();\n+//!\n+//! assert_eq!(props.writer_version(), WriterVersion::PARQUET_1_0);\n+//! assert_eq!(\n+//!     props.encoding(&ColumnPath::from(\"col1\")),\n+//!     Some(Encoding::DELTA_BINARY_PACKED)\n+//! );\n+//! assert_eq!(\n+//!     props.encoding(&ColumnPath::from(\"col2\")),\n+//!     Some(Encoding::PLAIN)\n+//! );\n+//! ```\n+\n+use std::{collections::HashMap, rc::Rc};\n+\n+use crate::parquet::basic::{Compression, Encoding};\n+use crate::parquet::schema::types::ColumnPath;\n+\n+const DEFAULT_PAGE_SIZE: usize = 1024 * 1024;\n+const DEFAULT_WRITE_BATCH_SIZE: usize = 1024;\n+const DEFAULT_WRITER_VERSION: WriterVersion = WriterVersion::PARQUET_1_0;\n+const DEFAULT_COMPRESSION: Compression = Compression::UNCOMPRESSED;\n+const DEFAULT_DICTIONARY_ENABLED: bool = true;\n+const DEFAULT_DICTIONARY_PAGE_SIZE_LIMIT: usize = DEFAULT_PAGE_SIZE;\n+const DEFAULT_STATISTICS_ENABLED: bool = true;\n+const DEFAULT_MAX_STATISTICS_SIZE: usize = 4096;\n+const DEFAULT_MAX_ROW_GROUP_SIZE: usize = 128 * 1024 * 1024;\n+const DEFAULT_CREATED_BY: &str = env!(\"PARQUET_CREATED_BY\");\n+\n+/// Parquet writer version.\n+///\n+/// Basic constant, which is not part of the Thrift definition.\n+#[derive(Debug, Clone, Copy, PartialEq)]\n+pub enum WriterVersion {\n+    PARQUET_1_0,\n+    PARQUET_2_0,\n+}\n+\n+impl WriterVersion {\n+    /// Returns writer version as `i32`.\n+    pub fn as_num(&self) -> i32 {\n+        match self {\n+            WriterVersion::PARQUET_1_0 => 1,\n+            WriterVersion::PARQUET_2_0 => 2,\n+        }\n+    }\n+}\n+\n+/// Reference counted writer properties.\n+pub type WriterPropertiesPtr = Rc<WriterProperties>;\n+\n+/// Writer properties.\n+///\n+/// It is created as an immutable data structure, use [`WriterPropertiesBuilder`] to\n+/// assemble the properties.\n+#[derive(Debug, Clone)]\n+pub struct WriterProperties {\n+    data_pagesize_limit: usize,\n+    dictionary_pagesize_limit: usize,\n+    write_batch_size: usize,\n+    max_row_group_size: usize,\n+    writer_version: WriterVersion,\n+    created_by: String,\n+    default_column_properties: ColumnProperties,\n+    column_properties: HashMap<ColumnPath, ColumnProperties>,\n+}\n+\n+impl WriterProperties {\n+    /// Returns builder for writer properties with default values.\n+    pub fn builder() -> WriterPropertiesBuilder {\n+        WriterPropertiesBuilder::with_defaults()\n+    }\n+\n+    /// Returns data page size limit.\n+    pub fn data_pagesize_limit(&self) -> usize {\n+        self.data_pagesize_limit\n+    }\n+\n+    /// Returns dictionary page size limit.\n+    pub fn dictionary_pagesize_limit(&self) -> usize {\n+        self.dictionary_pagesize_limit\n+    }\n+\n+    /// Returns configured batch size for writes.\n+    ///\n+    /// When writing a batch of data, this setting allows to split it internally into\n+    /// smaller batches so we can better estimate the size of a page currently being\n+    /// written.\n+    pub fn write_batch_size(&self) -> usize {\n+        self.write_batch_size\n+    }\n+\n+    /// Returns max size for a row group.\n+    pub fn max_row_group_size(&self) -> usize {\n+        self.max_row_group_size\n+    }\n+\n+    /// Returns configured writer version.\n+    pub fn writer_version(&self) -> WriterVersion {\n+        self.writer_version\n+    }\n+\n+    /// Returns `created_by` string.\n+    pub fn created_by(&self) -> &str {\n+        &self.created_by\n+    }\n+\n+    /// Returns encoding for a data page, when dictionary encoding is enabled.\n+    /// This is not configurable.\n+    #[inline]\n+    pub fn dictionary_data_page_encoding(&self) -> Encoding {\n+        // PLAIN_DICTIONARY encoding is deprecated in writer version 1.\n+        // Dictionary values are encoded using RLE_DICTIONARY encoding.\n+        Encoding::RLE_DICTIONARY\n+    }\n+\n+    /// Returns encoding for dictionary page, when dictionary encoding is enabled.\n+    /// This is not configurable.\n+    #[inline]\n+    pub fn dictionary_page_encoding(&self) -> Encoding {\n+        // PLAIN_DICTIONARY is deprecated in writer version 1.\n+        // Dictionary is encoded using plain encoding.\n+        Encoding::PLAIN\n+    }\n+\n+    /// Returns encoding for a column, if set.\n+    /// In case when dictionary is enabled, returns fallback encoding.\n+    ///\n+    /// If encoding is not set, then column writer will choose the best encoding\n+    /// based on the column type.\n+    pub fn encoding(&self, col: &ColumnPath) -> Option<Encoding> {\n+        self.column_properties\n+            .get(col)\n+            .and_then(|c| c.encoding())\n+            .or_else(|| self.default_column_properties.encoding())\n+    }\n+\n+    /// Returns compression codec for a column.\n+    pub fn compression(&self, col: &ColumnPath) -> Compression {\n+        self.column_properties\n+            .get(col)\n+            .and_then(|c| c.compression())\n+            .or_else(|| self.default_column_properties.compression())\n+            .unwrap_or(DEFAULT_COMPRESSION)\n+    }\n+\n+    /// Returns `true` if dictionary encoding is enabled for a column.\n+    pub fn dictionary_enabled(&self, col: &ColumnPath) -> bool {\n+        self.column_properties\n+            .get(col)\n+            .and_then(|c| c.dictionary_enabled())\n+            .or_else(|| self.default_column_properties.dictionary_enabled())\n+            .unwrap_or(DEFAULT_DICTIONARY_ENABLED)\n+    }\n+\n+    /// Returns `true` if statistics are enabled for a column.\n+    pub fn statistics_enabled(&self, col: &ColumnPath) -> bool {\n+        self.column_properties\n+            .get(col)\n+            .and_then(|c| c.statistics_enabled())\n+            .or_else(|| self.default_column_properties.statistics_enabled())\n+            .unwrap_or(DEFAULT_STATISTICS_ENABLED)\n+    }\n+\n+    /// Returns max size for statistics.\n+    /// Only applicable if statistics are enabled.\n+    pub fn max_statistics_size(&self, col: &ColumnPath) -> usize {\n+        self.column_properties\n+            .get(col)\n+            .and_then(|c| c.max_statistics_size())\n+            .or_else(|| self.default_column_properties.max_statistics_size())\n+            .unwrap_or(DEFAULT_MAX_STATISTICS_SIZE)\n+    }\n+}\n+\n+/// Writer properties builder.\n+pub struct WriterPropertiesBuilder {\n+    data_pagesize_limit: usize,\n+    dictionary_pagesize_limit: usize,\n+    write_batch_size: usize,\n+    max_row_group_size: usize,\n+    writer_version: WriterVersion,\n+    created_by: String,\n+    default_column_properties: ColumnProperties,\n+    column_properties: HashMap<ColumnPath, ColumnProperties>,\n+}\n+\n+impl WriterPropertiesBuilder {\n+    /// Returns default state of the builder.\n+    fn with_defaults() -> Self {\n+        Self {\n+            data_pagesize_limit: DEFAULT_PAGE_SIZE,\n+            dictionary_pagesize_limit: DEFAULT_DICTIONARY_PAGE_SIZE_LIMIT,\n+            write_batch_size: DEFAULT_WRITE_BATCH_SIZE,\n+            max_row_group_size: DEFAULT_MAX_ROW_GROUP_SIZE,\n+            writer_version: DEFAULT_WRITER_VERSION,\n+            created_by: DEFAULT_CREATED_BY.to_string(),\n+            default_column_properties: ColumnProperties::new(),\n+            column_properties: HashMap::new(),\n+        }\n+    }\n+\n+    /// Finalizes the configuration and returns immutable writer properties struct.\n+    pub fn build(self) -> WriterProperties {\n+        WriterProperties {\n+            data_pagesize_limit: self.data_pagesize_limit,\n+            dictionary_pagesize_limit: self.dictionary_pagesize_limit,\n+            write_batch_size: self.write_batch_size,\n+            max_row_group_size: self.max_row_group_size,\n+            writer_version: self.writer_version,\n+            created_by: self.created_by,\n+            default_column_properties: self.default_column_properties,\n+            column_properties: self.column_properties,\n+        }\n+    }\n+\n+    // ----------------------------------------------------------------------\n+    // Writer properies related to a file\n+\n+    /// Sets writer version.\n+    pub fn set_writer_version(mut self, value: WriterVersion) -> Self {\n+        self.writer_version = value;\n+        self\n+    }\n+\n+    /// Sets data page size limit.\n+    pub fn set_data_pagesize_limit(mut self, value: usize) -> Self {\n+        self.data_pagesize_limit = value;\n+        self\n+    }\n+\n+    /// Sets dictionary page size limit.\n+    pub fn set_dictionary_pagesize_limit(mut self, value: usize) -> Self {\n+        self.dictionary_pagesize_limit = value;\n+        self\n+    }\n+\n+    /// Sets write batch size.\n+    pub fn set_write_batch_size(mut self, value: usize) -> Self {\n+        self.write_batch_size = value;\n+        self\n+    }\n+\n+    /// Sets max size for a row group.\n+    pub fn set_max_row_group_size(mut self, value: usize) -> Self {\n+        self.max_row_group_size = value;\n+        self\n+    }\n+\n+    /// Sets \"created by\" property.\n+    pub fn set_created_by(mut self, value: String) -> Self {\n+        self.created_by = value;\n+        self\n+    }\n+\n+    // ----------------------------------------------------------------------\n+    // Setters for any column (global)\n+\n+    /// Sets encoding for any column.\n+    ///\n+    /// If dictionary is not enabled, this is treated as a primary encoding for all columns.\n+    /// In case when dictionary is enabled for any column, this value is considered to\n+    /// be a fallback encoding for that column.\n+    ///\n+    /// Panics if user tries to set dictionary encoding here, regardless of dictinoary\n+    /// encoding flag being set.\n+    pub fn set_encoding(mut self, value: Encoding) -> Self {\n+        self.default_column_properties.set_encoding(value);\n+        self\n+    }\n+\n+    /// Sets compression codec for any column.\n+    pub fn set_compression(mut self, value: Compression) -> Self {\n+        self.default_column_properties.set_compression(value);\n+        self\n+    }\n+\n+    /// Sets flag to enable/disable dictionary encoding for any column.\n+    ///\n+    /// Use this method to set dictionary encoding, instead of explicitly specifying\n+    /// encoding in `set_encoding` method.\n+    pub fn set_dictionary_enabled(mut self, value: bool) -> Self {\n+        self.default_column_properties.set_dictionary_enabled(value);\n+        self\n+    }\n+\n+    /// Sets flag to enable/disable statistics for any column.\n+    pub fn set_statistics_enabled(mut self, value: bool) -> Self {\n+        self.default_column_properties.set_statistics_enabled(value);\n+        self\n+    }\n+\n+    /// Sets max statistics size for any column.\n+    /// Applicable only if statistics are enabled.\n+    pub fn set_max_statistics_size(mut self, value: usize) -> Self {\n+        self.default_column_properties\n+            .set_max_statistics_size(value);\n+        self\n+    }\n+\n+    // ----------------------------------------------------------------------\n+    // Setters for a specific column\n+\n+    /// Helper method to get existing or new mutable reference of column properties.\n+    #[inline]\n+    fn get_mut_props(&mut self, col: ColumnPath) -> &mut ColumnProperties {\n+        self.column_properties\n+            .entry(col)\n+            .or_insert(ColumnProperties::new())\n+    }\n+\n+    /// Sets encoding for a column.\n+    /// Takes precedence over globally defined settings.\n+    ///\n+    /// If dictionary is not enabled, this is treated as a primary encoding for this column.\n+    /// In case when dictionary is enabled for this column, either through global defaults\n+    /// or explicitly, this value is considered to be a fallback encoding for this column.\n+    ///\n+    /// Panics if user tries to set dictionary encoding here, regardless of dictinoary\n+    /// encoding flag being set.\n+    pub fn set_column_encoding(mut self, col: ColumnPath, value: Encoding) -> Self {\n+        self.get_mut_props(col).set_encoding(value);\n+        self\n+    }\n+\n+    /// Sets compression codec for a column.\n+    /// Takes precedence over globally defined settings.\n+    pub fn set_column_compression(mut self, col: ColumnPath, value: Compression) -> Self {\n+        self.get_mut_props(col).set_compression(value);\n+        self\n+    }\n+\n+    /// Sets flag to enable/disable dictionary encoding for a column.\n+    /// Takes precedence over globally defined settings.\n+    pub fn set_column_dictionary_enabled(mut self, col: ColumnPath, value: bool) -> Self {\n+        self.get_mut_props(col).set_dictionary_enabled(value);\n+        self\n+    }\n+\n+    /// Sets flag to enable/disable statistics for a column.\n+    /// Takes precedence over globally defined settings.\n+    pub fn set_column_statistics_enabled(mut self, col: ColumnPath, value: bool) -> Self {\n+        self.get_mut_props(col).set_statistics_enabled(value);\n+        self\n+    }\n+\n+    /// Sets max size for statistics for a column.\n+    /// Takes precedence over globally defined settings.\n+    pub fn set_column_max_statistics_size(mut self, col: ColumnPath, value: usize) -> Self {\n+        self.get_mut_props(col).set_max_statistics_size(value);\n+        self\n+    }\n+}\n+\n+/// Container for column properties that can be changed as part of writer.\n+///\n+/// If a field is `None`, it means that no specific value has been set for this column,\n+/// so some subsequent or default value must be used.\n+#[derive(Debug, Clone, PartialEq)]\n+struct ColumnProperties {\n+    encoding: Option<Encoding>,\n+    codec: Option<Compression>,\n+    dictionary_enabled: Option<bool>,\n+    statistics_enabled: Option<bool>,\n+    max_statistics_size: Option<usize>,\n+}\n+\n+impl ColumnProperties {\n+    /// Initialise column properties with default values.\n+    fn new() -> Self {\n+        Self {\n+            encoding: None,\n+            codec: None,\n+            dictionary_enabled: None,\n+            statistics_enabled: None,\n+            max_statistics_size: None,\n+        }\n+    }\n+\n+    /// Sets encoding for this column.\n+    ///\n+    /// If dictionary is not enabled, this is treated as a primary encoding for a column.\n+    /// In case when dictionary is enabled for a column, this value is considered to\n+    /// be a fallback encoding.\n+    ///\n+    /// Panics if user tries to set dictionary encoding here, regardless of dictinoary\n+    /// encoding flag being set. Use `set_dictionary_enabled` method to enable dictionary\n+    /// for a column.\n+    fn set_encoding(&mut self, value: Encoding) {\n+        if value == Encoding::PLAIN_DICTIONARY || value == Encoding::RLE_DICTIONARY {\n+            panic!(\"Dictionary encoding can not be used as fallback encoding\");\n+        }\n+        self.encoding = Some(value);\n+    }\n+\n+    /// Sets compression codec for this column.\n+    fn set_compression(&mut self, value: Compression) {\n+        self.codec = Some(value);\n+    }\n+\n+    /// Sets whether or not dictionary encoding is enabled for this column.\n+    fn set_dictionary_enabled(&mut self, enabled: bool) {\n+        self.dictionary_enabled = Some(enabled);\n+    }\n+\n+    /// Sets whether or not statistics are enabled for this column.\n+    fn set_statistics_enabled(&mut self, enabled: bool) {\n+        self.statistics_enabled = Some(enabled);\n+    }\n+\n+    /// Sets max size for statistics for this column.\n+    fn set_max_statistics_size(&mut self, value: usize) {\n+        self.max_statistics_size = Some(value);\n+    }\n+\n+    /// Returns optional encoding for this column.\n+    fn encoding(&self) -> Option<Encoding> {\n+        self.encoding\n+    }\n+\n+    /// Returns optional compression codec for this column.\n+    fn compression(&self) -> Option<Compression> {\n+        self.codec\n+    }\n+\n+    /// Returns `Some(true)` if dictionary encoding is enabled for this column, if disabled\n+    /// then returns `Some(false)`. If result is `None`, then no setting has been provided.\n+    fn dictionary_enabled(&self) -> Option<bool> {\n+        self.dictionary_enabled\n+    }\n+\n+    /// Returns `Some(true)` if statistics are enabled for this column, if disabled then\n+    /// returns `Some(false)`. If result is `None`, then no setting has been provided.\n+    fn statistics_enabled(&self) -> Option<bool> {\n+        self.statistics_enabled\n+    }\n+\n+    /// Returns optional max size in bytes for statistics.\n+    fn max_statistics_size(&self) -> Option<usize> {\n+        self.max_statistics_size\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_writer_version() {\n+        assert_eq!(WriterVersion::PARQUET_1_0.as_num(), 1);\n+        assert_eq!(WriterVersion::PARQUET_2_0.as_num(), 2);\n+    }\n+\n+    #[test]\n+    fn test_writer_properties_default_settings() {\n+        let props = WriterProperties::builder().build();\n+        assert_eq!(props.data_pagesize_limit(), DEFAULT_PAGE_SIZE);\n+        assert_eq!(\n+            props.dictionary_pagesize_limit(),\n+            DEFAULT_DICTIONARY_PAGE_SIZE_LIMIT\n+        );\n+        assert_eq!(props.write_batch_size(), DEFAULT_WRITE_BATCH_SIZE);\n+        assert_eq!(props.max_row_group_size(), DEFAULT_MAX_ROW_GROUP_SIZE);\n+        assert_eq!(props.writer_version(), DEFAULT_WRITER_VERSION);\n+        assert_eq!(props.created_by(), DEFAULT_CREATED_BY);\n+        assert_eq!(props.encoding(&ColumnPath::from(\"col\")), None);\n+        assert_eq!(\n+            props.compression(&ColumnPath::from(\"col\")),\n+            DEFAULT_COMPRESSION\n+        );\n+        assert_eq!(\n+            props.dictionary_enabled(&ColumnPath::from(\"col\")),\n+            DEFAULT_DICTIONARY_ENABLED\n+        );\n+        assert_eq!(\n+            props.statistics_enabled(&ColumnPath::from(\"col\")),\n+            DEFAULT_STATISTICS_ENABLED\n+        );\n+        assert_eq!(\n+            props.max_statistics_size(&ColumnPath::from(\"col\")),\n+            DEFAULT_MAX_STATISTICS_SIZE\n+        );\n+    }\n+\n+    #[test]\n+    fn test_writer_properties_dictionary_encoding() {\n+        // dictionary encoding is not configurable, and it should be the same for both\n+        // writer version 1 and 2.\n+        for version in vec![WriterVersion::PARQUET_1_0, WriterVersion::PARQUET_2_0] {\n+            let props = WriterProperties::builder()\n+                .set_writer_version(version)\n+                .build();\n+            assert_eq!(props.dictionary_page_encoding(), Encoding::PLAIN);\n+            assert_eq!(\n+                props.dictionary_data_page_encoding(),\n+                Encoding::RLE_DICTIONARY\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Dictionary encoding can not be used as fallback encoding\")]\n+    fn test_writer_properties_panic_when_plain_dictionary_is_fallback() {\n+        // Should panic when user specifies dictionary encoding as fallback encoding.\n+        WriterProperties::builder()\n+            .set_encoding(Encoding::PLAIN_DICTIONARY)\n+            .build();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Dictionary encoding can not be used as fallback encoding\")]\n+    fn test_writer_properties_panic_when_rle_dictionary_is_fallback() {\n+        // Should panic when user specifies dictionary encoding as fallback encoding.\n+        WriterProperties::builder()\n+            .set_encoding(Encoding::RLE_DICTIONARY)\n+            .build();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Dictionary encoding can not be used as fallback encoding\")]\n+    fn test_writer_properties_panic_when_dictionary_is_enabled() {\n+        WriterProperties::builder()\n+            .set_dictionary_enabled(true)\n+            .set_column_encoding(ColumnPath::from(\"col\"), Encoding::RLE_DICTIONARY)\n+            .build();\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Dictionary encoding can not be used as fallback encoding\")]\n+    fn test_writer_properties_panic_when_dictionary_is_disabled() {\n+        WriterProperties::builder()\n+            .set_dictionary_enabled(false)\n+            .set_column_encoding(ColumnPath::from(\"col\"), Encoding::RLE_DICTIONARY)\n+            .build();\n+    }\n+\n+    #[test]\n+    fn test_writer_properties_builder() {\n+        let props = WriterProperties::builder()\n+            // file settings\n+            .set_writer_version(WriterVersion::PARQUET_2_0)\n+            .set_data_pagesize_limit(10)\n+            .set_dictionary_pagesize_limit(20)\n+            .set_write_batch_size(30)\n+            .set_max_row_group_size(40)\n+            .set_created_by(\"default\".to_owned())\n+            // global column settings\n+            .set_encoding(Encoding::DELTA_BINARY_PACKED)\n+            .set_compression(Compression::GZIP)\n+            .set_dictionary_enabled(false)\n+            .set_statistics_enabled(false)\n+            .set_max_statistics_size(50)\n+            // specific column settings\n+            .set_column_encoding(ColumnPath::from(\"col\"), Encoding::RLE)\n+            .set_column_compression(ColumnPath::from(\"col\"), Compression::SNAPPY)\n+            .set_column_dictionary_enabled(ColumnPath::from(\"col\"), true)\n+            .set_column_statistics_enabled(ColumnPath::from(\"col\"), true)\n+            .set_column_max_statistics_size(ColumnPath::from(\"col\"), 123)\n+            .build();\n+\n+        assert_eq!(props.writer_version(), WriterVersion::PARQUET_2_0);\n+        assert_eq!(props.data_pagesize_limit(), 10);\n+        assert_eq!(props.dictionary_pagesize_limit(), 20);\n+        assert_eq!(props.write_batch_size(), 30);\n+        assert_eq!(props.max_row_group_size(), 40);\n+        assert_eq!(props.created_by(), \"default\");\n+\n+        assert_eq!(\n+            props.encoding(&ColumnPath::from(\"a\")),\n+            Some(Encoding::DELTA_BINARY_PACKED)\n+        );\n+        assert_eq!(props.compression(&ColumnPath::from(\"a\")), Compression::GZIP);\n+        assert_eq!(props.dictionary_enabled(&ColumnPath::from(\"a\")), false);\n+        assert_eq!(props.statistics_enabled(&ColumnPath::from(\"a\")), false);\n+        assert_eq!(props.max_statistics_size(&ColumnPath::from(\"a\")), 50);\n+\n+        assert_eq!(\n+            props.encoding(&ColumnPath::from(\"col\")),\n+            Some(Encoding::RLE)\n+        );\n+        assert_eq!(\n+            props.compression(&ColumnPath::from(\"col\")),\n+            Compression::SNAPPY\n+        );\n+        assert_eq!(props.dictionary_enabled(&ColumnPath::from(\"col\")), true);\n+        assert_eq!(props.statistics_enabled(&ColumnPath::from(\"col\")), true);\n+        assert_eq!(props.max_statistics_size(&ColumnPath::from(\"col\")), 123);\n+    }\n+\n+    #[test]\n+    fn test_writer_properties_builder_partial_defaults() {\n+        let props = WriterProperties::builder()\n+            .set_encoding(Encoding::DELTA_BINARY_PACKED)\n+            .set_compression(Compression::GZIP)\n+            .set_column_encoding(ColumnPath::from(\"col\"), Encoding::RLE)\n+            .build();\n+\n+        assert_eq!(\n+            props.encoding(&ColumnPath::from(\"col\")),\n+            Some(Encoding::RLE)\n+        );\n+        assert_eq!(\n+            props.compression(&ColumnPath::from(\"col\")),\n+            Compression::GZIP\n+        );\n+        assert_eq!(\n+            props.dictionary_enabled(&ColumnPath::from(\"col\")),\n+            DEFAULT_DICTIONARY_ENABLED\n+        );\n+    }\n+}\ndiff --git a/rust/src/parquet/file/reader.rs b/rust/src/parquet/file/reader.rs\nnew file mode 100644\nindex 0000000000..c2e5dd176d\n--- /dev/null\n+++ b/rust/src/parquet/file/reader.rs\n@@ -0,0 +1,899 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains file reader API and provides methods to access file metadata, row group\n+//! readers to read individual column chunks, or access record iterator.\n+\n+use std::{\n+    convert::TryFrom,\n+    fs::File,\n+    io::{BufReader, Cursor, Read, Seek, SeekFrom},\n+    path::Path,\n+    rc::Rc,\n+};\n+\n+use byteorder::{ByteOrder, LittleEndian};\n+use parquet_format::{\n+    ColumnOrder as TColumnOrder, FileMetaData as TFileMetaData, PageHeader, PageType,\n+};\n+use thrift::protocol::TCompactInputProtocol;\n+\n+use crate::parquet::basic::{ColumnOrder, Compression, Encoding, Type};\n+use crate::parquet::column::{\n+    page::{Page, PageReader},\n+    reader::{ColumnReader, ColumnReaderImpl},\n+};\n+use crate::parquet::compression::{create_codec, Codec};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::file::{metadata::*, statistics, FOOTER_SIZE, PARQUET_MAGIC};\n+use crate::parquet::record::reader::RowIter;\n+use crate::parquet::schema::types::{self, SchemaDescriptor, Type as SchemaType};\n+use crate::parquet::util::{io::FileSource, memory::ByteBufferPtr};\n+\n+// ----------------------------------------------------------------------\n+// APIs for file & row group readers\n+\n+/// Parquet file reader API. With this, user can get metadata information about the\n+/// Parquet file, can get reader for each row group, and access record iterator.\n+pub trait FileReader {\n+    /// Get metadata information about this file.\n+    fn metadata(&self) -> ParquetMetaDataPtr;\n+\n+    /// Get the total number of row groups for this file.\n+    fn num_row_groups(&self) -> usize;\n+\n+    /// Get the `i`th row group reader. Note this doesn't do bound check.\n+    fn get_row_group(&self, i: usize) -> Result<Box<RowGroupReader>>;\n+\n+    /// Get full iterator of `Row`s from a file (over all row groups).\n+    ///\n+    /// Iterator will automatically load the next row group to advance.\n+    ///\n+    /// Projected schema can be a subset of or equal to the file schema, when it is None,\n+    /// full file schema is assumed.\n+    fn get_row_iter(&self, projection: Option<SchemaType>) -> Result<RowIter>;\n+}\n+\n+/// Parquet row group reader API. With this, user can get metadata information about the\n+/// row group, as well as readers for each individual column chunk.\n+pub trait RowGroupReader {\n+    /// Get metadata information about this row group.\n+    fn metadata(&self) -> RowGroupMetaDataPtr;\n+\n+    /// Get the total number of column chunks in this row group.\n+    fn num_columns(&self) -> usize;\n+\n+    /// Get page reader for the `i`th column chunk.\n+    fn get_column_page_reader(&self, i: usize) -> Result<Box<PageReader>>;\n+\n+    /// Get value reader for the `i`th column chunk.\n+    fn get_column_reader(&self, i: usize) -> Result<ColumnReader>;\n+\n+    /// Get iterator of `Row`s from this row group.\n+    ///\n+    /// Projected schema can be a subset of or equal to the file schema, when it is None,\n+    /// full file schema is assumed.\n+    fn get_row_iter(&self, projection: Option<SchemaType>) -> Result<RowIter>;\n+}\n+\n+// ----------------------------------------------------------------------\n+// Serialized impl for file & row group readers\n+\n+/// Length should return the amount of bytes that implementor contains.\n+/// It's mainly used to read the metadata, which is at the end of the source.\n+pub trait Length {\n+    /// Returns the amount of bytes of the inner source.\n+    fn len(&self) -> u64;\n+}\n+\n+/// TryClone tries to clone the type and should maintain the `Seek` position of the given\n+/// instance.\n+pub trait TryClone: Sized {\n+    /// Clones the type returning a new instance or an error if it's not possible\n+    /// to clone it.\n+    fn try_clone(&self) -> Result<Self>;\n+}\n+\n+impl Length for File {\n+    fn len(&self) -> u64 {\n+        self.metadata().map(|m| m.len()).unwrap_or(0u64)\n+    }\n+}\n+\n+impl TryClone for File {\n+    fn try_clone(&self) -> Result<Self> {\n+        self.try_clone().map_err(|e| e.into())\n+    }\n+}\n+\n+impl<'a> Length for Cursor<&'a [u8]> {\n+    fn len(&self) -> u64 {\n+        self.get_ref().len() as u64\n+    }\n+}\n+\n+impl<'a> TryClone for Cursor<&'a [u8]> {\n+    fn try_clone(&self) -> Result<Self> {\n+        Ok(self.clone())\n+    }\n+}\n+\n+/// ParquetReader is the interface which needs to be fulfilled to be able to parse a\n+/// parquet source.\n+pub trait ParquetReader: Read + Seek + Length + TryClone {}\n+impl<T: Read + Seek + Length + TryClone> ParquetReader for T {}\n+\n+/// A serialized implementation for Parquet [`FileReader`].\n+pub struct SerializedFileReader<R: ParquetReader> {\n+    buf: BufReader<R>,\n+    metadata: ParquetMetaDataPtr,\n+}\n+\n+impl<R: ParquetReader> SerializedFileReader<R> {\n+    /// Creates file reader from a Parquet file.\n+    /// Returns error if Parquet file does not exist or is corrupt.\n+    pub fn new(reader: R) -> Result<Self> {\n+        let mut buf = BufReader::new(reader);\n+        let metadata = Self::parse_metadata(&mut buf)?;\n+        Ok(Self {\n+            buf,\n+            metadata: Rc::new(metadata),\n+        })\n+    }\n+\n+    // Layout of Parquet file\n+    // +---------------------------+---+-----+\n+    // |      Rest of file         | B |  A  |\n+    // +---------------------------+---+-----+\n+    // where A: parquet footer, B: parquet metadata.\n+    //\n+    fn parse_metadata(buf: &mut BufReader<R>) -> Result<ParquetMetaData> {\n+        let file_size = buf.get_ref().len();\n+        if file_size < (FOOTER_SIZE as u64) {\n+            return Err(general_err!(\n+                \"Invalid Parquet file. Size is smaller than footer\"\n+            ));\n+        }\n+        let mut footer_buffer: [u8; FOOTER_SIZE] = [0; FOOTER_SIZE];\n+        buf.seek(SeekFrom::End(-(FOOTER_SIZE as i64)))?;\n+        buf.read_exact(&mut footer_buffer)?;\n+        if footer_buffer[4..] != PARQUET_MAGIC {\n+            return Err(general_err!(\"Invalid Parquet file. Corrupt footer\"));\n+        }\n+        let metadata_len = LittleEndian::read_i32(&footer_buffer[0..4]) as i64;\n+        if metadata_len < 0 {\n+            return Err(general_err!(\n+                \"Invalid Parquet file. Metadata length is less than zero ({})\",\n+                metadata_len\n+            ));\n+        }\n+        let metadata_start: i64 = file_size as i64 - FOOTER_SIZE as i64 - metadata_len;\n+        if metadata_start < 0 {\n+            return Err(general_err!(\n+                \"Invalid Parquet file. Metadata start is less than zero ({})\",\n+                metadata_start\n+            ));\n+        }\n+        buf.seek(SeekFrom::Start(metadata_start as u64))?;\n+        let metadata_buf = buf.take(metadata_len as u64).into_inner();\n+\n+        // TODO: row group filtering\n+        let mut prot = TCompactInputProtocol::new(metadata_buf);\n+        let mut t_file_metadata: TFileMetaData = TFileMetaData::read_from_in_protocol(&mut prot)\n+            .map_err(|e| ParquetError::General(format!(\"Could not parse metadata: {}\", e)))?;\n+        let schema = types::from_thrift(&mut t_file_metadata.schema)?;\n+        let schema_descr = Rc::new(SchemaDescriptor::new(schema.clone()));\n+        let mut row_groups = Vec::new();\n+        for rg in t_file_metadata.row_groups {\n+            row_groups.push(Rc::new(RowGroupMetaData::from_thrift(\n+                schema_descr.clone(),\n+                rg,\n+            )?));\n+        }\n+        let column_orders = Self::parse_column_orders(t_file_metadata.column_orders, &schema_descr);\n+\n+        let file_metadata = FileMetaData::new(\n+            t_file_metadata.version,\n+            t_file_metadata.num_rows,\n+            t_file_metadata.created_by,\n+            schema,\n+            schema_descr,\n+            column_orders,\n+        );\n+        Ok(ParquetMetaData::new(file_metadata, row_groups))\n+    }\n+\n+    /// Parses column orders from Thrift definition.\n+    /// If no column orders are defined, returns `None`.\n+    fn parse_column_orders(\n+        t_column_orders: Option<Vec<TColumnOrder>>,\n+        schema_descr: &SchemaDescriptor,\n+    ) -> Option<Vec<ColumnOrder>> {\n+        match t_column_orders {\n+            Some(orders) => {\n+                // Should always be the case\n+                assert_eq!(\n+                    orders.len(),\n+                    schema_descr.num_columns(),\n+                    \"Column order length mismatch\"\n+                );\n+                let mut res = Vec::new();\n+                for (i, column) in schema_descr.columns().iter().enumerate() {\n+                    match orders[i] {\n+                        TColumnOrder::TYPEORDER(_) => {\n+                            let sort_order = ColumnOrder::get_sort_order(\n+                                column.logical_type(),\n+                                column.physical_type(),\n+                            );\n+                            res.push(ColumnOrder::TYPE_DEFINED_ORDER(sort_order));\n+                        }\n+                    }\n+                }\n+                Some(res)\n+            }\n+            None => None,\n+        }\n+    }\n+}\n+\n+impl<R: 'static + ParquetReader> FileReader for SerializedFileReader<R> {\n+    fn metadata(&self) -> ParquetMetaDataPtr {\n+        self.metadata.clone()\n+    }\n+\n+    fn num_row_groups(&self) -> usize {\n+        self.metadata.num_row_groups()\n+    }\n+\n+    fn get_row_group(&self, i: usize) -> Result<Box<RowGroupReader>> {\n+        let row_group_metadata = self.metadata.row_group(i);\n+        // Row groups should be processed sequentially.\n+        let f = self.buf.get_ref().try_clone()?;\n+        Ok(Box::new(SerializedRowGroupReader::new(\n+            f,\n+            row_group_metadata,\n+        )))\n+    }\n+\n+    fn get_row_iter(&self, projection: Option<SchemaType>) -> Result<RowIter> {\n+        RowIter::from_file(projection, self)\n+    }\n+}\n+\n+impl TryFrom<File> for SerializedFileReader<File> {\n+    type Error = ParquetError;\n+\n+    fn try_from(file: File) -> Result<Self> {\n+        Self::new(file)\n+    }\n+}\n+\n+impl<'a> TryFrom<&'a Path> for SerializedFileReader<File> {\n+    type Error = ParquetError;\n+\n+    fn try_from(path: &Path) -> Result<Self> {\n+        let file = File::open(path)?;\n+        Self::try_from(file)\n+    }\n+}\n+\n+impl TryFrom<String> for SerializedFileReader<File> {\n+    type Error = ParquetError;\n+\n+    fn try_from(path: String) -> Result<Self> {\n+        Self::try_from(Path::new(&path))\n+    }\n+}\n+\n+impl<'a> TryFrom<&'a str> for SerializedFileReader<File> {\n+    type Error = ParquetError;\n+\n+    fn try_from(path: &str) -> Result<Self> {\n+        Self::try_from(Path::new(&path))\n+    }\n+}\n+\n+/// A serialized implementation for Parquet [`RowGroupReader`].\n+pub struct SerializedRowGroupReader<R: ParquetReader> {\n+    buf: BufReader<R>,\n+    metadata: RowGroupMetaDataPtr,\n+}\n+\n+impl<R: 'static + ParquetReader> SerializedRowGroupReader<R> {\n+    /// Creates new row group reader from a file and row group metadata.\n+    fn new(file: R, metadata: RowGroupMetaDataPtr) -> Self {\n+        let buf = BufReader::new(file);\n+        Self { buf, metadata }\n+    }\n+}\n+\n+impl<R: 'static + ParquetReader> RowGroupReader for SerializedRowGroupReader<R> {\n+    fn metadata(&self) -> RowGroupMetaDataPtr {\n+        self.metadata.clone()\n+    }\n+\n+    fn num_columns(&self) -> usize {\n+        self.metadata.num_columns()\n+    }\n+\n+    // TODO: fix PARQUET-816\n+    fn get_column_page_reader(&self, i: usize) -> Result<Box<PageReader>> {\n+        let col = self.metadata.column(i);\n+        let mut col_start = col.data_page_offset();\n+        if col.has_dictionary_page() {\n+            col_start = col.dictionary_page_offset().unwrap();\n+        }\n+        let col_length = col.compressed_size();\n+        let file_chunk = FileSource::new(self.buf.get_ref(), col_start as u64, col_length as usize);\n+        let page_reader = SerializedPageReader::new(\n+            file_chunk,\n+            col.num_values(),\n+            col.compression(),\n+            col.column_descr().physical_type(),\n+        )?;\n+        Ok(Box::new(page_reader))\n+    }\n+\n+    fn get_column_reader(&self, i: usize) -> Result<ColumnReader> {\n+        let schema_descr = self.metadata.schema_descr();\n+        let col_descr = schema_descr.column(i);\n+        let col_page_reader = self.get_column_page_reader(i)?;\n+        let col_reader = match col_descr.physical_type() {\n+            Type::BOOLEAN => {\n+                ColumnReader::BoolColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::INT32 => {\n+                ColumnReader::Int32ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::INT64 => {\n+                ColumnReader::Int64ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::INT96 => {\n+                ColumnReader::Int96ColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::FLOAT => {\n+                ColumnReader::FloatColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::DOUBLE => {\n+                ColumnReader::DoubleColumnReader(ColumnReaderImpl::new(col_descr, col_page_reader))\n+            }\n+            Type::BYTE_ARRAY => ColumnReader::ByteArrayColumnReader(ColumnReaderImpl::new(\n+                col_descr,\n+                col_page_reader,\n+            )),\n+            Type::FIXED_LEN_BYTE_ARRAY => ColumnReader::FixedLenByteArrayColumnReader(\n+                ColumnReaderImpl::new(col_descr, col_page_reader),\n+            ),\n+        };\n+        Ok(col_reader)\n+    }\n+\n+    fn get_row_iter(&self, projection: Option<SchemaType>) -> Result<RowIter> {\n+        RowIter::from_row_group(projection, self)\n+    }\n+}\n+\n+/// A serialized implementation for Parquet [`PageReader`].\n+pub struct SerializedPageReader<T: Read> {\n+    // The file source buffer which references exactly the bytes for the column trunk\n+    // to be read by this page reader.\n+    buf: T,\n+\n+    // The compression codec for this column chunk. Only set for non-PLAIN codec.\n+    decompressor: Option<Box<Codec>>,\n+\n+    // The number of values we have seen so far.\n+    seen_num_values: i64,\n+\n+    // The number of total values in this column chunk.\n+    total_num_values: i64,\n+\n+    // Column chunk type.\n+    physical_type: Type,\n+}\n+\n+impl<T: Read> SerializedPageReader<T> {\n+    /// Creates a new serialized page reader from file source.\n+    pub fn new(\n+        buf: T,\n+        total_num_values: i64,\n+        compression: Compression,\n+        physical_type: Type,\n+    ) -> Result<Self> {\n+        let decompressor = create_codec(compression)?;\n+        let result = Self {\n+            buf,\n+            total_num_values,\n+            seen_num_values: 0,\n+            decompressor,\n+            physical_type,\n+        };\n+        Ok(result)\n+    }\n+\n+    /// Reads Page header from Thrift.\n+    fn read_page_header(&mut self) -> Result<PageHeader> {\n+        let mut prot = TCompactInputProtocol::new(&mut self.buf);\n+        let page_header = PageHeader::read_from_in_protocol(&mut prot)?;\n+        Ok(page_header)\n+    }\n+}\n+\n+impl<T: Read> PageReader for SerializedPageReader<T> {\n+    fn get_next_page(&mut self) -> Result<Option<Page>> {\n+        while self.seen_num_values < self.total_num_values {\n+            let page_header = self.read_page_header()?;\n+\n+            // When processing data page v2, depending on enabled compression for the page, we\n+            // should account for uncompressed data ('offset') of repetition and definition\n+            // levels.\n+            //\n+            // We always use 0 offset for other pages other than v2, `true` flag means that\n+            // compression will be applied if decompressor is defined\n+            let mut offset: usize = 0;\n+            let mut can_decompress = true;\n+\n+            if let Some(ref header_v2) = page_header.data_page_header_v2 {\n+                offset = (header_v2.definition_levels_byte_length\n+                    + header_v2.repetition_levels_byte_length) as usize;\n+                // When is_compressed flag is missing the page is considered compressed\n+                can_decompress = header_v2.is_compressed.unwrap_or(true);\n+            }\n+\n+            let compressed_len = page_header.compressed_page_size as usize - offset;\n+            let uncompressed_len = page_header.uncompressed_page_size as usize - offset;\n+            // We still need to read all bytes from buffered stream\n+            let mut buffer = vec![0; offset + compressed_len];\n+            self.buf.read_exact(&mut buffer)?;\n+\n+            // TODO: page header could be huge because of statistics. We should set a maximum\n+            // page header size and abort if that is exceeded.\n+            if let Some(decompressor) = self.decompressor.as_mut() {\n+                if can_decompress {\n+                    let mut decompressed_buffer = Vec::with_capacity(uncompressed_len);\n+                    let decompressed_size =\n+                        decompressor.decompress(&buffer[offset..], &mut decompressed_buffer)?;\n+                    if decompressed_size != uncompressed_len {\n+                        return Err(general_err!(\n+                            \"Actual decompressed size doesn't match the expected one ({} vs {})\",\n+                            decompressed_size,\n+                            uncompressed_len\n+                        ));\n+                    }\n+                    if offset == 0 {\n+                        buffer = decompressed_buffer;\n+                    } else {\n+                        // Prepend saved offsets to the buffer\n+                        buffer.truncate(offset);\n+                        buffer.append(&mut decompressed_buffer);\n+                    }\n+                }\n+            }\n+\n+            let result = match page_header.type_ {\n+                PageType::DICTIONARY_PAGE => {\n+                    assert!(page_header.dictionary_page_header.is_some());\n+                    let dict_header = page_header.dictionary_page_header.as_ref().unwrap();\n+                    let is_sorted = dict_header.is_sorted.unwrap_or(false);\n+                    Page::DictionaryPage {\n+                        buf: ByteBufferPtr::new(buffer),\n+                        num_values: dict_header.num_values as u32,\n+                        encoding: Encoding::from(dict_header.encoding),\n+                        is_sorted,\n+                    }\n+                }\n+                PageType::DATA_PAGE => {\n+                    assert!(page_header.data_page_header.is_some());\n+                    let header = page_header.data_page_header.unwrap();\n+                    self.seen_num_values += header.num_values as i64;\n+                    Page::DataPage {\n+                        buf: ByteBufferPtr::new(buffer),\n+                        num_values: header.num_values as u32,\n+                        encoding: Encoding::from(header.encoding),\n+                        def_level_encoding: Encoding::from(header.definition_level_encoding),\n+                        rep_level_encoding: Encoding::from(header.repetition_level_encoding),\n+                        statistics: statistics::from_thrift(self.physical_type, header.statistics),\n+                    }\n+                }\n+                PageType::DATA_PAGE_V2 => {\n+                    assert!(page_header.data_page_header_v2.is_some());\n+                    let header = page_header.data_page_header_v2.unwrap();\n+                    let is_compressed = header.is_compressed.unwrap_or(true);\n+                    self.seen_num_values += header.num_values as i64;\n+                    Page::DataPageV2 {\n+                        buf: ByteBufferPtr::new(buffer),\n+                        num_values: header.num_values as u32,\n+                        encoding: Encoding::from(header.encoding),\n+                        num_nulls: header.num_nulls as u32,\n+                        num_rows: header.num_rows as u32,\n+                        def_levels_byte_len: header.definition_levels_byte_length as u32,\n+                        rep_levels_byte_len: header.repetition_levels_byte_length as u32,\n+                        is_compressed,\n+                        statistics: statistics::from_thrift(self.physical_type, header.statistics),\n+                    }\n+                }\n+                _ => {\n+                    // For unknown page type (e.g., INDEX_PAGE), skip and read next.\n+                    continue;\n+                }\n+            };\n+            return Ok(Some(result));\n+        }\n+\n+        // We are at the end of this column chunk and no more page left. Return None.\n+        Ok(None)\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use parquet_format::TypeDefinedOrder;\n+\n+    use crate::parquet::basic::SortOrder;\n+    use crate::parquet::util::test_common::{get_temp_file, get_test_file, get_test_path};\n+\n+    #[test]\n+    fn test_file_reader_metadata_size_smaller_than_footer() {\n+        let test_file = get_temp_file(\"corrupt-1.parquet\", &[]);\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_err());\n+        assert_eq!(\n+            reader_result.err().unwrap(),\n+            general_err!(\"Invalid Parquet file. Size is smaller than footer\")\n+        );\n+    }\n+\n+    // #[test]\n+    // fn test_cursor_and_file_has_the_same_behaviour() {\n+    //     let path = get_test_path(\"alltypes_plain.parquet\");\n+    //     let buffer = include_bytes!(path);\n+    //     let cursor = Cursor::new(buffer.as_ref());\n+\n+    //     let read_from_file =\n+    //         SerializedFileReader::new(File::open(\"testdata/alltypes_plain.parquet\").unwrap())\n+    //             .unwrap();\n+    //     let read_from_cursor = SerializedFileReader::new(cursor).unwrap();\n+\n+    //     let file_iter = read_from_file.get_row_iter(None).unwrap();\n+    //     let cursor_iter = read_from_cursor.get_row_iter(None).unwrap();\n+\n+    //     assert!(file_iter.eq(cursor_iter));\n+    // }\n+\n+    #[test]\n+    fn test_file_reader_metadata_corrupt_footer() {\n+        let test_file = get_temp_file(\"corrupt-2.parquet\", &[1, 2, 3, 4, 5, 6, 7, 8]);\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_err());\n+        assert_eq!(\n+            reader_result.err().unwrap(),\n+            general_err!(\"Invalid Parquet file. Corrupt footer\")\n+        );\n+    }\n+\n+    #[test]\n+    fn test_file_reader_metadata_invalid_length() {\n+        let test_file = get_temp_file(\"corrupt-3.parquet\", &[0, 0, 0, 255, b'P', b'A', b'R', b'1']);\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_err());\n+        assert_eq!(\n+            reader_result.err().unwrap(),\n+            general_err!(\"Invalid Parquet file. Metadata length is less than zero (-16777216)\")\n+        );\n+    }\n+\n+    #[test]\n+    fn test_file_reader_metadata_invalid_start() {\n+        let test_file = get_temp_file(\"corrupt-4.parquet\", &[255, 0, 0, 0, b'P', b'A', b'R', b'1']);\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_err());\n+        assert_eq!(\n+            reader_result.err().unwrap(),\n+            general_err!(\"Invalid Parquet file. Metadata start is less than zero (-255)\")\n+        );\n+    }\n+\n+    #[test]\n+    fn test_file_reader_column_orders_parse() {\n+        // Define simple schema, we do not need to provide logical types.\n+        let mut fields = vec![\n+            Rc::new(\n+                SchemaType::primitive_type_builder(\"col1\", Type::INT32)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                SchemaType::primitive_type_builder(\"col2\", Type::FLOAT)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+        ];\n+        let schema = SchemaType::group_type_builder(\"schema\")\n+            .with_fields(&mut fields)\n+            .build()\n+            .unwrap();\n+        let schema_descr = SchemaDescriptor::new(Rc::new(schema));\n+\n+        let t_column_orders = Some(vec![\n+            TColumnOrder::TYPEORDER(TypeDefinedOrder::new()),\n+            TColumnOrder::TYPEORDER(TypeDefinedOrder::new()),\n+        ]);\n+\n+        assert_eq!(\n+            SerializedFileReader::<File>::parse_column_orders(t_column_orders, &schema_descr),\n+            Some(vec![\n+                ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::SIGNED),\n+                ColumnOrder::TYPE_DEFINED_ORDER(SortOrder::SIGNED)\n+            ])\n+        );\n+\n+        // Test when no column orders are defined.\n+        assert_eq!(\n+            SerializedFileReader::<File>::parse_column_orders(None, &schema_descr),\n+            None\n+        );\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Column order length mismatch\")]\n+    fn test_file_reader_column_orders_len_mismatch() {\n+        let schema = SchemaType::group_type_builder(\"schema\").build().unwrap();\n+        let schema_descr = SchemaDescriptor::new(Rc::new(schema));\n+\n+        let t_column_orders = Some(vec![TColumnOrder::TYPEORDER(TypeDefinedOrder::new())]);\n+\n+        SerializedFileReader::<File>::parse_column_orders(t_column_orders, &schema_descr);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_try_from() {\n+        // Valid file path\n+        let test_file = get_test_file(\"alltypes_plain.parquet\");\n+        let test_path_buf = get_test_path(\"alltypes_plain.parquet\");\n+        let test_path = test_path_buf.as_path();\n+        let test_path_str = test_path.to_str().unwrap();\n+\n+        let reader = SerializedFileReader::try_from(test_file);\n+        assert!(reader.is_ok());\n+\n+        let reader = SerializedFileReader::try_from(test_path);\n+        assert!(reader.is_ok());\n+\n+        let reader = SerializedFileReader::try_from(test_path_str);\n+        assert!(reader.is_ok());\n+\n+        let reader = SerializedFileReader::try_from(test_path_str.to_string());\n+        assert!(reader.is_ok());\n+\n+        // Invalid file path\n+        let test_path = Path::new(\"invalid.parquet\");\n+        let test_path_str = test_path.to_str().unwrap();\n+\n+        let reader = SerializedFileReader::try_from(test_path);\n+        assert!(reader.is_err());\n+\n+        let reader = SerializedFileReader::try_from(test_path_str);\n+        assert!(reader.is_err());\n+\n+        let reader = SerializedFileReader::try_from(test_path_str.to_string());\n+        assert!(reader.is_err());\n+    }\n+\n+    #[test]\n+    fn test_reuse_file_chunk() {\n+        // This test covers the case of maintaining the correct start position in a file\n+        // stream for each column reader after initializing and moving to the next one\n+        // (without necessarily reading the entire column).\n+        let test_file = get_test_file(\"alltypes_plain.parquet\");\n+        let reader = SerializedFileReader::new(test_file).unwrap();\n+        let row_group = reader.get_row_group(0).unwrap();\n+\n+        let mut page_readers = Vec::new();\n+        for i in 0..row_group.num_columns() {\n+            page_readers.push(row_group.get_column_page_reader(i).unwrap());\n+        }\n+\n+        // Now buffer each col reader, we do not expect any failures like:\n+        // General(\"underlying Thrift error: end of file\")\n+        for mut page_reader in page_readers {\n+            assert!(page_reader.get_next_page().is_ok());\n+        }\n+    }\n+\n+    #[test]\n+    fn test_file_reader() {\n+        let test_file = get_test_file(\"alltypes_plain.parquet\");\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_ok());\n+        let reader = reader_result.unwrap();\n+\n+        // Test contents in Parquet metadata\n+        let metadata = reader.metadata();\n+        assert_eq!(metadata.num_row_groups(), 1);\n+\n+        // Test contents in file metadata\n+        let file_metadata = metadata.file_metadata();\n+        assert!(file_metadata.created_by().is_some());\n+        assert_eq!(\n+            file_metadata.created_by().as_ref().unwrap(),\n+            \"impala version 1.3.0-INTERNAL (build 8a48ddb1eff84592b3fc06bc6f51ec120e1fffc9)\"\n+        );\n+        assert_eq!(file_metadata.num_rows(), 8);\n+        assert_eq!(file_metadata.version(), 1);\n+        assert_eq!(file_metadata.column_orders(), None);\n+\n+        // Test contents in row group metadata\n+        let row_group_metadata = metadata.row_group(0);\n+        assert_eq!(row_group_metadata.num_columns(), 11);\n+        assert_eq!(row_group_metadata.num_rows(), 8);\n+        assert_eq!(row_group_metadata.total_byte_size(), 671);\n+        // Check each column order\n+        for i in 0..row_group_metadata.num_columns() {\n+            assert_eq!(file_metadata.column_order(i), ColumnOrder::UNDEFINED);\n+        }\n+\n+        // Test row group reader\n+        let row_group_reader_result = reader.get_row_group(0);\n+        assert!(row_group_reader_result.is_ok());\n+        let row_group_reader: Box<RowGroupReader> = row_group_reader_result.unwrap();\n+        assert_eq!(\n+            row_group_reader.num_columns(),\n+            row_group_metadata.num_columns()\n+        );\n+        assert_eq!(\n+            row_group_reader.metadata().total_byte_size(),\n+            row_group_metadata.total_byte_size()\n+        );\n+\n+        // Test page readers\n+        // TODO: test for every column\n+        let page_reader_0_result = row_group_reader.get_column_page_reader(0);\n+        assert!(page_reader_0_result.is_ok());\n+        let mut page_reader_0: Box<PageReader> = page_reader_0_result.unwrap();\n+        let mut page_count = 0;\n+        while let Ok(Some(page)) = page_reader_0.get_next_page() {\n+            let is_expected_page = match page {\n+                Page::DictionaryPage {\n+                    buf,\n+                    num_values,\n+                    encoding,\n+                    is_sorted,\n+                } => {\n+                    assert_eq!(buf.len(), 32);\n+                    assert_eq!(num_values, 8);\n+                    assert_eq!(encoding, Encoding::PLAIN_DICTIONARY);\n+                    assert_eq!(is_sorted, false);\n+                    true\n+                }\n+                Page::DataPage {\n+                    buf,\n+                    num_values,\n+                    encoding,\n+                    def_level_encoding,\n+                    rep_level_encoding,\n+                    statistics,\n+                } => {\n+                    assert_eq!(buf.len(), 11);\n+                    assert_eq!(num_values, 8);\n+                    assert_eq!(encoding, Encoding::PLAIN_DICTIONARY);\n+                    assert_eq!(def_level_encoding, Encoding::RLE);\n+                    assert_eq!(rep_level_encoding, Encoding::BIT_PACKED);\n+                    assert!(statistics.is_none());\n+                    true\n+                }\n+                _ => false,\n+            };\n+            assert!(is_expected_page);\n+            page_count += 1;\n+        }\n+        assert_eq!(page_count, 2);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_datapage_v2() {\n+        let test_file = get_test_file(\"datapage_v2.snappy.parquet\");\n+        let reader_result = SerializedFileReader::new(test_file);\n+        assert!(reader_result.is_ok());\n+        let reader = reader_result.unwrap();\n+\n+        // Test contents in Parquet metadata\n+        let metadata = reader.metadata();\n+        assert_eq!(metadata.num_row_groups(), 1);\n+\n+        // Test contents in file metadata\n+        let file_metadata = metadata.file_metadata();\n+        assert!(file_metadata.created_by().is_some());\n+        assert_eq!(\n+            file_metadata.created_by().as_ref().unwrap(),\n+            \"parquet-mr version 1.8.1 (build 4aba4dae7bb0d4edbcf7923ae1339f28fd3f7fcf)\"\n+        );\n+        assert_eq!(file_metadata.num_rows(), 5);\n+        assert_eq!(file_metadata.version(), 1);\n+        assert_eq!(file_metadata.column_orders(), None);\n+\n+        let row_group_metadata = metadata.row_group(0);\n+\n+        // Check each column order\n+        for i in 0..row_group_metadata.num_columns() {\n+            assert_eq!(file_metadata.column_order(i), ColumnOrder::UNDEFINED);\n+        }\n+\n+        // Test row group reader\n+        let row_group_reader_result = reader.get_row_group(0);\n+        assert!(row_group_reader_result.is_ok());\n+        let row_group_reader: Box<RowGroupReader> = row_group_reader_result.unwrap();\n+        assert_eq!(\n+            row_group_reader.num_columns(),\n+            row_group_metadata.num_columns()\n+        );\n+        assert_eq!(\n+            row_group_reader.metadata().total_byte_size(),\n+            row_group_metadata.total_byte_size()\n+        );\n+\n+        // Test page readers\n+        // TODO: test for every column\n+        let page_reader_0_result = row_group_reader.get_column_page_reader(0);\n+        assert!(page_reader_0_result.is_ok());\n+        let mut page_reader_0: Box<PageReader> = page_reader_0_result.unwrap();\n+        let mut page_count = 0;\n+        while let Ok(Some(page)) = page_reader_0.get_next_page() {\n+            let is_expected_page = match page {\n+                Page::DictionaryPage {\n+                    buf,\n+                    num_values,\n+                    encoding,\n+                    is_sorted,\n+                } => {\n+                    assert_eq!(buf.len(), 7);\n+                    assert_eq!(num_values, 1);\n+                    assert_eq!(encoding, Encoding::PLAIN);\n+                    assert_eq!(is_sorted, false);\n+                    true\n+                }\n+                Page::DataPageV2 {\n+                    buf,\n+                    num_values,\n+                    encoding,\n+                    num_nulls,\n+                    num_rows,\n+                    def_levels_byte_len,\n+                    rep_levels_byte_len,\n+                    is_compressed,\n+                    statistics,\n+                } => {\n+                    assert_eq!(buf.len(), 4);\n+                    assert_eq!(num_values, 5);\n+                    assert_eq!(encoding, Encoding::RLE_DICTIONARY);\n+                    assert_eq!(num_nulls, 1);\n+                    assert_eq!(num_rows, 5);\n+                    assert_eq!(def_levels_byte_len, 2);\n+                    assert_eq!(rep_levels_byte_len, 0);\n+                    assert_eq!(is_compressed, true);\n+                    assert!(statistics.is_some());\n+                    true\n+                }\n+                _ => false,\n+            };\n+            assert!(is_expected_page);\n+            page_count += 1;\n+        }\n+        assert_eq!(page_count, 2);\n+    }\n+}\ndiff --git a/rust/src/parquet/file/statistics.rs b/rust/src/parquet/file/statistics.rs\nnew file mode 100644\nindex 0000000000..ff4d731857\n--- /dev/null\n+++ b/rust/src/parquet/file/statistics.rs\n@@ -0,0 +1,692 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains definitions for working with Parquet statistics.\n+//!\n+//! Though some common methods are available on enum, use pattern match to extract\n+//! actual min and max values from statistics, see below:\n+//!\n+//! ```rust\n+//! use arrow::parquet::file::statistics::Statistics;\n+//!\n+//! let stats = Statistics::int32(Some(1), Some(10), None, 3, true);\n+//! assert_eq!(stats.null_count(), 3);\n+//! assert!(stats.has_min_max_set());\n+//! assert!(stats.is_min_max_deprecated());\n+//!\n+//! match stats {\n+//!     Statistics::Int32(ref typed) => {\n+//!         assert_eq!(*typed.min(), 1);\n+//!         assert_eq!(*typed.max(), 10);\n+//!     }\n+//!     _ => {}\n+//! }\n+//! ```\n+\n+use std::{cmp, fmt};\n+\n+use byteorder::{ByteOrder, LittleEndian};\n+use parquet_format::Statistics as TStatistics;\n+\n+use crate::parquet::basic::Type;\n+use crate::parquet::data_type::*;\n+\n+// Macro to generate methods create Statistics.\n+macro_rules! statistics_new_func {\n+    ($func:ident, $vtype:ty, $stat:ident) => {\n+        pub fn $func(\n+            min: $vtype,\n+            max: $vtype,\n+            distinct: Option<u64>,\n+            nulls: u64,\n+            is_deprecated: bool,\n+        ) -> Self {\n+            Statistics::$stat(TypedStatistics::new(\n+                min,\n+                max,\n+                distinct,\n+                nulls,\n+                is_deprecated,\n+            ))\n+        }\n+    };\n+}\n+\n+// Macro to generate getter functions for Statistics.\n+macro_rules! statistics_enum_func {\n+    ($self:ident, $func:ident) => {{\n+        match *$self {\n+            Statistics::Boolean(ref typed) => typed.$func(),\n+            Statistics::Int32(ref typed) => typed.$func(),\n+            Statistics::Int64(ref typed) => typed.$func(),\n+            Statistics::Int96(ref typed) => typed.$func(),\n+            Statistics::Float(ref typed) => typed.$func(),\n+            Statistics::Double(ref typed) => typed.$func(),\n+            Statistics::ByteArray(ref typed) => typed.$func(),\n+            Statistics::FixedLenByteArray(ref typed) => typed.$func(),\n+        }\n+    }};\n+}\n+\n+/// Converts Thrift definition into `Statistics`.\n+pub fn from_thrift(physical_type: Type, thrift_stats: Option<TStatistics>) -> Option<Statistics> {\n+    match thrift_stats {\n+        Some(stats) => {\n+            // Number of nulls recorded, when it is not available, we just mark it as 0.\n+            let null_count = stats.null_count.unwrap_or(0);\n+            assert!(\n+                null_count >= 0,\n+                \"Statistics null count is negative ({})\",\n+                null_count\n+            );\n+\n+            // Generic null count.\n+            let null_count = null_count as u64;\n+            // Generic distinct count (count of distinct values occurring)\n+            let distinct_count = stats.distinct_count.map(|value| value as u64);\n+            // Whether or not statistics use deprecated min/max fields.\n+            let old_format = stats.min_value.is_none() && stats.max_value.is_none();\n+            // Generic min value as bytes.\n+            let min = if old_format {\n+                stats.min\n+            } else {\n+                stats.min_value\n+            };\n+            // Generic max value as bytes.\n+            let max = if old_format {\n+                stats.max\n+            } else {\n+                stats.max_value\n+            };\n+\n+            // Values are encoded using PLAIN encoding definition, except that\n+            // variable-length byte arrays do not include a length prefix.\n+            //\n+            // Instead of using actual decoder, we manually convert values.\n+            let res = match physical_type {\n+                Type::BOOLEAN => Statistics::boolean(\n+                    min.map(|data| data[0] != 0),\n+                    max.map(|data| data[0] != 0),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::INT32 => Statistics::int32(\n+                    min.map(|data| LittleEndian::read_i32(&data)),\n+                    max.map(|data| LittleEndian::read_i32(&data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::INT64 => Statistics::int64(\n+                    min.map(|data| LittleEndian::read_i64(&data)),\n+                    max.map(|data| LittleEndian::read_i64(&data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::INT96 => {\n+                    // INT96 statistics may not be correct, because comparison is signed\n+                    // byte-wise, not actual timestamps. It is recommended to ignore min/max\n+                    // statistics for INT96 columns.\n+                    let min = min.map(|data| {\n+                        assert_eq!(data.len(), 12);\n+                        unsafe {\n+                            let raw = ::std::slice::from_raw_parts(data.as_ptr() as *mut u32, 3);\n+                            Int96::from(Vec::from(raw))\n+                        }\n+                    });\n+                    let max = max.map(|data| {\n+                        assert_eq!(data.len(), 12);\n+                        unsafe {\n+                            let raw = ::std::slice::from_raw_parts(data.as_ptr() as *mut u32, 3);\n+                            Int96::from(Vec::from(raw))\n+                        }\n+                    });\n+                    Statistics::int96(min, max, distinct_count, null_count, old_format)\n+                }\n+                Type::FLOAT => Statistics::float(\n+                    min.map(|data| LittleEndian::read_f32(&data)),\n+                    max.map(|data| LittleEndian::read_f32(&data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::DOUBLE => Statistics::double(\n+                    min.map(|data| LittleEndian::read_f64(&data)),\n+                    max.map(|data| LittleEndian::read_f64(&data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::BYTE_ARRAY => Statistics::byte_array(\n+                    min.map(|data| ByteArray::from(data)),\n+                    max.map(|data| ByteArray::from(data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+                Type::FIXED_LEN_BYTE_ARRAY => Statistics::fixed_len_byte_array(\n+                    min.map(|data| ByteArray::from(data)),\n+                    max.map(|data| ByteArray::from(data)),\n+                    distinct_count,\n+                    null_count,\n+                    old_format,\n+                ),\n+            };\n+\n+            Some(res)\n+        }\n+        None => None,\n+    }\n+}\n+\n+// Convert Statistics into Thrift definition.\n+pub fn to_thrift(stats: Option<&Statistics>) -> Option<TStatistics> {\n+    if stats.is_none() {\n+        return None;\n+    }\n+\n+    let stats = stats.unwrap();\n+\n+    let mut thrift_stats = TStatistics {\n+        max: None,\n+        min: None,\n+        null_count: if stats.has_nulls() {\n+            Some(stats.null_count() as i64)\n+        } else {\n+            None\n+        },\n+        distinct_count: stats.distinct_count().map(|value| value as i64),\n+        max_value: None,\n+        min_value: None,\n+    };\n+\n+    // Get min/max if set.\n+    let (min, max) = if stats.has_min_max_set() {\n+        (\n+            Some(stats.min_bytes().to_vec()),\n+            Some(stats.max_bytes().to_vec()),\n+        )\n+    } else {\n+        (None, None)\n+    };\n+\n+    if stats.is_min_max_deprecated() {\n+        thrift_stats.min = min;\n+        thrift_stats.max = max;\n+    } else {\n+        thrift_stats.min_value = min;\n+        thrift_stats.max_value = max;\n+    }\n+\n+    Some(thrift_stats)\n+}\n+\n+/// Statistics for a column chunk and data page.\n+#[derive(Debug, PartialEq)]\n+pub enum Statistics {\n+    Boolean(TypedStatistics<BoolType>),\n+    Int32(TypedStatistics<Int32Type>),\n+    Int64(TypedStatistics<Int64Type>),\n+    Int96(TypedStatistics<Int96Type>),\n+    Float(TypedStatistics<FloatType>),\n+    Double(TypedStatistics<DoubleType>),\n+    ByteArray(TypedStatistics<ByteArrayType>),\n+    FixedLenByteArray(TypedStatistics<FixedLenByteArrayType>),\n+}\n+\n+impl Statistics {\n+    statistics_new_func![boolean, Option<bool>, Boolean];\n+\n+    statistics_new_func![int32, Option<i32>, Int32];\n+\n+    statistics_new_func![int64, Option<i64>, Int64];\n+\n+    statistics_new_func![int96, Option<Int96>, Int96];\n+\n+    statistics_new_func![float, Option<f32>, Float];\n+\n+    statistics_new_func![double, Option<f64>, Double];\n+\n+    statistics_new_func![byte_array, Option<ByteArray>, ByteArray];\n+\n+    statistics_new_func![fixed_len_byte_array, Option<ByteArray>, FixedLenByteArray];\n+\n+    /// Returns `true` if statistics have old `min` and `max` fields set.\n+    /// This means that the column order is likely to be undefined, which, for old files\n+    /// could mean a signed sort order of values.\n+    ///\n+    /// Refer to [`ColumnOrder`](`::basic::ColumnOrder`) and\n+    /// [`SortOrder`](`::basic::SortOrder`) for more information.\n+    pub fn is_min_max_deprecated(&self) -> bool {\n+        statistics_enum_func![self, is_min_max_deprecated]\n+    }\n+\n+    /// Returns optional value of number of distinct values occurring.\n+    /// When it is `None`, the value should be ignored.\n+    pub fn distinct_count(&self) -> Option<u64> {\n+        statistics_enum_func![self, distinct_count]\n+    }\n+\n+    /// Returns number of null values for the column.\n+    /// Note that this includes all nulls when column is part of the complex type.\n+    pub fn null_count(&self) -> u64 {\n+        statistics_enum_func![self, null_count]\n+    }\n+\n+    /// Returns `true` if statistics collected any null values, `false` otherwise.\n+    pub fn has_nulls(&self) -> bool {\n+        self.null_count() > 0\n+    }\n+\n+    /// Returns `true` if min value and max value are set.\n+    /// Normally both min/max values will be set to `Some(value)` or `None`.\n+    pub fn has_min_max_set(&self) -> bool {\n+        statistics_enum_func![self, has_min_max_set]\n+    }\n+\n+    /// Returns slice of bytes that represent min value.\n+    /// Panics if min value is not set.\n+    pub fn min_bytes(&self) -> &[u8] {\n+        statistics_enum_func![self, min_bytes]\n+    }\n+\n+    /// Returns slice of bytes that represent max value.\n+    /// Panics if max value is not set.\n+    pub fn max_bytes(&self) -> &[u8] {\n+        statistics_enum_func![self, max_bytes]\n+    }\n+\n+    /// Returns physical type associated with statistics.\n+    pub fn physical_type(&self) -> Type {\n+        match self {\n+            Statistics::Boolean(_) => Type::BOOLEAN,\n+            Statistics::Int32(_) => Type::INT32,\n+            Statistics::Int64(_) => Type::INT64,\n+            Statistics::Int96(_) => Type::INT96,\n+            Statistics::Float(_) => Type::FLOAT,\n+            Statistics::Double(_) => Type::DOUBLE,\n+            Statistics::ByteArray(_) => Type::BYTE_ARRAY,\n+            Statistics::FixedLenByteArray(_) => Type::FIXED_LEN_BYTE_ARRAY,\n+        }\n+    }\n+}\n+\n+impl fmt::Display for Statistics {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        match self {\n+            Statistics::Boolean(typed) => write!(f, \"{}\", typed),\n+            Statistics::Int32(typed) => write!(f, \"{}\", typed),\n+            Statistics::Int64(typed) => write!(f, \"{}\", typed),\n+            Statistics::Int96(typed) => write!(f, \"{}\", typed),\n+            Statistics::Float(typed) => write!(f, \"{}\", typed),\n+            Statistics::Double(typed) => write!(f, \"{}\", typed),\n+            Statistics::ByteArray(typed) => write!(f, \"{}\", typed),\n+            Statistics::FixedLenByteArray(typed) => write!(f, \"{}\", typed),\n+        }\n+    }\n+}\n+\n+/// Typed implementation for [`Statistics`].\n+pub struct TypedStatistics<T: DataType> {\n+    min: Option<T::T>,\n+    max: Option<T::T>,\n+    // Distinct count could be omitted in some cases\n+    distinct_count: Option<u64>,\n+    null_count: u64,\n+    is_min_max_deprecated: bool,\n+}\n+\n+impl<T: DataType> TypedStatistics<T> {\n+    /// Creates new typed statistics.\n+    pub fn new(\n+        min: Option<T::T>,\n+        max: Option<T::T>,\n+        distinct_count: Option<u64>,\n+        null_count: u64,\n+        is_min_max_deprecated: bool,\n+    ) -> Self {\n+        Self {\n+            min,\n+            max,\n+            distinct_count,\n+            null_count,\n+            is_min_max_deprecated,\n+        }\n+    }\n+\n+    /// Returns min value of the statistics.\n+    ///\n+    /// Panics if min value is not set, e.g. all values are `null`.\n+    /// Use `has_min_max_set` method to check that.\n+    pub fn min(&self) -> &T::T {\n+        self.min.as_ref().unwrap()\n+    }\n+\n+    /// Returns max value of the statistics.\n+    ///\n+    /// Panics if max value is not set, e.g. all values are `null`.\n+    /// Use `has_min_max_set` method to check that.\n+    pub fn max(&self) -> &T::T {\n+        self.max.as_ref().unwrap()\n+    }\n+\n+    /// Returns min value as bytes of the statistics.\n+    ///\n+    /// Panics if min value is not set, use `has_min_max_set` method to check\n+    /// if values are set.\n+    pub fn min_bytes(&self) -> &[u8] {\n+        self.min().as_bytes()\n+    }\n+\n+    /// Returns max value as bytes of the statistics.\n+    ///\n+    /// Panics if max value is not set, use `has_min_max_set` method to check\n+    /// if values are set.\n+    pub fn max_bytes(&self) -> &[u8] {\n+        self.max().as_bytes()\n+    }\n+\n+    /// Whether or not min and max values are set.\n+    /// Normally both min/max values will be set to `Some(value)` or `None`.\n+    fn has_min_max_set(&self) -> bool {\n+        self.min.is_some() && self.max.is_some()\n+    }\n+\n+    /// Returns optional value of number of distinct values occurring.\n+    fn distinct_count(&self) -> Option<u64> {\n+        self.distinct_count\n+    }\n+\n+    /// Returns null count.\n+    fn null_count(&self) -> u64 {\n+        self.null_count\n+    }\n+\n+    /// Returns `true` if statistics were created using old min/max fields.\n+    fn is_min_max_deprecated(&self) -> bool {\n+        self.is_min_max_deprecated\n+    }\n+}\n+\n+impl<T: DataType> fmt::Display for TypedStatistics<T> {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{{\")?;\n+        write!(f, \"min: \")?;\n+        match self.min {\n+            Some(ref value) => self.value_fmt(f, value)?,\n+            None => write!(f, \"N/A\")?,\n+        }\n+        write!(f, \", max: \")?;\n+        match self.max {\n+            Some(ref value) => self.value_fmt(f, value)?,\n+            None => write!(f, \"N/A\")?,\n+        }\n+        write!(f, \", distinct_count: \")?;\n+        match self.distinct_count {\n+            Some(value) => write!(f, \"{}\", value)?,\n+            None => write!(f, \"N/A\")?,\n+        }\n+        write!(f, \", null_count: {}\", self.null_count)?;\n+        write!(f, \", min_max_deprecated: {}\", self.is_min_max_deprecated)?;\n+        write!(f, \"}}\")\n+    }\n+}\n+\n+impl<T: DataType> fmt::Debug for TypedStatistics<T> {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(\n+            f,\n+            \"{{min: {:?}, max: {:?}, distinct_count: {:?}, null_count: {}, \\\n+             min_max_deprecated: {}}}\",\n+            self.min, self.max, self.distinct_count, self.null_count, self.is_min_max_deprecated\n+        )\n+    }\n+}\n+\n+impl<T: DataType> cmp::PartialEq for TypedStatistics<T> {\n+    fn eq(&self, other: &TypedStatistics<T>) -> bool {\n+        self.min == other.min\n+            && self.max == other.max\n+            && self.distinct_count == other.distinct_count\n+            && self.null_count == other.null_count\n+            && self.is_min_max_deprecated == other.is_min_max_deprecated\n+    }\n+}\n+\n+/// Trait to provide a specific write format for values.\n+/// For example, we should display vector slices for byte array types, and original\n+/// values for other types.\n+trait ValueDisplay<T: DataType> {\n+    fn value_fmt(&self, f: &mut fmt::Formatter, value: &T::T) -> fmt::Result;\n+}\n+\n+impl<T: DataType> ValueDisplay<T> for TypedStatistics<T> {\n+    default fn value_fmt(&self, f: &mut fmt::Formatter, value: &T::T) -> fmt::Result {\n+        write!(f, \"{:?}\", value)\n+    }\n+}\n+\n+impl ValueDisplay<Int96Type> for TypedStatistics<Int96Type> {\n+    fn value_fmt(&self, f: &mut fmt::Formatter, value: &Int96) -> fmt::Result {\n+        write!(f, \"{:?}\", value.data())\n+    }\n+}\n+\n+impl ValueDisplay<ByteArrayType> for TypedStatistics<ByteArrayType> {\n+    fn value_fmt(&self, f: &mut fmt::Formatter, value: &ByteArray) -> fmt::Result {\n+        write!(f, \"{:?}\", value.data())\n+    }\n+}\n+\n+impl ValueDisplay<FixedLenByteArrayType> for TypedStatistics<FixedLenByteArrayType> {\n+    fn value_fmt(&self, f: &mut fmt::Formatter, value: &ByteArray) -> fmt::Result {\n+        write!(f, \"{:?}\", value.data())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_statistics_min_max_bytes() {\n+        let stats = Statistics::int32(Some(-123), Some(234), None, 1, false);\n+        assert!(stats.has_min_max_set());\n+        assert_eq!(stats.min_bytes(), (-123).as_bytes());\n+        assert_eq!(stats.max_bytes(), 234.as_bytes());\n+\n+        let stats = Statistics::byte_array(\n+            Some(ByteArray::from(vec![1, 2, 3])),\n+            Some(ByteArray::from(vec![3, 4, 5])),\n+            None,\n+            1,\n+            true,\n+        );\n+        assert!(stats.has_min_max_set());\n+        assert_eq!(stats.min_bytes(), &[1, 2, 3]);\n+        assert_eq!(stats.max_bytes(), &[3, 4, 5]);\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Statistics null count is negative (-10)\")]\n+    fn test_statistics_negative_null_count() {\n+        let thrift_stats = TStatistics {\n+            max: None,\n+            min: None,\n+            null_count: Some(-10),\n+            distinct_count: None,\n+            max_value: None,\n+            min_value: None,\n+        };\n+\n+        from_thrift(Type::INT32, Some(thrift_stats));\n+    }\n+\n+    #[test]\n+    fn test_statistics_thrift_none() {\n+        assert_eq!(from_thrift(Type::INT32, None), None);\n+        assert_eq!(from_thrift(Type::BYTE_ARRAY, None), None);\n+    }\n+\n+    #[test]\n+    fn test_statistics_debug() {\n+        let stats = Statistics::int32(Some(1), Some(12), None, 12, true);\n+        assert_eq!(\n+            format!(\"{:?}\", stats),\n+            \"Int32({min: Some(1), max: Some(12), distinct_count: None, null_count: 12, \\\n+             min_max_deprecated: true})\"\n+        );\n+\n+        let stats = Statistics::int32(None, None, None, 7, false);\n+        assert_eq!(\n+            format!(\"{:?}\", stats),\n+            \"Int32({min: None, max: None, distinct_count: None, null_count: 7, \\\n+             min_max_deprecated: false})\"\n+        )\n+    }\n+\n+    #[test]\n+    fn test_statistics_display() {\n+        let stats = Statistics::int32(Some(1), Some(12), None, 12, true);\n+        assert_eq!(\n+            format!(\"{}\", stats),\n+            \"{min: 1, max: 12, distinct_count: N/A, null_count: 12, min_max_deprecated: true}\"\n+        );\n+\n+        let stats = Statistics::int64(None, None, None, 7, false);\n+        assert_eq!(\n+            format!(\"{}\", stats),\n+            \"{min: N/A, max: N/A, distinct_count: N/A, null_count: 7, min_max_deprecated: \\\n+             false}\"\n+        );\n+\n+        let stats = Statistics::int96(\n+            Some(Int96::from(vec![1, 0, 0])),\n+            Some(Int96::from(vec![2, 3, 4])),\n+            None,\n+            3,\n+            true,\n+        );\n+        assert_eq!(\n+            format!(\"{}\", stats),\n+            \"{min: [1, 0, 0], max: [2, 3, 4], distinct_count: N/A, null_count: 3, \\\n+             min_max_deprecated: true}\"\n+        );\n+\n+        let stats = Statistics::byte_array(\n+            Some(ByteArray::from(vec![1u8])),\n+            Some(ByteArray::from(vec![2u8])),\n+            Some(5),\n+            7,\n+            false,\n+        );\n+        assert_eq!(\n+            format!(\"{}\", stats),\n+            \"{min: [1], max: [2], distinct_count: 5, null_count: 7, min_max_deprecated: false}\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_statistics_partial_eq() {\n+        let expected = Statistics::int32(Some(12), Some(45), None, 11, true);\n+\n+        assert!(Statistics::int32(Some(12), Some(45), None, 11, true) == expected);\n+        assert!(Statistics::int32(Some(11), Some(45), None, 11, true) != expected);\n+        assert!(Statistics::int32(Some(12), Some(44), None, 11, true) != expected);\n+        assert!(Statistics::int32(Some(12), Some(45), None, 23, true) != expected);\n+        assert!(Statistics::int32(Some(12), Some(45), None, 11, false) != expected);\n+\n+        assert!(\n+            Statistics::int32(Some(12), Some(45), None, 11, false)\n+                != Statistics::int64(Some(12), Some(45), None, 11, false)\n+        );\n+\n+        assert!(\n+            Statistics::boolean(Some(false), Some(true), None, 0, true)\n+                != Statistics::double(Some(1.2), Some(4.5), None, 0, true)\n+        );\n+\n+        assert!(\n+            Statistics::byte_array(\n+                Some(ByteArray::from(vec![1, 2, 3])),\n+                Some(ByteArray::from(vec![1, 2, 3])),\n+                None,\n+                0,\n+                true\n+            ) != Statistics::fixed_len_byte_array(\n+                Some(ByteArray::from(vec![1, 2, 3])),\n+                Some(ByteArray::from(vec![1, 2, 3])),\n+                None,\n+                0,\n+                true\n+            )\n+        );\n+    }\n+\n+    #[test]\n+    fn test_statistics_from_thrift() {\n+        // Helper method to check statistics conversion.\n+        fn check_stats(stats: Statistics) {\n+            let tpe = stats.physical_type();\n+            let thrift_stats = to_thrift(Some(&stats));\n+            assert_eq!(from_thrift(tpe, thrift_stats), Some(stats));\n+        }\n+\n+        check_stats(Statistics::boolean(Some(false), Some(true), None, 7, true));\n+        check_stats(Statistics::boolean(Some(false), Some(true), None, 7, true));\n+        check_stats(Statistics::boolean(Some(false), Some(true), None, 0, false));\n+        check_stats(Statistics::boolean(Some(true), Some(true), None, 7, true));\n+        check_stats(Statistics::boolean(Some(false), Some(false), None, 7, true));\n+        check_stats(Statistics::boolean(None, None, None, 7, true));\n+\n+        check_stats(Statistics::int32(Some(-100), Some(500), None, 7, true));\n+        check_stats(Statistics::int32(Some(-100), Some(500), None, 0, false));\n+        check_stats(Statistics::int32(None, None, None, 7, true));\n+\n+        check_stats(Statistics::int64(Some(-100), Some(200), None, 7, true));\n+        check_stats(Statistics::int64(Some(-100), Some(200), None, 0, false));\n+        check_stats(Statistics::int64(None, None, None, 7, true));\n+\n+        check_stats(Statistics::float(Some(1.2), Some(3.4), None, 7, true));\n+        check_stats(Statistics::float(Some(1.2), Some(3.4), None, 0, false));\n+        check_stats(Statistics::float(None, None, None, 7, true));\n+\n+        check_stats(Statistics::double(Some(1.2), Some(3.4), None, 7, true));\n+        check_stats(Statistics::double(Some(1.2), Some(3.4), None, 0, false));\n+        check_stats(Statistics::double(None, None, None, 7, true));\n+\n+        check_stats(Statistics::byte_array(\n+            Some(ByteArray::from(vec![1, 2, 3])),\n+            Some(ByteArray::from(vec![3, 4, 5])),\n+            None,\n+            7,\n+            true,\n+        ));\n+        check_stats(Statistics::byte_array(None, None, None, 7, true));\n+\n+        check_stats(Statistics::fixed_len_byte_array(\n+            Some(ByteArray::from(vec![1, 2, 3])),\n+            Some(ByteArray::from(vec![3, 4, 5])),\n+            None,\n+            7,\n+            true,\n+        ));\n+        check_stats(Statistics::fixed_len_byte_array(None, None, None, 7, true));\n+    }\n+}\ndiff --git a/rust/src/parquet/file/writer.rs b/rust/src/parquet/file/writer.rs\nnew file mode 100644\nindex 0000000000..1e0c11641f\n--- /dev/null\n+++ b/rust/src/parquet/file/writer.rs\n@@ -0,0 +1,936 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains file writer API, and provides methods to write row groups and columns by\n+//! using row group writers and column writers respectively.\n+\n+use std::{\n+    fs::File,\n+    io::{Seek, SeekFrom, Write},\n+    rc::Rc,\n+};\n+\n+use byteorder::{ByteOrder, LittleEndian};\n+use parquet_format as parquet;\n+use thrift::protocol::{TCompactOutputProtocol, TOutputProtocol};\n+\n+use crate::parquet::basic::PageType;\n+use crate::parquet::column::{\n+    page::{CompressedPage, Page, PageWriteSpec, PageWriter},\n+    writer::{get_column_writer, ColumnWriter},\n+};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::file::{\n+    metadata::*, properties::WriterPropertiesPtr, statistics::to_thrift as statistics_to_thrift,\n+    FOOTER_SIZE, PARQUET_MAGIC,\n+};\n+use crate::parquet::schema::types::{self, SchemaDescPtr, SchemaDescriptor, TypePtr};\n+use crate::parquet::util::io::{FileSink, Position};\n+\n+// ----------------------------------------------------------------------\n+// APIs for file & row group writers\n+\n+/// Parquet file writer API.\n+/// Provides methods to write row groups sequentially.\n+///\n+/// The main workflow should be as following:\n+/// - Create file writer, this will open a new file and potentially write some metadata.\n+/// - Request a new row group writer by calling `next_row_group`.\n+/// - Once finished writing row group, close row group writer by passing it into\n+/// `close_row_group` method - this will finalise row group metadata and update metrics.\n+/// - Write subsequent row groups, if necessary.\n+/// - After all row groups have been written, close the file writer using `close` method.\n+pub trait FileWriter {\n+    /// Creates new row group from this file writer.\n+    /// In case of IO error or Thrift error, returns `Err`.\n+    ///\n+    /// There is no limit on a number of row groups in a file; however, row groups have\n+    /// to be written sequentially. Every time the next row group is requested, the\n+    /// previous row group must be finalised and closed using `close_row_group` method.\n+    fn next_row_group(&mut self) -> Result<Box<RowGroupWriter>>;\n+\n+    /// Finalises and closes row group that was created using `next_row_group` method.\n+    /// After calling this method, the next row group is available for writes.\n+    fn close_row_group(&mut self, row_group_writer: Box<RowGroupWriter>) -> Result<()>;\n+\n+    /// Closes and finalises file writer.\n+    ///\n+    /// All row groups must be appended before this method is called.\n+    /// No writes are allowed after this point.\n+    ///\n+    /// Can be called multiple times. It is up to implementation to either result in no-op,\n+    /// or return an `Err` for subsequent calls.\n+    fn close(&mut self) -> Result<()>;\n+}\n+\n+/// Parquet row group writer API.\n+/// Provides methods to access column writers in an iterator-like fashion, order is\n+/// guaranteed to match the order of schema leaves (column descriptors).\n+///\n+/// All columns should be written sequentially; the main workflow is:\n+/// - Request the next column using `next_column` method - this will return `None` if no\n+/// more columns are available to write.\n+/// - Once done writing a column, close column writer with `close_column` method - this\n+/// will finalise column chunk metadata and update row group metrics.\n+/// - Once all columns have been written, close row group writer with `close` method -\n+/// it will return row group metadata and is no-op on already closed row group.\n+pub trait RowGroupWriter {\n+    /// Returns the next column writer, if available; otherwise returns `None`.\n+    /// In case of any IO error or Thrift error, or if row group writer has already been\n+    /// closed returns `Err`.\n+    ///\n+    /// To request the next column writer, the previous one must be finalised and closed\n+    /// using `close_column`.\n+    fn next_column(&mut self) -> Result<Option<ColumnWriter>>;\n+\n+    /// Closes column writer that was created using `next_column` method.\n+    /// This should be called before requesting the next column writer.\n+    fn close_column(&mut self, column_writer: ColumnWriter) -> Result<()>;\n+\n+    /// Closes this row group writer and returns row group metadata.\n+    /// After calling this method row group writer must not be used.\n+    ///\n+    /// It is recommended to call this method before requesting another row group, but it\n+    /// will be closed automatically before returning a new row group.\n+    ///\n+    /// Can be called multiple times. In subsequent calls will result in no-op and return\n+    /// already created row group metadata.\n+    fn close(&mut self) -> Result<RowGroupMetaDataPtr>;\n+}\n+\n+// ----------------------------------------------------------------------\n+// Serialized impl for file & row group writers\n+\n+/// A serialized implementation for Parquet [`FileWriter`].\n+/// See documentation on file writer for more information.\n+pub struct SerializedFileWriter {\n+    file: File,\n+    schema: TypePtr,\n+    descr: SchemaDescPtr,\n+    props: WriterPropertiesPtr,\n+    total_num_rows: u64,\n+    row_groups: Vec<RowGroupMetaDataPtr>,\n+    previous_writer_closed: bool,\n+    is_closed: bool,\n+}\n+\n+impl SerializedFileWriter {\n+    /// Creates new file writer.\n+    pub fn new(mut file: File, schema: TypePtr, properties: WriterPropertiesPtr) -> Result<Self> {\n+        Self::start_file(&mut file)?;\n+        Ok(Self {\n+            file,\n+            schema: schema.clone(),\n+            descr: Rc::new(SchemaDescriptor::new(schema)),\n+            props: properties,\n+            total_num_rows: 0,\n+            row_groups: Vec::new(),\n+            previous_writer_closed: true,\n+            is_closed: false,\n+        })\n+    }\n+\n+    /// Writes magic bytes at the beginning of the file.\n+    fn start_file(file: &mut File) -> Result<()> {\n+        file.write(&PARQUET_MAGIC)?;\n+        Ok(())\n+    }\n+\n+    /// Finalises active row group writer, otherwise no-op.\n+    fn finalise_row_group_writer(\n+        &mut self,\n+        mut row_group_writer: Box<RowGroupWriter>,\n+    ) -> Result<()> {\n+        let row_group_metadata = row_group_writer.close()?;\n+        self.row_groups.push(row_group_metadata);\n+        Ok(())\n+    }\n+\n+    /// Assembles and writes metadata at the end of the file.\n+    fn write_metadata(&mut self) -> Result<()> {\n+        let file_metadata = parquet::FileMetaData {\n+            version: self.props.writer_version().as_num(),\n+            schema: types::to_thrift(self.schema.as_ref())?,\n+            num_rows: self.total_num_rows as i64,\n+            row_groups: self\n+                .row_groups\n+                .as_slice()\n+                .into_iter()\n+                .map(|v| v.to_thrift())\n+                .collect(),\n+            key_value_metadata: None,\n+            created_by: Some(self.props.created_by().to_owned()),\n+            column_orders: None,\n+        };\n+\n+        // Write file metadata\n+        let start_pos = self.file.seek(SeekFrom::Current(0))?;\n+        {\n+            let mut protocol = TCompactOutputProtocol::new(&mut self.file);\n+            file_metadata.write_to_out_protocol(&mut protocol)?;\n+            protocol.flush()?;\n+        }\n+        let end_pos = self.file.seek(SeekFrom::Current(0))?;\n+\n+        // Write footer\n+        let mut footer_buffer: [u8; FOOTER_SIZE] = [0; FOOTER_SIZE];\n+        let metadata_len = (end_pos - start_pos) as i32;\n+        LittleEndian::write_i32(&mut footer_buffer, metadata_len);\n+        (&mut footer_buffer[4..]).write(&PARQUET_MAGIC)?;\n+        self.file.write(&footer_buffer)?;\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn assert_closed(&self) -> Result<()> {\n+        if self.is_closed {\n+            Err(general_err!(\"File writer is closed\"))\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n+    #[inline]\n+    fn assert_previous_writer_closed(&self) -> Result<()> {\n+        if !self.previous_writer_closed {\n+            Err(general_err!(\"Previous row group writer was not closed\"))\n+        } else {\n+            Ok(())\n+        }\n+    }\n+}\n+\n+impl FileWriter for SerializedFileWriter {\n+    #[inline]\n+    fn next_row_group(&mut self) -> Result<Box<RowGroupWriter>> {\n+        self.assert_closed()?;\n+        self.assert_previous_writer_closed()?;\n+        let row_group_writer =\n+            SerializedRowGroupWriter::new(self.descr.clone(), self.props.clone(), &self.file);\n+        self.previous_writer_closed = false;\n+        Ok(Box::new(row_group_writer))\n+    }\n+\n+    #[inline]\n+    fn close_row_group(&mut self, row_group_writer: Box<RowGroupWriter>) -> Result<()> {\n+        self.assert_closed()?;\n+        let res = self.finalise_row_group_writer(row_group_writer);\n+        self.previous_writer_closed = res.is_ok();\n+        res\n+    }\n+\n+    #[inline]\n+    fn close(&mut self) -> Result<()> {\n+        self.assert_closed()?;\n+        self.assert_previous_writer_closed()?;\n+        self.write_metadata()?;\n+        self.is_closed = true;\n+        Ok(())\n+    }\n+}\n+\n+/// A serialized implementation for Parquet [`RowGroupWriter`].\n+/// Coordinates writing of a row group with column writers.\n+/// See documentation on row group writer for more information.\n+pub struct SerializedRowGroupWriter {\n+    descr: SchemaDescPtr,\n+    props: WriterPropertiesPtr,\n+    file: File,\n+    total_rows_written: Option<u64>,\n+    total_bytes_written: u64,\n+    column_index: usize,\n+    previous_writer_closed: bool,\n+    row_group_metadata: Option<RowGroupMetaDataPtr>,\n+    column_chunks: Vec<ColumnChunkMetaDataPtr>,\n+}\n+\n+impl SerializedRowGroupWriter {\n+    pub fn new(schema_descr: SchemaDescPtr, properties: WriterPropertiesPtr, file: &File) -> Self {\n+        let num_columns = schema_descr.num_columns();\n+        Self {\n+            descr: schema_descr,\n+            props: properties,\n+            file: file.try_clone().unwrap(),\n+            total_rows_written: None,\n+            total_bytes_written: 0,\n+            column_index: 0,\n+            previous_writer_closed: true,\n+            row_group_metadata: None,\n+            column_chunks: Vec::with_capacity(num_columns),\n+        }\n+    }\n+\n+    /// Checks and finalises current column writer.\n+    fn finalise_column_writer(&mut self, writer: ColumnWriter) -> Result<()> {\n+        let (bytes_written, rows_written, metadata) = match writer {\n+            ColumnWriter::BoolColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::Int32ColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::Int64ColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::Int96ColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::FloatColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::DoubleColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::ByteArrayColumnWriter(typed) => typed.close()?,\n+            ColumnWriter::FixedLenByteArrayColumnWriter(typed) => typed.close()?,\n+        };\n+\n+        // Update row group writer metrics\n+        self.total_bytes_written += bytes_written;\n+        self.column_chunks.push(Rc::new(metadata));\n+        if let Some(rows) = self.total_rows_written {\n+            if rows != rows_written {\n+                return Err(general_err!(\n+                    \"Incorrect number of rows, expected {} != {} rows\",\n+                    rows,\n+                    rows_written\n+                ));\n+            }\n+        } else {\n+            self.total_rows_written = Some(rows_written);\n+        }\n+\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn assert_closed(&self) -> Result<()> {\n+        if self.row_group_metadata.is_some() {\n+            Err(general_err!(\"Row group writer is closed\"))\n+        } else {\n+            Ok(())\n+        }\n+    }\n+\n+    #[inline]\n+    fn assert_previous_writer_closed(&self) -> Result<()> {\n+        if !self.previous_writer_closed {\n+            Err(general_err!(\"Previous column writer was not closed\"))\n+        } else {\n+            Ok(())\n+        }\n+    }\n+}\n+\n+impl RowGroupWriter for SerializedRowGroupWriter {\n+    #[inline]\n+    fn next_column(&mut self) -> Result<Option<ColumnWriter>> {\n+        self.assert_closed()?;\n+        self.assert_previous_writer_closed()?;\n+\n+        if self.column_index >= self.descr.num_columns() {\n+            return Ok(None);\n+        }\n+        let sink = FileSink::new(&self.file);\n+        let page_writer = Box::new(SerializedPageWriter::new(sink));\n+        let column_writer = get_column_writer(\n+            self.descr.column(self.column_index),\n+            self.props.clone(),\n+            page_writer,\n+        );\n+        self.column_index += 1;\n+        self.previous_writer_closed = false;\n+\n+        Ok(Some(column_writer))\n+    }\n+\n+    #[inline]\n+    fn close_column(&mut self, column_writer: ColumnWriter) -> Result<()> {\n+        let res = self.finalise_column_writer(column_writer);\n+        self.previous_writer_closed = res.is_ok();\n+        res\n+    }\n+\n+    #[inline]\n+    fn close(&mut self) -> Result<RowGroupMetaDataPtr> {\n+        if self.row_group_metadata.is_none() {\n+            self.assert_previous_writer_closed()?;\n+\n+            let row_group_metadata = RowGroupMetaData::builder(self.descr.clone())\n+                .set_column_metadata(self.column_chunks.clone())\n+                .set_total_byte_size(self.total_bytes_written as i64)\n+                .set_num_rows(self.total_rows_written.unwrap_or(0) as i64)\n+                .build()?;\n+\n+            self.row_group_metadata = Some(Rc::new(row_group_metadata));\n+        }\n+\n+        let metadata = self.row_group_metadata.as_ref().unwrap().clone();\n+        Ok(metadata)\n+    }\n+}\n+\n+/// A serialized implementation for Parquet [`PageWriter`].\n+/// Writes and serializes pages and metadata into output stream.\n+///\n+/// `SerializedPageWriter` should not be used after calling `close()`.\n+pub struct SerializedPageWriter<T: Write + Position> {\n+    sink: T,\n+}\n+\n+impl<T: Write + Position> SerializedPageWriter<T> {\n+    /// Creates new page writer.\n+    pub fn new(sink: T) -> Self {\n+        Self { sink }\n+    }\n+\n+    /// Serializes page header into Thrift.\n+    /// Returns number of bytes that have been written into the sink.\n+    #[inline]\n+    fn serialize_page_header(&mut self, header: parquet::PageHeader) -> Result<usize> {\n+        let start_pos = self.sink.pos();\n+        {\n+            let mut protocol = TCompactOutputProtocol::new(&mut self.sink);\n+            header.write_to_out_protocol(&mut protocol)?;\n+            protocol.flush()?;\n+        }\n+        Ok((self.sink.pos() - start_pos) as usize)\n+    }\n+\n+    /// Serializes column chunk into Thrift.\n+    /// Returns Ok() if there are not errors serializing and writing data into the sink.\n+    #[inline]\n+    fn serialize_column_chunk(&mut self, chunk: parquet::ColumnChunk) -> Result<()> {\n+        let mut protocol = TCompactOutputProtocol::new(&mut self.sink);\n+        chunk.write_to_out_protocol(&mut protocol)?;\n+        protocol.flush()?;\n+        Ok(())\n+    }\n+}\n+\n+impl<T: Write + Position> PageWriter for SerializedPageWriter<T> {\n+    fn write_page(&mut self, page: CompressedPage) -> Result<PageWriteSpec> {\n+        let uncompressed_size = page.uncompressed_size();\n+        let compressed_size = page.compressed_size();\n+        let num_values = page.num_values();\n+        let encoding = page.encoding();\n+        let page_type = page.page_type();\n+\n+        let mut page_header = parquet::PageHeader {\n+            type_: page_type.into(),\n+            uncompressed_page_size: uncompressed_size as i32,\n+            compressed_page_size: compressed_size as i32,\n+            // TODO: Add support for crc checksum\n+            crc: None,\n+            data_page_header: None,\n+            index_page_header: None,\n+            dictionary_page_header: None,\n+            data_page_header_v2: None,\n+        };\n+\n+        match page.compressed_page() {\n+            &Page::DataPage {\n+                def_level_encoding,\n+                rep_level_encoding,\n+                ref statistics,\n+                ..\n+            } => {\n+                let data_page_header = parquet::DataPageHeader {\n+                    num_values: num_values as i32,\n+                    encoding: encoding.into(),\n+                    definition_level_encoding: def_level_encoding.into(),\n+                    repetition_level_encoding: rep_level_encoding.into(),\n+                    statistics: statistics_to_thrift(statistics.as_ref()),\n+                };\n+                page_header.data_page_header = Some(data_page_header);\n+            }\n+            &Page::DataPageV2 {\n+                num_nulls,\n+                num_rows,\n+                def_levels_byte_len,\n+                rep_levels_byte_len,\n+                is_compressed,\n+                ref statistics,\n+                ..\n+            } => {\n+                let data_page_header_v2 = parquet::DataPageHeaderV2 {\n+                    num_values: num_values as i32,\n+                    num_nulls: num_nulls as i32,\n+                    num_rows: num_rows as i32,\n+                    encoding: encoding.into(),\n+                    definition_levels_byte_length: def_levels_byte_len as i32,\n+                    repetition_levels_byte_length: rep_levels_byte_len as i32,\n+                    is_compressed: Some(is_compressed),\n+                    statistics: statistics_to_thrift(statistics.as_ref()),\n+                };\n+                page_header.data_page_header_v2 = Some(data_page_header_v2);\n+            }\n+            &Page::DictionaryPage { is_sorted, .. } => {\n+                let dictionary_page_header = parquet::DictionaryPageHeader {\n+                    num_values: num_values as i32,\n+                    encoding: encoding.into(),\n+                    is_sorted: Some(is_sorted),\n+                };\n+                page_header.dictionary_page_header = Some(dictionary_page_header);\n+            }\n+        }\n+\n+        let start_pos = self.sink.pos();\n+\n+        let header_size = self.serialize_page_header(page_header)?;\n+        self.sink.write_all(page.data())?;\n+\n+        let mut spec = PageWriteSpec::new();\n+        spec.page_type = page_type;\n+        spec.uncompressed_size = uncompressed_size + header_size;\n+        spec.compressed_size = compressed_size + header_size;\n+        spec.offset = start_pos;\n+        spec.bytes_written = self.sink.pos() - start_pos;\n+        // Number of values is incremented for data pages only\n+        if page_type == PageType::DATA_PAGE || page_type == PageType::DATA_PAGE_V2 {\n+            spec.num_values = num_values;\n+        }\n+\n+        Ok(spec)\n+    }\n+\n+    fn write_metadata(&mut self, metadata: &ColumnChunkMetaData) -> Result<()> {\n+        self.serialize_column_chunk(metadata.to_thrift())\n+    }\n+\n+    fn close(&mut self) -> Result<()> {\n+        self.sink.flush()?;\n+        Ok(())\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use std::{error::Error, io::Cursor};\n+\n+    use crate::parquet::basic::{Compression, Encoding, Repetition, Type};\n+    use crate::parquet::column::page::PageReader;\n+    use crate::parquet::compression::{create_codec, Codec};\n+    use crate::parquet::file::{\n+        properties::WriterProperties,\n+        reader::{FileReader, SerializedFileReader, SerializedPageReader},\n+        statistics::{from_thrift, to_thrift, Statistics},\n+    };\n+    use crate::parquet::record::RowAccessor;\n+    use crate::parquet::util::{memory::ByteBufferPtr, test_common::get_temp_file};\n+\n+    #[test]\n+    fn test_file_writer_error_after_close() {\n+        let file = get_temp_file(\"test_file_writer_error_after_close\", &[]);\n+        let schema = Rc::new(types::Type::group_type_builder(\"schema\").build().unwrap());\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+        writer.close().unwrap();\n+        {\n+            let res = writer.next_row_group();\n+            assert!(res.is_err());\n+            if let Err(err) = res {\n+                assert_eq!(err.description(), \"File writer is closed\");\n+            }\n+        }\n+        {\n+            let res = writer.close();\n+            assert!(res.is_err());\n+            if let Err(err) = res {\n+                assert_eq!(err.description(), \"File writer is closed\");\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn test_row_group_writer_error_after_close() {\n+        let file = get_temp_file(\"test_file_writer_row_group_error_after_close\", &[]);\n+        let schema = Rc::new(types::Type::group_type_builder(\"schema\").build().unwrap());\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+        let mut row_group_writer = writer.next_row_group().unwrap();\n+        row_group_writer.close().unwrap();\n+\n+        let res = row_group_writer.next_column();\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(err.description(), \"Row group writer is closed\");\n+        }\n+    }\n+\n+    #[test]\n+    fn test_row_group_writer_error_not_all_columns_written() {\n+        let file = get_temp_file(\"test_row_group_writer_error_not_all_columns_written\", &[]);\n+        let schema = Rc::new(\n+            types::Type::group_type_builder(\"schema\")\n+                .with_fields(&mut vec![Rc::new(\n+                    types::Type::primitive_type_builder(\"col1\", Type::INT32)\n+                        .build()\n+                        .unwrap(),\n+                )])\n+                .build()\n+                .unwrap(),\n+        );\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+        let mut row_group_writer = writer.next_row_group().unwrap();\n+        let res = row_group_writer.close();\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(err.description(), \"Column length mismatch: 1 != 0\");\n+        }\n+    }\n+\n+    #[test]\n+    fn test_row_group_writer_num_records_mismatch() {\n+        let file = get_temp_file(\"test_row_group_writer_num_records_mismatch\", &[]);\n+        let schema = Rc::new(\n+            types::Type::group_type_builder(\"schema\")\n+                .with_fields(&mut vec![\n+                    Rc::new(\n+                        types::Type::primitive_type_builder(\"col1\", Type::INT32)\n+                            .with_repetition(Repetition::REQUIRED)\n+                            .build()\n+                            .unwrap(),\n+                    ),\n+                    Rc::new(\n+                        types::Type::primitive_type_builder(\"col2\", Type::INT32)\n+                            .with_repetition(Repetition::REQUIRED)\n+                            .build()\n+                            .unwrap(),\n+                    ),\n+                ])\n+                .build()\n+                .unwrap(),\n+        );\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\n+        let mut row_group_writer = writer.next_row_group().unwrap();\n+\n+        let mut col_writer = row_group_writer.next_column().unwrap().unwrap();\n+        if let ColumnWriter::Int32ColumnWriter(ref mut typed) = col_writer {\n+            typed.write_batch(&[1, 2, 3], None, None).unwrap();\n+        }\n+        row_group_writer.close_column(col_writer).unwrap();\n+\n+        let mut col_writer = row_group_writer.next_column().unwrap().unwrap();\n+        if let ColumnWriter::Int32ColumnWriter(ref mut typed) = col_writer {\n+            typed.write_batch(&[1, 2], None, None).unwrap();\n+        }\n+\n+        let res = row_group_writer.close_column(col_writer);\n+        assert!(res.is_err());\n+        if let Err(err) = res {\n+            assert_eq!(\n+                err.description(),\n+                \"Incorrect number of rows, expected 3 != 2 rows\"\n+            );\n+        }\n+    }\n+\n+    #[test]\n+    fn test_file_writer_empty_file() {\n+        let file = get_temp_file(\"test_file_writer_write_empty_file\", &[]);\n+\n+        let schema = Rc::new(\n+            types::Type::group_type_builder(\"schema\")\n+                .with_fields(&mut vec![Rc::new(\n+                    types::Type::primitive_type_builder(\"col1\", Type::INT32)\n+                        .build()\n+                        .unwrap(),\n+                )])\n+                .build()\n+                .unwrap(),\n+        );\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut writer =\n+            SerializedFileWriter::new(file.try_clone().unwrap(), schema, props).unwrap();\n+        writer.close().unwrap();\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+        assert_eq!(reader.get_row_iter(None).unwrap().count(), 0);\n+    }\n+\n+    #[test]\n+    fn test_file_writer_empty_row_groups() {\n+        let file = get_temp_file(\"test_file_writer_write_empty_row_groups\", &[]);\n+        test_file_roundtrip(file, vec![]);\n+    }\n+\n+    #[test]\n+    fn test_file_writer_single_row_group() {\n+        let file = get_temp_file(\"test_file_writer_write_single_row_group\", &[]);\n+        test_file_roundtrip(file, vec![vec![1, 2, 3, 4, 5]]);\n+    }\n+\n+    #[test]\n+    fn test_file_writer_multiple_row_groups() {\n+        let file = get_temp_file(\"test_file_writer_write_multiple_row_groups\", &[]);\n+        test_file_roundtrip(\n+            file,\n+            vec![\n+                vec![1, 2, 3, 4, 5],\n+                vec![1, 2, 3],\n+                vec![1],\n+                vec![1, 2, 3, 4, 5, 6],\n+            ],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_file_writer_multiple_large_row_groups() {\n+        let file = get_temp_file(\"test_file_writer_multiple_large_row_groups\", &[]);\n+        test_file_roundtrip(\n+            file,\n+            vec![vec![123; 1024], vec![124; 1000], vec![125; 15], vec![]],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_page_writer_data_pages() {\n+        let pages = vec![\n+            Page::DataPage {\n+                buf: ByteBufferPtr::new(vec![1, 2, 3, 4, 5, 6, 7, 8]),\n+                num_values: 10,\n+                encoding: Encoding::DELTA_BINARY_PACKED,\n+                def_level_encoding: Encoding::RLE,\n+                rep_level_encoding: Encoding::RLE,\n+                statistics: Some(Statistics::int32(Some(1), Some(3), None, 7, true)),\n+            },\n+            Page::DataPageV2 {\n+                buf: ByteBufferPtr::new(vec![4; 128]),\n+                num_values: 10,\n+                encoding: Encoding::DELTA_BINARY_PACKED,\n+                num_nulls: 2,\n+                num_rows: 12,\n+                def_levels_byte_len: 24,\n+                rep_levels_byte_len: 32,\n+                is_compressed: false,\n+                statistics: Some(Statistics::int32(Some(1), Some(3), None, 7, true)),\n+            },\n+        ];\n+\n+        test_page_roundtrip(&pages[..], Compression::SNAPPY, Type::INT32);\n+        test_page_roundtrip(&pages[..], Compression::UNCOMPRESSED, Type::INT32);\n+    }\n+\n+    #[test]\n+    fn test_page_writer_dict_pages() {\n+        let pages = vec![\n+            Page::DictionaryPage {\n+                buf: ByteBufferPtr::new(vec![1, 2, 3, 4, 5]),\n+                num_values: 5,\n+                encoding: Encoding::RLE_DICTIONARY,\n+                is_sorted: false,\n+            },\n+            Page::DataPage {\n+                buf: ByteBufferPtr::new(vec![1, 2, 3, 4, 5, 6, 7, 8]),\n+                num_values: 10,\n+                encoding: Encoding::DELTA_BINARY_PACKED,\n+                def_level_encoding: Encoding::RLE,\n+                rep_level_encoding: Encoding::RLE,\n+                statistics: Some(Statistics::int32(Some(1), Some(3), None, 7, true)),\n+            },\n+            Page::DataPageV2 {\n+                buf: ByteBufferPtr::new(vec![4; 128]),\n+                num_values: 10,\n+                encoding: Encoding::DELTA_BINARY_PACKED,\n+                num_nulls: 2,\n+                num_rows: 12,\n+                def_levels_byte_len: 24,\n+                rep_levels_byte_len: 32,\n+                is_compressed: false,\n+                statistics: None,\n+            },\n+        ];\n+\n+        test_page_roundtrip(&pages[..], Compression::SNAPPY, Type::INT32);\n+        test_page_roundtrip(&pages[..], Compression::UNCOMPRESSED, Type::INT32);\n+    }\n+\n+    /// Tests writing and reading pages.\n+    /// Physical type is for statistics only, should match any defined statistics type in\n+    /// pages.\n+    fn test_page_roundtrip(pages: &[Page], codec: Compression, physical_type: Type) {\n+        let mut compressed_pages = vec![];\n+        let mut total_num_values = 0i64;\n+        let mut compressor = create_codec(codec).unwrap();\n+\n+        for page in pages {\n+            let uncompressed_len = page.buffer().len();\n+\n+            let compressed_page = match page {\n+                &Page::DataPage {\n+                    ref buf,\n+                    num_values,\n+                    encoding,\n+                    def_level_encoding,\n+                    rep_level_encoding,\n+                    ref statistics,\n+                } => {\n+                    total_num_values += num_values as i64;\n+                    let output_buf = compress_helper(compressor.as_mut(), buf.data());\n+\n+                    Page::DataPage {\n+                        buf: ByteBufferPtr::new(output_buf),\n+                        num_values,\n+                        encoding,\n+                        def_level_encoding,\n+                        rep_level_encoding,\n+                        statistics: from_thrift(physical_type, to_thrift(statistics.as_ref())),\n+                    }\n+                }\n+                &Page::DataPageV2 {\n+                    ref buf,\n+                    num_values,\n+                    encoding,\n+                    num_nulls,\n+                    num_rows,\n+                    def_levels_byte_len,\n+                    rep_levels_byte_len,\n+                    ref statistics,\n+                    ..\n+                } => {\n+                    total_num_values += num_values as i64;\n+                    let offset = (def_levels_byte_len + rep_levels_byte_len) as usize;\n+                    let cmp_buf = compress_helper(compressor.as_mut(), &buf.data()[offset..]);\n+                    let mut output_buf = Vec::from(&buf.data()[..offset]);\n+                    output_buf.extend_from_slice(&cmp_buf[..]);\n+\n+                    Page::DataPageV2 {\n+                        buf: ByteBufferPtr::new(output_buf),\n+                        num_values,\n+                        encoding,\n+                        num_nulls,\n+                        num_rows,\n+                        def_levels_byte_len,\n+                        rep_levels_byte_len,\n+                        is_compressed: compressor.is_some(),\n+                        statistics: from_thrift(physical_type, to_thrift(statistics.as_ref())),\n+                    }\n+                }\n+                &Page::DictionaryPage {\n+                    ref buf,\n+                    num_values,\n+                    encoding,\n+                    is_sorted,\n+                } => {\n+                    let output_buf = compress_helper(compressor.as_mut(), buf.data());\n+\n+                    Page::DictionaryPage {\n+                        buf: ByteBufferPtr::new(output_buf),\n+                        num_values,\n+                        encoding,\n+                        is_sorted,\n+                    }\n+                }\n+            };\n+\n+            let compressed_page = CompressedPage::new(compressed_page, uncompressed_len);\n+            compressed_pages.push(compressed_page);\n+        }\n+\n+        let mut buffer: Vec<u8> = vec![];\n+        let mut result_pages: Vec<Page> = vec![];\n+        {\n+            let cursor = Cursor::new(&mut buffer);\n+            let mut page_writer = SerializedPageWriter::new(cursor);\n+\n+            for page in compressed_pages {\n+                page_writer.write_page(page).unwrap();\n+            }\n+            page_writer.close().unwrap();\n+        }\n+        {\n+            let mut page_reader = SerializedPageReader::new(\n+                Cursor::new(&buffer),\n+                total_num_values,\n+                codec,\n+                physical_type,\n+            )\n+            .unwrap();\n+\n+            while let Some(page) = page_reader.get_next_page().unwrap() {\n+                result_pages.push(page);\n+            }\n+        }\n+\n+        assert_eq!(result_pages.len(), pages.len());\n+        for i in 0..result_pages.len() {\n+            assert_page(&result_pages[i], &pages[i]);\n+        }\n+    }\n+\n+    /// Helper function to compress a slice\n+    fn compress_helper(compressor: Option<&mut Box<Codec>>, data: &[u8]) -> Vec<u8> {\n+        let mut output_buf = vec![];\n+        if let Some(cmpr) = compressor {\n+            cmpr.compress(data, &mut output_buf).unwrap();\n+        } else {\n+            output_buf.extend_from_slice(data);\n+        }\n+        output_buf\n+    }\n+\n+    /// Check if pages match.\n+    fn assert_page(left: &Page, right: &Page) {\n+        assert_eq!(left.page_type(), right.page_type());\n+        assert_eq!(left.buffer().data(), right.buffer().data());\n+        assert_eq!(left.num_values(), right.num_values());\n+        assert_eq!(left.encoding(), right.encoding());\n+        assert_eq!(to_thrift(left.statistics()), to_thrift(right.statistics()));\n+    }\n+\n+    /// File write-read roundtrip.\n+    /// `data` consists of arrays of values for each row group.\n+    fn test_file_roundtrip(file: File, data: Vec<Vec<i32>>) {\n+        let schema = Rc::new(\n+            types::Type::group_type_builder(\"schema\")\n+                .with_fields(&mut vec![Rc::new(\n+                    types::Type::primitive_type_builder(\"col1\", Type::INT32)\n+                        .with_repetition(Repetition::REQUIRED)\n+                        .build()\n+                        .unwrap(),\n+                )])\n+                .build()\n+                .unwrap(),\n+        );\n+        let props = Rc::new(WriterProperties::builder().build());\n+        let mut file_writer =\n+            SerializedFileWriter::new(file.try_clone().unwrap(), schema, props).unwrap();\n+\n+        for subset in &data {\n+            let mut row_group_writer = file_writer.next_row_group().unwrap();\n+            let col_writer = row_group_writer.next_column().unwrap();\n+            if let Some(mut writer) = col_writer {\n+                match writer {\n+                    ColumnWriter::Int32ColumnWriter(ref mut typed) => {\n+                        typed.write_batch(&subset[..], None, None).unwrap();\n+                    }\n+                    _ => {\n+                        unimplemented!();\n+                    }\n+                }\n+                row_group_writer.close_column(writer).unwrap();\n+            }\n+            file_writer.close_row_group(row_group_writer).unwrap();\n+        }\n+\n+        file_writer.close().unwrap();\n+\n+        let reader = SerializedFileReader::new(file).unwrap();\n+        assert_eq!(reader.num_row_groups(), data.len());\n+        for i in 0..reader.num_row_groups() {\n+            let row_group_reader = reader.get_row_group(i).unwrap();\n+            let iter = row_group_reader.get_row_iter(None).unwrap();\n+            let res = iter\n+                .map(|elem| elem.get_int(0).unwrap())\n+                .collect::<Vec<i32>>();\n+            assert_eq!(res, data[i]);\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/mod.rs b/rust/src/parquet/mod.rs\nnew file mode 100644\nindex 0000000000..58cc7b13df\n--- /dev/null\n+++ b/rust/src/parquet/mod.rs\n@@ -0,0 +1,34 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#[macro_use]\n+pub mod errors;\n+pub mod basic;\n+pub mod data_type;\n+\n+// Exported for external use, such as benchmarks\n+pub use self::encodings::{decoding, encoding};\n+pub use self::util::memory;\n+\n+#[macro_use]\n+mod util;\n+pub mod column;\n+pub mod compression;\n+mod encodings;\n+pub mod file;\n+pub mod record;\n+pub mod schema;\ndiff --git a/rust/src/parquet/record/api.rs b/rust/src/parquet/record/api.rs\nnew file mode 100644\nindex 0000000000..d6e3ec19b7\n--- /dev/null\n+++ b/rust/src/parquet/record/api.rs\n@@ -0,0 +1,1439 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains Row enum that is used to represent record in Rust.\n+\n+use std::fmt;\n+\n+use chrono::{Local, TimeZone};\n+use num_bigint::{BigInt, Sign};\n+\n+use crate::parquet::basic::{LogicalType, Type as PhysicalType};\n+use crate::parquet::data_type::{ByteArray, Decimal, Int96};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::schema::types::ColumnDescPtr;\n+\n+/// Macro as a shortcut to generate 'not yet implemented' panic error.\n+macro_rules! nyi {\n+    ($column_descr:ident, $value:ident) => {{\n+        unimplemented!(\n+            \"Conversion for physical type {}, logical type {}, value {:?}\",\n+            $column_descr.physical_type(),\n+            $column_descr.logical_type(),\n+            $value\n+        );\n+    }};\n+}\n+\n+/// `Row` represents a nested Parquet record.\n+#[derive(Clone, Debug, PartialEq)]\n+pub struct Row {\n+    fields: Vec<(String, Field)>,\n+}\n+\n+impl Row {\n+    /// Get the number of fields in this row.\n+    pub fn len(&self) -> usize {\n+        self.fields.len()\n+    }\n+}\n+\n+/// Trait for type-safe convenient access to fields within a Row.\n+pub trait RowAccessor {\n+    fn get_bool(&self, i: usize) -> Result<bool>;\n+    fn get_byte(&self, i: usize) -> Result<i8>;\n+    fn get_short(&self, i: usize) -> Result<i16>;\n+    fn get_int(&self, i: usize) -> Result<i32>;\n+    fn get_long(&self, i: usize) -> Result<i64>;\n+    fn get_ubyte(&self, i: usize) -> Result<u8>;\n+    fn get_ushort(&self, i: usize) -> Result<u16>;\n+    fn get_uint(&self, i: usize) -> Result<u32>;\n+    fn get_ulong(&self, i: usize) -> Result<u64>;\n+    fn get_float(&self, i: usize) -> Result<f32>;\n+    fn get_double(&self, i: usize) -> Result<f64>;\n+    fn get_timestamp(&self, i: usize) -> Result<u64>;\n+    fn get_decimal(&self, i: usize) -> Result<&Decimal>;\n+    fn get_string(&self, i: usize) -> Result<&String>;\n+    fn get_bytes(&self, i: usize) -> Result<&ByteArray>;\n+    fn get_group(&self, i: usize) -> Result<&Row>;\n+    fn get_list(&self, i: usize) -> Result<&List>;\n+    fn get_map(&self, i: usize) -> Result<&Map>;\n+}\n+\n+/// Macro to generate type-safe get_xxx methods for primitive types,\n+/// e.g. `get_bool`, `get_short`.\n+macro_rules! row_primitive_accessor {\n+  ($METHOD:ident, $VARIANT:ident, $TY:ty) => {\n+    fn $METHOD(&self, i: usize) -> Result<$TY> {\n+      match self.fields[i].1 {\n+        Field::$VARIANT(v) => Ok(v),\n+        _ => Err(general_err!(\"Cannot access {} as {}\",\n+          self.fields[i].1.get_type_name(), stringify!($VARIANT)))\n+      }\n+    }\n+  }\n+}\n+\n+/// Macro to generate type-safe get_xxx methods for reference types,\n+/// e.g. `get_list`, `get_map`.\n+macro_rules! row_complex_accessor {\n+  ($METHOD:ident, $VARIANT:ident, $TY:ty) => {\n+    fn $METHOD(&self, i: usize) -> Result<&$TY> {\n+      match self.fields[i].1 {\n+        Field::$VARIANT(ref v) => Ok(v),\n+        _ => Err(general_err!(\"Cannot access {} as {}\",\n+          self.fields[i].1.get_type_name(), stringify!($VARIANT)))\n+      }\n+    }\n+  }\n+}\n+\n+impl RowAccessor for Row {\n+    row_primitive_accessor!(get_bool, Bool, bool);\n+\n+    row_primitive_accessor!(get_byte, Byte, i8);\n+\n+    row_primitive_accessor!(get_short, Short, i16);\n+\n+    row_primitive_accessor!(get_int, Int, i32);\n+\n+    row_primitive_accessor!(get_long, Long, i64);\n+\n+    row_primitive_accessor!(get_ubyte, UByte, u8);\n+\n+    row_primitive_accessor!(get_ushort, UShort, u16);\n+\n+    row_primitive_accessor!(get_uint, UInt, u32);\n+\n+    row_primitive_accessor!(get_ulong, ULong, u64);\n+\n+    row_primitive_accessor!(get_float, Float, f32);\n+\n+    row_primitive_accessor!(get_double, Double, f64);\n+\n+    row_primitive_accessor!(get_timestamp, Timestamp, u64);\n+\n+    row_complex_accessor!(get_decimal, Decimal, Decimal);\n+\n+    row_complex_accessor!(get_string, Str, String);\n+\n+    row_complex_accessor!(get_bytes, Bytes, ByteArray);\n+\n+    row_complex_accessor!(get_group, Group, Row);\n+\n+    row_complex_accessor!(get_list, ListInternal, List);\n+\n+    row_complex_accessor!(get_map, MapInternal, Map);\n+}\n+\n+/// Constructs a `Row` from the list of `fields` and returns it.\n+#[inline]\n+pub fn make_row(fields: Vec<(String, Field)>) -> Row {\n+    Row { fields }\n+}\n+\n+impl fmt::Display for Row {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"{{\")?;\n+        for (i, &(ref key, ref value)) in self.fields.iter().enumerate() {\n+            key.fmt(f)?;\n+            write!(f, \": \")?;\n+            value.fmt(f)?;\n+            if i < self.fields.len() - 1 {\n+                write!(f, \", \")?;\n+            }\n+        }\n+        write!(f, \"}}\")\n+    }\n+}\n+\n+/// `List` represents a list which contains an array of elements.\n+#[derive(Clone, Debug, PartialEq)]\n+pub struct List {\n+    elements: Vec<Field>,\n+}\n+\n+impl List {\n+    /// Get the number of fields in this row\n+    pub fn len(&self) -> usize {\n+        self.elements.len()\n+    }\n+}\n+\n+/// Constructs a `List` from the list of `fields` and returns it.\n+#[inline]\n+pub fn make_list(elements: Vec<Field>) -> List {\n+    List { elements }\n+}\n+\n+/// Trait for type-safe access of an index for a `List`.\n+/// Note that the get_XXX methods do not do bound checking.\n+pub trait ListAccessor {\n+    fn get_bool(&self, i: usize) -> Result<bool>;\n+    fn get_byte(&self, i: usize) -> Result<i8>;\n+    fn get_short(&self, i: usize) -> Result<i16>;\n+    fn get_int(&self, i: usize) -> Result<i32>;\n+    fn get_long(&self, i: usize) -> Result<i64>;\n+    fn get_ubyte(&self, i: usize) -> Result<u8>;\n+    fn get_ushort(&self, i: usize) -> Result<u16>;\n+    fn get_uint(&self, i: usize) -> Result<u32>;\n+    fn get_ulong(&self, i: usize) -> Result<u64>;\n+    fn get_float(&self, i: usize) -> Result<f32>;\n+    fn get_double(&self, i: usize) -> Result<f64>;\n+    fn get_timestamp(&self, i: usize) -> Result<u64>;\n+    fn get_decimal(&self, i: usize) -> Result<&Decimal>;\n+    fn get_string(&self, i: usize) -> Result<&String>;\n+    fn get_bytes(&self, i: usize) -> Result<&ByteArray>;\n+    fn get_group(&self, i: usize) -> Result<&Row>;\n+    fn get_list(&self, i: usize) -> Result<&List>;\n+    fn get_map(&self, i: usize) -> Result<&Map>;\n+}\n+\n+/// Macro to generate type-safe get_xxx methods for primitive types,\n+/// e.g. get_bool, get_short\n+macro_rules! list_primitive_accessor {\n+  ($METHOD:ident, $VARIANT:ident, $TY:ty) => {\n+    fn $METHOD(&self, i: usize) -> Result<$TY> {\n+      match self.elements[i] {\n+        Field::$VARIANT(v) => Ok(v),\n+        _ => Err(general_err!(\n+          \"Cannot access {} as {}\",\n+          self.elements[i].get_type_name(), stringify!($VARIANT))\n+        )\n+      }\n+    }\n+  }\n+}\n+\n+/// Macro to generate type-safe get_xxx methods for reference types\n+/// e.g. get_list, get_map\n+macro_rules! list_complex_accessor {\n+  ($METHOD:ident, $VARIANT:ident, $TY:ty) => {\n+    fn $METHOD(&self, i: usize) -> Result<&$TY> {\n+      match self.elements[i] {\n+        Field::$VARIANT(ref v) => Ok(v),\n+        _ => Err(general_err!(\n+          \"Cannot access {} as {}\",\n+          self.elements[i].get_type_name(), stringify!($VARIANT))\n+        )\n+      }\n+    }\n+  }\n+}\n+\n+impl ListAccessor for List {\n+    list_primitive_accessor!(get_bool, Bool, bool);\n+\n+    list_primitive_accessor!(get_byte, Byte, i8);\n+\n+    list_primitive_accessor!(get_short, Short, i16);\n+\n+    list_primitive_accessor!(get_int, Int, i32);\n+\n+    list_primitive_accessor!(get_long, Long, i64);\n+\n+    list_primitive_accessor!(get_ubyte, UByte, u8);\n+\n+    list_primitive_accessor!(get_ushort, UShort, u16);\n+\n+    list_primitive_accessor!(get_uint, UInt, u32);\n+\n+    list_primitive_accessor!(get_ulong, ULong, u64);\n+\n+    list_primitive_accessor!(get_float, Float, f32);\n+\n+    list_primitive_accessor!(get_double, Double, f64);\n+\n+    list_primitive_accessor!(get_timestamp, Timestamp, u64);\n+\n+    list_complex_accessor!(get_decimal, Decimal, Decimal);\n+\n+    list_complex_accessor!(get_string, Str, String);\n+\n+    list_complex_accessor!(get_bytes, Bytes, ByteArray);\n+\n+    list_complex_accessor!(get_group, Group, Row);\n+\n+    list_complex_accessor!(get_list, ListInternal, List);\n+\n+    list_complex_accessor!(get_map, MapInternal, Map);\n+}\n+\n+/// `Map` represents a map which contains an list of key->value pairs.\n+#[derive(Clone, Debug, PartialEq)]\n+pub struct Map {\n+    entries: Vec<(Field, Field)>,\n+}\n+\n+impl Map {\n+    /// Get the number of fields in this row\n+    pub fn len(&self) -> usize {\n+        self.entries.len()\n+    }\n+}\n+\n+/// Constructs a `Map` from the list of `entries` and returns it.\n+#[inline]\n+pub fn make_map(entries: Vec<(Field, Field)>) -> Map {\n+    Map { entries }\n+}\n+\n+/// Trait for type-safe access of an index for a `Map`\n+pub trait MapAccessor {\n+    fn get_keys<'a>(&'a self) -> Box<ListAccessor + 'a>;\n+    fn get_values<'a>(&'a self) -> Box<ListAccessor + 'a>;\n+}\n+\n+struct MapList<'a> {\n+    elements: Vec<&'a Field>,\n+}\n+\n+/// Macro to generate type-safe get_xxx methods for primitive types,\n+/// e.g. get_bool, get_short\n+macro_rules! map_list_primitive_accessor {\n+  ($METHOD:ident, $VARIANT:ident, $TY:ty) => {\n+    fn $METHOD(&self, i: usize) -> Result<$TY> {\n+      match self.elements[i] {\n+        Field::$VARIANT(v) => Ok(*v),\n+        _ => Err(general_err!(\n+          \"Cannot access {} as {}\",\n+          self.elements[i].get_type_name(), stringify!($VARIANT))\n+        )\n+      }\n+    }\n+  }\n+}\n+\n+impl<'a> ListAccessor for MapList<'a> {\n+    map_list_primitive_accessor!(get_bool, Bool, bool);\n+\n+    map_list_primitive_accessor!(get_byte, Byte, i8);\n+\n+    map_list_primitive_accessor!(get_short, Short, i16);\n+\n+    map_list_primitive_accessor!(get_int, Int, i32);\n+\n+    map_list_primitive_accessor!(get_long, Long, i64);\n+\n+    map_list_primitive_accessor!(get_ubyte, UByte, u8);\n+\n+    map_list_primitive_accessor!(get_ushort, UShort, u16);\n+\n+    map_list_primitive_accessor!(get_uint, UInt, u32);\n+\n+    map_list_primitive_accessor!(get_ulong, ULong, u64);\n+\n+    map_list_primitive_accessor!(get_float, Float, f32);\n+\n+    map_list_primitive_accessor!(get_double, Double, f64);\n+\n+    map_list_primitive_accessor!(get_timestamp, Timestamp, u64);\n+\n+    list_complex_accessor!(get_decimal, Decimal, Decimal);\n+\n+    list_complex_accessor!(get_string, Str, String);\n+\n+    list_complex_accessor!(get_bytes, Bytes, ByteArray);\n+\n+    list_complex_accessor!(get_group, Group, Row);\n+\n+    list_complex_accessor!(get_list, ListInternal, List);\n+\n+    list_complex_accessor!(get_map, MapInternal, Map);\n+}\n+\n+impl MapAccessor for Map {\n+    fn get_keys<'a>(&'a self) -> Box<ListAccessor + 'a> {\n+        let map_list = MapList {\n+            elements: self.entries.iter().map(|v| &v.0).collect(),\n+        };\n+        Box::new(map_list)\n+    }\n+\n+    fn get_values<'a>(&'a self) -> Box<ListAccessor + 'a> {\n+        let map_list = MapList {\n+            elements: self.entries.iter().map(|v| &v.1).collect(),\n+        };\n+        Box::new(map_list)\n+    }\n+}\n+\n+/// API to represent a single field in a `Row`.\n+#[derive(Clone, Debug, PartialEq)]\n+pub enum Field {\n+    // Primitive types\n+    /// Null value.\n+    Null,\n+    /// Boolean value (`true`, `false`).\n+    Bool(bool),\n+    /// Signed integer INT_8.\n+    Byte(i8),\n+    /// Signed integer INT_16.\n+    Short(i16),\n+    /// Signed integer INT_32.\n+    Int(i32),\n+    /// Signed integer INT_64.\n+    Long(i64),\n+    // Unsigned integer UINT_8.\n+    UByte(u8),\n+    // Unsigned integer UINT_16.\n+    UShort(u16),\n+    // Unsigned integer UINT_32.\n+    UInt(u32),\n+    // Unsigned integer UINT_64.\n+    ULong(u64),\n+    /// IEEE 32-bit floating point value.\n+    Float(f32),\n+    /// IEEE 64-bit floating point value.\n+    Double(f64),\n+    /// Decimal value.\n+    Decimal(Decimal),\n+    /// UTF-8 encoded character string.\n+    Str(String),\n+    /// General binary value.\n+    Bytes(ByteArray),\n+    /// Date without a time of day, stores the number of days from the\n+    /// Unix epoch, 1 January 1970.\n+    Date(u32),\n+    /// Milliseconds from the Unix epoch, 1 January 1970.\n+    Timestamp(u64),\n+\n+    // ----------------------------------------------------------------------\n+    // Complex types\n+    /// Struct, child elements are tuples of field-value pairs.\n+    Group(Row),\n+    /// List of elements.\n+    ListInternal(List),\n+    /// List of key-value pairs.\n+    MapInternal(Map),\n+}\n+\n+impl Field {\n+    /// Get the type name.\n+    fn get_type_name(&self) -> &'static str {\n+        match *self {\n+            Field::Null => \"Null\",\n+            Field::Bool(_) => \"Bool\",\n+            Field::Byte(_) => \"Byte\",\n+            Field::Short(_) => \"Short\",\n+            Field::Int(_) => \"Int\",\n+            Field::Long(_) => \"Long\",\n+            Field::UByte(_) => \"UByte\",\n+            Field::UShort(_) => \"UShort\",\n+            Field::UInt(_) => \"UInt\",\n+            Field::ULong(_) => \"ULong\",\n+            Field::Float(_) => \"Float\",\n+            Field::Double(_) => \"Double\",\n+            Field::Decimal(_) => \"Decimal\",\n+            Field::Date(_) => \"Date\",\n+            Field::Str(_) => \"Str\",\n+            Field::Bytes(_) => \"Bytes\",\n+            Field::Timestamp(_) => \"Timestamp\",\n+            Field::Group(_) => \"Group\",\n+            Field::ListInternal(_) => \"ListInternal\",\n+            Field::MapInternal(_) => \"MapInternal\",\n+        }\n+    }\n+\n+    /// Determines if this Row represents a primitive value.\n+    pub fn is_primitive(&self) -> bool {\n+        match *self {\n+            Field::Group(_) => false,\n+            Field::ListInternal(_) => false,\n+            Field::MapInternal(_) => false,\n+            _ => true,\n+        }\n+    }\n+\n+    /// Converts Parquet BOOLEAN type with logical type into `bool` value.\n+    #[inline]\n+    pub fn convert_bool(_descr: &ColumnDescPtr, value: bool) -> Self {\n+        Field::Bool(value)\n+    }\n+\n+    /// Converts Parquet INT32 type with logical type into `i32` value.\n+    #[inline]\n+    pub fn convert_int32(descr: &ColumnDescPtr, value: i32) -> Self {\n+        match descr.logical_type() {\n+            LogicalType::INT_8 => Field::Byte(value as i8),\n+            LogicalType::INT_16 => Field::Short(value as i16),\n+            LogicalType::INT_32 | LogicalType::NONE => Field::Int(value),\n+            LogicalType::UINT_8 => Field::UByte(value as u8),\n+            LogicalType::UINT_16 => Field::UShort(value as u16),\n+            LogicalType::UINT_32 => Field::UInt(value as u32),\n+            LogicalType::DATE => Field::Date(value as u32),\n+            LogicalType::DECIMAL => Field::Decimal(Decimal::from_i32(\n+                value,\n+                descr.type_precision(),\n+                descr.type_scale(),\n+            )),\n+            _ => nyi!(descr, value),\n+        }\n+    }\n+\n+    /// Converts Parquet INT64 type with logical type into `i64` value.\n+    #[inline]\n+    pub fn convert_int64(descr: &ColumnDescPtr, value: i64) -> Self {\n+        match descr.logical_type() {\n+            LogicalType::INT_64 | LogicalType::NONE => Field::Long(value),\n+            LogicalType::UINT_64 => Field::ULong(value as u64),\n+            LogicalType::TIMESTAMP_MILLIS => Field::Timestamp(value as u64),\n+            LogicalType::DECIMAL => Field::Decimal(Decimal::from_i64(\n+                value,\n+                descr.type_precision(),\n+                descr.type_scale(),\n+            )),\n+            _ => nyi!(descr, value),\n+        }\n+    }\n+\n+    /// Converts Parquet INT96 (nanosecond timestamps) type and logical type into\n+    /// `Timestamp` value.\n+    #[inline]\n+    pub fn convert_int96(_descr: &ColumnDescPtr, value: Int96) -> Self {\n+        const JULIAN_DAY_OF_EPOCH: i64 = 2_440_588;\n+        const SECONDS_PER_DAY: i64 = 86_400;\n+        const MILLIS_PER_SECOND: i64 = 1_000;\n+\n+        let day = value.data()[2] as i64;\n+        let nanoseconds = ((value.data()[1] as i64) << 32) + value.data()[0] as i64;\n+        let seconds = (day - JULIAN_DAY_OF_EPOCH) * SECONDS_PER_DAY;\n+        let millis = seconds * MILLIS_PER_SECOND + nanoseconds / 1_000_000;\n+\n+        // TODO: Add support for negative milliseconds.\n+        // Chrono library does not handle negative timestamps, but we could probably write\n+        // something similar to java.util.Date and java.util.Calendar.\n+        if millis < 0 {\n+            panic!(\n+                \"Expected non-negative milliseconds when converting Int96, found {}\",\n+                millis\n+            );\n+        }\n+\n+        Field::Timestamp(millis as u64)\n+    }\n+\n+    /// Converts Parquet FLOAT type with logical type into `f32` value.\n+    #[inline]\n+    pub fn convert_float(_descr: &ColumnDescPtr, value: f32) -> Self {\n+        Field::Float(value)\n+    }\n+\n+    /// Converts Parquet DOUBLE type with logical type into `f64` value.\n+    #[inline]\n+    pub fn convert_double(_descr: &ColumnDescPtr, value: f64) -> Self {\n+        Field::Double(value)\n+    }\n+\n+    /// Converts Parquet BYTE_ARRAY type with logical type into either UTF8 string or\n+    /// array of bytes.\n+    #[inline]\n+    pub fn convert_byte_array(descr: &ColumnDescPtr, value: ByteArray) -> Self {\n+        match descr.physical_type() {\n+            PhysicalType::BYTE_ARRAY => match descr.logical_type() {\n+                LogicalType::UTF8 | LogicalType::ENUM | LogicalType::JSON => {\n+                    let value = unsafe { String::from_utf8_unchecked(value.data().to_vec()) };\n+                    Field::Str(value)\n+                }\n+                LogicalType::BSON | LogicalType::NONE => Field::Bytes(value),\n+                LogicalType::DECIMAL => Field::Decimal(Decimal::from_bytes(\n+                    value,\n+                    descr.type_precision(),\n+                    descr.type_scale(),\n+                )),\n+                _ => nyi!(descr, value),\n+            },\n+            PhysicalType::FIXED_LEN_BYTE_ARRAY => match descr.logical_type() {\n+                LogicalType::DECIMAL => Field::Decimal(Decimal::from_bytes(\n+                    value,\n+                    descr.type_precision(),\n+                    descr.type_scale(),\n+                )),\n+                LogicalType::NONE => Field::Bytes(value),\n+                _ => nyi!(descr, value),\n+            },\n+            _ => nyi!(descr, value),\n+        }\n+    }\n+}\n+\n+impl fmt::Display for Field {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        match *self {\n+            Field::Null => write!(f, \"null\"),\n+            Field::Bool(value) => write!(f, \"{}\", value),\n+            Field::Byte(value) => write!(f, \"{}\", value),\n+            Field::Short(value) => write!(f, \"{}\", value),\n+            Field::Int(value) => write!(f, \"{}\", value),\n+            Field::Long(value) => write!(f, \"{}\", value),\n+            Field::UByte(value) => write!(f, \"{}\", value),\n+            Field::UShort(value) => write!(f, \"{}\", value),\n+            Field::UInt(value) => write!(f, \"{}\", value),\n+            Field::ULong(value) => write!(f, \"{}\", value),\n+            Field::Float(value) => {\n+                if value > 1e19 || value < 1e-15 {\n+                    write!(f, \"{:E}\", value)\n+                } else {\n+                    write!(f, \"{:?}\", value)\n+                }\n+            }\n+            Field::Double(value) => {\n+                if value > 1e19 || value < 1e-15 {\n+                    write!(f, \"{:E}\", value)\n+                } else {\n+                    write!(f, \"{:?}\", value)\n+                }\n+            }\n+            Field::Decimal(ref value) => write!(f, \"{}\", convert_decimal_to_string(value)),\n+            Field::Str(ref value) => write!(f, \"\\\"{}\\\"\", value),\n+            Field::Bytes(ref value) => write!(f, \"{:?}\", value.data()),\n+            Field::Date(value) => write!(f, \"{}\", convert_date_to_string(value)),\n+            Field::Timestamp(value) => write!(f, \"{}\", convert_timestamp_to_string(value)),\n+            Field::Group(ref fields) => write!(f, \"{}\", fields),\n+            Field::ListInternal(ref list) => {\n+                let elems = &list.elements;\n+                write!(f, \"[\")?;\n+                for (i, field) in elems.iter().enumerate() {\n+                    field.fmt(f)?;\n+                    if i < elems.len() - 1 {\n+                        write!(f, \", \")?;\n+                    }\n+                }\n+                write!(f, \"]\")\n+            }\n+            Field::MapInternal(ref map) => {\n+                let entries = &map.entries;\n+                write!(f, \"{{\")?;\n+                for (i, &(ref key, ref value)) in entries.iter().enumerate() {\n+                    key.fmt(f)?;\n+                    write!(f, \" -> \")?;\n+                    value.fmt(f)?;\n+                    if i < entries.len() - 1 {\n+                        write!(f, \", \")?;\n+                    }\n+                }\n+                write!(f, \"}}\")\n+            }\n+        }\n+    }\n+}\n+\n+/// Helper method to convert Parquet date into a string.\n+/// Input `value` is a number of days since the epoch in UTC.\n+/// Date is displayed in local timezone.\n+#[inline]\n+fn convert_date_to_string(value: u32) -> String {\n+    static NUM_SECONDS_IN_DAY: i64 = 60 * 60 * 24;\n+    let dt = Local.timestamp(value as i64 * NUM_SECONDS_IN_DAY, 0).date();\n+    format!(\"{}\", dt.format(\"%Y-%m-%d %:z\"))\n+}\n+\n+/// Helper method to convert Parquet timestamp into a string.\n+/// Input `value` is a number of milliseconds since the epoch in UTC.\n+/// Datetime is displayed in local timezone.\n+#[inline]\n+fn convert_timestamp_to_string(value: u64) -> String {\n+    let dt = Local.timestamp((value / 1000) as i64, 0);\n+    format!(\"{}\", dt.format(\"%Y-%m-%d %H:%M:%S %:z\"))\n+}\n+\n+/// Helper method to convert Parquet decimal into a string.\n+/// We assert that `scale >= 0` and `precision > scale`, but this will be enforced\n+/// when constructing Parquet schema.\n+#[inline]\n+fn convert_decimal_to_string(decimal: &Decimal) -> String {\n+    assert!(decimal.scale() >= 0 && decimal.precision() > decimal.scale());\n+\n+    // Specify as signed bytes to resolve sign as part of conversion.\n+    let num = BigInt::from_signed_bytes_be(decimal.data());\n+\n+    // Offset of the first digit in a string.\n+    let negative = if num.sign() == Sign::Minus { 1 } else { 0 };\n+    let mut num_str = num.to_string();\n+    let mut point = num_str.len() as i32 - decimal.scale() - negative;\n+\n+    // Convert to string form without scientific notation.\n+    if point <= 0 {\n+        // Zeros need to be prepended to the unscaled value.\n+        while point < 0 {\n+            num_str.insert(negative as usize, '0');\n+            point += 1;\n+        }\n+        num_str.insert_str(negative as usize, \"0.\");\n+    } else {\n+        // No zeroes need to be prepended to the unscaled value, simply insert decimal point.\n+        num_str.insert((point + negative) as usize, '.');\n+    }\n+\n+    num_str\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use chrono;\n+    use std::rc::Rc;\n+\n+    use crate::parquet::schema::types::{ColumnDescriptor, ColumnPath, PrimitiveTypeBuilder};\n+\n+    /// Creates test column descriptor based on provided type parameters.\n+    macro_rules! make_column_descr {\n+        ($physical_type:expr, $logical_type:expr) => {{\n+            let tpe = PrimitiveTypeBuilder::new(\"col\", $physical_type)\n+                .with_logical_type($logical_type)\n+                .build()\n+                .unwrap();\n+            Rc::new(ColumnDescriptor::new(\n+                Rc::new(tpe),\n+                None,\n+                0,\n+                0,\n+                ColumnPath::from(\"col\"),\n+            ))\n+        }};\n+        ($physical_type:expr, $logical_type:expr, $len:expr, $prec:expr, $scale:expr) => {{\n+            let tpe = PrimitiveTypeBuilder::new(\"col\", $physical_type)\n+                .with_logical_type($logical_type)\n+                .with_length($len)\n+                .with_precision($prec)\n+                .with_scale($scale)\n+                .build()\n+                .unwrap();\n+            Rc::new(ColumnDescriptor::new(\n+                Rc::new(tpe),\n+                None,\n+                0,\n+                0,\n+                ColumnPath::from(\"col\"),\n+            ))\n+        }};\n+    }\n+\n+    #[test]\n+    fn test_row_convert_bool() {\n+        // BOOLEAN value does not depend on logical type\n+        let descr = make_column_descr![PhysicalType::BOOLEAN, LogicalType::NONE];\n+\n+        let row = Field::convert_bool(&descr, true);\n+        assert_eq!(row, Field::Bool(true));\n+\n+        let row = Field::convert_bool(&descr, false);\n+        assert_eq!(row, Field::Bool(false));\n+    }\n+\n+    #[test]\n+    fn test_row_convert_int32() {\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::INT_8];\n+        let row = Field::convert_int32(&descr, 111);\n+        assert_eq!(row, Field::Byte(111));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::INT_16];\n+        let row = Field::convert_int32(&descr, 222);\n+        assert_eq!(row, Field::Short(222));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::INT_32];\n+        let row = Field::convert_int32(&descr, 333);\n+        assert_eq!(row, Field::Int(333));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::UINT_8];\n+        let row = Field::convert_int32(&descr, -1);\n+        assert_eq!(row, Field::UByte(255));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::UINT_16];\n+        let row = Field::convert_int32(&descr, 256);\n+        assert_eq!(row, Field::UShort(256));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::UINT_32];\n+        let row = Field::convert_int32(&descr, 1234);\n+        assert_eq!(row, Field::UInt(1234));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::NONE];\n+        let row = Field::convert_int32(&descr, 444);\n+        assert_eq!(row, Field::Int(444));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::DATE];\n+        let row = Field::convert_int32(&descr, 14611);\n+        assert_eq!(row, Field::Date(14611));\n+\n+        let descr = make_column_descr![PhysicalType::INT32, LogicalType::DECIMAL, 0, 8, 2];\n+        let row = Field::convert_int32(&descr, 444);\n+        assert_eq!(row, Field::Decimal(Decimal::from_i32(444, 8, 2)));\n+    }\n+\n+    #[test]\n+    fn test_row_convert_int64() {\n+        let descr = make_column_descr![PhysicalType::INT64, LogicalType::INT_64];\n+        let row = Field::convert_int64(&descr, 1111);\n+        assert_eq!(row, Field::Long(1111));\n+\n+        let descr = make_column_descr![PhysicalType::INT64, LogicalType::UINT_64];\n+        let row = Field::convert_int64(&descr, 78239823);\n+        assert_eq!(row, Field::ULong(78239823));\n+\n+        let descr = make_column_descr![PhysicalType::INT64, LogicalType::TIMESTAMP_MILLIS];\n+        let row = Field::convert_int64(&descr, 1541186529153);\n+        assert_eq!(row, Field::Timestamp(1541186529153));\n+\n+        let descr = make_column_descr![PhysicalType::INT64, LogicalType::NONE];\n+        let row = Field::convert_int64(&descr, 2222);\n+        assert_eq!(row, Field::Long(2222));\n+\n+        let descr = make_column_descr![PhysicalType::INT64, LogicalType::DECIMAL, 0, 8, 2];\n+        let row = Field::convert_int64(&descr, 3333);\n+        assert_eq!(row, Field::Decimal(Decimal::from_i64(3333, 8, 2)));\n+    }\n+\n+    #[test]\n+    fn test_row_convert_int96() {\n+        // INT96 value does not depend on logical type\n+        let descr = make_column_descr![PhysicalType::INT96, LogicalType::NONE];\n+\n+        let value = Int96::from(vec![0, 0, 2454923]);\n+        let row = Field::convert_int96(&descr, value);\n+        assert_eq!(row, Field::Timestamp(1238544000000));\n+\n+        let value = Int96::from(vec![4165425152, 13, 2454923]);\n+        let row = Field::convert_int96(&descr, value);\n+        assert_eq!(row, Field::Timestamp(1238544060000));\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Expected non-negative milliseconds when converting Int96\")]\n+    fn test_row_convert_int96_invalid() {\n+        // INT96 value does not depend on logical type\n+        let descr = make_column_descr![PhysicalType::INT96, LogicalType::NONE];\n+\n+        let value = Int96::from(vec![0, 0, 0]);\n+        Field::convert_int96(&descr, value);\n+    }\n+\n+    #[test]\n+    fn test_row_convert_float() {\n+        // FLOAT value does not depend on logical type\n+        let descr = make_column_descr![PhysicalType::FLOAT, LogicalType::NONE];\n+        let row = Field::convert_float(&descr, 2.31);\n+        assert_eq!(row, Field::Float(2.31));\n+    }\n+\n+    #[test]\n+    fn test_row_convert_double() {\n+        // DOUBLE value does not depend on logical type\n+        let descr = make_column_descr![PhysicalType::DOUBLE, LogicalType::NONE];\n+        let row = Field::convert_double(&descr, 1.56);\n+        assert_eq!(row, Field::Double(1.56));\n+    }\n+\n+    #[test]\n+    fn test_row_convert_byte_array() {\n+        // UTF8\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::UTF8];\n+        let value = ByteArray::from(vec![b'A', b'B', b'C', b'D']);\n+        let row = Field::convert_byte_array(&descr, value);\n+        assert_eq!(row, Field::Str(\"ABCD\".to_string()));\n+\n+        // ENUM\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::ENUM];\n+        let value = ByteArray::from(vec![b'1', b'2', b'3']);\n+        let row = Field::convert_byte_array(&descr, value);\n+        assert_eq!(row, Field::Str(\"123\".to_string()));\n+\n+        // JSON\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::JSON];\n+        let value = ByteArray::from(vec![b'{', b'\"', b'a', b'\"', b':', b'1', b'}']);\n+        let row = Field::convert_byte_array(&descr, value);\n+        assert_eq!(row, Field::Str(\"{\\\"a\\\":1}\".to_string()));\n+\n+        // NONE\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::NONE];\n+        let value = ByteArray::from(vec![1, 2, 3, 4, 5]);\n+        let row = Field::convert_byte_array(&descr, value.clone());\n+        assert_eq!(row, Field::Bytes(value));\n+\n+        // BSON\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::BSON];\n+        let value = ByteArray::from(vec![1, 2, 3, 4, 5]);\n+        let row = Field::convert_byte_array(&descr, value.clone());\n+        assert_eq!(row, Field::Bytes(value));\n+\n+        // DECIMAL\n+        let descr = make_column_descr![PhysicalType::BYTE_ARRAY, LogicalType::DECIMAL, 0, 8, 2];\n+        let value = ByteArray::from(vec![207, 200]);\n+        let row = Field::convert_byte_array(&descr, value.clone());\n+        assert_eq!(row, Field::Decimal(Decimal::from_bytes(value, 8, 2)));\n+\n+        // DECIMAL (FIXED_LEN_BYTE_ARRAY)\n+        let descr = make_column_descr![\n+            PhysicalType::FIXED_LEN_BYTE_ARRAY,\n+            LogicalType::DECIMAL,\n+            8,\n+            17,\n+            5\n+        ];\n+        let value = ByteArray::from(vec![0, 0, 0, 0, 0, 4, 147, 224]);\n+        let row = Field::convert_byte_array(&descr, value.clone());\n+        assert_eq!(row, Field::Decimal(Decimal::from_bytes(value, 17, 5)));\n+\n+        // NONE (FIXED_LEN_BYTE_ARRAY)\n+        let descr = make_column_descr![\n+            PhysicalType::FIXED_LEN_BYTE_ARRAY,\n+            LogicalType::NONE,\n+            6,\n+            0,\n+            0\n+        ];\n+        let value = ByteArray::from(vec![1, 2, 3, 4, 5, 6]);\n+        let row = Field::convert_byte_array(&descr, value.clone());\n+        assert_eq!(row, Field::Bytes(value));\n+    }\n+\n+    #[test]\n+    fn test_convert_date_to_string() {\n+        fn check_date_conversion(y: u32, m: u32, d: u32) {\n+            let datetime = chrono::NaiveDate::from_ymd(y as i32, m, d).and_hms(0, 0, 0);\n+            let dt = Local.from_utc_datetime(&datetime);\n+            let res = convert_date_to_string((dt.timestamp() / 60 / 60 / 24) as u32);\n+            let exp = format!(\"{}\", dt.format(\"%Y-%m-%d %:z\"));\n+            assert_eq!(res, exp);\n+        }\n+\n+        check_date_conversion(2010, 01, 02);\n+        check_date_conversion(2014, 05, 01);\n+        check_date_conversion(2016, 02, 29);\n+        check_date_conversion(2017, 09, 12);\n+        check_date_conversion(2018, 03, 31);\n+    }\n+\n+    #[test]\n+    fn test_convert_timestamp_to_string() {\n+        fn check_datetime_conversion(y: u32, m: u32, d: u32, h: u32, mi: u32, s: u32) {\n+            let datetime = chrono::NaiveDate::from_ymd(y as i32, m, d).and_hms(h, mi, s);\n+            let dt = Local.from_utc_datetime(&datetime);\n+            let res = convert_timestamp_to_string(dt.timestamp_millis() as u64);\n+            let exp = format!(\"{}\", dt.format(\"%Y-%m-%d %H:%M:%S %:z\"));\n+            assert_eq!(res, exp);\n+        }\n+\n+        check_datetime_conversion(2010, 01, 02, 13, 12, 54);\n+        check_datetime_conversion(2011, 01, 03, 08, 23, 01);\n+        check_datetime_conversion(2012, 04, 05, 11, 06, 32);\n+        check_datetime_conversion(2013, 05, 12, 16, 38, 00);\n+        check_datetime_conversion(2014, 11, 28, 21, 15, 12);\n+    }\n+\n+    #[test]\n+    fn test_convert_float_to_string() {\n+        assert_eq!(format!(\"{}\", Field::Float(1.0)), \"1.0\");\n+        assert_eq!(format!(\"{}\", Field::Float(9.63)), \"9.63\");\n+        assert_eq!(format!(\"{}\", Field::Float(1e-15)), \"0.000000000000001\");\n+        assert_eq!(format!(\"{}\", Field::Float(1e-16)), \"1E-16\");\n+        assert_eq!(format!(\"{}\", Field::Float(1e19)), \"10000000000000000000.0\");\n+        assert_eq!(format!(\"{}\", Field::Float(1e20)), \"1E20\");\n+        assert_eq!(format!(\"{}\", Field::Float(1.7976931E30)), \"1.7976931E30\");\n+        assert_eq!(format!(\"{}\", Field::Float(-1.7976931E30)), \"-1.7976931E30\");\n+    }\n+\n+    #[test]\n+    fn test_convert_double_to_string() {\n+        assert_eq!(format!(\"{}\", Field::Double(1.0)), \"1.0\");\n+        assert_eq!(format!(\"{}\", Field::Double(9.63)), \"9.63\");\n+        assert_eq!(format!(\"{}\", Field::Double(1e-15)), \"0.000000000000001\");\n+        assert_eq!(format!(\"{}\", Field::Double(1e-16)), \"1E-16\");\n+        assert_eq!(format!(\"{}\", Field::Double(1e19)), \"10000000000000000000.0\");\n+        assert_eq!(format!(\"{}\", Field::Double(1e20)), \"1E20\");\n+        assert_eq!(\n+            format!(\"{}\", Field::Double(1.79769313486E308)),\n+            \"1.79769313486E308\"\n+        );\n+        assert_eq!(\n+            format!(\"{}\", Field::Double(-1.79769313486E308)),\n+            \"-1.79769313486E308\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_convert_decimal_to_string() {\n+        // Helper method to compare decimal\n+        fn check_decimal(bytes: Vec<u8>, precision: i32, scale: i32, res: &str) {\n+            let decimal = Decimal::from_bytes(ByteArray::from(bytes), precision, scale);\n+            assert_eq!(convert_decimal_to_string(&decimal), res);\n+        }\n+\n+        // This example previously used to fail in some engines\n+        check_decimal(\n+            vec![0, 0, 0, 0, 0, 0, 0, 0, 13, 224, 182, 179, 167, 100, 0, 0],\n+            38,\n+            18,\n+            \"1.000000000000000000\",\n+        );\n+        check_decimal(\n+            vec![\n+                249, 233, 247, 16, 185, 192, 202, 223, 215, 165, 192, 166, 67, 72,\n+            ],\n+            36,\n+            28,\n+            \"-12344.0242342304923409234234293432\",\n+        );\n+        check_decimal(vec![0, 0, 0, 0, 0, 4, 147, 224], 17, 5, \"3.00000\");\n+        check_decimal(vec![0, 0, 0, 0, 1, 201, 195, 140], 18, 2, \"300000.12\");\n+        check_decimal(vec![207, 200], 10, 2, \"-123.44\");\n+        check_decimal(vec![207, 200], 10, 8, \"-0.00012344\");\n+    }\n+\n+    #[test]\n+    fn test_row_display() {\n+        // Primitive types\n+        assert_eq!(format!(\"{}\", Field::Null), \"null\");\n+        assert_eq!(format!(\"{}\", Field::Bool(true)), \"true\");\n+        assert_eq!(format!(\"{}\", Field::Bool(false)), \"false\");\n+        assert_eq!(format!(\"{}\", Field::Byte(1)), \"1\");\n+        assert_eq!(format!(\"{}\", Field::Short(2)), \"2\");\n+        assert_eq!(format!(\"{}\", Field::Int(3)), \"3\");\n+        assert_eq!(format!(\"{}\", Field::Long(4)), \"4\");\n+        assert_eq!(format!(\"{}\", Field::UByte(1)), \"1\");\n+        assert_eq!(format!(\"{}\", Field::UShort(2)), \"2\");\n+        assert_eq!(format!(\"{}\", Field::UInt(3)), \"3\");\n+        assert_eq!(format!(\"{}\", Field::ULong(4)), \"4\");\n+        assert_eq!(format!(\"{}\", Field::Float(5.0)), \"5.0\");\n+        assert_eq!(format!(\"{}\", Field::Float(5.1234)), \"5.1234\");\n+        assert_eq!(format!(\"{}\", Field::Double(6.0)), \"6.0\");\n+        assert_eq!(format!(\"{}\", Field::Double(6.1234)), \"6.1234\");\n+        assert_eq!(format!(\"{}\", Field::Str(\"abc\".to_string())), \"\\\"abc\\\"\");\n+        assert_eq!(\n+            format!(\"{}\", Field::Bytes(ByteArray::from(vec![1, 2, 3]))),\n+            \"[1, 2, 3]\"\n+        );\n+        assert_eq!(\n+            format!(\"{}\", Field::Date(14611)),\n+            convert_date_to_string(14611)\n+        );\n+        assert_eq!(\n+            format!(\"{}\", Field::Timestamp(1262391174000)),\n+            convert_timestamp_to_string(1262391174000)\n+        );\n+        assert_eq!(\n+            format!(\"{}\", Field::Decimal(Decimal::from_i32(4, 8, 2))),\n+            convert_decimal_to_string(&Decimal::from_i32(4, 8, 2))\n+        );\n+\n+        // Complex types\n+        let fields = vec![\n+            (\"x\".to_string(), Field::Null),\n+            (\"Y\".to_string(), Field::Int(2)),\n+            (\"z\".to_string(), Field::Float(3.1)),\n+            (\"a\".to_string(), Field::Str(\"abc\".to_string())),\n+        ];\n+        let row = Field::Group(make_row(fields));\n+        assert_eq!(format!(\"{}\", row), \"{x: null, Y: 2, z: 3.1, a: \\\"abc\\\"}\");\n+\n+        let row = Field::ListInternal(make_list(vec![\n+            Field::Int(2),\n+            Field::Int(1),\n+            Field::Null,\n+            Field::Int(12),\n+        ]));\n+        assert_eq!(format!(\"{}\", row), \"[2, 1, null, 12]\");\n+\n+        let row = Field::MapInternal(make_map(vec![\n+            (Field::Int(1), Field::Float(1.2)),\n+            (Field::Int(2), Field::Float(4.5)),\n+            (Field::Int(3), Field::Float(2.3)),\n+        ]));\n+        assert_eq!(format!(\"{}\", row), \"{1 -> 1.2, 2 -> 4.5, 3 -> 2.3}\");\n+    }\n+\n+    #[test]\n+    fn test_is_primitive() {\n+        // primitives\n+        assert!(Field::Null.is_primitive());\n+        assert!(Field::Bool(true).is_primitive());\n+        assert!(Field::Bool(false).is_primitive());\n+        assert!(Field::Byte(1).is_primitive());\n+        assert!(Field::Short(2).is_primitive());\n+        assert!(Field::Int(3).is_primitive());\n+        assert!(Field::Long(4).is_primitive());\n+        assert!(Field::UByte(1).is_primitive());\n+        assert!(Field::UShort(2).is_primitive());\n+        assert!(Field::UInt(3).is_primitive());\n+        assert!(Field::ULong(4).is_primitive());\n+        assert!(Field::Float(5.0).is_primitive());\n+        assert!(Field::Float(5.1234).is_primitive());\n+        assert!(Field::Double(6.0).is_primitive());\n+        assert!(Field::Double(6.1234).is_primitive());\n+        assert!(Field::Str(\"abc\".to_string()).is_primitive());\n+        assert!(Field::Bytes(ByteArray::from(vec![1, 2, 3])).is_primitive());\n+        assert!(Field::Timestamp(12345678).is_primitive());\n+        assert!(Field::Decimal(Decimal::from_i32(4, 8, 2)).is_primitive());\n+\n+        // complex types\n+        assert_eq!(\n+            false,\n+            Field::Group(make_row(vec![\n+                (\"x\".to_string(), Field::Null),\n+                (\"Y\".to_string(), Field::Int(2)),\n+                (\"z\".to_string(), Field::Float(3.1)),\n+                (\"a\".to_string(), Field::Str(\"abc\".to_string()))\n+            ]))\n+            .is_primitive()\n+        );\n+\n+        assert_eq!(\n+            false,\n+            Field::ListInternal(make_list(vec![\n+                Field::Int(2),\n+                Field::Int(1),\n+                Field::Null,\n+                Field::Int(12)\n+            ]))\n+            .is_primitive()\n+        );\n+\n+        assert_eq!(\n+            false,\n+            Field::MapInternal(make_map(vec![\n+                (Field::Int(1), Field::Float(1.2)),\n+                (Field::Int(2), Field::Float(4.5)),\n+                (Field::Int(3), Field::Float(2.3))\n+            ]))\n+            .is_primitive()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_row_primitive_accessors() {\n+        // primitives\n+        let row = make_row(vec![\n+            (\"a\".to_string(), Field::Null),\n+            (\"b\".to_string(), Field::Bool(false)),\n+            (\"c\".to_string(), Field::Byte(3)),\n+            (\"d\".to_string(), Field::Short(4)),\n+            (\"e\".to_string(), Field::Int(5)),\n+            (\"f\".to_string(), Field::Long(6)),\n+            (\"g\".to_string(), Field::UByte(3)),\n+            (\"h\".to_string(), Field::UShort(4)),\n+            (\"i\".to_string(), Field::UInt(5)),\n+            (\"j\".to_string(), Field::ULong(6)),\n+            (\"k\".to_string(), Field::Float(7.1)),\n+            (\"l\".to_string(), Field::Double(8.1)),\n+            (\"m\".to_string(), Field::Str(\"abc\".to_string())),\n+            (\n+                \"n\".to_string(),\n+                Field::Bytes(ByteArray::from(vec![1, 2, 3, 4, 5])),\n+            ),\n+            (\"o\".to_string(), Field::Decimal(Decimal::from_i32(4, 7, 2))),\n+        ]);\n+\n+        assert_eq!(false, row.get_bool(1).unwrap());\n+        assert_eq!(3, row.get_byte(2).unwrap());\n+        assert_eq!(4, row.get_short(3).unwrap());\n+        assert_eq!(5, row.get_int(4).unwrap());\n+        assert_eq!(6, row.get_long(5).unwrap());\n+        assert_eq!(3, row.get_ubyte(6).unwrap());\n+        assert_eq!(4, row.get_ushort(7).unwrap());\n+        assert_eq!(5, row.get_uint(8).unwrap());\n+        assert_eq!(6, row.get_ulong(9).unwrap());\n+        assert_eq!(7.1, row.get_float(10).unwrap());\n+        assert_eq!(8.1, row.get_double(11).unwrap());\n+        assert_eq!(\"abc\", row.get_string(12).unwrap());\n+        assert_eq!(5, row.get_bytes(13).unwrap().len());\n+        assert_eq!(7, row.get_decimal(14).unwrap().precision());\n+    }\n+\n+    #[test]\n+    fn test_row_primitive_invalid_accessors() {\n+        // primitives\n+        let row = make_row(vec![\n+            (\"a\".to_string(), Field::Null),\n+            (\"b\".to_string(), Field::Bool(false)),\n+            (\"c\".to_string(), Field::Byte(3)),\n+            (\"d\".to_string(), Field::Short(4)),\n+            (\"e\".to_string(), Field::Int(5)),\n+            (\"f\".to_string(), Field::Long(6)),\n+            (\"g\".to_string(), Field::UByte(3)),\n+            (\"h\".to_string(), Field::UShort(4)),\n+            (\"i\".to_string(), Field::UInt(5)),\n+            (\"j\".to_string(), Field::ULong(6)),\n+            (\"k\".to_string(), Field::Float(7.1)),\n+            (\"l\".to_string(), Field::Double(8.1)),\n+            (\"m\".to_string(), Field::Str(\"abc\".to_string())),\n+            (\n+                \"n\".to_string(),\n+                Field::Bytes(ByteArray::from(vec![1, 2, 3, 4, 5])),\n+            ),\n+            (\"o\".to_string(), Field::Decimal(Decimal::from_i32(4, 7, 2))),\n+        ]);\n+\n+        for i in 0..row.len() {\n+            assert!(row.get_group(i).is_err());\n+        }\n+    }\n+\n+    #[test]\n+    fn test_row_complex_accessors() {\n+        let row = make_row(vec![\n+            (\n+                \"a\".to_string(),\n+                Field::Group(make_row(vec![\n+                    (\"x\".to_string(), Field::Null),\n+                    (\"Y\".to_string(), Field::Int(2)),\n+                ])),\n+            ),\n+            (\n+                \"b\".to_string(),\n+                Field::ListInternal(make_list(vec![\n+                    Field::Int(2),\n+                    Field::Int(1),\n+                    Field::Null,\n+                    Field::Int(12),\n+                ])),\n+            ),\n+            (\n+                \"c\".to_string(),\n+                Field::MapInternal(make_map(vec![\n+                    (Field::Int(1), Field::Float(1.2)),\n+                    (Field::Int(2), Field::Float(4.5)),\n+                    (Field::Int(3), Field::Float(2.3)),\n+                ])),\n+            ),\n+        ]);\n+\n+        assert_eq!(2, row.get_group(0).unwrap().len());\n+        assert_eq!(4, row.get_list(1).unwrap().len());\n+        assert_eq!(3, row.get_map(2).unwrap().len());\n+    }\n+\n+    #[test]\n+    fn test_row_complex_invalid_accessors() {\n+        let row = make_row(vec![\n+            (\n+                \"a\".to_string(),\n+                Field::Group(make_row(vec![\n+                    (\"x\".to_string(), Field::Null),\n+                    (\"Y\".to_string(), Field::Int(2)),\n+                ])),\n+            ),\n+            (\n+                \"b\".to_string(),\n+                Field::ListInternal(make_list(vec![\n+                    Field::Int(2),\n+                    Field::Int(1),\n+                    Field::Null,\n+                    Field::Int(12),\n+                ])),\n+            ),\n+            (\n+                \"c\".to_string(),\n+                Field::MapInternal(make_map(vec![\n+                    (Field::Int(1), Field::Float(1.2)),\n+                    (Field::Int(2), Field::Float(4.5)),\n+                    (Field::Int(3), Field::Float(2.3)),\n+                ])),\n+            ),\n+        ]);\n+\n+        assert_eq!(\n+            ParquetError::General(\"Cannot access Group as Float\".to_string()),\n+            row.get_float(0).unwrap_err()\n+        );\n+        assert_eq!(\n+            ParquetError::General(\"Cannot access ListInternal as Float\".to_string()),\n+            row.get_float(1).unwrap_err()\n+        );\n+        assert_eq!(\n+            ParquetError::General(\"Cannot access MapInternal as Float\".to_string()),\n+            row.get_float(2).unwrap_err()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_list_primitive_accessors() {\n+        // primitives\n+        let list = make_list(vec![Field::Bool(false)]);\n+        assert_eq!(false, list.get_bool(0).unwrap());\n+\n+        let list = make_list(vec![Field::Byte(3), Field::Byte(4)]);\n+        assert_eq!(4, list.get_byte(1).unwrap());\n+\n+        let list = make_list(vec![Field::Short(4), Field::Short(5), Field::Short(6)]);\n+        assert_eq!(6, list.get_short(2).unwrap());\n+\n+        let list = make_list(vec![Field::Int(5)]);\n+        assert_eq!(5, list.get_int(0).unwrap());\n+\n+        let list = make_list(vec![Field::Long(6), Field::Long(7)]);\n+        assert_eq!(7, list.get_long(1).unwrap());\n+\n+        let list = make_list(vec![Field::UByte(3), Field::UByte(4)]);\n+        assert_eq!(4, list.get_ubyte(1).unwrap());\n+\n+        let list = make_list(vec![Field::UShort(4), Field::UShort(5), Field::UShort(6)]);\n+        assert_eq!(6, list.get_ushort(2).unwrap());\n+\n+        let list = make_list(vec![Field::UInt(5)]);\n+        assert_eq!(5, list.get_uint(0).unwrap());\n+\n+        let list = make_list(vec![Field::ULong(6), Field::ULong(7)]);\n+        assert_eq!(7, list.get_ulong(1).unwrap());\n+\n+        let list = make_list(vec![\n+            Field::Float(8.1),\n+            Field::Float(9.2),\n+            Field::Float(10.3),\n+        ]);\n+        assert_eq!(10.3, list.get_float(2).unwrap());\n+\n+        let list = make_list(vec![Field::Double(3.1415)]);\n+        assert_eq!(3.1415, list.get_double(0).unwrap());\n+\n+        let list = make_list(vec![Field::Str(\"abc\".to_string())]);\n+        assert_eq!(&\"abc\".to_string(), list.get_string(0).unwrap());\n+\n+        let list = make_list(vec![Field::Bytes(ByteArray::from(vec![1, 2, 3, 4, 5]))]);\n+        assert_eq!(&[1, 2, 3, 4, 5], list.get_bytes(0).unwrap().data());\n+\n+        let list = make_list(vec![Field::Decimal(Decimal::from_i32(4, 5, 2))]);\n+        assert_eq!(&[0, 0, 0, 4], list.get_decimal(0).unwrap().data());\n+    }\n+\n+    #[test]\n+    fn test_list_primitive_invalid_accessors() {\n+        // primitives\n+        let list = make_list(vec![Field::Bool(false)]);\n+        assert!(list.get_byte(0).is_err());\n+\n+        let list = make_list(vec![Field::Byte(3), Field::Byte(4)]);\n+        assert!(list.get_short(1).is_err());\n+\n+        let list = make_list(vec![Field::Short(4), Field::Short(5), Field::Short(6)]);\n+        assert!(list.get_int(2).is_err());\n+\n+        let list = make_list(vec![Field::Int(5)]);\n+        assert!(list.get_long(0).is_err());\n+\n+        let list = make_list(vec![Field::Long(6), Field::Long(7)]);\n+        assert!(list.get_float(1).is_err());\n+\n+        let list = make_list(vec![Field::UByte(3), Field::UByte(4)]);\n+        assert!(list.get_short(1).is_err());\n+\n+        let list = make_list(vec![Field::UShort(4), Field::UShort(5), Field::UShort(6)]);\n+        assert!(list.get_int(2).is_err());\n+\n+        let list = make_list(vec![Field::UInt(5)]);\n+        assert!(list.get_long(0).is_err());\n+\n+        let list = make_list(vec![Field::ULong(6), Field::ULong(7)]);\n+        assert!(list.get_float(1).is_err());\n+\n+        let list = make_list(vec![\n+            Field::Float(8.1),\n+            Field::Float(9.2),\n+            Field::Float(10.3),\n+        ]);\n+        assert!(list.get_double(2).is_err());\n+\n+        let list = make_list(vec![Field::Double(3.1415)]);\n+        assert!(list.get_string(0).is_err());\n+\n+        let list = make_list(vec![Field::Str(\"abc\".to_string())]);\n+        assert!(list.get_bytes(0).is_err());\n+\n+        let list = make_list(vec![Field::Bytes(ByteArray::from(vec![1, 2, 3, 4, 5]))]);\n+        assert!(list.get_bool(0).is_err());\n+\n+        let list = make_list(vec![Field::Decimal(Decimal::from_i32(4, 5, 2))]);\n+        assert!(list.get_bool(0).is_err());\n+    }\n+\n+    #[test]\n+    fn test_list_complex_accessors() {\n+        let list = make_list(vec![Field::Group(make_row(vec![\n+            (\"x\".to_string(), Field::Null),\n+            (\"Y\".to_string(), Field::Int(2)),\n+        ]))]);\n+        assert_eq!(2, list.get_group(0).unwrap().len());\n+\n+        let list = make_list(vec![Field::ListInternal(make_list(vec![\n+            Field::Int(2),\n+            Field::Int(1),\n+            Field::Null,\n+            Field::Int(12),\n+        ]))]);\n+        assert_eq!(4, list.get_list(0).unwrap().len());\n+\n+        let list = make_list(vec![Field::MapInternal(make_map(vec![\n+            (Field::Int(1), Field::Float(1.2)),\n+            (Field::Int(2), Field::Float(4.5)),\n+            (Field::Int(3), Field::Float(2.3)),\n+        ]))]);\n+        assert_eq!(3, list.get_map(0).unwrap().len());\n+    }\n+\n+    #[test]\n+    fn test_list_complex_invalid_accessors() {\n+        let list = make_list(vec![Field::Group(make_row(vec![\n+            (\"x\".to_string(), Field::Null),\n+            (\"Y\".to_string(), Field::Int(2)),\n+        ]))]);\n+        assert_eq!(\n+            general_err!(\"Cannot access Group as Float\".to_string()),\n+            list.get_float(0).unwrap_err()\n+        );\n+\n+        let list = make_list(vec![Field::ListInternal(make_list(vec![\n+            Field::Int(2),\n+            Field::Int(1),\n+            Field::Null,\n+            Field::Int(12),\n+        ]))]);\n+        assert_eq!(\n+            general_err!(\"Cannot access ListInternal as Float\".to_string()),\n+            list.get_float(0).unwrap_err()\n+        );\n+\n+        let list = make_list(vec![Field::MapInternal(make_map(vec![\n+            (Field::Int(1), Field::Float(1.2)),\n+            (Field::Int(2), Field::Float(4.5)),\n+            (Field::Int(3), Field::Float(2.3)),\n+        ]))]);\n+        assert_eq!(\n+            general_err!(\"Cannot access MapInternal as Float\".to_string()),\n+            list.get_float(0).unwrap_err()\n+        );\n+    }\n+\n+    #[test]\n+    fn test_map_accessors() {\n+        // a map from int to string\n+        let map = make_map(vec![\n+            (Field::Int(1), Field::Str(\"a\".to_string())),\n+            (Field::Int(2), Field::Str(\"b\".to_string())),\n+            (Field::Int(3), Field::Str(\"c\".to_string())),\n+            (Field::Int(4), Field::Str(\"d\".to_string())),\n+            (Field::Int(5), Field::Str(\"e\".to_string())),\n+        ]);\n+\n+        assert_eq!(5, map.len());\n+        for i in 0..5 {\n+            assert_eq!((i + 1) as i32, map.get_keys().get_int(i).unwrap());\n+            assert_eq!(\n+                &((i as u8 + 'a' as u8) as char).to_string(),\n+                map.get_values().get_string(i).unwrap()\n+            );\n+        }\n+    }\n+}\ndiff --git a/rust/src/parquet/record/mod.rs b/rust/src/parquet/record/mod.rs\nnew file mode 100644\nindex 0000000000..0dba8a78bd\n--- /dev/null\n+++ b/rust/src/parquet/record/mod.rs\n@@ -0,0 +1,24 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains record-based API for reading Parquet files.\n+\n+mod api;\n+pub mod reader;\n+mod triplet;\n+\n+pub use self::api::{List, ListAccessor, Map, MapAccessor, Row, RowAccessor};\ndiff --git a/rust/src/parquet/record/reader.rs b/rust/src/parquet/record/reader.rs\nnew file mode 100644\nindex 0000000000..d9f3d6fea1\n--- /dev/null\n+++ b/rust/src/parquet/record/reader.rs\n@@ -0,0 +1,1464 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains implementation of record assembly and converting Parquet types into\n+//! [`Row`](`::record::api::Row`)s.\n+\n+use std::{collections::HashMap, fmt, rc::Rc};\n+\n+use crate::parquet::basic::{LogicalType, Repetition};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::file::reader::{FileReader, RowGroupReader};\n+use crate::parquet::record::{\n+    api::{make_list, make_map, make_row, Field, Row},\n+    triplet::TripletIter,\n+};\n+use crate::parquet::schema::types::{ColumnPath, SchemaDescPtr, SchemaDescriptor, Type, TypePtr};\n+\n+/// Default batch size for a reader\n+const DEFAULT_BATCH_SIZE: usize = 1024;\n+\n+/// Tree builder for `Reader` enum.\n+/// Serves as a container of options for building a reader tree and a builder, and\n+/// accessing a records iterator [`RowIter`].\n+pub struct TreeBuilder {\n+    // Batch size (>= 1) for triplet iterators\n+    batch_size: usize,\n+}\n+\n+impl TreeBuilder {\n+    /// Creates new tree builder with default parameters.\n+    pub fn new() -> Self {\n+        Self {\n+            batch_size: DEFAULT_BATCH_SIZE,\n+        }\n+    }\n+\n+    /// Sets batch size for this tree builder.\n+    pub fn with_batch_size(mut self, batch_size: usize) -> Self {\n+        self.batch_size = batch_size;\n+        self\n+    }\n+\n+    /// Creates new root reader for provided schema and row group.\n+    pub fn build(&self, descr: SchemaDescPtr, row_group_reader: &RowGroupReader) -> Reader {\n+        // Prepare lookup table of column path -> original column index\n+        // This allows to prune columns and map schema leaf nodes to the column readers\n+        let mut paths: HashMap<ColumnPath, usize> = HashMap::new();\n+        let row_group_metadata = row_group_reader.metadata();\n+\n+        for col_index in 0..row_group_reader.num_columns() {\n+            let col_meta = row_group_metadata.column(col_index);\n+            let col_path = col_meta.column_path().clone();\n+            paths.insert(col_path, col_index);\n+        }\n+\n+        // Build child readers for the message type\n+        let mut readers = Vec::new();\n+        let mut path = Vec::new();\n+\n+        for field in descr.root_schema().get_fields() {\n+            let reader = self.reader_tree(field.clone(), &mut path, 0, 0, &paths, row_group_reader);\n+            readers.push(reader);\n+        }\n+\n+        // Return group reader for message type,\n+        // it is always required with definition level 0\n+        Reader::GroupReader(None, 0, readers)\n+    }\n+\n+    /// Creates iterator of `Row`s directly from schema descriptor and row group.\n+    pub fn as_iter(&self, descr: SchemaDescPtr, row_group_reader: &RowGroupReader) -> ReaderIter {\n+        let num_records = row_group_reader.metadata().num_rows() as usize;\n+        ReaderIter::new(self.build(descr, row_group_reader), num_records)\n+    }\n+\n+    /// Builds tree of readers for the current schema recursively.\n+    fn reader_tree(\n+        &self,\n+        field: TypePtr,\n+        mut path: &mut Vec<String>,\n+        mut curr_def_level: i16,\n+        mut curr_rep_level: i16,\n+        paths: &HashMap<ColumnPath, usize>,\n+        row_group_reader: &RowGroupReader,\n+    ) -> Reader {\n+        assert!(field.get_basic_info().has_repetition());\n+        // Update current definition and repetition levels for this type\n+        let repetition = field.get_basic_info().repetition();\n+        match repetition {\n+            Repetition::OPTIONAL => {\n+                curr_def_level += 1;\n+            }\n+            Repetition::REPEATED => {\n+                curr_def_level += 1;\n+                curr_rep_level += 1;\n+            }\n+            _ => {}\n+        }\n+\n+        path.push(String::from(field.name()));\n+        let reader = if field.is_primitive() {\n+            let col_path = ColumnPath::new(path.to_vec());\n+            let orig_index = *paths.get(&col_path).unwrap();\n+            let col_descr = row_group_reader\n+                .metadata()\n+                .column(orig_index)\n+                .column_descr_ptr();\n+            let col_reader = row_group_reader.get_column_reader(orig_index).unwrap();\n+            let column = TripletIter::new(col_descr, col_reader, self.batch_size);\n+            Reader::PrimitiveReader(field, column)\n+        } else {\n+            match field.get_basic_info().logical_type() {\n+                // List types\n+                LogicalType::LIST => {\n+                    assert_eq!(field.get_fields().len(), 1, \"Invalid list type {:?}\", field);\n+\n+                    let repeated_field = field.get_fields()[0].clone();\n+                    assert_eq!(\n+                        repeated_field.get_basic_info().repetition(),\n+                        Repetition::REPEATED,\n+                        \"Invalid list type {:?}\",\n+                        field\n+                    );\n+\n+                    if Reader::is_element_type(&repeated_field) {\n+                        // Support for backward compatible lists\n+                        let reader = self.reader_tree(\n+                            repeated_field.clone(),\n+                            &mut path,\n+                            curr_def_level,\n+                            curr_rep_level,\n+                            paths,\n+                            row_group_reader,\n+                        );\n+\n+                        Reader::RepeatedReader(\n+                            field,\n+                            curr_def_level,\n+                            curr_rep_level,\n+                            Box::new(reader),\n+                        )\n+                    } else {\n+                        let child_field = repeated_field.get_fields()[0].clone();\n+\n+                        path.push(String::from(repeated_field.name()));\n+\n+                        let reader = self.reader_tree(\n+                            child_field,\n+                            &mut path,\n+                            curr_def_level + 1,\n+                            curr_rep_level + 1,\n+                            paths,\n+                            row_group_reader,\n+                        );\n+\n+                        path.pop();\n+\n+                        Reader::RepeatedReader(\n+                            field,\n+                            curr_def_level,\n+                            curr_rep_level,\n+                            Box::new(reader),\n+                        )\n+                    }\n+                }\n+                // Map types (key-value pairs)\n+                LogicalType::MAP | LogicalType::MAP_KEY_VALUE => {\n+                    assert_eq!(field.get_fields().len(), 1, \"Invalid map type: {:?}\", field);\n+                    assert!(\n+                        !field.get_fields()[0].is_primitive(),\n+                        \"Invalid map type: {:?}\",\n+                        field\n+                    );\n+\n+                    let key_value_type = field.get_fields()[0].clone();\n+                    assert_eq!(\n+                        key_value_type.get_basic_info().repetition(),\n+                        Repetition::REPEATED,\n+                        \"Invalid map type: {:?}\",\n+                        field\n+                    );\n+                    assert_eq!(\n+                        key_value_type.get_fields().len(),\n+                        2,\n+                        \"Invalid map type: {:?}\",\n+                        field\n+                    );\n+\n+                    path.push(String::from(key_value_type.name()));\n+\n+                    let key_type = &key_value_type.get_fields()[0];\n+                    assert!(\n+                        key_type.is_primitive(),\n+                        \"Map key type is expected to be a primitive type, but found {:?}\",\n+                        key_type\n+                    );\n+                    let key_reader = self.reader_tree(\n+                        key_type.clone(),\n+                        &mut path,\n+                        curr_def_level + 1,\n+                        curr_rep_level + 1,\n+                        paths,\n+                        row_group_reader,\n+                    );\n+\n+                    let value_type = &key_value_type.get_fields()[1];\n+                    let value_reader = self.reader_tree(\n+                        value_type.clone(),\n+                        &mut path,\n+                        curr_def_level + 1,\n+                        curr_rep_level + 1,\n+                        paths,\n+                        row_group_reader,\n+                    );\n+\n+                    path.pop();\n+\n+                    Reader::KeyValueReader(\n+                        field,\n+                        curr_def_level,\n+                        curr_rep_level,\n+                        Box::new(key_reader),\n+                        Box::new(value_reader),\n+                    )\n+                }\n+                // A repeated field that is neither contained by a `LIST`- or `MAP`-annotated\n+                // group nor annotated by `LIST` or `MAP` should be interpreted as a required\n+                // list of required elements where the element type is the type of the field.\n+                _ if repetition == Repetition::REPEATED => {\n+                    let required_field = Type::group_type_builder(field.name())\n+                        .with_repetition(Repetition::REQUIRED)\n+                        .with_logical_type(field.get_basic_info().logical_type())\n+                        .with_fields(&mut Vec::from(field.get_fields()))\n+                        .build()\n+                        .unwrap();\n+\n+                    path.pop();\n+\n+                    let reader = self.reader_tree(\n+                        Rc::new(required_field),\n+                        &mut path,\n+                        curr_def_level,\n+                        curr_rep_level,\n+                        paths,\n+                        row_group_reader,\n+                    );\n+\n+                    Reader::RepeatedReader(\n+                        field,\n+                        curr_def_level - 1,\n+                        curr_rep_level - 1,\n+                        Box::new(reader),\n+                    )\n+                }\n+                // Group types (structs)\n+                _ => {\n+                    let mut readers = Vec::new();\n+                    for child in field.get_fields() {\n+                        let reader = self.reader_tree(\n+                            child.clone(),\n+                            &mut path,\n+                            curr_def_level,\n+                            curr_rep_level,\n+                            paths,\n+                            row_group_reader,\n+                        );\n+                        readers.push(reader);\n+                    }\n+                    Reader::GroupReader(Some(field), curr_def_level, readers)\n+                }\n+            }\n+        };\n+        path.pop();\n+\n+        Reader::option(repetition, curr_def_level, reader)\n+    }\n+}\n+\n+/// Reader tree for record assembly\n+pub enum Reader {\n+    // Primitive reader with type information and triplet iterator\n+    PrimitiveReader(TypePtr, TripletIter),\n+    // Optional reader with definition level of a parent and a reader\n+    OptionReader(i16, Box<Reader>),\n+    // Group (struct) reader with type information, definition level and list of child\n+    // readers. When it represents message type, type information is None\n+    GroupReader(Option<TypePtr>, i16, Vec<Reader>),\n+    // Reader for repeated values, e.g. lists, contains type information, definition\n+    // level, repetition level and a child reader\n+    RepeatedReader(TypePtr, i16, i16, Box<Reader>),\n+    // Reader of key-value pairs, e.g. maps, contains type information, definition level,\n+    // repetition level, child reader for keys and child reader for values\n+    KeyValueReader(TypePtr, i16, i16, Box<Reader>, Box<Reader>),\n+}\n+\n+impl Reader {\n+    /// Wraps reader in option reader based on repetition.\n+    fn option(repetition: Repetition, def_level: i16, reader: Reader) -> Self {\n+        if repetition == Repetition::OPTIONAL {\n+            Reader::OptionReader(def_level - 1, Box::new(reader))\n+        } else {\n+            reader\n+        }\n+    }\n+\n+    /// Returns true if repeated type is an element type for the list.\n+    /// Used to determine legacy list types.\n+    /// This method is copied from Spark Parquet reader and is based on the reference:\n+    /// https://github.com/apache/parquet-format/blob/master/LogicalTypes.md\n+    ///   #backward-compatibility-rules\n+    fn is_element_type(repeated_type: &Type) -> bool {\n+        // For legacy 2-level list types with primitive element type, e.g.:\n+        //\n+        //    // ARRAY<INT> (nullable list, non-null elements)\n+        //    optional group my_list (LIST) {\n+        //      repeated int32 element;\n+        //    }\n+        //\n+        repeated_type.is_primitive() ||\n+    // For legacy 2-level list types whose element type is a group type with 2 or more\n+    // fields, e.g.:\n+    //\n+    //    // ARRAY<STRUCT<str: STRING, num: INT>> (nullable list, non-null elements)\n+    //    optional group my_list (LIST) {\n+    //      repeated group element {\n+    //        required binary str (UTF8);\n+    //        required int32 num;\n+    //      };\n+    //    }\n+    //\n+    repeated_type.is_group() && repeated_type.get_fields().len() > 1 ||\n+    // For legacy 2-level list types generated by parquet-avro (Parquet version < 1.6.0),\n+    // e.g.:\n+    //\n+    //    // ARRAY<STRUCT<str: STRING>> (nullable list, non-null elements)\n+    //    optional group my_list (LIST) {\n+    //      repeated group array {\n+    //        required binary str (UTF8);\n+    //      };\n+    //    }\n+    //\n+    repeated_type.name() == \"array\" ||\n+    // For Parquet data generated by parquet-thrift, e.g.:\n+    //\n+    //    // ARRAY<STRUCT<str: STRING>> (nullable list, non-null elements)\n+    //    optional group my_list (LIST) {\n+    //      repeated group my_list_tuple {\n+    //        required binary str (UTF8);\n+    //      };\n+    //    }\n+    //\n+    repeated_type.name().ends_with(\"_tuple\")\n+    }\n+\n+    /// Reads current record as `Row` from the reader tree.\n+    /// Automatically advances all necessary readers.\n+    /// This must be called on the root level reader (i.e., for Message type).\n+    /// Otherwise, it will panic.\n+    fn read(&mut self) -> Row {\n+        match *self {\n+            Reader::GroupReader(_, _, ref mut readers) => {\n+                let mut fields = Vec::new();\n+                for reader in readers {\n+                    fields.push((String::from(reader.field_name()), reader.read_field()));\n+                }\n+                make_row(fields)\n+            }\n+            _ => panic!(\"Cannot call read() on {}\", self),\n+        }\n+    }\n+\n+    /// Reads current record as `Field` from the reader tree.\n+    /// Automatically advances all necessary readers.\n+    fn read_field(&mut self) -> Field {\n+        match *self {\n+            Reader::PrimitiveReader(_, ref mut column) => {\n+                let value = column.current_value();\n+                column.read_next().unwrap();\n+                value\n+            }\n+            Reader::OptionReader(def_level, ref mut reader) => {\n+                if reader.current_def_level() > def_level {\n+                    reader.read_field()\n+                } else {\n+                    reader.advance_columns();\n+                    Field::Null\n+                }\n+            }\n+            Reader::GroupReader(_, def_level, ref mut readers) => {\n+                let mut fields = Vec::new();\n+                for reader in readers {\n+                    if reader.repetition() != Repetition::OPTIONAL\n+                        || reader.current_def_level() > def_level\n+                    {\n+                        fields.push((String::from(reader.field_name()), reader.read_field()));\n+                    } else {\n+                        reader.advance_columns();\n+                        fields.push((String::from(reader.field_name()), Field::Null));\n+                    }\n+                }\n+                let row = make_row(fields);\n+                Field::Group(row)\n+            }\n+            Reader::RepeatedReader(_, def_level, rep_level, ref mut reader) => {\n+                let mut elements = Vec::new();\n+                loop {\n+                    if reader.current_def_level() > def_level {\n+                        elements.push(reader.read_field());\n+                    } else {\n+                        reader.advance_columns();\n+                        // If the current definition level is equal to the definition level of this\n+                        // repeated type, then the result is an empty list and the repetition level\n+                        // will always be <= rl.\n+                        break;\n+                    }\n+\n+                    // This covers case when we are out of repetition levels and should close the\n+                    // group, or there are no values left to buffer.\n+                    if !reader.has_next() || reader.current_rep_level() <= rep_level {\n+                        break;\n+                    }\n+                }\n+                Field::ListInternal(make_list(elements))\n+            }\n+            Reader::KeyValueReader(_, def_level, rep_level, ref mut keys, ref mut values) => {\n+                let mut pairs = Vec::new();\n+                loop {\n+                    if keys.current_def_level() > def_level {\n+                        pairs.push((keys.read_field(), values.read_field()));\n+                    } else {\n+                        keys.advance_columns();\n+                        values.advance_columns();\n+                        // If the current definition level is equal to the definition level of this\n+                        // repeated type, then the result is an empty list and the repetition level\n+                        // will always be <= rl.\n+                        break;\n+                    }\n+\n+                    // This covers case when we are out of repetition levels and should close the\n+                    // group, or there are no values left to buffer.\n+                    if !keys.has_next() || keys.current_rep_level() <= rep_level {\n+                        break;\n+                    }\n+                }\n+\n+                Field::MapInternal(make_map(pairs))\n+            }\n+        }\n+    }\n+\n+    /// Returns field name for the current reader.\n+    fn field_name(&self) -> &str {\n+        match *self {\n+            Reader::PrimitiveReader(ref field, _) => field.name(),\n+            Reader::OptionReader(_, ref reader) => reader.field_name(),\n+            Reader::GroupReader(ref opt, ..) => match opt {\n+                &Some(ref field) => field.name(),\n+                &None => panic!(\"Field is None for group reader\"),\n+            },\n+            Reader::RepeatedReader(ref field, ..) => field.name(),\n+            Reader::KeyValueReader(ref field, ..) => field.name(),\n+        }\n+    }\n+\n+    /// Returns repetition for the current reader.\n+    fn repetition(&self) -> Repetition {\n+        match *self {\n+            Reader::PrimitiveReader(ref field, _) => field.get_basic_info().repetition(),\n+            Reader::OptionReader(_, ref reader) => reader.repetition(),\n+            Reader::GroupReader(ref opt, ..) => match opt {\n+                &Some(ref field) => field.get_basic_info().repetition(),\n+                &None => panic!(\"Field is None for group reader\"),\n+            },\n+            Reader::RepeatedReader(ref field, ..) => field.get_basic_info().repetition(),\n+            Reader::KeyValueReader(ref field, ..) => field.get_basic_info().repetition(),\n+        }\n+    }\n+\n+    /// Returns true, if current reader has more values, false otherwise.\n+    /// Method does not advance internal iterator.\n+    fn has_next(&self) -> bool {\n+        match *self {\n+            Reader::PrimitiveReader(_, ref column) => column.has_next(),\n+            Reader::OptionReader(_, ref reader) => reader.has_next(),\n+            Reader::GroupReader(_, _, ref readers) => readers.first().unwrap().has_next(),\n+            Reader::RepeatedReader(_, _, _, ref reader) => reader.has_next(),\n+            Reader::KeyValueReader(_, _, _, ref keys, _) => keys.has_next(),\n+        }\n+    }\n+\n+    /// Returns current definition level,\n+    /// Method does not advance internal iterator.\n+    fn current_def_level(&self) -> i16 {\n+        match *self {\n+            Reader::PrimitiveReader(_, ref column) => column.current_def_level(),\n+            Reader::OptionReader(_, ref reader) => reader.current_def_level(),\n+            Reader::GroupReader(_, _, ref readers) => match readers.first() {\n+                Some(reader) => reader.current_def_level(),\n+                None => panic!(\"Current definition level: empty group reader\"),\n+            },\n+            Reader::RepeatedReader(_, _, _, ref reader) => reader.current_def_level(),\n+            Reader::KeyValueReader(_, _, _, ref keys, _) => keys.current_def_level(),\n+        }\n+    }\n+\n+    /// Returns current repetition level.\n+    /// Method does not advance internal iterator.\n+    fn current_rep_level(&self) -> i16 {\n+        match *self {\n+            Reader::PrimitiveReader(_, ref column) => column.current_rep_level(),\n+            Reader::OptionReader(_, ref reader) => reader.current_rep_level(),\n+            Reader::GroupReader(_, _, ref readers) => match readers.first() {\n+                Some(reader) => reader.current_rep_level(),\n+                None => panic!(\"Current repetition level: empty group reader\"),\n+            },\n+            Reader::RepeatedReader(_, _, _, ref reader) => reader.current_rep_level(),\n+            Reader::KeyValueReader(_, _, _, ref keys, _) => keys.current_rep_level(),\n+        }\n+    }\n+\n+    /// Advances leaf columns for the current reader.\n+    fn advance_columns(&mut self) {\n+        match *self {\n+            Reader::PrimitiveReader(_, ref mut column) => {\n+                column.read_next().unwrap();\n+            }\n+            Reader::OptionReader(_, ref mut reader) => {\n+                reader.advance_columns();\n+            }\n+            Reader::GroupReader(_, _, ref mut readers) => {\n+                for reader in readers {\n+                    reader.advance_columns();\n+                }\n+            }\n+            Reader::RepeatedReader(_, _, _, ref mut reader) => {\n+                reader.advance_columns();\n+            }\n+            Reader::KeyValueReader(_, _, _, ref mut keys, ref mut values) => {\n+                keys.advance_columns();\n+                values.advance_columns();\n+            }\n+        }\n+    }\n+}\n+\n+impl fmt::Display for Reader {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        let s = match self {\n+            Reader::PrimitiveReader(..) => \"PrimitiveReader\",\n+            Reader::OptionReader(..) => \"OptionReader\",\n+            Reader::GroupReader(..) => \"GroupReader\",\n+            Reader::RepeatedReader(..) => \"RepeatedReader\",\n+            Reader::KeyValueReader(..) => \"KeyValueReader\",\n+        };\n+        write!(f, \"{}\", s)\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// Row iterators\n+\n+/// Iterator of [`Row`](`::record::api::Row`)s.\n+/// It is used either for a single row group to iterate over data in that row group, or\n+/// an entire file with auto buffering of all row groups.\n+pub struct RowIter<'a> {\n+    descr: SchemaDescPtr,\n+    tree_builder: TreeBuilder,\n+    file_reader: Option<&'a FileReader>,\n+    current_row_group: usize,\n+    num_row_groups: usize,\n+    row_iter: Option<ReaderIter>,\n+}\n+\n+impl<'a> RowIter<'a> {\n+    /// Creates iterator of [`Row`](`::record::api::Row`)s for all row groups in a file.\n+    pub fn from_file(proj: Option<Type>, reader: &'a FileReader) -> Result<Self> {\n+        let descr =\n+            Self::get_proj_descr(proj, reader.metadata().file_metadata().schema_descr_ptr())?;\n+        let num_row_groups = reader.num_row_groups();\n+\n+        Ok(Self {\n+            descr,\n+            tree_builder: Self::tree_builder(),\n+            file_reader: Some(reader),\n+            current_row_group: 0,\n+            num_row_groups,\n+            row_iter: None,\n+        })\n+    }\n+\n+    /// Creates iterator of [`Row`](`::record::api::Row`)s for a specific row group.\n+    pub fn from_row_group(proj: Option<Type>, reader: &'a RowGroupReader) -> Result<Self> {\n+        let descr = Self::get_proj_descr(proj, reader.metadata().schema_descr_ptr())?;\n+        let tree_builder = Self::tree_builder();\n+        let row_iter = tree_builder.as_iter(descr.clone(), reader);\n+\n+        // For row group we need to set `current_row_group` >= `num_row_groups`, because we\n+        // only have one row group and can't buffer more.\n+        Ok(Self {\n+            descr,\n+            tree_builder,\n+            file_reader: None,\n+            current_row_group: 0,\n+            num_row_groups: 0,\n+            row_iter: Some(row_iter),\n+        })\n+    }\n+\n+    /// Returns common tree builder, so the same settings are applied to both iterators\n+    /// from file reader and row group.\n+    #[inline]\n+    fn tree_builder() -> TreeBuilder {\n+        TreeBuilder::new()\n+    }\n+\n+    /// Helper method to get schema descriptor for projected schema.\n+    /// If projection is None, then full schema is returned.\n+    #[inline]\n+    fn get_proj_descr(proj: Option<Type>, root_descr: SchemaDescPtr) -> Result<SchemaDescPtr> {\n+        match proj {\n+            Some(projection) => {\n+                // check if projection is part of file schema\n+                let root_schema = root_descr.root_schema();\n+                if !root_schema.check_contains(&projection) {\n+                    return Err(general_err!(\"Root schema does not contain projection\"));\n+                }\n+                Ok(Rc::new(SchemaDescriptor::new(Rc::new(projection))))\n+            }\n+            None => Ok(root_descr),\n+        }\n+    }\n+}\n+\n+impl<'a> Iterator for RowIter<'a> {\n+    type Item = Row;\n+\n+    fn next(&mut self) -> Option<Row> {\n+        let mut row = None;\n+        if let Some(ref mut iter) = self.row_iter {\n+            row = iter.next();\n+        }\n+\n+        while row.is_none() && self.current_row_group < self.num_row_groups {\n+            // We do not expect any failures when accessing a row group, and file reader\n+            // must be set for selecting next row group.\n+            let row_group_reader = &*self\n+                .file_reader\n+                .as_ref()\n+                .expect(\"File reader is required to advance row group\")\n+                .get_row_group(self.current_row_group)\n+                .unwrap();\n+            self.current_row_group += 1;\n+            let mut iter = self\n+                .tree_builder\n+                .as_iter(self.descr.clone(), row_group_reader);\n+            row = iter.next();\n+            self.row_iter = Some(iter);\n+        }\n+\n+        row\n+    }\n+}\n+\n+/// Internal iterator of [`Row`](`::record::api::Row`)s for a reader.\n+pub struct ReaderIter {\n+    root_reader: Reader,\n+    records_left: usize,\n+}\n+\n+impl ReaderIter {\n+    fn new(mut root_reader: Reader, num_records: usize) -> Self {\n+        // Prepare root reader by advancing all column vectors\n+        root_reader.advance_columns();\n+        Self {\n+            root_reader,\n+            records_left: num_records,\n+        }\n+    }\n+}\n+\n+impl Iterator for ReaderIter {\n+    type Item = Row;\n+\n+    fn next(&mut self) -> Option<Row> {\n+        if self.records_left > 0 {\n+            self.records_left -= 1;\n+            Some(self.root_reader.read())\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use crate::parquet::errors::{ParquetError, Result};\n+    use crate::parquet::file::reader::{FileReader, SerializedFileReader};\n+    use crate::parquet::record::api::{Field, Row};\n+    use crate::parquet::schema::parser::parse_message_type;\n+    use crate::parquet::util::test_common::get_test_file;\n+\n+    // Convenient macros to assemble row, list, map, and group.\n+\n+    macro_rules! row {\n+        () => {\n+            {\n+                let result = Vec::new();\n+                make_row(result)\n+            }\n+        };\n+        ( $( $e:expr ), + ) => {\n+            {\n+                let mut result = Vec::new();\n+                $(\n+                    result.push($e);\n+                )*\n+                    make_row(result)\n+            }\n+        }\n+    }\n+\n+    macro_rules! list {\n+        () => {\n+            {\n+                let result = Vec::new();\n+                Field::ListInternal(make_list(result))\n+            }\n+        };\n+        ( $( $e:expr ), + ) => {\n+            {\n+                let mut result = Vec::new();\n+                $(\n+                    result.push($e);\n+                )*\n+                    Field::ListInternal(make_list(result))\n+            }\n+        }\n+    }\n+\n+    macro_rules! map {\n+        () => {\n+            {\n+                let result = Vec::new();\n+                Field::MapInternal(make_map(result))\n+            }\n+        };\n+        ( $( $e:expr ), + ) => {\n+            {\n+                let mut result = Vec::new();\n+                $(\n+                    result.push($e);\n+                )*\n+                    Field::MapInternal(make_map(result))\n+            }\n+        }\n+    }\n+\n+    macro_rules! group {\n+        ( $( $e:expr ), * ) => {\n+            {\n+                Field::Group(row!($( $e ), *))\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_nulls() {\n+        let rows = test_file_reader_rows(\"nulls.snappy.parquet\", None).unwrap();\n+        let expected_rows = vec![\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+            row![(\n+                \"b_struct\".to_string(),\n+                group![(\"b_c_int\".to_string(), Field::Null)]\n+            )],\n+        ];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_nonnullable() {\n+        let rows = test_file_reader_rows(\"nonnullable.impala.parquet\", None).unwrap();\n+        let expected_rows = vec![row![\n+            (\"ID\".to_string(), Field::Long(8)),\n+            (\"Int_Array\".to_string(), list![Field::Int(-1)]),\n+            (\n+                \"int_array_array\".to_string(),\n+                list![list![Field::Int(-1), Field::Int(-2)], list![]]\n+            ),\n+            (\n+                \"Int_Map\".to_string(),\n+                map![(Field::Str(\"k1\".to_string()), Field::Int(-1))]\n+            ),\n+            (\n+                \"int_map_array\".to_string(),\n+                list![\n+                    map![],\n+                    map![(Field::Str(\"k1\".to_string()), Field::Int(1))],\n+                    map![],\n+                    map![]\n+                ]\n+            ),\n+            (\n+                \"nested_Struct\".to_string(),\n+                group![\n+                    (\"a\".to_string(), Field::Int(-1)),\n+                    (\"B\".to_string(), list![Field::Int(-1)]),\n+                    (\n+                        \"c\".to_string(),\n+                        group![(\n+                            \"D\".to_string(),\n+                            list![list![group![\n+                                (\"e\".to_string(), Field::Int(-1)),\n+                                (\"f\".to_string(), Field::Str(\"nonnullable\".to_string()))\n+                            ]]]\n+                        )]\n+                    ),\n+                    (\"G\".to_string(), map![])\n+                ]\n+            )\n+        ]];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_nullable() {\n+        let rows = test_file_reader_rows(\"nullable.impala.parquet\", None).unwrap();\n+        let expected_rows = vec![\n+            row![\n+                (\"id\".to_string(), Field::Long(1)),\n+                (\n+                    \"int_array\".to_string(),\n+                    list![Field::Int(1), Field::Int(2), Field::Int(3)]\n+                ),\n+                (\n+                    \"int_array_Array\".to_string(),\n+                    list![\n+                        list![Field::Int(1), Field::Int(2)],\n+                        list![Field::Int(3), Field::Int(4)]\n+                    ]\n+                ),\n+                (\n+                    \"int_map\".to_string(),\n+                    map![\n+                        (Field::Str(\"k1\".to_string()), Field::Int(1)),\n+                        (Field::Str(\"k2\".to_string()), Field::Int(100))\n+                    ]\n+                ),\n+                (\n+                    \"int_Map_Array\".to_string(),\n+                    list![map![(Field::Str(\"k1\".to_string()), Field::Int(1))]]\n+                ),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Int(1)),\n+                        (\"b\".to_string(), list![Field::Int(1)]),\n+                        (\n+                            \"C\".to_string(),\n+                            group![(\n+                                \"d\".to_string(),\n+                                list![\n+                                    list![\n+                                        group![\n+                                            (\"E\".to_string(), Field::Int(10)),\n+                                            (\"F\".to_string(), Field::Str(\"aaa\".to_string()))\n+                                        ],\n+                                        group![\n+                                            (\"E\".to_string(), Field::Int(-10)),\n+                                            (\"F\".to_string(), Field::Str(\"bbb\".to_string()))\n+                                        ]\n+                                    ],\n+                                    list![group![\n+                                        (\"E\".to_string(), Field::Int(11)),\n+                                        (\"F\".to_string(), Field::Str(\"c\".to_string()))\n+                                    ]]\n+                                ]\n+                            )]\n+                        ),\n+                        (\n+                            \"g\".to_string(),\n+                            map![(\n+                                Field::Str(\"foo\".to_string()),\n+                                group![(\n+                                    \"H\".to_string(),\n+                                    group![(\"i\".to_string(), list![Field::Double(1.1)])]\n+                                )]\n+                            )]\n+                        )\n+                    ]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(2)),\n+                (\n+                    \"int_array\".to_string(),\n+                    list![\n+                        Field::Null,\n+                        Field::Int(1),\n+                        Field::Int(2),\n+                        Field::Null,\n+                        Field::Int(3),\n+                        Field::Null\n+                    ]\n+                ),\n+                (\n+                    \"int_array_Array\".to_string(),\n+                    list![\n+                        list![Field::Null, Field::Int(1), Field::Int(2), Field::Null],\n+                        list![Field::Int(3), Field::Null, Field::Int(4)],\n+                        list![],\n+                        Field::Null\n+                    ]\n+                ),\n+                (\n+                    \"int_map\".to_string(),\n+                    map![\n+                        (Field::Str(\"k1\".to_string()), Field::Int(2)),\n+                        (Field::Str(\"k2\".to_string()), Field::Null)\n+                    ]\n+                ),\n+                (\n+                    \"int_Map_Array\".to_string(),\n+                    list![\n+                        map![\n+                            (Field::Str(\"k3\".to_string()), Field::Null),\n+                            (Field::Str(\"k1\".to_string()), Field::Int(1))\n+                        ],\n+                        Field::Null,\n+                        map![]\n+                    ]\n+                ),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Null),\n+                        (\"b\".to_string(), list![Field::Null]),\n+                        (\n+                            \"C\".to_string(),\n+                            group![(\n+                                \"d\".to_string(),\n+                                list![\n+                                    list![\n+                                        group![\n+                                            (\"E\".to_string(), Field::Null),\n+                                            (\"F\".to_string(), Field::Null)\n+                                        ],\n+                                        group![\n+                                            (\"E\".to_string(), Field::Int(10)),\n+                                            (\"F\".to_string(), Field::Str(\"aaa\".to_string()))\n+                                        ],\n+                                        group![\n+                                            (\"E\".to_string(), Field::Null),\n+                                            (\"F\".to_string(), Field::Null)\n+                                        ],\n+                                        group![\n+                                            (\"E\".to_string(), Field::Int(-10)),\n+                                            (\"F\".to_string(), Field::Str(\"bbb\".to_string()))\n+                                        ],\n+                                        group![\n+                                            (\"E\".to_string(), Field::Null),\n+                                            (\"F\".to_string(), Field::Null)\n+                                        ]\n+                                    ],\n+                                    list![\n+                                        group![\n+                                            (\"E\".to_string(), Field::Int(11)),\n+                                            (\"F\".to_string(), Field::Str(\"c\".to_string()))\n+                                        ],\n+                                        Field::Null\n+                                    ],\n+                                    list![],\n+                                    Field::Null\n+                                ]\n+                            )]\n+                        ),\n+                        (\n+                            \"g\".to_string(),\n+                            map![\n+                                (\n+                                    Field::Str(\"g1\".to_string()),\n+                                    group![(\n+                                        \"H\".to_string(),\n+                                        group![(\n+                                            \"i\".to_string(),\n+                                            list![Field::Double(2.2), Field::Null]\n+                                        )]\n+                                    )]\n+                                ),\n+                                (\n+                                    Field::Str(\"g2\".to_string()),\n+                                    group![(\"H\".to_string(), group![(\"i\".to_string(), list![])])]\n+                                ),\n+                                (Field::Str(\"g3\".to_string()), Field::Null),\n+                                (\n+                                    Field::Str(\"g4\".to_string()),\n+                                    group![(\n+                                        \"H\".to_string(),\n+                                        group![(\"i\".to_string(), Field::Null)]\n+                                    )]\n+                                ),\n+                                (\n+                                    Field::Str(\"g5\".to_string()),\n+                                    group![(\"H\".to_string(), Field::Null)]\n+                                )\n+                            ]\n+                        )\n+                    ]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(3)),\n+                (\"int_array\".to_string(), list![]),\n+                (\"int_array_Array\".to_string(), list![Field::Null]),\n+                (\"int_map\".to_string(), map![]),\n+                (\"int_Map_Array\".to_string(), list![Field::Null, Field::Null]),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Null),\n+                        (\"b\".to_string(), Field::Null),\n+                        (\"C\".to_string(), group![(\"d\".to_string(), list![])]),\n+                        (\"g\".to_string(), map![])\n+                    ]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(4)),\n+                (\"int_array\".to_string(), Field::Null),\n+                (\"int_array_Array\".to_string(), list![]),\n+                (\"int_map\".to_string(), map![]),\n+                (\"int_Map_Array\".to_string(), list![]),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Null),\n+                        (\"b\".to_string(), Field::Null),\n+                        (\"C\".to_string(), group![(\"d\".to_string(), Field::Null)]),\n+                        (\"g\".to_string(), Field::Null)\n+                    ]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(5)),\n+                (\"int_array\".to_string(), Field::Null),\n+                (\"int_array_Array\".to_string(), Field::Null),\n+                (\"int_map\".to_string(), map![]),\n+                (\"int_Map_Array\".to_string(), Field::Null),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Null),\n+                        (\"b\".to_string(), Field::Null),\n+                        (\"C\".to_string(), Field::Null),\n+                        (\n+                            \"g\".to_string(),\n+                            map![(\n+                                Field::Str(\"foo\".to_string()),\n+                                group![(\n+                                    \"H\".to_string(),\n+                                    group![(\n+                                        \"i\".to_string(),\n+                                        list![Field::Double(2.2), Field::Double(3.3)]\n+                                    )]\n+                                )]\n+                            )]\n+                        )\n+                    ]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(6)),\n+                (\"int_array\".to_string(), Field::Null),\n+                (\"int_array_Array\".to_string(), Field::Null),\n+                (\"int_map\".to_string(), Field::Null),\n+                (\"int_Map_Array\".to_string(), Field::Null),\n+                (\"nested_struct\".to_string(), Field::Null)\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Long(7)),\n+                (\"int_array\".to_string(), Field::Null),\n+                (\n+                    \"int_array_Array\".to_string(),\n+                    list![Field::Null, list![Field::Int(5), Field::Int(6)]]\n+                ),\n+                (\n+                    \"int_map\".to_string(),\n+                    map![\n+                        (Field::Str(\"k1\".to_string()), Field::Null),\n+                        (Field::Str(\"k3\".to_string()), Field::Null)\n+                    ]\n+                ),\n+                (\"int_Map_Array\".to_string(), Field::Null),\n+                (\n+                    \"nested_struct\".to_string(),\n+                    group![\n+                        (\"A\".to_string(), Field::Int(7)),\n+                        (\n+                            \"b\".to_string(),\n+                            list![Field::Int(2), Field::Int(3), Field::Null]\n+                        ),\n+                        (\n+                            \"C\".to_string(),\n+                            group![(\n+                                \"d\".to_string(),\n+                                list![list![], list![Field::Null], Field::Null]\n+                            )]\n+                        ),\n+                        (\"g\".to_string(), Field::Null)\n+                    ]\n+                )\n+            ],\n+        ];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_projection() {\n+        let schema = \"\n+      message spark_schema {\n+        REQUIRED DOUBLE c;\n+        REQUIRED INT32 b;\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        let rows = test_file_reader_rows(\"nested_maps.snappy.parquet\", Some(schema)).unwrap();\n+        let expected_rows = vec![\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+            row![\n+                (\"c\".to_string(), Field::Double(1.0)),\n+                (\"b\".to_string(), Field::Int(1))\n+            ],\n+        ];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_projection_map() {\n+        let schema = \"\n+      message spark_schema {\n+        OPTIONAL group a (MAP) {\n+          REPEATED group key_value {\n+            REQUIRED BYTE_ARRAY key (UTF8);\n+            OPTIONAL group value (MAP) {\n+              REPEATED group key_value {\n+                REQUIRED INT32 key;\n+                REQUIRED BOOLEAN value;\n+              }\n+            }\n+          }\n+        }\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        let rows = test_file_reader_rows(\"nested_maps.snappy.parquet\", Some(schema)).unwrap();\n+        let expected_rows = vec![\n+            row![(\n+                \"a\".to_string(),\n+                map![(\n+                    Field::Str(\"a\".to_string()),\n+                    map![\n+                        (Field::Int(1), Field::Bool(true)),\n+                        (Field::Int(2), Field::Bool(false))\n+                    ]\n+                )]\n+            )],\n+            row![(\n+                \"a\".to_string(),\n+                map![(\n+                    Field::Str(\"b\".to_string()),\n+                    map![(Field::Int(1), Field::Bool(true))]\n+                )]\n+            )],\n+            row![(\n+                \"a\".to_string(),\n+                map![(Field::Str(\"c\".to_string()), Field::Null)]\n+            )],\n+            row![(\"a\".to_string(), map![(Field::Str(\"d\".to_string()), map![])])],\n+            row![(\n+                \"a\".to_string(),\n+                map![(\n+                    Field::Str(\"e\".to_string()),\n+                    map![(Field::Int(1), Field::Bool(true))]\n+                )]\n+            )],\n+            row![(\n+                \"a\".to_string(),\n+                map![(\n+                    Field::Str(\"f\".to_string()),\n+                    map![\n+                        (Field::Int(3), Field::Bool(true)),\n+                        (Field::Int(4), Field::Bool(false)),\n+                        (Field::Int(5), Field::Bool(true))\n+                    ]\n+                )]\n+            )],\n+        ];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_projection_list() {\n+        let schema = \"\n+      message spark_schema {\n+        OPTIONAL group a (LIST) {\n+          REPEATED group list {\n+            OPTIONAL group element (LIST) {\n+              REPEATED group list {\n+                OPTIONAL group element (LIST) {\n+                  REPEATED group list {\n+                    OPTIONAL BYTE_ARRAY element (UTF8);\n+                  }\n+                }\n+              }\n+            }\n+          }\n+        }\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        let rows = test_file_reader_rows(\"nested_lists.snappy.parquet\", Some(schema)).unwrap();\n+        let expected_rows = vec![\n+            row![(\n+                \"a\".to_string(),\n+                list![\n+                    list![\n+                        list![Field::Str(\"a\".to_string()), Field::Str(\"b\".to_string())],\n+                        list![Field::Str(\"c\".to_string())]\n+                    ],\n+                    list![Field::Null, list![Field::Str(\"d\".to_string())]]\n+                ]\n+            )],\n+            row![(\n+                \"a\".to_string(),\n+                list![\n+                    list![\n+                        list![Field::Str(\"a\".to_string()), Field::Str(\"b\".to_string())],\n+                        list![Field::Str(\"c\".to_string()), Field::Str(\"d\".to_string())]\n+                    ],\n+                    list![Field::Null, list![Field::Str(\"e\".to_string())]]\n+                ]\n+            )],\n+            row![(\n+                \"a\".to_string(),\n+                list![\n+                    list![\n+                        list![Field::Str(\"a\".to_string()), Field::Str(\"b\".to_string())],\n+                        list![Field::Str(\"c\".to_string()), Field::Str(\"d\".to_string())],\n+                        list![Field::Str(\"e\".to_string())]\n+                    ],\n+                    list![Field::Null, list![Field::Str(\"f\".to_string())]]\n+                ]\n+            )],\n+        ];\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    #[test]\n+    fn test_file_reader_rows_invalid_projection() {\n+        let schema = \"\n+      message spark_schema {\n+        REQUIRED INT32 key;\n+        REQUIRED BOOLEAN value;\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        let res = test_file_reader_rows(\"nested_maps.snappy.parquet\", Some(schema));\n+        assert!(res.is_err());\n+        assert_eq!(\n+            res.unwrap_err(),\n+            general_err!(\"Root schema does not contain projection\")\n+        );\n+    }\n+\n+    #[test]\n+    fn test_row_group_rows_invalid_projection() {\n+        let schema = \"\n+      message spark_schema {\n+        REQUIRED INT32 key;\n+        REQUIRED BOOLEAN value;\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        let res = test_row_group_rows(\"nested_maps.snappy.parquet\", Some(schema));\n+        assert!(res.is_err());\n+        assert_eq!(\n+            res.unwrap_err(),\n+            general_err!(\"Root schema does not contain projection\")\n+        );\n+    }\n+\n+    #[test]\n+    #[should_panic(expected = \"Invalid map type\")]\n+    fn test_file_reader_rows_invalid_map_type() {\n+        let schema = \"\n+      message spark_schema {\n+        OPTIONAL group a (MAP) {\n+          REPEATED group key_value {\n+            REQUIRED BYTE_ARRAY key (UTF8);\n+            OPTIONAL group value (MAP) {\n+              REPEATED group key_value {\n+                REQUIRED INT32 key;\n+              }\n+            }\n+          }\n+        }\n+      }\n+    \";\n+        let schema = parse_message_type(&schema).unwrap();\n+        test_file_reader_rows(\"nested_maps.snappy.parquet\", Some(schema)).unwrap();\n+    }\n+\n+    #[test]\n+    fn test_tree_reader_handle_repeated_fields_with_no_annotation() {\n+        // Array field `phoneNumbers` does not contain LIST annotation.\n+        // We parse it as struct with `phone` repeated field as array.\n+        let rows = test_file_reader_rows(\"repeated_no_annotation.parquet\", None).unwrap();\n+        let expected_rows = vec![\n+            row![\n+                (\"id\".to_string(), Field::Int(1)),\n+                (\"phoneNumbers\".to_string(), Field::Null)\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Int(2)),\n+                (\"phoneNumbers\".to_string(), Field::Null)\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Int(3)),\n+                (\n+                    \"phoneNumbers\".to_string(),\n+                    group![(\"phone\".to_string(), list![])]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Int(4)),\n+                (\n+                    \"phoneNumbers\".to_string(),\n+                    group![(\n+                        \"phone\".to_string(),\n+                        list![group![\n+                            (\"number\".to_string(), Field::Long(5555555555)),\n+                            (\"kind\".to_string(), Field::Null)\n+                        ]]\n+                    )]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Int(5)),\n+                (\n+                    \"phoneNumbers\".to_string(),\n+                    group![(\n+                        \"phone\".to_string(),\n+                        list![group![\n+                            (\"number\".to_string(), Field::Long(1111111111)),\n+                            (\"kind\".to_string(), Field::Str(\"home\".to_string()))\n+                        ]]\n+                    )]\n+                )\n+            ],\n+            row![\n+                (\"id\".to_string(), Field::Int(6)),\n+                (\n+                    \"phoneNumbers\".to_string(),\n+                    group![(\n+                        \"phone\".to_string(),\n+                        list![\n+                            group![\n+                                (\"number\".to_string(), Field::Long(1111111111)),\n+                                (\"kind\".to_string(), Field::Str(\"home\".to_string()))\n+                            ],\n+                            group![\n+                                (\"number\".to_string(), Field::Long(2222222222)),\n+                                (\"kind\".to_string(), Field::Null)\n+                            ],\n+                            group![\n+                                (\"number\".to_string(), Field::Long(3333333333)),\n+                                (\"kind\".to_string(), Field::Str(\"mobile\".to_string()))\n+                            ]\n+                        ]\n+                    )]\n+                )\n+            ],\n+        ];\n+\n+        assert_eq!(rows, expected_rows);\n+    }\n+\n+    fn test_file_reader_rows(file_name: &str, schema: Option<Type>) -> Result<Vec<Row>> {\n+        let file = get_test_file(file_name);\n+        let file_reader: Box<FileReader> = Box::new(SerializedFileReader::new(file)?);\n+        let iter = file_reader.get_row_iter(schema)?;\n+        Ok(iter.collect())\n+    }\n+\n+    fn test_row_group_rows(file_name: &str, schema: Option<Type>) -> Result<Vec<Row>> {\n+        let file = get_test_file(file_name);\n+        let file_reader: Box<FileReader> = Box::new(SerializedFileReader::new(file)?);\n+        // Check the first row group only, because files will contain only single row group\n+        let row_group_reader = file_reader.get_row_group(0).unwrap();\n+        let iter = row_group_reader.get_row_iter(schema)?;\n+        Ok(iter.collect())\n+    }\n+}\ndiff --git a/rust/src/parquet/record/triplet.rs b/rust/src/parquet/record/triplet.rs\nnew file mode 100644\nindex 0000000000..fadcbbce9b\n--- /dev/null\n+++ b/rust/src/parquet/record/triplet.rs\n@@ -0,0 +1,561 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+use crate::parquet::basic::Type as PhysicalType;\n+use crate::parquet::column::reader::{get_typed_column_reader, ColumnReader, ColumnReaderImpl};\n+use crate::parquet::data_type::*;\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::record::api::Field;\n+use crate::parquet::schema::types::ColumnDescPtr;\n+\n+/// Macro to generate simple functions that cover all types of triplet iterator.\n+/// $func is a function of a typed triplet iterator and $token is a either {`ref`} or\n+/// {`ref`, `mut`}\n+macro_rules! triplet_enum_func {\n+  ($self:ident, $func:ident, $( $token:tt ),*) => ({\n+    match *$self {\n+      TripletIter::BoolTripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::Int32TripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::Int64TripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::Int96TripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::FloatTripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::DoubleTripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::ByteArrayTripletIter($($token)* typed) => typed.$func(),\n+      TripletIter::FixedLenByteArrayTripletIter($($token)* typed) => typed.$func()\n+    }\n+  });\n+}\n+\n+/// High level API wrapper on column reader.\n+/// Provides per-element access for each primitive column.\n+pub enum TripletIter {\n+    BoolTripletIter(TypedTripletIter<BoolType>),\n+    Int32TripletIter(TypedTripletIter<Int32Type>),\n+    Int64TripletIter(TypedTripletIter<Int64Type>),\n+    Int96TripletIter(TypedTripletIter<Int96Type>),\n+    FloatTripletIter(TypedTripletIter<FloatType>),\n+    DoubleTripletIter(TypedTripletIter<DoubleType>),\n+    ByteArrayTripletIter(TypedTripletIter<ByteArrayType>),\n+    FixedLenByteArrayTripletIter(TypedTripletIter<FixedLenByteArrayType>),\n+}\n+\n+impl TripletIter {\n+    /// Creates new triplet for column reader\n+    pub fn new(descr: ColumnDescPtr, reader: ColumnReader, batch_size: usize) -> Self {\n+        match descr.physical_type() {\n+            PhysicalType::BOOLEAN => {\n+                TripletIter::BoolTripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::INT32 => {\n+                TripletIter::Int32TripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::INT64 => {\n+                TripletIter::Int64TripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::INT96 => {\n+                TripletIter::Int96TripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::FLOAT => {\n+                TripletIter::FloatTripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::DOUBLE => {\n+                TripletIter::DoubleTripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::BYTE_ARRAY => {\n+                TripletIter::ByteArrayTripletIter(TypedTripletIter::new(descr, batch_size, reader))\n+            }\n+            PhysicalType::FIXED_LEN_BYTE_ARRAY => TripletIter::FixedLenByteArrayTripletIter(\n+                TypedTripletIter::new(descr, batch_size, reader),\n+            ),\n+        }\n+    }\n+\n+    /// Invokes underlying typed triplet iterator to buffer current value.\n+    /// Should be called once - either before `is_null` or `current_value`.\n+    #[inline]\n+    pub fn read_next(&mut self) -> Result<bool> {\n+        triplet_enum_func!(self, read_next, ref, mut)\n+    }\n+\n+    /// Provides check on values/levels left without invoking the underlying typed triplet\n+    /// iterator.\n+    /// Returns true if more values/levels exist, false otherwise.\n+    /// It is always in sync with `read_next` method.\n+    #[inline]\n+    pub fn has_next(&self) -> bool {\n+        triplet_enum_func!(self, has_next, ref)\n+    }\n+\n+    /// Returns current definition level for a leaf triplet iterator\n+    #[inline]\n+    pub fn current_def_level(&self) -> i16 {\n+        triplet_enum_func!(self, current_def_level, ref)\n+    }\n+\n+    /// Returns max definition level for a leaf triplet iterator\n+    #[inline]\n+    pub fn max_def_level(&self) -> i16 {\n+        triplet_enum_func!(self, max_def_level, ref)\n+    }\n+\n+    /// Returns current repetition level for a leaf triplet iterator\n+    #[inline]\n+    pub fn current_rep_level(&self) -> i16 {\n+        triplet_enum_func!(self, current_rep_level, ref)\n+    }\n+\n+    /// Returns max repetition level for a leaf triplet iterator\n+    #[inline]\n+    pub fn max_rep_level(&self) -> i16 {\n+        triplet_enum_func!(self, max_rep_level, ref)\n+    }\n+\n+    /// Returns true, if current value is null.\n+    /// Based on the fact that for non-null value current definition level\n+    /// equals to max definition level.\n+    #[inline]\n+    pub fn is_null(&self) -> bool {\n+        self.current_def_level() < self.max_def_level()\n+    }\n+\n+    /// Updates non-null value for current row.\n+    pub fn current_value(&self) -> Field {\n+        assert!(!self.is_null(), \"Value is null\");\n+        match *self {\n+            TripletIter::BoolTripletIter(ref typed) => {\n+                Field::convert_bool(typed.column_descr(), *typed.current_value())\n+            }\n+            TripletIter::Int32TripletIter(ref typed) => {\n+                Field::convert_int32(typed.column_descr(), *typed.current_value())\n+            }\n+            TripletIter::Int64TripletIter(ref typed) => {\n+                Field::convert_int64(typed.column_descr(), *typed.current_value())\n+            }\n+            TripletIter::Int96TripletIter(ref typed) => {\n+                Field::convert_int96(typed.column_descr(), typed.current_value().clone())\n+            }\n+            TripletIter::FloatTripletIter(ref typed) => {\n+                Field::convert_float(typed.column_descr(), *typed.current_value())\n+            }\n+            TripletIter::DoubleTripletIter(ref typed) => {\n+                Field::convert_double(typed.column_descr(), *typed.current_value())\n+            }\n+            TripletIter::ByteArrayTripletIter(ref typed) => {\n+                Field::convert_byte_array(typed.column_descr(), typed.current_value().clone())\n+            }\n+            TripletIter::FixedLenByteArrayTripletIter(ref typed) => {\n+                Field::convert_byte_array(typed.column_descr(), typed.current_value().clone())\n+            }\n+        }\n+    }\n+}\n+\n+/// Internal typed triplet iterator as a wrapper for column reader\n+/// (primitive leaf column), provides per-element access.\n+pub struct TypedTripletIter<T: DataType> {\n+    reader: ColumnReaderImpl<T>,\n+    column_descr: ColumnDescPtr,\n+    batch_size: usize,\n+    // type properties\n+    max_def_level: i16,\n+    max_rep_level: i16,\n+    // values and levels\n+    values: Vec<T::T>,\n+    def_levels: Option<Vec<i16>>,\n+    rep_levels: Option<Vec<i16>>,\n+    // current index for the triplet (value, def, rep)\n+    curr_triplet_index: usize,\n+    // how many triplets are left before we need to buffer\n+    triplets_left: usize,\n+    // helper flag to quickly check if we have more values/levels to read\n+    has_next: bool,\n+}\n+\n+impl<T: DataType> TypedTripletIter<T> {\n+    /// Creates new typed triplet iterator based on provided column reader.\n+    /// Use batch size to specify the amount of values to buffer from column reader.\n+    fn new(descr: ColumnDescPtr, batch_size: usize, column_reader: ColumnReader) -> Self {\n+        assert!(\n+            batch_size > 0,\n+            \"Expected positive batch size, found: {}\",\n+            batch_size\n+        );\n+\n+        let max_def_level = descr.max_def_level();\n+        let max_rep_level = descr.max_rep_level();\n+\n+        let def_levels = if max_def_level == 0 {\n+            None\n+        } else {\n+            Some(vec![0; batch_size])\n+        };\n+        let rep_levels = if max_rep_level == 0 {\n+            None\n+        } else {\n+            Some(vec![0; batch_size])\n+        };\n+\n+        Self {\n+            reader: get_typed_column_reader(column_reader),\n+            column_descr: descr,\n+            batch_size,\n+            max_def_level,\n+            max_rep_level,\n+            values: vec![T::T::default(); batch_size],\n+            def_levels,\n+            rep_levels,\n+            curr_triplet_index: 0,\n+            triplets_left: 0,\n+            has_next: false,\n+        }\n+    }\n+\n+    /// Returns column descriptor reference for the current typed triplet iterator.\n+    #[inline]\n+    pub fn column_descr(&self) -> &ColumnDescPtr {\n+        &self.column_descr\n+    }\n+\n+    /// Returns maximum definition level for the triplet iterator (leaf column).\n+    #[inline]\n+    fn max_def_level(&self) -> i16 {\n+        self.max_def_level\n+    }\n+\n+    /// Returns maximum repetition level for the triplet iterator (leaf column).\n+    #[inline]\n+    fn max_rep_level(&self) -> i16 {\n+        self.max_rep_level\n+    }\n+\n+    /// Returns current value.\n+    /// Method does not advance the iterator, therefore can be called multiple times.\n+    #[inline]\n+    fn current_value(&self) -> &T::T {\n+        assert!(\n+            self.current_def_level() == self.max_def_level(),\n+            \"Cannot extract value, max definition level: {}, current level: {}\",\n+            self.max_def_level(),\n+            self.current_def_level()\n+        );\n+        &self.values[self.curr_triplet_index]\n+    }\n+\n+    /// Returns current definition level.\n+    /// If field is required, then maximum definition level is returned.\n+    #[inline]\n+    fn current_def_level(&self) -> i16 {\n+        match self.def_levels {\n+            Some(ref vec) => vec[self.curr_triplet_index],\n+            None => self.max_def_level,\n+        }\n+    }\n+\n+    /// Returns current repetition level.\n+    /// If field is required, then maximum repetition level is returned.\n+    #[inline]\n+    fn current_rep_level(&self) -> i16 {\n+        match self.rep_levels {\n+            Some(ref vec) => vec[self.curr_triplet_index],\n+            None => self.max_rep_level,\n+        }\n+    }\n+\n+    /// Quick check if iterator has more values/levels to read.\n+    /// It is updated as a result of `read_next` method, so they are synchronized.\n+    #[inline]\n+    fn has_next(&self) -> bool {\n+        self.has_next\n+    }\n+\n+    /// Advances to the next triplet.\n+    /// Returns true, if there are more records to read, false there are no records left.\n+    fn read_next(&mut self) -> Result<bool> {\n+        self.curr_triplet_index += 1;\n+\n+        if self.curr_triplet_index >= self.triplets_left {\n+            let (values_read, levels_read) = {\n+                // Get slice of definition levels, if available\n+                let def_levels = match self.def_levels {\n+                    Some(ref mut vec) => Some(&mut vec[..]),\n+                    None => None,\n+                };\n+\n+                // Get slice of repetition levels, if available\n+                let rep_levels = match self.rep_levels {\n+                    Some(ref mut vec) => Some(&mut vec[..]),\n+                    None => None,\n+                };\n+\n+                // Buffer triplets\n+                self.reader\n+                    .read_batch(self.batch_size, def_levels, rep_levels, &mut self.values)?\n+            };\n+\n+            // No more values or levels to read\n+            if values_read == 0 && levels_read == 0 {\n+                self.has_next = false;\n+                return Ok(false);\n+            }\n+\n+            // We never read values more than levels\n+            if levels_read == 0 || values_read == levels_read {\n+                // There are no definition levels to read, column is required\n+                // or definition levels match values, so it does not require spacing\n+                self.curr_triplet_index = 0;\n+                self.triplets_left = values_read;\n+            } else if values_read < levels_read {\n+                // Add spacing for triplets.\n+                // The idea is setting values for positions in def_levels when current definition\n+                // level equals to maximum definition level. Values and levels are guaranteed to\n+                // line up, because of the column reader method.\n+\n+                // Note: if values_read == 0, then spacing will not be triggered\n+                let mut idx = values_read;\n+                let def_levels = self.def_levels.as_ref().unwrap();\n+                for i in 0..levels_read {\n+                    if def_levels[levels_read - i - 1] == self.max_def_level {\n+                        idx -= 1; // This is done to avoid usize becoming a negative value\n+                        self.values.swap(levels_read - i - 1, idx);\n+                    }\n+                }\n+                self.curr_triplet_index = 0;\n+                self.triplets_left = levels_read;\n+            } else {\n+                return Err(general_err!(\n+                    \"Spacing of values/levels is wrong, values_read: {}, levels_read: {}\",\n+                    values_read,\n+                    levels_read\n+                ));\n+            }\n+        }\n+\n+        self.has_next = true;\n+        Ok(true)\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use crate::parquet::file::reader::{FileReader, SerializedFileReader};\n+    use crate::parquet::schema::types::ColumnPath;\n+    use crate::parquet::util::test_common::get_test_file;\n+\n+    #[test]\n+    #[should_panic(expected = \"Expected positive batch size, found: 0\")]\n+    fn test_triplet_zero_batch_size() {\n+        let column_path = ColumnPath::from(vec![\"b_struct\".to_string(), \"b_c_int\".to_string()]);\n+        test_column_in_file(\n+            \"nulls.snappy.parquet\",\n+            0,\n+            &column_path,\n+            &vec![],\n+            &vec![],\n+            &vec![],\n+        );\n+    }\n+\n+    #[test]\n+    fn test_triplet_null_column() {\n+        let path = vec![\"b_struct\", \"b_c_int\"];\n+        let values = vec![];\n+        let def_levels = vec![1, 1, 1, 1, 1, 1, 1, 1];\n+        let rep_levels = vec![0, 0, 0, 0, 0, 0, 0, 0];\n+        test_triplet_iter(\n+            \"nulls.snappy.parquet\",\n+            path,\n+            &values,\n+            &def_levels,\n+            &rep_levels,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_triplet_required_column() {\n+        let path = vec![\"ID\"];\n+        let values = vec![Field::Long(8)];\n+        let def_levels = vec![0];\n+        let rep_levels = vec![0];\n+        test_triplet_iter(\n+            \"nonnullable.impala.parquet\",\n+            path,\n+            &values,\n+            &def_levels,\n+            &rep_levels,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_triplet_optional_column() {\n+        let path = vec![\"nested_struct\", \"A\"];\n+        let values = vec![Field::Int(1), Field::Int(7)];\n+        let def_levels = vec![2, 1, 1, 1, 1, 0, 2];\n+        let rep_levels = vec![0, 0, 0, 0, 0, 0, 0];\n+        test_triplet_iter(\n+            \"nullable.impala.parquet\",\n+            path,\n+            &values,\n+            &def_levels,\n+            &rep_levels,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_triplet_optional_list_column() {\n+        let path = vec![\"a\", \"list\", \"element\", \"list\", \"element\", \"list\", \"element\"];\n+        let values = vec![\n+            Field::Str(\"a\".to_string()),\n+            Field::Str(\"b\".to_string()),\n+            Field::Str(\"c\".to_string()),\n+            Field::Str(\"d\".to_string()),\n+            Field::Str(\"a\".to_string()),\n+            Field::Str(\"b\".to_string()),\n+            Field::Str(\"c\".to_string()),\n+            Field::Str(\"d\".to_string()),\n+            Field::Str(\"e\".to_string()),\n+            Field::Str(\"a\".to_string()),\n+            Field::Str(\"b\".to_string()),\n+            Field::Str(\"c\".to_string()),\n+            Field::Str(\"d\".to_string()),\n+            Field::Str(\"e\".to_string()),\n+            Field::Str(\"f\".to_string()),\n+        ];\n+        let def_levels = vec![7, 7, 7, 4, 7, 7, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 4, 7];\n+        let rep_levels = vec![0, 3, 2, 1, 2, 0, 3, 2, 3, 1, 2, 0, 3, 2, 3, 2, 1, 2];\n+        test_triplet_iter(\n+            \"nested_lists.snappy.parquet\",\n+            path,\n+            &values,\n+            &def_levels,\n+            &rep_levels,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_triplet_optional_map_column() {\n+        let path = vec![\"a\", \"key_value\", \"value\", \"key_value\", \"key\"];\n+        let values = vec![\n+            Field::Int(1),\n+            Field::Int(2),\n+            Field::Int(1),\n+            Field::Int(1),\n+            Field::Int(3),\n+            Field::Int(4),\n+            Field::Int(5),\n+        ];\n+        let def_levels = vec![4, 4, 4, 2, 3, 4, 4, 4, 4];\n+        let rep_levels = vec![0, 2, 0, 0, 0, 0, 0, 2, 2];\n+        test_triplet_iter(\n+            \"nested_maps.snappy.parquet\",\n+            path,\n+            &values,\n+            &def_levels,\n+            &rep_levels,\n+        );\n+    }\n+\n+    // Check triplet iterator across different batch sizes\n+    fn test_triplet_iter(\n+        file_name: &str,\n+        column_path: Vec<&str>,\n+        expected_values: &[Field],\n+        expected_def_levels: &[i16],\n+        expected_rep_levels: &[i16],\n+    ) {\n+        // Convert path into column path\n+        let path: Vec<String> = column_path.iter().map(|x| x.to_string()).collect();\n+        let column_path = ColumnPath::from(path);\n+\n+        let batch_sizes = vec![1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 128, 256];\n+        for batch_size in batch_sizes {\n+            test_column_in_file(\n+                file_name,\n+                batch_size,\n+                &column_path,\n+                expected_values,\n+                expected_def_levels,\n+                expected_rep_levels,\n+            );\n+        }\n+    }\n+\n+    // Check values of a selectd column in a file\n+    fn test_column_in_file(\n+        file_name: &str,\n+        batch_size: usize,\n+        column_path: &ColumnPath,\n+        expected_values: &[Field],\n+        expected_def_levels: &[i16],\n+        expected_rep_levels: &[i16],\n+    ) {\n+        let file = get_test_file(file_name);\n+        let file_reader = SerializedFileReader::new(file).unwrap();\n+        // Get schema descriptor\n+        let file_metadata = file_reader.metadata().file_metadata();\n+        let schema = file_metadata.schema_descr();\n+        // Get first row group\n+        let row_group_reader = file_reader.get_row_group(0).unwrap();\n+\n+        for i in 0..schema.num_columns() {\n+            let descr = schema.column(i);\n+            if descr.path() == column_path {\n+                let reader = row_group_reader.get_column_reader(i).unwrap();\n+                test_triplet_column(\n+                    descr,\n+                    reader,\n+                    batch_size,\n+                    expected_values,\n+                    expected_def_levels,\n+                    expected_rep_levels,\n+                );\n+            }\n+        }\n+    }\n+\n+    // Check values for individual triplet iterator\n+    fn test_triplet_column(\n+        descr: ColumnDescPtr,\n+        reader: ColumnReader,\n+        batch_size: usize,\n+        expected_values: &[Field],\n+        expected_def_levels: &[i16],\n+        expected_rep_levels: &[i16],\n+    ) {\n+        let mut iter = TripletIter::new(descr.clone(), reader, batch_size);\n+        let mut values: Vec<Field> = Vec::new();\n+        let mut def_levels: Vec<i16> = Vec::new();\n+        let mut rep_levels: Vec<i16> = Vec::new();\n+\n+        assert_eq!(iter.max_def_level(), descr.max_def_level());\n+        assert_eq!(iter.max_rep_level(), descr.max_rep_level());\n+\n+        while let Ok(true) = iter.read_next() {\n+            assert!(iter.has_next());\n+            if !iter.is_null() {\n+                values.push(iter.current_value());\n+            }\n+            def_levels.push(iter.current_def_level());\n+            rep_levels.push(iter.current_rep_level());\n+        }\n+\n+        assert_eq!(values, expected_values);\n+        assert_eq!(def_levels, expected_def_levels);\n+        assert_eq!(rep_levels, expected_rep_levels);\n+    }\n+}\ndiff --git a/rust/src/parquet/schema/mod.rs b/rust/src/parquet/schema/mod.rs\nnew file mode 100644\nindex 0000000000..5319504964\n--- /dev/null\n+++ b/rust/src/parquet/schema/mod.rs\n@@ -0,0 +1,66 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet schema definitions and methods to print and parse schema.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use arrow::parquet::{\n+//!     basic::{LogicalType, Repetition, Type as PhysicalType},\n+//!     schema::{parser, printer, types::Type},\n+//! };\n+//! use std::rc::Rc;\n+//!\n+//! // Create the following schema:\n+//! //\n+//! // message schema {\n+//! //   OPTIONAL BYTE_ARRAY a (UTF8);\n+//! //   REQUIRED INT32 b;\n+//! // }\n+//!\n+//! let field_a = Type::primitive_type_builder(\"a\", PhysicalType::BYTE_ARRAY)\n+//!     .with_logical_type(LogicalType::UTF8)\n+//!     .with_repetition(Repetition::OPTIONAL)\n+//!     .build()\n+//!     .unwrap();\n+//!\n+//! let field_b = Type::primitive_type_builder(\"b\", PhysicalType::INT32)\n+//!     .with_repetition(Repetition::REQUIRED)\n+//!     .build()\n+//!     .unwrap();\n+//!\n+//! let schema = Type::group_type_builder(\"schema\")\n+//!     .with_fields(&mut vec![Rc::new(field_a), Rc::new(field_b)])\n+//!     .build()\n+//!     .unwrap();\n+//!\n+//! let mut buf = Vec::new();\n+//!\n+//! // Print schema into buffer\n+//! printer::print_schema(&mut buf, &schema);\n+//!\n+//! // Parse schema from the string\n+//! let string_schema = String::from_utf8(buf).unwrap();\n+//! let parsed_schema = parser::parse_message_type(&string_schema).unwrap();\n+//!\n+//! assert_eq!(schema, parsed_schema);\n+//! ```\n+\n+pub mod parser;\n+pub mod printer;\n+pub mod types;\ndiff --git a/rust/src/parquet/schema/parser.rs b/rust/src/parquet/schema/parser.rs\nnew file mode 100644\nindex 0000000000..2890c84a75\n--- /dev/null\n+++ b/rust/src/parquet/schema/parser.rs\n@@ -0,0 +1,764 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet schema parser.\n+//! Provides methods to parse and validate string message type into Parquet\n+//! [`Type`](`::schema::types::Type`).\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use arrow::parquet::schema::parser::parse_message_type;\n+//!\n+//! let message_type = \"\n+//!   message spark_schema {\n+//!     OPTIONAL BYTE_ARRAY a (UTF8);\n+//!     REQUIRED INT32 b;\n+//!     REQUIRED DOUBLE c;\n+//!     REQUIRED BOOLEAN d;\n+//!     OPTIONAL group e (LIST) {\n+//!       REPEATED group list {\n+//!         REQUIRED INT32 element;\n+//!       }\n+//!     }\n+//!   }\n+//! \";\n+//!\n+//! let schema = parse_message_type(message_type).expect(\"Expected valid schema\");\n+//! println!(\"{:?}\", schema);\n+//! ```\n+\n+use std::rc::Rc;\n+\n+use crate::parquet::basic::{LogicalType, Repetition, Type as PhysicalType};\n+use crate::parquet::errors::{ParquetError, Result};\n+use crate::parquet::schema::types::{Type, TypePtr};\n+\n+/// Parses message type as string into a Parquet [`Type`](`::schema::types::Type`) which,\n+/// for example, could be used to extract individual columns. Returns Parquet general\n+/// error when parsing or validation fails.\n+pub fn parse_message_type<'a>(message_type: &'a str) -> Result<Type> {\n+    let mut parser = Parser {\n+        tokenizer: &mut Tokenizer::from_str(message_type),\n+    };\n+    parser.parse_message_type()\n+}\n+\n+/// Tokenizer to split message type string into tokens that are separated using characters\n+/// defined in `is_schema_delim` method. Tokenizer also preserves delimiters as tokens.\n+/// Tokenizer provides Iterator interface to process tokens; it also allows to step back\n+/// to reprocess previous tokens.\n+struct Tokenizer<'a> {\n+    // List of all tokens for a string\n+    tokens: Vec<&'a str>,\n+    // Current index of vector\n+    index: usize,\n+}\n+\n+impl<'a> Tokenizer<'a> {\n+    // Create tokenizer from message type string\n+    pub fn from_str(string: &'a str) -> Self {\n+        let vec = string\n+            .split_whitespace()\n+            .flat_map(|t| Self::split_token(t))\n+            .collect();\n+        Tokenizer {\n+            tokens: vec,\n+            index: 0,\n+        }\n+    }\n+\n+    // List of all special characters in schema\n+    fn is_schema_delim(c: char) -> bool {\n+        c == ';' || c == '{' || c == '}' || c == '(' || c == ')' || c == '=' || c == ','\n+    }\n+\n+    /// Splits string into tokens; input string can already be token or can contain\n+    /// delimiters, e.g. required\" -> Vec(\"required\") and\n+    /// \"(UTF8);\" -> Vec(\"(\", \"UTF8\", \")\", \";\")\n+    fn split_token(string: &str) -> Vec<&str> {\n+        let mut buffer: Vec<&str> = Vec::new();\n+        let mut tail = string;\n+        while let Some(index) = tail.find(Self::is_schema_delim) {\n+            let (h, t) = tail.split_at(index);\n+            if !h.is_empty() {\n+                buffer.push(h);\n+            }\n+            buffer.push(&t[0..1]);\n+            tail = &t[1..];\n+        }\n+        if !tail.is_empty() {\n+            buffer.push(tail);\n+        }\n+        buffer\n+    }\n+\n+    // Move pointer to a previous element\n+    fn backtrack(&mut self) {\n+        self.index -= 1;\n+    }\n+}\n+\n+impl<'a> Iterator for Tokenizer<'a> {\n+    type Item = &'a str;\n+\n+    fn next(&mut self) -> Option<&'a str> {\n+        if self.index < self.tokens.len() {\n+            self.index += 1;\n+            Some(self.tokens[self.index - 1])\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+/// Internal Schema parser.\n+/// Traverses message type using tokenizer and parses each group/primitive type\n+/// recursively.\n+struct Parser<'a> {\n+    tokenizer: &'a mut Tokenizer<'a>,\n+}\n+\n+// Utility function to assert token on validity.\n+fn assert_token(token: Option<&str>, expected: &str) -> Result<()> {\n+    match token {\n+        Some(value) if value == expected => Ok(()),\n+        Some(other) => Err(general_err!(\n+            \"Expected '{}', found token '{}'\",\n+            expected,\n+            other\n+        )),\n+        None => Err(general_err!(\n+            \"Expected '{}', but no token found (None)\",\n+            expected\n+        )),\n+    }\n+}\n+\n+// Utility function to parse i32 or return general error.\n+fn parse_i32(value: Option<&str>, not_found_msg: &str, parse_fail_msg: &str) -> Result<i32> {\n+    value\n+        .ok_or(general_err!(not_found_msg))\n+        .and_then(|v| v.parse::<i32>().map_err(|_| general_err!(parse_fail_msg)))\n+}\n+\n+impl<'a> Parser<'a> {\n+    // Entry function to parse message type, uses internal tokenizer.\n+    fn parse_message_type(&mut self) -> Result<Type> {\n+        // Check that message type starts with \"message\".\n+        match self.tokenizer.next() {\n+            Some(\"message\") => {\n+                let name = self\n+                    .tokenizer\n+                    .next()\n+                    .ok_or(general_err!(\"Expected name, found None\"))?;\n+                let mut fields = self.parse_child_types()?;\n+                Type::group_type_builder(name)\n+                    .with_fields(&mut fields)\n+                    .build()\n+            }\n+            _ => Err(general_err!(\"Message type does not start with 'message'\")),\n+        }\n+    }\n+\n+    // Parses child types for a current group type.\n+    // This is only invoked on root and group types.\n+    fn parse_child_types(&mut self) -> Result<Vec<TypePtr>> {\n+        assert_token(self.tokenizer.next(), \"{\")?;\n+        let mut vec = Vec::new();\n+        while let Some(value) = self.tokenizer.next() {\n+            if value == \"}\" {\n+                break;\n+            } else {\n+                self.tokenizer.backtrack();\n+                vec.push(Rc::new(self.add_type()?));\n+            }\n+        }\n+        Ok(vec)\n+    }\n+\n+    fn add_type(&mut self) -> Result<Type> {\n+        // Parse repetition\n+        let repetition = self\n+            .tokenizer\n+            .next()\n+            .ok_or(general_err!(\"Expected repetition, found None\"))\n+            .and_then(|v| v.to_uppercase().parse::<Repetition>())?;\n+\n+        match self.tokenizer.next() {\n+            Some(group) if group.to_uppercase() == \"GROUP\" => self.add_group_type(Some(repetition)),\n+            Some(type_string) => {\n+                let physical_type = type_string.to_uppercase().parse::<PhysicalType>()?;\n+                self.add_primitive_type(repetition, physical_type)\n+            }\n+            None => Err(general_err!(\"Invalid type, could not extract next token\")),\n+        }\n+    }\n+\n+    fn add_group_type(&mut self, repetition: Option<Repetition>) -> Result<Type> {\n+        // Parse name of the group type\n+        let name = self\n+            .tokenizer\n+            .next()\n+            .ok_or(general_err!(\"Expected name, found None\"))?;\n+\n+        // Parse logical type if exists\n+        let logical_type = if let Some(\"(\") = self.tokenizer.next() {\n+            let tpe = self\n+                .tokenizer\n+                .next()\n+                .ok_or(general_err!(\"Expected logical type, found None\"))\n+                .and_then(|v| v.to_uppercase().parse::<LogicalType>())?;\n+            assert_token(self.tokenizer.next(), \")\")?;\n+            tpe\n+        } else {\n+            self.tokenizer.backtrack();\n+            LogicalType::NONE\n+        };\n+\n+        // Parse optional id\n+        let id = if let Some(\"=\") = self.tokenizer.next() {\n+            self.tokenizer.next().and_then(|v| v.parse::<i32>().ok())\n+        } else {\n+            self.tokenizer.backtrack();\n+            None\n+        };\n+\n+        let mut fields = self.parse_child_types()?;\n+        let mut builder = Type::group_type_builder(name)\n+            .with_logical_type(logical_type)\n+            .with_fields(&mut fields);\n+        if let Some(rep) = repetition {\n+            builder = builder.with_repetition(rep);\n+        }\n+        if let Some(id) = id {\n+            builder = builder.with_id(id);\n+        }\n+        builder.build()\n+    }\n+\n+    fn add_primitive_type(\n+        &mut self,\n+        repetition: Repetition,\n+        physical_type: PhysicalType,\n+    ) -> Result<Type> {\n+        // Read type length if the type is FIXED_LEN_BYTE_ARRAY.\n+        let mut length: i32 = -1;\n+        if physical_type == PhysicalType::FIXED_LEN_BYTE_ARRAY {\n+            assert_token(self.tokenizer.next(), \"(\")?;\n+            length = parse_i32(\n+                self.tokenizer.next(),\n+                \"Expected length for FIXED_LEN_BYTE_ARRAY, found None\",\n+                \"Failed to parse length for FIXED_LEN_BYTE_ARRAY\",\n+            )?;\n+            assert_token(self.tokenizer.next(), \")\")?;\n+        }\n+\n+        // Parse name of the primitive type\n+        let name = self\n+            .tokenizer\n+            .next()\n+            .ok_or(general_err!(\"Expected name, found None\"))?;\n+\n+        // Parse logical type\n+        let (logical_type, precision, scale) = if let Some(\"(\") = self.tokenizer.next() {\n+            let tpe = self\n+                .tokenizer\n+                .next()\n+                .ok_or(general_err!(\"Expected logical type, found None\"))\n+                .and_then(|v| v.to_uppercase().parse::<LogicalType>())?;\n+\n+            // Parse precision and scale for decimals\n+            let mut precision: i32 = -1;\n+            let mut scale: i32 = -1;\n+\n+            if tpe == LogicalType::DECIMAL {\n+                if let Some(\"(\") = self.tokenizer.next() {\n+                    // Parse precision\n+                    precision = parse_i32(\n+                        self.tokenizer.next(),\n+                        \"Expected precision, found None\",\n+                        \"Failed to parse precision for DECIMAL type\",\n+                    )?;\n+\n+                    // Parse scale\n+                    scale = if let Some(\",\") = self.tokenizer.next() {\n+                        parse_i32(\n+                            self.tokenizer.next(),\n+                            \"Expected scale, found None\",\n+                            \"Failed to parse scale for DECIMAL type\",\n+                        )?\n+                    } else {\n+                        // Scale is not provided, set it to 0.\n+                        self.tokenizer.backtrack();\n+                        0\n+                    };\n+\n+                    assert_token(self.tokenizer.next(), \")\")?;\n+                } else {\n+                    self.tokenizer.backtrack();\n+                }\n+            }\n+\n+            assert_token(self.tokenizer.next(), \")\")?;\n+            (tpe, precision, scale)\n+        } else {\n+            self.tokenizer.backtrack();\n+            (LogicalType::NONE, -1, -1)\n+        };\n+\n+        // Parse optional id\n+        let id = if let Some(\"=\") = self.tokenizer.next() {\n+            self.tokenizer.next().and_then(|v| v.parse::<i32>().ok())\n+        } else {\n+            self.tokenizer.backtrack();\n+            None\n+        };\n+        assert_token(self.tokenizer.next(), \";\")?;\n+\n+        let mut builder = Type::primitive_type_builder(name, physical_type)\n+            .with_repetition(repetition)\n+            .with_logical_type(logical_type)\n+            .with_length(length)\n+            .with_precision(precision)\n+            .with_scale(scale);\n+        if let Some(id) = id {\n+            builder = builder.with_id(id);\n+        }\n+        Ok(builder.build()?)\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn test_tokenize_empty_string() {\n+        assert_eq!(Tokenizer::from_str(\"\").next(), None);\n+    }\n+\n+    #[test]\n+    fn test_tokenize_delimiters() {\n+        let mut iter = Tokenizer::from_str(\",;{}()=\");\n+        assert_eq!(iter.next(), Some(\",\"));\n+        assert_eq!(iter.next(), Some(\";\"));\n+        assert_eq!(iter.next(), Some(\"{\"));\n+        assert_eq!(iter.next(), Some(\"}\"));\n+        assert_eq!(iter.next(), Some(\"(\"));\n+        assert_eq!(iter.next(), Some(\")\"));\n+        assert_eq!(iter.next(), Some(\"=\"));\n+        assert_eq!(iter.next(), None);\n+    }\n+\n+    #[test]\n+    fn test_tokenize_delimiters_with_whitespaces() {\n+        let mut iter = Tokenizer::from_str(\" , ; { } ( ) = \");\n+        assert_eq!(iter.next(), Some(\",\"));\n+        assert_eq!(iter.next(), Some(\";\"));\n+        assert_eq!(iter.next(), Some(\"{\"));\n+        assert_eq!(iter.next(), Some(\"}\"));\n+        assert_eq!(iter.next(), Some(\"(\"));\n+        assert_eq!(iter.next(), Some(\")\"));\n+        assert_eq!(iter.next(), Some(\"=\"));\n+        assert_eq!(iter.next(), None);\n+    }\n+\n+    #[test]\n+    fn test_tokenize_words() {\n+        let mut iter = Tokenizer::from_str(\"abc def ghi jkl mno\");\n+        assert_eq!(iter.next(), Some(\"abc\"));\n+        assert_eq!(iter.next(), Some(\"def\"));\n+        assert_eq!(iter.next(), Some(\"ghi\"));\n+        assert_eq!(iter.next(), Some(\"jkl\"));\n+        assert_eq!(iter.next(), Some(\"mno\"));\n+        assert_eq!(iter.next(), None);\n+    }\n+\n+    #[test]\n+    fn test_tokenize_backtrack() {\n+        let mut iter = Tokenizer::from_str(\"abc;\");\n+        assert_eq!(iter.next(), Some(\"abc\"));\n+        assert_eq!(iter.next(), Some(\";\"));\n+        iter.backtrack();\n+        assert_eq!(iter.next(), Some(\";\"));\n+        assert_eq!(iter.next(), None);\n+    }\n+\n+    #[test]\n+    fn test_tokenize_message_type() {\n+        let schema = \"\n+    message schema {\n+      required int32 a;\n+      optional binary c (UTF8);\n+      required group d {\n+        required int32 a;\n+        optional binary c (UTF8);\n+      }\n+      required group e (LIST) {\n+        repeated group list {\n+          required int32 element;\n+        }\n+      }\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let mut res = Vec::new();\n+        while let Some(token) = iter.next() {\n+            res.push(token);\n+        }\n+        assert_eq!(\n+            res,\n+            vec![\n+                \"message\", \"schema\", \"{\", \"required\", \"int32\", \"a\", \";\", \"optional\", \"binary\", \"c\",\n+                \"(\", \"UTF8\", \")\", \";\", \"required\", \"group\", \"d\", \"{\", \"required\", \"int32\", \"a\",\n+                \";\", \"optional\", \"binary\", \"c\", \"(\", \"UTF8\", \")\", \";\", \"}\", \"required\", \"group\",\n+                \"e\", \"(\", \"LIST\", \")\", \"{\", \"repeated\", \"group\", \"list\", \"{\", \"required\", \"int32\",\n+                \"element\", \";\", \"}\", \"}\", \"}\"\n+            ]\n+        );\n+    }\n+\n+    #[test]\n+    fn test_assert_token() {\n+        assert!(assert_token(Some(\"a\"), \"a\").is_ok());\n+        assert!(assert_token(Some(\"a\"), \"b\").is_err());\n+        assert!(assert_token(None, \"b\").is_err());\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_invalid() {\n+        let mut iter = Tokenizer::from_str(\"test\");\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().to_string(),\n+            \"Parquet error: Message type does not start with 'message'\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_no_name() {\n+        let mut iter = Tokenizer::from_str(\"message\");\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+        assert_eq!(\n+            result.unwrap_err().to_string(),\n+            \"Parquet error: Expected name, found None\"\n+        );\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_fixed_byte_array() {\n+        let schema = \"\n+    message schema {\n+      REQUIRED FIXED_LEN_BYTE_ARRAY col;\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+\n+        let schema = \"\n+    message schema {\n+      REQUIRED FIXED_LEN_BYTE_ARRAY(16) col;\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_ok());\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_decimal() {\n+        // It is okay for decimal to omit precision and scale with right syntax.\n+        // Here we test wrong syntax of decimal type\n+\n+        // Invalid decimal syntax\n+        let schema = \"\n+    message root {\n+      optional int32 f1 (DECIMAL();\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+\n+        // Invalid decimal, need precision and scale\n+        let schema = \"\n+    message root {\n+      optional int32 f1 (DECIMAL());\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+\n+        // Invalid decimal because of `,` - has precision, needs scale\n+        let schema = \"\n+    message root {\n+      optional int32 f1 (DECIMAL(8,));\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+\n+        // Invalid decimal because, we always require either precision or scale to be\n+        // specified as part of logical type\n+        let schema = \"\n+    message root {\n+      optional int32 f3 (DECIMAL);\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_err());\n+\n+        // Valid decimal (precision, scale)\n+        let schema = \"\n+    message root {\n+      optional int32 f1 (DECIMAL(8, 3));\n+      optional int32 f2 (DECIMAL(8));\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let result = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type();\n+        assert!(result.is_ok());\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_compare_1() {\n+        let schema = \"\n+    message root {\n+      optional fixed_len_byte_array(5) f1 (DECIMAL(9, 3));\n+      optional fixed_len_byte_array (16) f2 (DECIMAL (38, 18));\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let message = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type()\n+        .unwrap();\n+\n+        let expected = Type::group_type_builder(\"root\")\n+            .with_fields(&mut vec![\n+                Rc::new(\n+                    Type::primitive_type_builder(\"f1\", PhysicalType::FIXED_LEN_BYTE_ARRAY)\n+                        .with_logical_type(LogicalType::DECIMAL)\n+                        .with_length(5)\n+                        .with_precision(9)\n+                        .with_scale(3)\n+                        .build()\n+                        .unwrap(),\n+                ),\n+                Rc::new(\n+                    Type::primitive_type_builder(\"f2\", PhysicalType::FIXED_LEN_BYTE_ARRAY)\n+                        .with_logical_type(LogicalType::DECIMAL)\n+                        .with_length(16)\n+                        .with_precision(38)\n+                        .with_scale(18)\n+                        .build()\n+                        .unwrap(),\n+                ),\n+            ])\n+            .build()\n+            .unwrap();\n+\n+        assert_eq!(message, expected);\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_compare_2() {\n+        let schema = \"\n+    message root {\n+      required group a0 {\n+        optional group a1 (LIST) {\n+          repeated binary a2 (UTF8);\n+        }\n+\n+        optional group b1 (LIST) {\n+          repeated group b2 {\n+            optional int32 b3;\n+            optional double b4;\n+          }\n+        }\n+      }\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let message = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type()\n+        .unwrap();\n+\n+        let expected = Type::group_type_builder(\"root\")\n+            .with_fields(&mut vec![Rc::new(\n+                Type::group_type_builder(\"a0\")\n+                    .with_repetition(Repetition::REQUIRED)\n+                    .with_fields(&mut vec![\n+                        Rc::new(\n+                            Type::group_type_builder(\"a1\")\n+                                .with_repetition(Repetition::OPTIONAL)\n+                                .with_logical_type(LogicalType::LIST)\n+                                .with_fields(&mut vec![Rc::new(\n+                                    Type::primitive_type_builder(\"a2\", PhysicalType::BYTE_ARRAY)\n+                                        .with_repetition(Repetition::REPEATED)\n+                                        .with_logical_type(LogicalType::UTF8)\n+                                        .build()\n+                                        .unwrap(),\n+                                )])\n+                                .build()\n+                                .unwrap(),\n+                        ),\n+                        Rc::new(\n+                            Type::group_type_builder(\"b1\")\n+                                .with_repetition(Repetition::OPTIONAL)\n+                                .with_logical_type(LogicalType::LIST)\n+                                .with_fields(&mut vec![Rc::new(\n+                                    Type::group_type_builder(\"b2\")\n+                                        .with_repetition(Repetition::REPEATED)\n+                                        .with_fields(&mut vec![\n+                                            Rc::new(\n+                                                Type::primitive_type_builder(\n+                                                    \"b3\",\n+                                                    PhysicalType::INT32,\n+                                                )\n+                                                .build()\n+                                                .unwrap(),\n+                                            ),\n+                                            Rc::new(\n+                                                Type::primitive_type_builder(\n+                                                    \"b4\",\n+                                                    PhysicalType::DOUBLE,\n+                                                )\n+                                                .build()\n+                                                .unwrap(),\n+                                            ),\n+                                        ])\n+                                        .build()\n+                                        .unwrap(),\n+                                )])\n+                                .build()\n+                                .unwrap(),\n+                        ),\n+                    ])\n+                    .build()\n+                    .unwrap(),\n+            )])\n+            .build()\n+            .unwrap();\n+\n+        assert_eq!(message, expected);\n+    }\n+\n+    #[test]\n+    fn test_parse_message_type_compare_3() {\n+        let schema = \"\n+    message root {\n+      required int32 _1 (INT_8);\n+      required int32 _2 (INT_16);\n+      required float _3;\n+      required double _4;\n+      optional int32 _5 (DATE);\n+      optional binary _6 (UTF8);\n+    }\n+    \";\n+        let mut iter = Tokenizer::from_str(schema);\n+        let message = Parser {\n+            tokenizer: &mut iter,\n+        }\n+        .parse_message_type()\n+        .unwrap();\n+\n+        let mut fields = vec![\n+            Rc::new(\n+                Type::primitive_type_builder(\"_1\", PhysicalType::INT32)\n+                    .with_repetition(Repetition::REQUIRED)\n+                    .with_logical_type(LogicalType::INT_8)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                Type::primitive_type_builder(\"_2\", PhysicalType::INT32)\n+                    .with_repetition(Repetition::REQUIRED)\n+                    .with_logical_type(LogicalType::INT_16)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                Type::primitive_type_builder(\"_3\", PhysicalType::FLOAT)\n+                    .with_repetition(Repetition::REQUIRED)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                Type::primitive_type_builder(\"_4\", PhysicalType::DOUBLE)\n+                    .with_repetition(Repetition::REQUIRED)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                Type::primitive_type_builder(\"_5\", PhysicalType::INT32)\n+                    .with_logical_type(LogicalType::DATE)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+            Rc::new(\n+                Type::primitive_type_builder(\"_6\", PhysicalType::BYTE_ARRAY)\n+                    .with_logical_type(LogicalType::UTF8)\n+                    .build()\n+                    .unwrap(),\n+            ),\n+        ];\n+\n+        let expected = Type::group_type_builder(\"root\")\n+            .with_fields(&mut fields)\n+            .build()\n+            .unwrap();\n+        assert_eq!(message, expected);\n+    }\n+}\ndiff --git a/rust/src/parquet/schema/printer.rs b/rust/src/parquet/schema/printer.rs\nnew file mode 100644\nindex 0000000000..d61f116eb9\n--- /dev/null\n+++ b/rust/src/parquet/schema/printer.rs\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Parquet schema printer.\n+//! Provides methods to print Parquet file schema and list file metadata.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use arrow::parquet::{\n+//!     file::reader::{FileReader, SerializedFileReader},\n+//!     schema::printer::{print_file_metadata, print_parquet_metadata, print_schema},\n+//! };\n+//! use std::{fs::File, path::Path};\n+//!\n+//! // Open a file\n+//! let path = Path::new(\"test.parquet\");\n+//! if let Ok(file) = File::open(&path) {\n+//!     let reader = SerializedFileReader::new(file).unwrap();\n+//!     let parquet_metadata = reader.metadata();\n+//!\n+//!     print_parquet_metadata(&mut std::io::stdout(), &parquet_metadata);\n+//!     print_file_metadata(&mut std::io::stdout(), &parquet_metadata.file_metadata());\n+//!\n+//!     print_schema(\n+//!         &mut std::io::stdout(),\n+//!         &parquet_metadata.file_metadata().schema(),\n+//!     );\n+//! }\n+//! ```\n+\n+use std::{fmt, io};\n+\n+use crate::parquet::basic::{LogicalType, Type as PhysicalType};\n+use crate::parquet::file::metadata::{\n+    ColumnChunkMetaData, FileMetaData, ParquetMetaData, RowGroupMetaData,\n+};\n+use crate::parquet::schema::types::Type;\n+\n+/// Prints Parquet metadata [`ParquetMetaData`](`::file::metadata::ParquetMetaData`)\n+/// information.\n+#[allow(unused_must_use)]\n+pub fn print_parquet_metadata(out: &mut io::Write, metadata: &ParquetMetaData) {\n+    print_file_metadata(out, &metadata.file_metadata());\n+    writeln!(out, \"\");\n+    writeln!(out, \"\");\n+    writeln!(out, \"num of row groups: {}\", metadata.num_row_groups());\n+    writeln!(out, \"row groups:\");\n+    writeln!(out, \"\");\n+    for (i, rg) in metadata.row_groups().iter().enumerate() {\n+        writeln!(out, \"row group {}:\", i);\n+        print_dashes(out, 80);\n+        print_row_group_metadata(out, rg);\n+    }\n+}\n+\n+/// Prints file metadata [`FileMetaData`](`::file::metadata::FileMetaData`) information.\n+#[allow(unused_must_use)]\n+pub fn print_file_metadata(out: &mut io::Write, file_metadata: &FileMetaData) {\n+    writeln!(out, \"version: {}\", file_metadata.version());\n+    writeln!(out, \"num of rows: {}\", file_metadata.num_rows());\n+    if let Some(created_by) = file_metadata.created_by().as_ref() {\n+        writeln!(out, \"created by: {}\", created_by);\n+    }\n+    let schema = file_metadata.schema();\n+    print_schema(out, schema);\n+}\n+\n+/// Prints Parquet [`Type`](`::schema::types::Type`) information.\n+#[allow(unused_must_use)]\n+pub fn print_schema(out: &mut io::Write, tp: &Type) {\n+    // TODO: better if we can pass fmt::Write to Printer.\n+    // But how can we make it to accept both io::Write & fmt::Write?\n+    let mut s = String::new();\n+    {\n+        let mut printer = Printer::new(&mut s);\n+        printer.print(tp);\n+    }\n+    writeln!(out, \"{}\", s);\n+}\n+\n+#[allow(unused_must_use)]\n+fn print_row_group_metadata(out: &mut io::Write, rg_metadata: &RowGroupMetaData) {\n+    writeln!(out, \"total byte size: {}\", rg_metadata.total_byte_size());\n+    writeln!(out, \"num of rows: {}\", rg_metadata.num_rows());\n+    writeln!(out, \"\");\n+    writeln!(out, \"num of columns: {}\", rg_metadata.num_columns());\n+    writeln!(out, \"columns: \");\n+    for (i, cc) in rg_metadata.columns().iter().enumerate() {\n+        writeln!(out, \"\");\n+        writeln!(out, \"column {}:\", i);\n+        print_dashes(out, 80);\n+        print_column_chunk_metadata(out, cc);\n+    }\n+}\n+\n+#[allow(unused_must_use)]\n+fn print_column_chunk_metadata(out: &mut io::Write, cc_metadata: &ColumnChunkMetaData) {\n+    writeln!(out, \"column type: {}\", cc_metadata.column_type());\n+    writeln!(out, \"column path: {}\", cc_metadata.column_path());\n+    let encoding_strs: Vec<_> = cc_metadata\n+        .encodings()\n+        .iter()\n+        .map(|e| format!(\"{}\", e))\n+        .collect();\n+    writeln!(out, \"encodings: {}\", encoding_strs.join(\" \"));\n+    let file_path_str = match cc_metadata.file_path() {\n+        None => \"N/A\",\n+        Some(ref fp) => *fp,\n+    };\n+    writeln!(out, \"file path: {}\", file_path_str);\n+    writeln!(out, \"file offset: {}\", cc_metadata.file_offset());\n+    writeln!(out, \"num of values: {}\", cc_metadata.num_values());\n+    writeln!(\n+        out,\n+        \"total compressed size (in bytes): {}\",\n+        cc_metadata.compressed_size()\n+    );\n+    writeln!(\n+        out,\n+        \"total uncompressed size (in bytes): {}\",\n+        cc_metadata.uncompressed_size()\n+    );\n+    writeln!(out, \"data page offset: {}\", cc_metadata.data_page_offset());\n+    let index_page_offset_str = match cc_metadata.index_page_offset() {\n+        None => \"N/A\".to_owned(),\n+        Some(ipo) => ipo.to_string(),\n+    };\n+    writeln!(out, \"index page offset: {}\", index_page_offset_str);\n+    let dict_page_offset_str = match cc_metadata.dictionary_page_offset() {\n+        None => \"N/A\".to_owned(),\n+        Some(dpo) => dpo.to_string(),\n+    };\n+    writeln!(out, \"dictionary page offset: {}\", dict_page_offset_str);\n+    let statistics_str = match cc_metadata.statistics() {\n+        None => \"N/A\".to_owned(),\n+        Some(stats) => stats.to_string(),\n+    };\n+    writeln!(out, \"statistics: {}\", statistics_str);\n+    writeln!(out, \"\");\n+}\n+\n+#[allow(unused_must_use)]\n+fn print_dashes(out: &mut io::Write, num: i32) {\n+    for _ in 0..num {\n+        write!(out, \"-\");\n+    }\n+    writeln!(out, \"\");\n+}\n+\n+const INDENT_WIDTH: i32 = 2;\n+\n+/// Struct for printing Parquet message type.\n+struct Printer<'a> {\n+    output: &'a mut fmt::Write,\n+    indent: i32,\n+}\n+\n+#[allow(unused_must_use)]\n+impl<'a> Printer<'a> {\n+    fn new(output: &'a mut fmt::Write) -> Self {\n+        Printer { output, indent: 0 }\n+    }\n+\n+    fn print_indent(&mut self) {\n+        for _ in 0..self.indent {\n+            write!(self.output, \" \");\n+        }\n+    }\n+}\n+\n+#[allow(unused_must_use)]\n+impl<'a> Printer<'a> {\n+    pub fn print(&mut self, tp: &Type) {\n+        self.print_indent();\n+        match tp {\n+            &Type::PrimitiveType {\n+                ref basic_info,\n+                physical_type,\n+                type_length,\n+                scale,\n+                precision,\n+            } => {\n+                let phys_type_str = match physical_type {\n+                    PhysicalType::FIXED_LEN_BYTE_ARRAY => {\n+                        // We need to include length for fixed byte array\n+                        format!(\"{} ({})\", physical_type, type_length)\n+                    }\n+                    _ => format!(\"{}\", physical_type),\n+                };\n+                // Also print logical type if it is available\n+                let logical_type_str = match basic_info.logical_type() {\n+                    LogicalType::NONE => format!(\"\"),\n+                    decimal @ LogicalType::DECIMAL => {\n+                        // For decimal type we should print precision and scale if they are > 0, e.g.\n+                        // DECIMAL(9, 2) - DECIMAL(9) - DECIMAL\n+                        let precision_scale = match (precision, scale) {\n+                            (p, s) if p > 0 && s > 0 => format!(\" ({}, {})\", p, s),\n+                            (p, 0) if p > 0 => format!(\" ({})\", p),\n+                            _ => format!(\"\"),\n+                        };\n+                        format!(\" ({}{})\", decimal, precision_scale)\n+                    }\n+                    other_logical_type => format!(\" ({})\", other_logical_type),\n+                };\n+                write!(\n+                    self.output,\n+                    \"{} {} {}{};\",\n+                    basic_info.repetition(),\n+                    phys_type_str,\n+                    basic_info.name(),\n+                    logical_type_str\n+                );\n+            }\n+            &Type::GroupType {\n+                ref basic_info,\n+                ref fields,\n+            } => {\n+                if basic_info.has_repetition() {\n+                    let r = basic_info.repetition();\n+                    write!(self.output, \"{} group {} \", r, basic_info.name());\n+                    if basic_info.logical_type() != LogicalType::NONE {\n+                        write!(self.output, \"({}) \", basic_info.logical_type());\n+                    }\n+                    writeln!(self.output, \"{{\");\n+                } else {\n+                    writeln!(self.output, \"message {} {{\", basic_info.name());\n+                }\n+\n+                self.indent += INDENT_WIDTH;\n+                for c in fields {\n+                    self.print(&c);\n+                    writeln!(self.output, \"\");\n+                }\n+                self.indent -= INDENT_WIDTH;\n+                self.print_indent();\n+                write!(self.output, \"}}\");\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    use std::rc::Rc;\n+\n+    use crate::parquet::basic::{Repetition, Type as PhysicalType};\n+    use crate::parquet::schema::{parser::parse_message_type, types::Type};\n+\n+    fn assert_print_parse_message(message: Type) {\n+        let mut s = String::new();\n+        {\n+            let mut p = Printer::new(&mut s);\n+            p.print(&message);\n+        }\n+        let parsed = parse_message_type(&s).unwrap();\n+        assert_eq!(message, parsed);\n+    }\n+\n+    #[test]\n+    fn test_print_primitive_type() {\n+        let mut s = String::new();\n+        {\n+            let mut p = Printer::new(&mut s);\n+            let foo = Type::primitive_type_builder(\"foo\", PhysicalType::INT32)\n+                .with_repetition(Repetition::REQUIRED)\n+                .with_logical_type(LogicalType::INT_32)\n+                .build()\n+                .unwrap();\n+            p.print(&foo);\n+        }\n+        assert_eq!(&mut s, \"REQUIRED INT32 foo (INT_32);\");\n+    }\n+\n+    #[test]\n+    fn test_print_primitive_type_without_logical() {\n+        let mut s = String::new();\n+        {\n+            let mut p = Printer::new(&mut s);\n+            let foo = Type::primitive_type_builder(\"foo\", PhysicalType::DOUBLE)\n+                .with_repetition(Repetition::REQUIRED)\n+                .build()\n+                .unwrap();\n+            p.print(&foo);\n+        }\n+        assert_eq!(&mut s, \"REQUIRED DOUBLE foo;\");\n+    }\n+\n+    #[test]\n+    fn test_print_group_type() {\n+        let mut s = String::new();\n+        {\n+            let mut p = Printer::new(&mut s);\n+            let f1 = Type::primitive_type_builder(\"f1\", PhysicalType::INT32)\n+                .with_repetition(Repetition::REQUIRED)\n+                .with_logical_type(LogicalType::INT_32)\n+                .with_id(0)\n+                .build();\n+            let f2 = Type::primitive_type_builder(\"f2\", PhysicalType::BYTE_ARRAY)\n+                .with_logical_type(LogicalType::UTF8)\n+                .with_id(1)\n+                .build();\n+            let f3 = Type::primitive_type_builder(\"f3\", PhysicalType::FIXED_LEN_BYTE_ARRAY)\n+                .with_repetition(Repetition::REPEATED)\n+                .with_logical_type(LogicalType::INTERVAL)\n+                .with_length(12)\n+                .with_id(2)\n+                .build();\n+            let mut struct_fields = Vec::new();\n+            struct_fields.push(Rc::new(f1.unwrap()));\n+            struct_fields.push(Rc::new(f2.unwrap()));\n+            let foo = Type::group_type_builder(\"foo\")\n+                .with_repetition(Repetition::OPTIONAL)\n+                .with_fields(&mut struct_fields)\n+                .with_id(1)\n+                .build()\n+                .unwrap();\n+            let mut fields = Vec::new();\n+            fields.push(Rc::new(foo));\n+            fields.push(Rc::new(f3.unwrap()));\n+            let message = Type::group_type_builder(\"schema\")\n+                .with_fields(&mut fields)\n+                .with_id(2)\n+                .build()\n+                .unwrap();\n+            p.print(&message);\n+        }\n+        let expected = \"message schema {\n+  OPTIONAL group foo {\n+    REQUIRED INT32 f1 (INT_32);\n+    OPTIONAL BYTE_ARRAY f2 (UTF8);\n+  }\n+  REPEATED FIXED_LEN_BYTE_ARRAY (12) f3 (INTERVAL);\n+}\";\n+        assert_eq!(&mut s, expected);\n+    }\n+\n+    #[test]\n+    fn test_print_and_parse_primitive() {\n+        let a2 = Type::primitive_type_builder(\"a2\", PhysicalType::BYTE_ARRAY)\n+            .with_repetition(Repetition::REPEATED)\n+            .with_logical_type(LogicalType::UTF8)\n+            .build()\n+            .unwrap();\n+\n+        let a1 = Type::group_type_builder(\"a1\")\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_logical_type(LogicalType::LIST)\n+            .with_fields(&mut vec![Rc::new(a2)])\n+            .build()\n+            .unwrap();\n+\n+        let b3 = Type::primitive_type_builder(\"b3\", PhysicalType::INT32)\n+            .with_repetition(Repetition::OPTIONAL)\n+            .build()\n+            .unwrap();\n+\n+        let b4 = Type::primitive_type_builder(\"b4\", PhysicalType::DOUBLE)\n+            .with_repetition(Repetition::OPTIONAL)\n+            .build()\n+            .unwrap();\n+\n+        let b2 = Type::group_type_builder(\"b2\")\n+            .with_repetition(Repetition::REPEATED)\n+            .with_logical_type(LogicalType::NONE)\n+            .with_fields(&mut vec![Rc::new(b3), Rc::new(b4)])\n+            .build()\n+            .unwrap();\n+\n+        let b1 = Type::group_type_builder(\"b1\")\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_logical_type(LogicalType::LIST)\n+            .with_fields(&mut vec![Rc::new(b2)])\n+            .build()\n+            .unwrap();\n+\n+        let a0 = Type::group_type_builder(\"a0\")\n+            .with_repetition(Repetition::REQUIRED)\n+            .with_fields(&mut vec![Rc::new(a1), Rc::new(b1)])\n+            .build()\n+            .unwrap();\n+\n+        let message = Type::group_type_builder(\"root\")\n+            .with_fields(&mut vec![Rc::new(a0)])\n+            .build()\n+            .unwrap();\n+\n+        assert_print_parse_message(message);\n+    }\n+\n+    #[test]\n+    fn test_print_and_parse_nested() {\n+        let f1 = Type::primitive_type_builder(\"f1\", PhysicalType::INT32)\n+            .with_repetition(Repetition::REQUIRED)\n+            .with_logical_type(LogicalType::INT_32)\n+            .build()\n+            .unwrap();\n+\n+        let f2 = Type::primitive_type_builder(\"f2\", PhysicalType::BYTE_ARRAY)\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_logical_type(LogicalType::UTF8)\n+            .build()\n+            .unwrap();\n+\n+        let foo = Type::group_type_builder(\"foo\")\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_fields(&mut vec![Rc::new(f1), Rc::new(f2)])\n+            .build()\n+            .unwrap();\n+\n+        let f3 = Type::primitive_type_builder(\"f3\", PhysicalType::FIXED_LEN_BYTE_ARRAY)\n+            .with_repetition(Repetition::REPEATED)\n+            .with_logical_type(LogicalType::INTERVAL)\n+            .with_length(12)\n+            .build()\n+            .unwrap();\n+\n+        let message = Type::group_type_builder(\"schema\")\n+            .with_fields(&mut vec![Rc::new(foo), Rc::new(f3)])\n+            .build()\n+            .unwrap();\n+\n+        assert_print_parse_message(message);\n+    }\n+\n+    #[test]\n+    fn test_print_and_parse_decimal() {\n+        let f1 = Type::primitive_type_builder(\"f1\", PhysicalType::INT32)\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_logical_type(LogicalType::DECIMAL)\n+            .with_precision(9)\n+            .with_scale(2)\n+            .build()\n+            .unwrap();\n+\n+        let f2 = Type::primitive_type_builder(\"f2\", PhysicalType::INT32)\n+            .with_repetition(Repetition::OPTIONAL)\n+            .with_logical_type(LogicalType::DECIMAL)\n+            .with_precision(9)\n+            .with_scale(0)\n+            .build()\n+            .unwrap();\n+\n+        let message = Type::group_type_builder(\"schema\")\n+            .with_fields(&mut vec![Rc::new(f1), Rc::new(f2)])\n+            .build()\n+            .unwrap();\n+\n+        assert_print_parse_message(message);\n+    }\n+}\ndiff --git a/rust/src/parquet/schema/types.rs b/rust/src/parquet/schema/types.rs\nnew file mode 100644\nindex 0000000000..90c767c093\n--- /dev/null\n+++ b/rust/src/parquet/schema/types.rs\n@@ -0,0 +1,1830 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Contains structs and methods to build Parquet schema and schema descriptors.\n+\n+use std::{collections::HashMap, convert::From, fmt, rc::Rc};\n+\n+use parquet_format::SchemaElement;\n+\n+use crate::parquet::basic::{LogicalType, Repetition, Type as PhysicalType};\n+use crate::parquet::errors::{ParquetError, Result};\n+\n+// ----------------------------------------------------------------------\n+// Parquet Type definitions\n+\n+/// Type alias for `Rc<Type>`.\n+pub type TypePtr = Rc<Type>;\n+/// Type alias for `Rc<SchemaDescriptor>`.\n+pub type SchemaDescPtr = Rc<SchemaDescriptor>;\n+/// Type alias for `Rc<ColumnDescriptor>`.\n+pub type ColumnDescPtr = Rc<ColumnDescriptor>;\n+\n+/// Representation of a Parquet type.\n+/// Used to describe primitive leaf fields and structs, including top-level schema.\n+/// Note that the top-level schema type is represented using `GroupType` whose\n+/// repetition is `None`.\n+#[derive(Debug, PartialEq)]\n+pub enum Type {\n+    PrimitiveType {\n+        basic_info: BasicTypeInfo,\n+        physical_type: PhysicalType,\n+        type_length: i32,\n+        scale: i32,\n+        precision: i32,\n+    },\n+    GroupType {\n+        basic_info: BasicTypeInfo,\n+        fields: Vec<TypePtr>,\n+    },\n+}\n+\n+impl Type {\n+    /// Creates primitive type builder with provided field name and physical type.\n+    pub fn primitive_type_builder(name: &str, physical_type: PhysicalType) -> PrimitiveTypeBuilder {\n+        PrimitiveTypeBuilder::new(name, physical_type)\n+    }\n+\n+    /// Creates group type builder with provided column name.\n+    pub fn group_type_builder(name: &str) -> GroupTypeBuilder {\n+        GroupTypeBuilder::new(name)\n+    }\n+\n+    /// Returns [`BasicTypeInfo`] information about the type.\n+    pub fn get_basic_info(&self) -> &BasicTypeInfo {\n+        match *self {\n+            Type::PrimitiveType { ref basic_info, .. } => &basic_info,\n+            Type::GroupType { ref basic_info, .. } => &basic_info,\n+        }\n+    }\n+\n+    /// Returns this type's field name.\n+    pub fn name(&self) -> &str {\n+        self.get_basic_info().name()\n+    }\n+\n+    /// Gets the fields from this group type.\n+    /// Note that this will panic if called on a non-group type.\n+    // TODO: should we return `&[&Type]` here?\n+    pub fn get_fields(&self) -> &[TypePtr] {\n+        match *self {\n+            Type::GroupType { ref fields, .. } => &fields[..],\n+            _ => panic!(\"Cannot call get_fields() on a non-group type\"),\n+        }\n+    }\n+\n+    /// Gets physical type of this primitive type.\n+    /// Note that this will panic if called on a non-primitive type.\n+    pub fn get_physical_type(&self) -> PhysicalType {\n+        match *self {\n+            Type::PrimitiveType {\n+                basic_info: _,\n+                physical_type,\n+                ..\n+            } => physical_type,\n+            _ => panic!(\"Cannot call get_physical_type() on a non-primitive type\"),\n+        }\n+    }\n+\n+    /// Checks if `sub_type` schema is part of current schema.\n+    /// This method can be used to check if projected columns are part of the root schema.\n+    pub fn check_contains(&self, sub_type: &Type) -> bool {\n+        // Names match, and repetitions match or not set for both\n+        let basic_match = self.get_basic_info().name() == sub_type.get_basic_info().name()\n+            && (self.is_schema() && sub_type.is_schema()\n+                || !self.is_schema()\n+                    && !sub_type.is_schema()\n+                    && self.get_basic_info().repetition()\n+                        == sub_type.get_basic_info().repetition());\n+\n+        match *self {\n+            Type::PrimitiveType { .. } if basic_match && sub_type.is_primitive() => {\n+                self.get_physical_type() == sub_type.get_physical_type()\n+            }\n+            Type::GroupType { .. } if basic_match && sub_type.is_group() => {\n+                // build hashmap of name -> TypePtr\n+                let mut field_map = HashMap::new();\n+                for field in self.get_fields() {\n+                    field_map.insert(field.name(), field);\n+                }\n+\n+                for field in sub_type.get_fields() {\n+                    if !field_map\n+                        .get(field.name())\n+                        .map(|tpe| tpe.check_contains(field))\n+                        .unwrap_or(false)\n+                    {\n+                        return false;\n+                    }\n+                }\n+                true\n+            }\n+            _ => false,\n+        }\n+    }\n+\n+    /// Returns `true` if this type is a primitive type, `false` otherwise.\n+    pub fn is_primitive(&self) -> bool {\n+        match *self {\n+            Type::PrimitiveType { .. } => true,\n+            _ => false,\n+        }\n+    }\n+\n+    /// Returns `true` if this type is a group type, `false` otherwise.\n+    pub fn is_group(&self) -> bool {\n+        match *self {\n+            Type::GroupType { .. } => true,\n+            _ => false,\n+        }\n+    }\n+\n+    /// Returns `true` if this type is the top-level schema type (message type).\n+    pub fn is_schema(&self) -> bool {\n+        match *self {\n+            Type::GroupType { ref basic_info, .. } => !basic_info.has_repetition(),\n+            _ => false,\n+        }\n+    }\n+}\n+\n+/// A builder for primitive types. All attributes are optional\n+/// except the name and physical type.\n+/// Note that if not specified explicitly, `Repetition::OPTIONAL` is used.\n+pub struct PrimitiveTypeBuilder<'a> {\n+    name: &'a str,\n+    repetition: Repetition,\n+    physical_type: PhysicalType,\n+    logical_type: LogicalType,\n+    length: i32,\n+    precision: i32,\n+    scale: i32,\n+    id: Option<i32>,\n+}\n+\n+impl<'a> PrimitiveTypeBuilder<'a> {\n+    /// Creates new primitive type builder with provided field name and physical type.\n+    pub fn new(name: &'a str, physical_type: PhysicalType) -> Self {\n+        Self {\n+            name,\n+            repetition: Repetition::OPTIONAL,\n+            physical_type,\n+            logical_type: LogicalType::NONE,\n+            length: -1,\n+            precision: -1,\n+            scale: -1,\n+            id: None,\n+        }\n+    }\n+\n+    /// Sets [`Repetition`](`::basic::Repetition`) for this field and returns itself.\n+    pub fn with_repetition(mut self, repetition: Repetition) -> Self {\n+        self.repetition = repetition;\n+        self\n+    }\n+\n+    /// Sets [`LogicalType`](`::basic::LogicalType`) for this field and returns itself.\n+    pub fn with_logical_type(mut self, logical_type: LogicalType) -> Self {\n+        self.logical_type = logical_type;\n+        self\n+    }\n+\n+    /// Sets type length and returns itself.\n+    /// This is only applied to FIXED_LEN_BYTE_ARRAY and INT96 (INTERVAL) types, because\n+    /// they maintain fixed size underlying byte array.\n+    /// By default, value is `0`.\n+    pub fn with_length(mut self, length: i32) -> Self {\n+        self.length = length;\n+        self\n+    }\n+\n+    /// Sets precision for Parquet DECIMAL physical type and returns itself.\n+    /// By default, it equals to `0` and used only for decimal context.\n+    pub fn with_precision(mut self, precision: i32) -> Self {\n+        self.precision = precision;\n+        self\n+    }\n+\n+    /// Sets scale for Parquet DECIMAL physical type and returns itself.\n+    /// By default, it equals to `0` and used only for decimal context.\n+    pub fn with_scale(mut self, scale: i32) -> Self {\n+        self.scale = scale;\n+        self\n+    }\n+\n+    /// Sets optional field id and returns itself.\n+    pub fn with_id(mut self, id: i32) -> Self {\n+        self.id = Some(id);\n+        self\n+    }\n+\n+    /// Creates a new `PrimitiveType` instance from the collected attributes.\n+    /// Returns `Err` in case of any building conditions are not met.\n+    pub fn build(self) -> Result<Type> {\n+        let basic_info = BasicTypeInfo {\n+            name: String::from(self.name),\n+            repetition: Some(self.repetition),\n+            logical_type: self.logical_type,\n+            id: self.id,\n+        };\n+\n+        // Check length before logical type, since it is used for logical type validation.\n+        if self.physical_type == PhysicalType::FIXED_LEN_BYTE_ARRAY && self.length < 0 {\n+            return Err(general_err!(\n+                \"Invalid FIXED_LEN_BYTE_ARRAY length: {}\",\n+                self.length\n+            ));\n+        }\n+\n+        match self.logical_type {\n+            LogicalType::NONE => {}\n+            LogicalType::UTF8 | LogicalType::BSON | LogicalType::JSON => {\n+                if self.physical_type != PhysicalType::BYTE_ARRAY {\n+                    return Err(general_err!(\n+                        \"{} can only annotate BYTE_ARRAY fields\",\n+                        self.logical_type\n+                    ));\n+                }\n+            }\n+            LogicalType::DECIMAL => {\n+                match self.physical_type {\n+                    PhysicalType::INT32\n+                    | PhysicalType::INT64\n+                    | PhysicalType::BYTE_ARRAY\n+                    | PhysicalType::FIXED_LEN_BYTE_ARRAY => (),\n+                    _ => {\n+                        return Err(general_err!(\n+                            \"DECIMAL can only annotate INT32, INT64, BYTE_ARRAY and FIXED\"\n+                        ));\n+                    }\n+                }\n+\n+                // Precision is required and must be a non-zero positive integer.\n+                if self.precision < 1 {\n+                    return Err(general_err!(\n+                        \"Invalid DECIMAL precision: {}\",\n+                        self.precision\n+                    ));\n+                }\n+\n+                // Scale must be zero or a positive integer less than the precision.\n+                if self.scale < 0 {\n+                    return Err(general_err!(\"Invalid DECIMAL scale: {}\", self.scale));\n+                }\n+\n+                if self.scale >= self.precision {\n+                    return Err(general_err!(\n+                        \"Invalid DECIMAL: scale ({}) cannot be greater than or equal to precision \\\n+                         ({})\",\n+                        self.scale,\n+                        self.precision\n+                    ));\n+                }\n+\n+                // Check precision and scale based on physical type limitations.\n+                match self.physical_type {\n+                    PhysicalType::INT32 => {\n+                        if self.precision > 9 {\n+                            return Err(general_err!(\n+                                \"Cannot represent INT32 as DECIMAL with precision {}\",\n+                                self.precision\n+                            ));\n+                        }\n+                    }\n+                    PhysicalType::INT64 => {\n+                        if self.precision > 18 {\n+                            return Err(general_err!(\n+                                \"Cannot represent INT64 as DECIMAL with precision {}\",\n+                                self.precision\n+                            ));\n+                        }\n+                    }\n+                    PhysicalType::FIXED_LEN_BYTE_ARRAY => {\n+                        let max_precision =\n+                            (2f64.powi(8 * self.length - 1) - 1f64).log10().floor() as i32;\n+\n+                        if self.precision > max_precision {\n+                            return Err(general_err!(\n+                \"Cannot represent FIXED_LEN_BYTE_ARRAY as DECIMAL with length {} and \\\n+                 precision {}\",\n+                self.length,\n+                self.precision\n+              ));\n+                        }\n+                    }\n+                    _ => (), // For BYTE_ARRAY precision is not limited\n+                }\n+            }\n+            LogicalType::DATE\n+            | LogicalType::TIME_MILLIS\n+            | LogicalType::UINT_8\n+            | LogicalType::UINT_16\n+            | LogicalType::UINT_32\n+            | LogicalType::INT_8\n+            | LogicalType::INT_16\n+            | LogicalType::INT_32 => {\n+                if self.physical_type != PhysicalType::INT32 {\n+                    return Err(general_err!(\n+                        \"{} can only annotate INT32\",\n+                        self.logical_type\n+                    ));\n+                }\n+            }\n+            LogicalType::TIME_MICROS\n+            | LogicalType::TIMESTAMP_MILLIS\n+            | LogicalType::TIMESTAMP_MICROS\n+            | LogicalType::UINT_64\n+            | LogicalType::INT_64 => {\n+                if self.physical_type != PhysicalType::INT64 {\n+                    return Err(general_err!(\n+                        \"{} can only annotate INT64\",\n+                        self.logical_type\n+                    ));\n+                }\n+            }\n+            LogicalType::INTERVAL => {\n+                if self.physical_type != PhysicalType::FIXED_LEN_BYTE_ARRAY || self.length != 12 {\n+                    return Err(general_err!(\n+                        \"INTERVAL can only annotate FIXED_LEN_BYTE_ARRAY(12)\"\n+                    ));\n+                }\n+            }\n+            LogicalType::ENUM => {\n+                if self.physical_type != PhysicalType::BYTE_ARRAY {\n+                    return Err(general_err!(\"ENUM can only annotate BYTE_ARRAY fields\"));\n+                }\n+            }\n+            _ => {\n+                return Err(general_err!(\n+                    \"{} cannot be applied to a primitive type\",\n+                    self.logical_type\n+                ));\n+            }\n+        }\n+\n+        Ok(Type::PrimitiveType {\n+            basic_info,\n+            physical_type: self.physical_type,\n+            type_length: self.length,\n+            scale: self.scale,\n+            precision: self.precision,\n+        })\n+    }\n+}\n+\n+/// A builder for group types. All attributes are optional except the name.\n+/// Note that if not specified explicitly, `None` is used as the repetition of the group,\n+/// which means it is a root (message) type.\n+pub struct GroupTypeBuilder<'a> {\n+    name: &'a str,\n+    repetition: Option<Repetition>,\n+    logical_type: LogicalType,\n+    fields: Vec<TypePtr>,\n+    id: Option<i32>,\n+}\n+\n+impl<'a> GroupTypeBuilder<'a> {\n+    /// Creates new group type builder with provided field name.\n+    pub fn new(name: &'a str) -> Self {\n+        Self {\n+            name,\n+            repetition: None,\n+            logical_type: LogicalType::NONE,\n+            fields: Vec::new(),\n+            id: None,\n+        }\n+    }\n+\n+    /// Sets [`Repetition`](`::basic::Repetition`) for this field and returns itself.\n+    pub fn with_repetition(mut self, repetition: Repetition) -> Self {\n+        self.repetition = Some(repetition);\n+        self\n+    }\n+\n+    /// Sets [`LogicalType`](`::basic::LogicalType`) for this field and returns itself.\n+    pub fn with_logical_type(mut self, logical_type: LogicalType) -> Self {\n+        self.logical_type = logical_type;\n+        self\n+    }\n+\n+    /// Sets a list of fields that should be child nodes of this field.\n+    /// Returns updated self.\n+    pub fn with_fields(mut self, fields: &mut Vec<TypePtr>) -> Self {\n+        self.fields.append(fields);\n+        self\n+    }\n+\n+    /// Sets optional field id and returns itself.\n+    pub fn with_id(mut self, id: i32) -> Self {\n+        self.id = Some(id);\n+        self\n+    }\n+\n+    /// Creates a new `GroupType` instance from the gathered attributes.\n+    pub fn build(self) -> Result<Type> {\n+        let basic_info = BasicTypeInfo {\n+            name: String::from(self.name),\n+            repetition: self.repetition,\n+            logical_type: self.logical_type,\n+            id: self.id,\n+        };\n+        Ok(Type::GroupType {\n+            basic_info,\n+            fields: self.fields,\n+        })\n+    }\n+}\n+\n+/// Basic type info. This contains information such as the name of the type,\n+/// the repetition level, the logical type and the kind of the type (group, primitive).\n+#[derive(Debug, PartialEq)]\n+pub struct BasicTypeInfo {\n+    name: String,\n+    repetition: Option<Repetition>,\n+    logical_type: LogicalType,\n+    id: Option<i32>,\n+}\n+\n+impl BasicTypeInfo {\n+    /// Returns field name.\n+    pub fn name(&self) -> &str {\n+        &self.name\n+    }\n+\n+    /// Returns `true` if type has repetition field set, `false` otherwise.\n+    /// This is mostly applied to group type, because primitive type always has\n+    /// repetition set.\n+    pub fn has_repetition(&self) -> bool {\n+        self.repetition.is_some()\n+    }\n+\n+    /// Returns [`Repetition`](`::basic::Repetition`) value for the type.\n+    pub fn repetition(&self) -> Repetition {\n+        assert!(self.repetition.is_some());\n+        self.repetition.unwrap()\n+    }\n+\n+    /// Returns [`LogicalType`](`::basic::LogicalType`) value for the type.\n+    pub fn logical_type(&self) -> LogicalType {\n+        self.logical_type\n+    }\n+\n+    /// Returns `true` if id is set, `false` otherwise.\n+    pub fn has_id(&self) -> bool {\n+        self.id.is_some()\n+    }\n+\n+    /// Returns id value for the type.\n+    pub fn id(&self) -> i32 {\n+        assert!(self.id.is_some());\n+        self.id.unwrap()\n+    }\n+}\n+\n+// ----------------------------------------------------------------------\n+// Parquet descriptor definitions\n+\n+/// Represents a path in a nested schema\n+#[derive(Clone, PartialEq, Debug, Eq, Hash)]\n+pub struct ColumnPath {\n+    parts: Vec<String>,\n+}\n+\n+impl ColumnPath {\n+    /// Creates new column path from vector of field names.\n+    pub fn new(parts: Vec<String>) -> Self {\n+        ColumnPath { parts }\n+    }\n+\n+    /// Returns string representation of this column path.\n+    /// ```rust\n+    /// use arrow::parquet::schema::types::ColumnPath;\n+    ///\n+    /// let path = ColumnPath::new(vec![\"a\".to_string(), \"b\".to_string(), \"c\".to_string()]);\n\n  (This diff was longer than 20,000 lines, and has been truncated...)\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-12-17T21:35:54.870+0000",
                    "updated": "2018-12-17T21:35:54.870+0000",
                    "started": "2018-12-17T21:35:54.870+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "176255",
                    "issueId": "13204408"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408/worklog/176392",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on issue #3050: ARROW-4028: [Rust] Merge parquet-rs codebase\nURL: https://github.com/apache/arrow/pull/3050#issuecomment-448115538\n \n \n   Thanks everyone!!\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-12-18T06:50:40.500+0000",
                    "updated": "2018-12-18T06:50:40.500+0000",
                    "started": "2018-12-18T06:50:40.499+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "176392",
                    "issueId": "13204408"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 2400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@73f7476d[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2756daea[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@18a36f78[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@933770c[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7dcade19[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@1839b158[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1b62c588[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@9fb4aff[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@22ea77cf[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@778ca1bb[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7a4f6447[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@29b06543[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 2400,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Dec 17 21:35:44 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-12-17T21:35:44.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4028/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2018-12-14T05:52:12.000+0000",
        "updated": "2018-12-18T06:50:40.000+0000",
        "timeoriginalestimate": null,
        "description": "Initial donation of [parquet-rs|https://github.com/sunchao/parquet-rs], an Apache Parquet implementation in Rust. This subjects to ASF IP clearance. ",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 2400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust] Merge parquet-rs codebase",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13204408/comment/16723386",
                    "id": "16723386",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 3050\n[https://github.com/apache/arrow/pull/3050]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-12-17T21:35:44.573+0000",
                    "updated": "2018-12-17T21:35:44.573+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|s01ik8:",
        "customfield_12314139": null
    }
}