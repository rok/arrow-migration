{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13404217",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217",
    "key": "ARROW-14181",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350712",
                "id": "12350712",
                "description": "",
                "name": "6.0.1",
                "archived": false,
                "released": true,
                "releaseDate": "2021-11-18"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/2",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/critical.svg",
            "name": "Critical",
            "id": "2"
        },
        "labels": [
            "pull-request-available",
            "query-engine"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350323",
                "id": "12350323",
                "description": "",
                "name": "6.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-10-26"
            }
        ],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12623883",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12623883",
                "type": {
                    "id": "12310460",
                    "name": "Child-Issue",
                    "inward": "is a child of",
                    "outward": "is a parent of",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/12310460"
                },
                "inwardIssue": {
                    "id": "13376404",
                    "key": "ARROW-12633",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13376404",
                    "fields": {
                        "summary": "[C++] Query engine umbrella issue",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/1",
                            "description": "The issue is open and ready for the assignee to start work on it.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/open.png",
                            "name": "Open",
                            "id": "1",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/2",
                                "id": 2,
                                "key": "new",
                                "colorName": "blue-gray",
                                "name": "To Do"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
                            "id": "2",
                            "description": "A new feature of the product, which has yet to be developed.",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
                            "name": "New Feature",
                            "subtask": false,
                            "avatarId": 21141
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328935",
                "id": "12328935",
                "name": "C++"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=michalno",
            "name": "michalno",
            "key": "michalno",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
            },
            "displayName": "Michal Nowakiewicz",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 12000,
            "total": 12000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 12000,
            "total": 12000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14181/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 20,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/666126",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa opened a new pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446\n\n\n   Supporting dictionary arrays and dictionary scalars as inputs to hash join on both its sides, in key columns and non-key columns. \r\n   \r\n   A key column from probe side of the join can be matched against a key column from build side of the join, as long as the underlying value types are equal, that means that: \r\n   - dictionary column (on either side) can be matched against non-dictionary column (on the other side) if underlying value\r\n   types are equal\r\n   - dictionary column can be matched against dictionary column with a different index type, and potentially using a different dictionary, as long as the underlying value types are equal\r\n   \r\n   We keep the same limitation that is present in hash group by with respect to dictionaries, that is the same dictionary must be used for a given column in all input exec batches. The values in the dictionary do not have to be unique - it can contain duplicate entries and/or null entries.\r\n   \r\n   This change is build on top of https://github.com/apache/arrow/pull/11350 (fixing thread sanitizer problems in hash join node).\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-18T04:56:33.220+0000",
                    "updated": "2021-10-18T04:56:33.220+0000",
                    "started": "2021-10-18T04:56:33.219+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "666126",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/666127",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-945364465\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-10-18T04:56:47.095+0000",
                    "updated": "2021-10-18T04:56:47.095+0000",
                    "started": "2021-10-18T04:56:47.095+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "666127",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/673805",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741430380\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n\nReview comment:\n       nit: remove this TODO?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+};\n+\n+/// Implements processing of dictionary arrays/scalars in key columns on the build side of\n+/// a hash join.\n+/// Each instance of this class corresponds to a single column and stores and\n+/// processes only the information related to that column.\n+/// Const methods are thread-safe, non-const methods are not (the caller must make sure\n+/// that only one thread at any time will access them).\n+///\n\nReview comment:\n       FWIW, thanks for the detailed comments in this file - they help a lot in understanding what's going on here.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n\nReview comment:\n       n.b. arrow::dictionary(index_ty, value_ty) is shorthand for this\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n\nReview comment:\n       I might be dense but what does \"Cvt\" stand for?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n\nReview comment:\n       Does this need to be in the header at all then? It seems it could live solely in the .cc file.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n+  auto uint8_utf8 = std::make_shared<DictionaryType>(uint8(), utf8());\n+  auto int16_utf8 = std::make_shared<DictionaryType>(int16(), utf8());\n+  auto uint16_utf8 = std::make_shared<DictionaryType>(uint16(), utf8());\n+  auto int32_utf8 = std::make_shared<DictionaryType>(int32(), utf8());\n+  auto uint32_utf8 = std::make_shared<DictionaryType>(uint32(), utf8());\n+  auto int64_utf8 = std::make_shared<DictionaryType>(int64(), utf8());\n+  auto uint64_utf8 = std::make_shared<DictionaryType>(uint64(), utf8());\n+  std::shared_ptr<DictionaryType> dict_types[] = {int8_utf8,   uint8_utf8, int16_utf8,\n+                                                  uint16_utf8, int32_utf8, uint32_utf8,\n+                                                  int64_utf8,  uint64_utf8};\n+\n+  Random64Bit rng(43);\n+\n+  // Dictionaries in payload columns\n+  for (auto parallel : {false, true})\n\nReview comment:\n       nit: can we insert braces here for readability?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T20:23:06.772+0000",
                    "updated": "2021-11-02T20:23:06.772+0000",
                    "started": "2021-11-02T20:23:06.771+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "673805",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/674067",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741430380\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n\nReview comment:\n       nit: remove this TODO?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+};\n+\n+/// Implements processing of dictionary arrays/scalars in key columns on the build side of\n+/// a hash join.\n+/// Each instance of this class corresponds to a single column and stores and\n+/// processes only the information related to that column.\n+/// Const methods are thread-safe, non-const methods are not (the caller must make sure\n+/// that only one thread at any time will access them).\n+///\n\nReview comment:\n       FWIW, thanks for the detailed comments in this file - they help a lot in understanding what's going on here.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n\nReview comment:\n       n.b. arrow::dictionary(index_ty, value_ty) is shorthand for this\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n\nReview comment:\n       I might be dense but what does \"Cvt\" stand for?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n\nReview comment:\n       Does this need to be in the header at all then? It seems it could live solely in the .cc file.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n+  auto uint8_utf8 = std::make_shared<DictionaryType>(uint8(), utf8());\n+  auto int16_utf8 = std::make_shared<DictionaryType>(int16(), utf8());\n+  auto uint16_utf8 = std::make_shared<DictionaryType>(uint16(), utf8());\n+  auto int32_utf8 = std::make_shared<DictionaryType>(int32(), utf8());\n+  auto uint32_utf8 = std::make_shared<DictionaryType>(uint32(), utf8());\n+  auto int64_utf8 = std::make_shared<DictionaryType>(int64(), utf8());\n+  auto uint64_utf8 = std::make_shared<DictionaryType>(uint64(), utf8());\n+  std::shared_ptr<DictionaryType> dict_types[] = {int8_utf8,   uint8_utf8, int16_utf8,\n+                                                  uint16_utf8, int32_utf8, uint32_utf8,\n+                                                  int64_utf8,  uint64_utf8};\n+\n+  Random64Bit rng(43);\n+\n+  // Dictionaries in payload columns\n+  for (auto parallel : {false, true})\n\nReview comment:\n       nit: can we insert braces here for readability?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-02T21:36:36.092+0000",
                    "updated": "2021-11-02T21:36:36.092+0000",
                    "started": "2021-11-02T21:36:36.091+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674067",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/674283",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741544566\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n\nReview comment:\n       Moved out of the header\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n\nReview comment:\n       Cvt stands for \"Convert\". I probably wrote it like that, because I am used to SIMD intrinsics with \"cvt\" in the name doing conversion between integers.\r\n   \r\n   I changed \"Cvt\"s in the names to \"Convert\"s.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n+  auto uint8_utf8 = std::make_shared<DictionaryType>(uint8(), utf8());\n+  auto int16_utf8 = std::make_shared<DictionaryType>(int16(), utf8());\n+  auto uint16_utf8 = std::make_shared<DictionaryType>(uint16(), utf8());\n+  auto int32_utf8 = std::make_shared<DictionaryType>(int32(), utf8());\n+  auto uint32_utf8 = std::make_shared<DictionaryType>(uint32(), utf8());\n+  auto int64_utf8 = std::make_shared<DictionaryType>(int64(), utf8());\n+  auto uint64_utf8 = std::make_shared<DictionaryType>(uint64(), utf8());\n+  std::shared_ptr<DictionaryType> dict_types[] = {int8_utf8,   uint8_utf8, int16_utf8,\n+                                                  uint16_utf8, int32_utf8, uint32_utf8,\n+                                                  int64_utf8,  uint64_utf8};\n+\n+  Random64Bit rng(43);\n+\n+  // Dictionaries in payload columns\n+  for (auto parallel : {false, true})\n\nReview comment:\n       added\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n\nReview comment:\n       changed\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n\nReview comment:\n       done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T00:13:14.020+0000",
                    "updated": "2021-11-03T00:13:14.020+0000",
                    "started": "2021-11-03T00:13:14.020+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674283",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/674630",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741977708\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       This leads to a linter error. Maybe an anonymous namespace above would be more convenient?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-03T14:10:29.335+0000",
                    "updated": "2021-11-03T14:10:29.335+0000",
                    "started": "2021-11-03T14:10:29.334+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "674630",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/675183",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741544566\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n+      const std::shared_ptr<DataType>& from_type, const Datum& input,\n+      int64_t batch_length, ExecContext* ctx);\n+\n+  // Return an array that contains elements of input int32() array after casting to a\n+  // given integer type. This is used for mapping unified representation stored in the\n+  // hash table on build side back to original input data type of hash join, when\n+  // outputting hash join results to parent exec node.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> CvtFromInt32(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n+\n+  // Return dictionary referenced in either dictionary array or dictionary scalar\n+  static std::shared_ptr<Array> ExtractDictionary(const Datum& data);\n+\n+ private:\n+  template <typename FROM, typename TO>\n+  static Result<std::shared_ptr<ArrayData>> CvtImp(\n+      const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+      ExecContext* ctx);\n\nReview comment:\n       Moved out of the header\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.h\n##########\n@@ -0,0 +1,321 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#pragma once\n+\n+#include <memory>\n+#include <unordered_map>\n+\n+#include \"arrow/compute/exec.h\"\n+#include \"arrow/compute/exec/schema_util.h\"\n+#include \"arrow/compute/kernels/row_encoder.h\"\n+#include \"arrow/result.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+\n+// This file contains hash join logic related to handling of dictionary encoded key\n+// columns.\n+//\n+// A key column from probe side of the join can be matched against a key column from build\n+// side of the join, as long as the underlying value types are equal. That means that:\n+// - both scalars and arrays can be used and even mixed in the same column\n+// - dictionary column can be matched against non-dictionary column if underlying value\n+// types are equal\n+// - dictionary column can be matched against dictionary column with a different index\n+// type, and potentially using a different dictionary, if underlying value types are equal\n+//\n+// We currently require in hash that for all dictionary encoded columns, the same\n+// dictionary is used in all input exec batches.\n+//\n+// In order to allow matching columns with different dictionaries, different dictionary\n+// index types, and dictionary key against non-dictionary key, internally comparisons will\n+// be evaluated after remapping values on both sides of the join to a common\n+// representation (which will be called \"unified representation\"). This common\n+// representation is a column of int32() type (not a dictionary column). It represents an\n+// index in the unified dictionary computed for the (only) dictionary present on build\n+// side (an empty dictionary is still created for an empty build side). Null value is\n+// always represented in this common representation as null int32 value, unified\n+// dictionary will never contain a null value (so there is no ambiguity of representing\n+// nulls as either index to a null entry in the dictionary or null index).\n+//\n+// Unified dictionary represents values present on build side. There may be values on\n+// probe side that are not present in it. All such values, that are not null, are mapped\n+// in the common representation to a special constant kMissingValueId.\n+//\n+\n+namespace arrow {\n+namespace compute {\n+\n+using internal::RowEncoder;\n+\n+/// Helper class with operations that are stateless and common to processing of dictionary\n+/// keys on both build and probe side.\n+class HashJoinDictUtil {\n+ public:\n+  // Null values in unified representation are always represented as null that has\n+  // corresponding integer set to this constant\n+  static constexpr int32_t kNullId = 0;\n+  // Constant representing a value, that is not null, missing on the build side, in\n+  // unified representation.\n+  static constexpr int32_t kMissingValueId = -1;\n+\n+  // Check if data types of corresponding pair of key column on build and probe side are\n+  // compatible\n+  static bool KeyDataTypesValid(const std::shared_ptr<DataType>& probe_data_type,\n+                                const std::shared_ptr<DataType>& build_data_type);\n+\n+  // Input must be dictionary array or dictionary scalar.\n+  // A precomputed and provided here lookup table in the form of int32() array will be\n+  // used to remap input indices to unified representation.\n+  //\n+  static Result<std::shared_ptr<ArrayData>> IndexRemapUsingLUT(\n+      ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+      const std::shared_ptr<ArrayData>& map_array,\n+      const std::shared_ptr<DataType>& data_type);\n+\n+  // Return int32() array that contains indices of input dictionary array or scalar after\n+  // type casting.\n+  static Result<std::shared_ptr<ArrayData>> CvtToInt32(\n\nReview comment:\n       Cvt stands for \"Convert\". I probably wrote it like that, because I am used to SIMD intrinsics with \"cvt\" in the name doing conversion between integers.\r\n   \r\n   I changed \"Cvt\"s in the names to \"Convert\"s.\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n+  auto uint8_utf8 = std::make_shared<DictionaryType>(uint8(), utf8());\n+  auto int16_utf8 = std::make_shared<DictionaryType>(int16(), utf8());\n+  auto uint16_utf8 = std::make_shared<DictionaryType>(uint16(), utf8());\n+  auto int32_utf8 = std::make_shared<DictionaryType>(int32(), utf8());\n+  auto uint32_utf8 = std::make_shared<DictionaryType>(uint32(), utf8());\n+  auto int64_utf8 = std::make_shared<DictionaryType>(int64(), utf8());\n+  auto uint64_utf8 = std::make_shared<DictionaryType>(uint64(), utf8());\n+  std::shared_ptr<DictionaryType> dict_types[] = {int8_utf8,   uint8_utf8, int16_utf8,\n+                                                  uint16_utf8, int32_utf8, uint32_utf8,\n+                                                  int64_utf8,  uint64_utf8};\n+\n+  Random64Bit rng(43);\n+\n+  // Dictionaries in payload columns\n+  for (auto parallel : {false, true})\n\nReview comment:\n       added\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n+}\n+\n+TEST(HashJoin, Dictionary) {\n+  auto int8_utf8 = std::make_shared<DictionaryType>(int8(), utf8());\n\nReview comment:\n       changed\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_node_test.cc\n##########\n@@ -1113,5 +1113,539 @@ TEST(HashJoin, Random) {\n   }\n }\n \n+void DecodeScalarsAndDictionariesInBatch(ExecBatch* batch, MemoryPool* pool) {\n+  for (size_t i = 0; i < batch->values.size(); ++i) {\n+    if (batch->values[i].is_scalar()) {\n+      ASSERT_OK_AND_ASSIGN(\n+          std::shared_ptr<Array> col,\n+          MakeArrayFromScalar(*(batch->values[i].scalar()), batch->length, pool));\n+      batch->values[i] = Datum(col);\n+    }\n+    if (batch->values[i].type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type =\n+          checked_cast<const DictionaryType&>(*batch->values[i].type());\n+      std::shared_ptr<ArrayData> indices =\n+          ArrayData::Make(dict_type.index_type(), batch->values[i].array()->length,\n+                          batch->values[i].array()->buffers);\n+      const std::shared_ptr<ArrayData>& dictionary = batch->values[i].array()->dictionary;\n+      ASSERT_OK_AND_ASSIGN(Datum col, Take(*dictionary, *indices));\n+      batch->values[i] = col;\n+    }\n+  }\n+}\n+\n+std::shared_ptr<Schema> UpdateSchemaAfterDecodingDictionaries(\n+    const std::shared_ptr<Schema>& schema) {\n+  std::vector<std::shared_ptr<Field>> output_fields(schema->num_fields());\n+  for (int i = 0; i < schema->num_fields(); ++i) {\n+    const std::shared_ptr<Field>& field = schema->field(i);\n+    if (field->type()->id() == Type::DICTIONARY) {\n+      const auto& dict_type = checked_cast<const DictionaryType&>(*field->type());\n+      output_fields[i] = std::make_shared<Field>(field->name(), dict_type.value_type(),\n+                                                 true /* nullable */);\n+    } else {\n+      output_fields[i] = field->Copy();\n+    }\n+  }\n+  return std::make_shared<Schema>(std::move(output_fields));\n+}\n+\n+void TestHashJoinDictionaryHelper(\n+    JoinType join_type, JoinKeyCmp cmp,\n+    // Whether to run parallel hash join.\n+    // This requires generating multiple copies of each input batch on one side of the\n+    // join. Expected results will be automatically adjusted to reflect the multiplication\n+    // of input batches.\n+    bool parallel, Datum l_key, Datum l_payload, Datum r_key, Datum r_payload,\n+    Datum l_out_key, Datum l_out_payload, Datum r_out_key, Datum r_out_payload,\n+    // Number of rows at the end of expected output that represent rows from the right\n+    // side that do not have a match on the left side. This number is needed to\n+    // automatically adjust expected result when multiplying input batches on the left\n+    // side.\n+    int expected_num_r_no_match,\n+    // Whether to swap two inputs to the hash join\n+    bool swap_sides) {\n+  int64_t l_length = l_key.is_array()\n+                         ? l_key.array()->length\n+                         : l_payload.is_array() ? l_payload.array()->length : -1;\n+  int64_t r_length = r_key.is_array()\n+                         ? r_key.array()->length\n+                         : r_payload.is_array() ? r_payload.array()->length : -1;\n+  ARROW_DCHECK(l_length >= 0 && r_length >= 0);\n+\n+  constexpr int batch_multiplicity_for_parallel = 2;\n+\n+  // Split both sides into exactly two batches\n+  int64_t l_first_length = l_length / 2;\n+  int64_t r_first_length = r_length / 2;\n+  BatchesWithSchema l_batches, r_batches;\n+  l_batches.batches.resize(2);\n+  r_batches.batches.resize(2);\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[0],\n+      ExecBatch::Make({l_key.is_array() ? l_key.array()->Slice(0, l_first_length) : l_key,\n+                       l_payload.is_array() ? l_payload.array()->Slice(0, l_first_length)\n+                                            : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      l_batches.batches[1],\n+      ExecBatch::Make(\n+          {l_key.is_array()\n+               ? l_key.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_key,\n+           l_payload.is_array()\n+               ? l_payload.array()->Slice(l_first_length, l_length - l_first_length)\n+               : l_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[0],\n+      ExecBatch::Make({r_key.is_array() ? r_key.array()->Slice(0, r_first_length) : r_key,\n+                       r_payload.is_array() ? r_payload.array()->Slice(0, r_first_length)\n+                                            : r_payload}));\n+  ASSERT_OK_AND_ASSIGN(\n+      r_batches.batches[1],\n+      ExecBatch::Make(\n+          {r_key.is_array()\n+               ? r_key.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_key,\n+           r_payload.is_array()\n+               ? r_payload.array()->Slice(r_first_length, r_length - r_first_length)\n+               : r_payload}));\n+  l_batches.schema =\n+      schema({field(\"l_key\", l_key.type()), field(\"l_payload\", l_payload.type())});\n+  r_batches.schema =\n+      schema({field(\"r_key\", r_key.type()), field(\"r_payload\", r_payload.type())});\n+\n+  // Add copies of input batches on originally left side of the hash join\n+  if (parallel) {\n+    for (int i = 0; i < batch_multiplicity_for_parallel - 1; ++i) {\n+      l_batches.batches.push_back(l_batches.batches[0]);\n+      l_batches.batches.push_back(l_batches.batches[1]);\n+    }\n+  }\n+\n+  auto exec_ctx = arrow::internal::make_unique<ExecContext>(\n+      default_memory_pool(), parallel ? arrow::internal::GetCpuThreadPool() : nullptr);\n+  ASSERT_OK_AND_ASSIGN(auto plan, ExecPlan::Make(exec_ctx.get()));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * l_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{l_batches.schema, l_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  ASSERT_OK_AND_ASSIGN(\n+      ExecNode * r_source,\n+      MakeExecNode(\"source\", plan.get(), {},\n+                   SourceNodeOptions{r_batches.schema, r_batches.gen(parallel,\n+                                                                     /*slow=*/false)}));\n+  HashJoinNodeOptions join_options{join_type,\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\")},\n+                                   {FieldRef(swap_sides ? \"r_key\" : \"l_key\"),\n+                                    FieldRef(swap_sides ? \"r_payload\" : \"l_payload\")},\n+                                   {FieldRef(swap_sides ? \"l_key\" : \"r_key\"),\n+                                    FieldRef(swap_sides ? \"l_payload\" : \"r_payload\")},\n+                                   {cmp}};\n+  ASSERT_OK_AND_ASSIGN(ExecNode * join, MakeExecNode(\"hashjoin\", plan.get(),\n+                                                     {(swap_sides ? r_source : l_source),\n+                                                      (swap_sides ? l_source : r_source)},\n+                                                     join_options));\n+  AsyncGenerator<util::optional<ExecBatch>> sink_gen;\n+  ASSERT_OK_AND_ASSIGN(\n+      std::ignore, MakeExecNode(\"sink\", plan.get(), {join}, SinkNodeOptions{&sink_gen}));\n+  ASSERT_FINISHES_OK_AND_ASSIGN(auto res, StartAndCollect(plan.get(), sink_gen));\n+\n+  for (auto& batch : res) {\n+    DecodeScalarsAndDictionariesInBatch(&batch, exec_ctx->memory_pool());\n+  }\n+  std::shared_ptr<Schema> output_schema =\n+      UpdateSchemaAfterDecodingDictionaries(join->output_schema());\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> output,\n+                       TableFromExecBatches(output_schema, res));\n+\n+  ExecBatch expected_batch;\n+  if (swap_sides) {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({r_out_key, r_out_payload,\n+                                                          l_out_key, l_out_payload}));\n+  } else {\n+    ASSERT_OK_AND_ASSIGN(expected_batch, ExecBatch::Make({l_out_key, l_out_payload,\n+                                                          r_out_key, r_out_payload}));\n+  }\n+\n+  DecodeScalarsAndDictionariesInBatch(&expected_batch, exec_ctx->memory_pool());\n+\n+  // Slice expected batch into two to separate rows on right side with no matches from\n+  // everything else.\n+  //\n+  std::vector<ExecBatch> expected_batches;\n+  ASSERT_OK_AND_ASSIGN(\n+      auto prefix_batch,\n+      ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[1].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[2].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match),\n+                       expected_batch.values[3].array()->Slice(\n+                           0, expected_batch.length - expected_num_r_no_match)}));\n+  for (int i = 0; i < (parallel ? batch_multiplicity_for_parallel : 1); ++i) {\n+    expected_batches.push_back(prefix_batch);\n+  }\n+  if (expected_num_r_no_match > 0) {\n+    ASSERT_OK_AND_ASSIGN(\n+        auto suffix_batch,\n+        ExecBatch::Make({expected_batch.values[0].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[1].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[2].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match),\n+                         expected_batch.values[3].array()->Slice(\n+                             expected_batch.length - expected_num_r_no_match,\n+                             expected_num_r_no_match)}));\n+    expected_batches.push_back(suffix_batch);\n+  }\n+\n+  ASSERT_OK_AND_ASSIGN(std::shared_ptr<Table> expected,\n+                       TableFromExecBatches(output_schema, expected_batches));\n+\n+  // Compare results\n+  AssertTablesEqual(expected, output);\n+\n+  // TODO: This was added for debugging. Remove in the final version.\n+  // std::cout << output->ToString();\n\nReview comment:\n       done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T00:21:53.356+0000",
                    "updated": "2021-11-04T00:21:53.356+0000",
                    "started": "2021-11-04T00:21:53.356+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675183",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/675292",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741977708\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       This leads to a linter error. Maybe an anonymous namespace above would be more convenient?\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       This leads to a linter error. Maybe an anonymous namespace above would be more convenient?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T00:37:07.652+0000",
                    "updated": "2021-11-04T00:37:07.652+0000",
                    "started": "2021-11-04T00:37:07.652+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675292",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/675775",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r741977708\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       This leads to a linter error. Maybe an anonymous namespace above would be more convenient?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T01:27:27.327+0000",
                    "updated": "2021-11-04T01:27:27.327+0000",
                    "started": "2021-11-04T01:27:27.326+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "675775",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/676720",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r743189813\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-04T20:44:25.939+0000",
                    "updated": "2021-11-04T20:44:25.939+0000",
                    "started": "2021-11-04T20:44:25.939+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "676720",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677150",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson closed pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T18:37:43.115+0000",
                    "updated": "2021-11-05T18:37:43.115+0000",
                    "started": "2021-11-05T18:37:43.114+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677150",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677151",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot commented on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n   Benchmark runs are scheduled for baseline = 528625e6ed4bf1f6540f8a410a496a14712252e7 and contender = 230afef57f0ccc2135ced23093bac4298d5ba9e4. 230afef57f0ccc2135ced23093bac4298d5ba9e4 is a master commit associated with this PR. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Scheduled] [ec2-t3-xlarge-us-east-2](https://conbench.ursa.dev/compare/runs/342138961f8b4c3ea7ef3c5e411f86da...bbe0648555f74c47b4b6d3076d897bc3/)\n   [Scheduled] [ursa-i9-9960x](https://conbench.ursa.dev/compare/runs/b5e838509c2149e397eae3509995ed05...d782fdc52e314ffdb21748ee29313c18/)\n   [Scheduled] [ursa-thinkcentre-m75q](https://conbench.ursa.dev/compare/runs/f387dda1324749c19e8847e02c43aa40...cf78f551db64443799c96410f6bb6ac9/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R, JavaScript\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T18:38:26.585+0000",
                    "updated": "2021-11-05T18:38:26.585+0000",
                    "started": "2021-11-05T18:38:26.585+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677151",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677161",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n   Benchmark runs are scheduled for baseline = 528625e6ed4bf1f6540f8a410a496a14712252e7 and contender = 230afef57f0ccc2135ced23093bac4298d5ba9e4. 230afef57f0ccc2135ced23093bac4298d5ba9e4 is a master commit associated with this PR. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2](https://conbench.ursa.dev/compare/runs/342138961f8b4c3ea7ef3c5e411f86da...bbe0648555f74c47b4b6d3076d897bc3/)\n   [Scheduled] [ursa-i9-9960x](https://conbench.ursa.dev/compare/runs/b5e838509c2149e397eae3509995ed05...d782fdc52e314ffdb21748ee29313c18/)\n   [Scheduled] [ursa-thinkcentre-m75q](https://conbench.ursa.dev/compare/runs/f387dda1324749c19e8847e02c43aa40...cf78f551db64443799c96410f6bb6ac9/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R, JavaScript\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T18:55:21.144+0000",
                    "updated": "2021-11-05T18:55:21.144+0000",
                    "started": "2021-11-05T18:55:21.144+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677161",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677486",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson closed pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T20:00:44.113+0000",
                    "updated": "2021-11-05T20:00:44.113+0000",
                    "started": "2021-11-05T20:00:44.113+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677486",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677531",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "michalursa commented on a change in pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#discussion_r743189813\n\n\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       done\n\n##########\nFile path: cpp/src/arrow/compute/exec/hash_join_dict.cc\n##########\n@@ -0,0 +1,667 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/compute/exec/hash_join_dict.h\"\n+\n+#include <algorithm>\n+#include <cstdint>\n+#include <memory>\n+#include <unordered_map>\n+#include <vector>\n+\n+#include \"arrow/buffer.h\"\n+#include \"arrow/util/bit_util.h\"\n+#include \"arrow/util/checked_cast.h\"\n+\n+namespace arrow {\n+namespace compute {\n+\n+bool HashJoinDictUtil::KeyDataTypesValid(\n+    const std::shared_ptr<DataType>& probe_data_type,\n+    const std::shared_ptr<DataType>& build_data_type) {\n+  bool l_is_dict = (probe_data_type->id() == Type::DICTIONARY);\n+  bool r_is_dict = (build_data_type->id() == Type::DICTIONARY);\n+  DataType* l_type;\n+  if (l_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*probe_data_type);\n+    l_type = dict_type.value_type().get();\n+  } else {\n+    l_type = probe_data_type.get();\n+  }\n+  DataType* r_type;\n+  if (r_is_dict) {\n+    const auto& dict_type = checked_cast<const DictionaryType&>(*build_data_type);\n+    r_type = dict_type.value_type().get();\n+  } else {\n+    r_type = build_data_type.get();\n+  }\n+  return l_type->Equals(*r_type);\n+}\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::IndexRemapUsingLUT(\n+    ExecContext* ctx, const Datum& indices, int64_t batch_length,\n+    const std::shared_ptr<ArrayData>& map_array,\n+    const std::shared_ptr<DataType>& data_type) {\n+  ARROW_DCHECK(indices.is_array() || indices.is_scalar());\n+\n+  const uint8_t* map_non_nulls = map_array->buffers[0]->data();\n+  const int32_t* map = reinterpret_cast<const int32_t*>(map_array->buffers[1]->data());\n+\n+  ARROW_DCHECK(data_type->id() == Type::DICTIONARY);\n+  const auto& dict_type = checked_cast<const DictionaryType&>(*data_type);\n+\n+  ARROW_ASSIGN_OR_RAISE(\n+      std::shared_ptr<ArrayData> result,\n+      ConvertToInt32(dict_type.index_type(), indices, batch_length, ctx));\n+\n+  uint8_t* nns = result->buffers[0]->mutable_data();\n+  int32_t* ids = reinterpret_cast<int32_t*>(result->buffers[1]->mutable_data());\n+  for (int64_t i = 0; i < batch_length; ++i) {\n+    bool is_null = !BitUtil::GetBit(nns, i);\n+    if (is_null) {\n+      ids[i] = kNullId;\n+    } else {\n+      ARROW_DCHECK(ids[i] >= 0 && ids[i] < map_array->length);\n+      if (!BitUtil::GetBit(map_non_nulls, ids[i])) {\n+        BitUtil::ClearBit(nns, i);\n+        ids[i] = kNullId;\n+      } else {\n+        ids[i] = map[ids[i]];\n+      }\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+namespace HashJoinDictUtilImp {\n+template <typename FROM, typename TO>\n+static Result<std::shared_ptr<ArrayData>> ConvertImp(\n+    const std::shared_ptr<DataType>& to_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  ARROW_DCHECK(input.is_array() || input.is_scalar());\n+  bool is_scalar = input.is_scalar();\n+\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_buf,\n+                        AllocateBuffer(batch_length * sizeof(TO), ctx->memory_pool()));\n+  TO* to = reinterpret_cast<TO*>(to_buf->mutable_data());\n+  ARROW_ASSIGN_OR_RAISE(std::shared_ptr<Buffer> to_nn_buf,\n+                        AllocateBitmap(batch_length, ctx->memory_pool()));\n+  uint8_t* to_nn = to_nn_buf->mutable_data();\n+  memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+\n+  if (!is_scalar) {\n+    const ArrayData& arr = *input.array();\n+    const FROM* from = arr.GetValues<FROM>(1);\n+    DCHECK_EQ(arr.length, batch_length);\n+\n+    for (int64_t i = 0; i < arr.length; ++i) {\n+      to[i] = static_cast<TO>(from[i]);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to[i]) == from[i]);\n+\n+      bool is_null = (arr.buffers[0] != NULLPTR) &&\n+                     !BitUtil::GetBit(arr.buffers[0]->data(), arr.offset + i);\n+      if (is_null) {\n+        BitUtil::ClearBit(to_nn, i);\n+      }\n+    }\n+\n+    // Pass null buffer unchanged\n+    return ArrayData::Make(to_type, arr.length,\n+                           {std::move(to_nn_buf), std::move(to_buf)});\n+  } else {\n+    const auto& scalar = input.scalar_as<arrow::internal::PrimitiveScalarBase>();\n+    if (scalar.is_valid) {\n+      const util::string_view data = scalar.view();\n+      DCHECK_EQ(data.size(), sizeof(FROM));\n+      const FROM from = *reinterpret_cast<const FROM*>(data.data());\n+      const TO to_value = static_cast<TO>(from);\n+      // Make sure we did not lose information during cast\n+      ARROW_DCHECK(static_cast<FROM>(to_value) == from);\n+\n+      for (int64_t i = 0; i < batch_length; ++i) {\n+        to[i] = to_value;\n+      }\n+\n+      memset(to_nn, 0xff, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    } else {\n+      memset(to_nn, 0, BitUtil::BytesForBits(batch_length));\n+      return ArrayData::Make(to_type, batch_length,\n+                             {std::move(to_nn_buf), std::move(to_buf)});\n+    }\n+  }\n+}\n+}  // namespace HashJoinDictUtilImp\n+\n+Result<std::shared_ptr<ArrayData>> HashJoinDictUtil::ConvertToInt32(\n+    const std::shared_ptr<DataType>& from_type, const Datum& input, int64_t batch_length,\n+    ExecContext* ctx) {\n+  using namespace HashJoinDictUtilImp;\n\nReview comment:\n       done\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T20:05:59.372+0000",
                    "updated": "2021-11-05T20:05:59.372+0000",
                    "started": "2021-11-05T20:05:59.372+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677531",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677572",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot commented on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T20:10:38.781+0000",
                    "updated": "2021-11-05T20:10:38.781+0000",
                    "started": "2021-11-05T20:10:38.781+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677572",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677573",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T20:10:48.661+0000",
                    "updated": "2021-11-05T20:10:48.661+0000",
                    "started": "2021-11-05T20:10:48.661+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677573",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677856",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson closed pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446\n\n\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-05T20:45:29.555+0000",
                    "updated": "2021-11-05T20:45:29.555+0000",
                    "started": "2021-11-05T20:45:29.554+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677856",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/677995",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n   Benchmark runs are scheduled for baseline = 528625e6ed4bf1f6540f8a410a496a14712252e7 and contender = 230afef57f0ccc2135ced23093bac4298d5ba9e4. 230afef57f0ccc2135ced23093bac4298d5ba9e4 is a master commit associated with this PR. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2](https://conbench.ursa.dev/compare/runs/342138961f8b4c3ea7ef3c5e411f86da...bbe0648555f74c47b4b6d3076d897bc3/)\n   [Scheduled] [ursa-i9-9960x](https://conbench.ursa.dev/compare/runs/b5e838509c2149e397eae3509995ed05...d782fdc52e314ffdb21748ee29313c18/)\n   [Finished :arrow_down:0.27% :arrow_up:0.04%] [ursa-thinkcentre-m75q](https://conbench.ursa.dev/compare/runs/f387dda1324749c19e8847e02c43aa40...cf78f551db64443799c96410f6bb6ac9/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R, JavaScript\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-06T02:07:28.044+0000",
                    "updated": "2021-11-06T02:07:28.044+0000",
                    "started": "2021-11-06T02:07:28.044+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "677995",
                    "issueId": "13404217"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/worklog/678040",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "ursabot edited a comment on pull request #11446:\nURL: https://github.com/apache/arrow/pull/11446#issuecomment-962128103\n\n\n   Benchmark runs are scheduled for baseline = 528625e6ed4bf1f6540f8a410a496a14712252e7 and contender = 230afef57f0ccc2135ced23093bac4298d5ba9e4. 230afef57f0ccc2135ced23093bac4298d5ba9e4 is a master commit associated with this PR. Results will be available as each benchmark for each run completes.\n   Conbench compare runs links:\n   [Finished :arrow_down:0.0% :arrow_up:0.0%] [ec2-t3-xlarge-us-east-2](https://conbench.ursa.dev/compare/runs/342138961f8b4c3ea7ef3c5e411f86da...bbe0648555f74c47b4b6d3076d897bc3/)\n   [Failed :arrow_down:3.08% :arrow_up:0.0%] [ursa-i9-9960x](https://conbench.ursa.dev/compare/runs/b5e838509c2149e397eae3509995ed05...d782fdc52e314ffdb21748ee29313c18/)\n   [Finished :arrow_down:0.27% :arrow_up:0.04%] [ursa-thinkcentre-m75q](https://conbench.ursa.dev/compare/runs/f387dda1324749c19e8847e02c43aa40...cf78f551db64443799c96410f6bb6ac9/)\n   Supported benchmarks:\n   ursa-i9-9960x: langs = Python, R, JavaScript\n   ursa-thinkcentre-m75q: langs = C++, Java\n   ec2-t3-xlarge-us-east-2: cloud = True\n   \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-11-06T07:23:34.553+0000",
                    "updated": "2021-11-06T07:23:34.553+0000",
                    "started": "2021-11-06T07:23:34.553+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "678040",
                    "issueId": "13404217"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 12000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@4db143d1[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5ef922db[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@714d6d39[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@66b57631[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@28dd9aae[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@36996275[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2ca7f53a[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@616e1a61[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@25a2f408[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@5855a003[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4bc89016[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@5a64361c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 12000,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Nov 10 13:19:15 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-11-05T18:37:29.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-14181/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-09-30T07:50:30.000+0000",
        "updated": "2021-11-10T13:19:15.000+0000",
        "timeoriginalestimate": null,
        "description": "Currently dictionary arrays are not supported at all as input columns to hash join.\r\n\r\nAdd support for dictionary arrays in hash join for both key columns and payload columns.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "3h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 12000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[C++][Compute] Hash Join support for dictionary\t",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17439417",
                    "id": "17439417",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
                        "name": "npr",
                        "key": "npr",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Neal Richardson",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 11446\n[https://github.com/apache/arrow/pull/11446]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
                        "name": "npr",
                        "key": "npr",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Neal Richardson",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-11-05T18:37:29.707+0000",
                    "updated": "2021-11-05T18:37:29.707+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17441447",
                    "id": "17441447",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "body": "This has conflicts on maint-6.0.x branch.\r\nIs this really needed for 6.0.1? It seems that this is not a bug fix.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "created": "2021-11-10T00:58:55.590+0000",
                    "updated": "2021-11-10T00:58:55.590+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17441448",
                    "id": "17441448",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "body": "This requires ARROW-14310 and ARROW-14310 requires something. Is this intended?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "created": "2021-11-10T01:06:36.598+0000",
                    "updated": "2021-11-10T01:06:36.598+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17441451",
                    "id": "17441451",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "body": "ARROW-14310 requires ARROW-13156.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=kou",
                        "name": "kou",
                        "key": "kou",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=kou&avatarId=30762",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=kou&avatarId=30762",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=kou&avatarId=30762",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=kou&avatarId=30762"
                        },
                        "displayName": "Kouhei Sutou",
                        "active": true,
                        "timeZone": "Asia/Tokyo"
                    },
                    "created": "2021-11-10T01:08:52.976+0000",
                    "updated": "2021-11-10T01:08:52.976+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17441479",
                    "id": "17441479",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "[~npr] [~michalno] [~jonkeane]\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-11-10T03:09:53.152+0000",
                    "updated": "2021-11-10T03:09:53.152+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13404217/comment/17441730",
                    "id": "17441730",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
                        "name": "npr",
                        "key": "npr",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Neal Richardson",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "I would like this in the patch release, it will appear as a bug to users. The conflict sounds like it is only in the R tests I updated in r/tests/testthat/test-dplyr-join.R (from https://github.com/apache/arrow/pull/11446/commits/8e688d6d4f146b0d76bf74002b1023447a351c77), so feel free to drop that R test change, it is not strictly necessary.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=npr",
                        "name": "npr",
                        "key": "npr",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Neal Richardson",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2021-11-10T13:19:15.519+0000",
                    "updated": "2021-11-10T13:19:15.519+0000"
                }
            ],
            "maxResults": 6,
            "total": 6,
            "startAt": 0
        },
        "customfield_12311820": "0|z0vg08:",
        "customfield_12314139": null
    }
}