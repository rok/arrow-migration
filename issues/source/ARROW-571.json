{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13044605",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605",
    "key": "ARROW-571",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12341352",
                "id": "12341352",
                "name": "0.8.0",
                "archived": false,
                "released": true,
                "releaseDate": "2017-12-18"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": null,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": null,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 0,
            "total": 0
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-571/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 0,
            "worklogs": []
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": null,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@552ee0a6[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7ad0a10b[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@1469da95[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@6bcee9ea[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@22bf882b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@15519a9c[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4f3647d1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@532b7664[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@d8b42fa[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@1417acad[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@69e27931[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@61f2ba3f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": null,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Oct 23 13:24:05 UTC 2017",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2017-10-23T13:24:00.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-571/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2017-02-20T17:03:03.000+0000",
        "updated": "2017-10-23T13:24:05.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {},
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Add APIs to build Parquet files incrementally from Arrow tables",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605/comment/15874824",
                    "id": "15874824",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "[~xhochy] there's probably more work to do here in Parquet than in Arrow, let me know what you think",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-02-20T17:08:12.737+0000",
                    "updated": "2017-02-20T17:08:12.737+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605/comment/15874884",
                    "id": "15874884",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "body": "Writing RowGroup-wise is already supported by {{parquet_arrow}}, just not exposed in Python. One thing we're missing in C++ is incrementally building up the schema.\n\nFor example we have the use case that we already know 20 columns that shall be written into the Parquet file. We can serialise these columns and free the memory associated with them. But several other columns (of same length of course) will be generated later in a pipeline but that first part of the pipeline is unaware how many and of which type. Currently we build a Pandas DataFrame until we have reached the end of the pipeline. Subsequent jobs also only read a subset of columns (but different combinations thereof). Directly writing out these columns are they are computed would help us save a lot of RAM. Related issue for that: https://issues.apache.org/jira/browse/PARQUET-749",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=uwe",
                        "name": "uwe",
                        "key": "xhochy",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=xhochy&avatarId=30652",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=xhochy&avatarId=30652",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=xhochy&avatarId=30652",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=xhochy&avatarId=30652"
                        },
                        "displayName": "Uwe Korn",
                        "active": true,
                        "timeZone": "Europe/Berlin"
                    },
                    "created": "2017-02-20T17:59:10.494+0000",
                    "updated": "2017-02-20T17:59:10.494+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605/comment/16213000",
                    "id": "16213000",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "GitHub user wesm opened a pull request:\n\n    https://github.com/apache/arrow/pull/1218\n\n    ARROW-571: [Python] Add unit test for incremental Parquet file building, improve docs\n\n    This was actually already documented. I improved the docstring for `ParquetWriter` and added a unit test to validate the user API. Added ParquetWriter to the API listing also\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/wesm/arrow ARROW-571\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/arrow/pull/1218.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #1218\n    \n----\ncommit f73dd3531aef24dc2375dbab3ee41f0362848be9\nAuthor: Wes McKinney <wes.mckinney@twosigma.com>\nDate:   2017-10-20T18:18:53Z\n\n    Add unit test for incremental Parquet file building\n    \n    Change-Id: I2e1c7b45f5b373ff2296db1dff4a074aa4b65f6e\n\n----\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-20T18:20:49.320+0000",
                    "updated": "2017-10-20T18:20:49.320+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605/comment/16215130",
                    "id": "16215130",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 1218\n[https://github.com/apache/arrow/pull/1218]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2017-10-23T13:24:00.836+0000",
                    "updated": "2017-10-23T13:24:00.836+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13044605/comment/16215131",
                    "id": "16215131",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "wesm closed pull request #1218: ARROW-571: [Python] Add unit test for incremental Parquet file building, improve docs\nURL: https://github.com/apache/arrow/pull/1218\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/python/doc/source/api.rst b/python/doc/source/api.rst\nindex 26ccb98ed..6bceba3c6 100644\n--- a/python/doc/source/api.rst\n+++ b/python/doc/source/api.rst\n@@ -312,6 +312,7 @@ Apache Parquet\n \n    ParquetDataset\n    ParquetFile\n+   ParquetWriter\n    read_table\n    read_metadata\n    read_pandas\ndiff --git a/python/pyarrow/parquet.py b/python/pyarrow/parquet.py\nindex 1584b849a..b6a7b1244 100644\n--- a/python/pyarrow/parquet.py\n+++ b/python/pyarrow/parquet.py\n@@ -202,17 +202,47 @@ def _sanitize_table(table, new_schema, flavor):\n         return table\n \n \n+_parquet_writer_arg_docs = \"\"\"version : {\"1.0\", \"2.0\"}, default \"1.0\"\n+    The Parquet format version, defaults to 1.0\n+use_dictionary : bool or list\n+    Specify if we should use dictionary encoding in general or only for\n+    some columns.\n+use_deprecated_int96_timestamps : boolean, default None\n+    Write nanosecond resolution timestamps to INT96 Parquet\n+    format. Defaults to False unless enabled by flavor argument\n+coerce_timestamps : string, default None\n+    Cast timestamps a particular resolution.\n+    Valid values: {None, 'ms', 'us'}\n+compression : str or dict\n+    Specify the compression codec, either on a general basis or per-column.\n+flavor : {'spark'}, default None\n+    Sanitize schema or set other compatibility options for compatibility\"\"\"\n+\n+\n class ParquetWriter(object):\n-    \"\"\"\n \n-    Parameters\n-    ----------\n-    where\n-    schema\n-    flavor : {'spark', ...}\n-        Set options for compatibility with a particular reader\n-    \"\"\"\n-    def __init__(self, where, schema, flavor=None, **options):\n+    __doc__ = \"\"\"\n+Class for incrementally building a Parquet file for Arrow tables\n+\n+Parameters\n+----------\n+where : path or file-like object\n+schema : arrow Schema\n+{0}\n+\"\"\".format(_parquet_writer_arg_docs)\n+\n+    def __init__(self, where, schema, flavor=None,\n+                 version='1.0',\n+                 use_dictionary=True,\n+                 compression='snappy',\n+                 use_deprecated_int96_timestamps=None, **options):\n+        if use_deprecated_int96_timestamps is None:\n+            # Use int96 timestamps for Spark\n+            if flavor is not None and 'spark' in flavor:\n+                use_deprecated_int96_timestamps = True\n+            else:\n+                use_deprecated_int96_timestamps = False\n+\n         self.flavor = flavor\n         if flavor is not None:\n             schema, self.schema_changed = _sanitize_schema(schema, flavor)\n@@ -220,15 +250,29 @@ def __init__(self, where, schema, flavor=None, **options):\n             self.schema_changed = False\n \n         self.schema = schema\n-        self.writer = _parquet.ParquetWriter(where, schema, **options)\n+        self.writer = _parquet.ParquetWriter(\n+            where, schema,\n+            version=version,\n+            compression=compression,\n+            use_dictionary=use_dictionary,\n+            use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n+            **options)\n+        self.is_open = True\n+\n+    def __del__(self):\n+        if self.is_open:\n+            self.close()\n \n     def write_table(self, table, row_group_size=None):\n         if self.schema_changed:\n             table = _sanitize_table(table, self.schema, self.flavor)\n+        assert self.is_open\n         self.writer.write_table(table, row_group_size=row_group_size)\n \n     def close(self):\n-        self.writer.close()\n+        if self.is_open:\n+            self.writer.close()\n+            self.is_open = False\n \n \n def _get_pandas_index_columns(keyvalues):\n@@ -857,52 +901,19 @@ def write_table(table, where, row_group_size=None, version='1.0',\n                 use_deprecated_int96_timestamps=None,\n                 coerce_timestamps=None,\n                 flavor=None, **kwargs):\n-    \"\"\"\n-    Write a Table to Parquet format\n-\n-    Parameters\n-    ----------\n-    table : pyarrow.Table\n-    where: string or pyarrow.io.NativeFile\n-    row_group_size : int, default None\n-        The maximum number of rows in each Parquet RowGroup. As a default,\n-        we will write a single RowGroup per file.\n-    version : {\"1.0\", \"2.0\"}, default \"1.0\"\n-        The Parquet format version, defaults to 1.0\n-    use_dictionary : bool or list\n-        Specify if we should use dictionary encoding in general or only for\n-        some columns.\n-    use_deprecated_int96_timestamps : boolean, default None\n-        Write nanosecond resolution timestamps to INT96 Parquet\n-        format. Defaults to False unless enabled by flavor argument\n-    coerce_timestamps : string, default None\n-        Cast timestamps a particular resolution.\n-        Valid values: {None, 'ms', 'us'}\n-    compression : str or dict\n-        Specify the compression codec, either on a general basis or per-column.\n-    flavor : {'spark'}, default None\n-        Sanitize schema or set other compatibility options for compatibility\n-    \"\"\"\n-    row_group_size = kwargs.get('chunk_size', row_group_size)\n-\n-    if use_deprecated_int96_timestamps is None:\n-        # Use int96 timestamps for Spark\n-        if flavor is not None and 'spark' in flavor:\n-            use_deprecated_int96_timestamps = True\n-        else:\n-            use_deprecated_int96_timestamps = False\n-\n-    options = dict(\n-        use_dictionary=use_dictionary,\n-        compression=compression,\n-        version=version,\n-        use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n-        coerce_timestamps=coerce_timestamps)\n+    row_group_size = kwargs.pop('chunk_size', row_group_size)\n \n     writer = None\n     try:\n-        writer = ParquetWriter(where, table.schema, flavor=flavor,\n-                               **options)\n+        writer = ParquetWriter(\n+            where, table.schema,\n+            version=version,\n+            flavor=flavor,\n+            use_dictionary=use_dictionary,\n+            coerce_timestamps=coerce_timestamps,\n+            compression=compression,\n+            use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n+            **kwargs)\n         writer.write_table(table, row_group_size=row_group_size)\n     except:\n         if writer is not None:\n@@ -917,6 +928,17 @@ def write_table(table, where, row_group_size=None, version='1.0',\n         writer.close()\n \n \n+write_table.__doc__ = \"\"\"\n+Write a Table to Parquet format\n+\n+Parameters\n+----------\n+table : pyarrow.Table\n+where: string or pyarrow.io.NativeFile\n+{0}\n+\"\"\".format(_parquet_writer_arg_docs)\n+\n+\n def write_to_dataset(table, root_path, partition_cols=None,\n                      filesystem=None, preserve_index=True, **kwargs):\n     \"\"\"\n@@ -1013,12 +1035,10 @@ def write_metadata(schema, where, version='1.0',\n         Cast timestamps a particular resolution.\n         Valid values: {None, 'ms', 'us'}\n     \"\"\"\n-    options = dict(\n-        version=version,\n+    writer = ParquetWriter(\n+        where, schema, version=version,\n         use_deprecated_int96_timestamps=use_deprecated_int96_timestamps,\n-        coerce_timestamps=coerce_timestamps\n-    )\n-    writer = ParquetWriter(where, schema, **options)\n+        coerce_timestamps=coerce_timestamps)\n     writer.close()\n \n \ndiff --git a/python/pyarrow/tests/test_parquet.py b/python/pyarrow/tests/test_parquet.py\nindex deb4b3f35..09184cc05 100644\n--- a/python/pyarrow/tests/test_parquet.py\n+++ b/python/pyarrow/tests/test_parquet.py\n@@ -301,6 +301,35 @@ def test_pandas_parquet_native_file_roundtrip(tmpdir):\n \n \n @parquet\n+def test_parquet_incremental_file_build(tmpdir):\n+    import pyarrow.parquet as pq\n+\n+    df = _test_dataframe(100)\n+    df['unique_id'] = 0\n+\n+    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n+    out = pa.BufferOutputStream()\n+\n+    writer = pq.ParquetWriter(out, arrow_table.schema, version='2.0')\n+\n+    frames = []\n+    for i in range(10):\n+        df['unique_id'] = i\n+        arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n+        writer.write_table(arrow_table)\n+\n+        frames.append(df.copy())\n+\n+    writer.close()\n+\n+    buf = out.get_result()\n+    result = _read_table(pa.BufferReader(buf))\n+\n+    expected = pd.concat(frames, ignore_index=True)\n+    tm.assert_frame_equal(result.to_pandas(), expected)\n+\n+\n+@parquet\n def test_read_pandas_column_subset(tmpdir):\n     import pyarrow.parquet as pq\n \ndiff --git a/python/pyarrow/tests/test_serialization.py b/python/pyarrow/tests/test_serialization.py\nindex 9321ebc34..67798ac31 100644\n--- a/python/pyarrow/tests/test_serialization.py\n+++ b/python/pyarrow/tests/test_serialization.py\n@@ -266,30 +266,36 @@ def test_default_dict_serialization(large_memory_map):\n \n def test_numpy_serialization(large_memory_map):\n     with pa.memory_map(large_memory_map, mode=\"r+\") as mmap:\n-        for t in [\"bool\", \"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\", \"uint32\",\n-                  \"float16\", \"float32\", \"float64\"]:\n+        for t in [\"bool\", \"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\",\n+                  \"uint32\", \"float16\", \"float32\", \"float64\"]:\n             obj = np.random.randint(0, 10, size=(100, 100)).astype(t)\n             serialization_roundtrip(obj, mmap)\n \n \n def test_datetime_serialization(large_memory_map):\n-    data = [# Principia Mathematica published\n-            datetime.datetime(year=1687, month=7, day=5),\n-            # Some random date\n-            datetime.datetime(year=1911, month=6, day=3, hour=4,\n-                              minute=55, second=44),\n-            # End of WWI\n-            datetime.datetime(year=1918, month=11, day=11),\n-            # Beginning of UNIX time\n-            datetime.datetime(year=1970, month=1, day=1),\n-            # The Berlin wall falls\n-            datetime.datetime(year=1989, month=11, day=9),\n-            # Another random date\n-            datetime.datetime(year=2011, month=6, day=3, hour=4,\n-                              minute=0, second=3),\n-            # Another random date\n-            datetime.datetime(year=1970, month=1, day=3, hour=4,\n-                              minute=0, second=0)]\n+    data = [\n+        #  Principia Mathematica published\n+        datetime.datetime(year=1687, month=7, day=5),\n+\n+        # Some random date\n+        datetime.datetime(year=1911, month=6, day=3, hour=4,\n+                          minute=55, second=44),\n+        # End of WWI\n+        datetime.datetime(year=1918, month=11, day=11),\n+\n+        # Beginning of UNIX time\n+        datetime.datetime(year=1970, month=1, day=1),\n+\n+        # The Berlin wall falls\n+        datetime.datetime(year=1989, month=11, day=9),\n+\n+        # Another random date\n+        datetime.datetime(year=2011, month=6, day=3, hour=4,\n+                          minute=0, second=3),\n+        # Another random date\n+        datetime.datetime(year=1970, month=1, day=3, hour=4,\n+                          minute=0, second=0)\n+    ]\n     with pa.memory_map(large_memory_map, mode=\"r+\") as mmap:\n         for d in data:\n             serialization_roundtrip(d, mmap)\n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2017-10-23T13:24:05.306+0000",
                    "updated": "2017-10-23T13:24:05.306+0000"
                }
            ],
            "maxResults": 5,
            "total": 5,
            "startAt": 0
        },
        "customfield_12311820": "0|i3ac6v:",
        "customfield_12314139": null
    }
}