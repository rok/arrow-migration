{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13176744",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744",
    "key": "ARROW-2971",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343066",
                "id": "12343066",
                "description": "",
                "name": "0.11.0",
                "archived": false,
                "released": true,
                "releaseDate": "2018-10-08"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 2400,
            "total": 2400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-2971/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 4,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744/worklog/135933",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm opened a new pull request #2446: ARROW-2971: [Python] Give some modules in arrow/python more descriptive names\nURL: https://github.com/apache/arrow/pull/2446\n \n \n   Follow-up work to ARROW-2814. Renaming files only\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-18T00:00:17.329+0000",
                    "updated": "2018-08-18T00:00:17.329+0000",
                    "started": "2018-08-18T00:00:17.328+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135933",
                    "issueId": "13176744"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744/worklog/135999",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on issue #2446: ARROW-2971: [Python] Give some modules in arrow/python more descriptive names\nURL: https://github.com/apache/arrow/pull/2446#issuecomment-414071982\n \n \n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=h1) Report\n   > Merging [#2446](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=desc) into [master](https://codecov.io/gh/apache/arrow/commit/16bbec8da439879870649e94d07b3d571b2b3907?src=pr&el=desc) will **increase** coverage by `1.28%`.\n   > The diff coverage is `89.11%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/2446/graphs/tree.svg?width=650&token=LpTCFbqVT1&height=150&src=pr)](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #2446      +/-   ##\n   ==========================================\n   + Coverage   85.49%   86.78%   +1.28%     \n   ==========================================\n     Files         301      241      -60     \n     Lines       46273    42748    -3525     \n   ==========================================\n   - Hits        39563    37097    -2466     \n   + Misses       6636     5651     -985     \n   + Partials       74        0      -74\n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [cpp/src/arrow/python/arrow\\_to\\_pandas.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vYXJyb3dfdG9fcGFuZGFzLmNj) | `88.72% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/python/python-test.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vcHl0aG9uLXRlc3QuY2M=) | `100% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/python/numpy\\_to\\_arrow.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vbnVtcHlfdG9fYXJyb3cuY2M=) | `94.11% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/python/deserialize.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vZGVzZXJpYWxpemUuY2M=) | `91.62% <\u00f8> (\u00f8)` | |\n   | [cpp/src/arrow/python/decimal.h](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vZGVjaW1hbC5o) | `100% <\u00f8> (\u00f8)` | :arrow_up: |\n   | [cpp/src/arrow/python/serialize.h](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vc2VyaWFsaXplLmg=) | `0% <0%> (\u00f8)` | |\n   | [cpp/src/arrow/python/python\\_to\\_arrow.h](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vcHl0aG9uX3RvX2Fycm93Lmg=) | `100% <100%> (+100%)` | :arrow_up: |\n   | [cpp/src/arrow/python/serialize.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vc2VyaWFsaXplLmNj) | `88.88% <88.88%> (\u00f8)` | |\n   | [cpp/src/arrow/python/python\\_to\\_arrow.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9weXRob24vcHl0aG9uX3RvX2Fycm93LmNj) | `89.52% <89.51%> (+0.63%)` | :arrow_up: |\n   | [cpp/src/arrow/compute/kernels/cast.cc](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree#diff-Y3BwL3NyYy9hcnJvdy9jb21wdXRlL2tlcm5lbHMvY2FzdC5jYw==) | `90.82% <0%> (-0.72%)` | :arrow_down: |\n   | ... and [69 more](https://codecov.io/gh/apache/arrow/pull/2446/diff?src=pr&el=tree-more) | |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=footer). Last update [16bbec8...d13016b](https://codecov.io/gh/apache/arrow/pull/2446?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-18T17:00:42.122+0000",
                    "updated": "2018-08-18T17:00:42.122+0000",
                    "started": "2018-08-18T17:00:42.121+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "135999",
                    "issueId": "13176744"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744/worklog/136002",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #2446: ARROW-2971: [Python] Give some modules in arrow/python more descriptive names\nURL: https://github.com/apache/arrow/pull/2446#issuecomment-414074850\n \n \n   +1\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-18T17:44:33.044+0000",
                    "updated": "2018-08-18T17:44:33.044+0000",
                    "started": "2018-08-18T17:44:33.043+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136002",
                    "issueId": "13176744"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744/worklog/136003",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm closed pull request #2446: ARROW-2971: [Python] Give some modules in arrow/python more descriptive names\nURL: https://github.com/apache/arrow/pull/2446\n \n \n   \n\nThis is a PR merged from a forked repository.\nAs GitHub hides the original diff on merge, it is displayed below for\nthe sake of provenance:\n\nAs this is a foreign pull request (from a fork), the diff is supplied\nbelow (as it won't show otherwise due to GitHub magic):\n\ndiff --git a/cpp/src/arrow/python/CMakeLists.txt b/cpp/src/arrow/python/CMakeLists.txt\nindex 198e8ed432..d6dcf2f32d 100644\n--- a/cpp/src/arrow/python/CMakeLists.txt\n+++ b/cpp/src/arrow/python/CMakeLists.txt\n@@ -24,12 +24,11 @@ find_package(NumPy REQUIRED)\n \n set(ARROW_PYTHON_SRCS\n   arrow_to_pandas.cc\n-  arrow_to_python.cc\n   benchmark.cc\n-  builtin_convert.cc\n   common.cc\n   config.cc\n   decimal.cc\n+  deserialize.cc\n   helpers.cc\n   inference.cc\n   init.cc\n@@ -38,6 +37,7 @@ set(ARROW_PYTHON_SRCS\n   numpy_to_arrow.cc\n   python_to_arrow.cc\n   pyarrow.cc\n+  serialize.cc\n )\n \n if (\"${COMPILER_FAMILY}\" STREQUAL \"clang\")\n@@ -82,12 +82,11 @@ endif()\n install(FILES\n   api.h\n   arrow_to_pandas.h\n-  arrow_to_python.h\n   benchmark.h\n-  builtin_convert.h\n   common.h\n   config.h\n   decimal.h\n+  deserialize.h\n   helpers.h\n   inference.h\n   init.h\n@@ -99,6 +98,7 @@ install(FILES\n   python_to_arrow.h\n   platform.h\n   pyarrow.h\n+  serialize.h\n   type_traits.h\n   DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}/arrow/python\")\n \ndiff --git a/cpp/src/arrow/python/api.h b/cpp/src/arrow/python/api.h\nindex fd3895b2f4..6bbfcbfa34 100644\n--- a/cpp/src/arrow/python/api.h\n+++ b/cpp/src/arrow/python/api.h\n@@ -19,14 +19,14 @@\n #define ARROW_PYTHON_API_H\n \n #include \"arrow/python/arrow_to_pandas.h\"\n-#include \"arrow/python/arrow_to_python.h\"\n-#include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/common.h\"\n+#include \"arrow/python/deserialize.h\"\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/inference.h\"\n #include \"arrow/python/io.h\"\n #include \"arrow/python/numpy_convert.h\"\n #include \"arrow/python/numpy_to_arrow.h\"\n #include \"arrow/python/python_to_arrow.h\"\n+#include \"arrow/python/serialize.h\"\n \n #endif  // ARROW_PYTHON_API_H\ndiff --git a/cpp/src/arrow/python/arrow_to_pandas.cc b/cpp/src/arrow/python/arrow_to_pandas.cc\nindex eefeacc92b..9fb919463a 100644\n--- a/cpp/src/arrow/python/arrow_to_pandas.cc\n+++ b/cpp/src/arrow/python/arrow_to_pandas.cc\n@@ -45,13 +45,13 @@\n \n #include \"arrow/compute/api.h\"\n \n-#include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/common.h\"\n #include \"arrow/python/config.h\"\n #include \"arrow/python/decimal.h\"\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/numpy-internal.h\"\n #include \"arrow/python/numpy_convert.h\"\n+#include \"arrow/python/python_to_arrow.h\"\n #include \"arrow/python/type_traits.h\"\n #include \"arrow/python/util/datetime.h\"\n \ndiff --git a/cpp/src/arrow/python/builtin_convert.cc b/cpp/src/arrow/python/builtin_convert.cc\ndeleted file mode 100644\nindex ffc1a5f459..0000000000\n--- a/cpp/src/arrow/python/builtin_convert.cc\n+++ /dev/null\n@@ -1,970 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-#include \"arrow/python/builtin_convert.h\"\n-#include \"arrow/python/numpy_interop.h\"\n-\n-#include <datetime.h>\n-\n-#include <algorithm>\n-#include <limits>\n-#include <map>\n-#include <sstream>\n-#include <string>\n-#include <utility>\n-#include <vector>\n-\n-#include \"arrow/array.h\"\n-#include \"arrow/builder.h\"\n-#include \"arrow/status.h\"\n-#include \"arrow/table.h\"\n-#include \"arrow/type.h\"\n-#include \"arrow/type_traits.h\"\n-#include \"arrow/util/checked_cast.h\"\n-#include \"arrow/util/decimal.h\"\n-#include \"arrow/util/logging.h\"\n-\n-#include \"arrow/python/decimal.h\"\n-#include \"arrow/python/helpers.h\"\n-#include \"arrow/python/inference.h\"\n-#include \"arrow/python/iterators.h\"\n-#include \"arrow/python/numpy_convert.h\"\n-#include \"arrow/python/type_traits.h\"\n-#include \"arrow/python/util/datetime.h\"\n-\n-namespace arrow {\n-namespace py {\n-\n-// ----------------------------------------------------------------------\n-// Sequence converter base and CRTP \"middle\" subclasses\n-\n-class SeqConverter;\n-\n-// Forward-declare converter factory\n-Status GetConverter(const std::shared_ptr<DataType>& type, bool from_pandas,\n-                    bool strict_conversions, std::unique_ptr<SeqConverter>* out);\n-\n-// Marshal Python sequence (list, tuple, etc.) to Arrow array\n-class SeqConverter {\n- public:\n-  virtual ~SeqConverter() = default;\n-\n-  // Initialize the sequence converter with an ArrayBuilder created\n-  // externally. The reason for this interface is that we have\n-  // arrow::MakeBuilder which also creates child builders for nested types, so\n-  // we have to pass in the child builders to child SeqConverter in the case of\n-  // converting Python objects to Arrow nested types\n-  virtual Status Init(ArrayBuilder* builder) = 0;\n-\n-  // Append a single (non-sequence) Python datum to the underlying builder,\n-  // virtual function\n-  virtual Status AppendSingleVirtual(PyObject* obj) = 0;\n-\n-  // Append the contents of a Python sequence to the underlying builder,\n-  // virtual version\n-  virtual Status AppendMultiple(PyObject* seq, int64_t size) = 0;\n-\n-  // Append the contents of a Python sequence to the underlying builder,\n-  // virtual version\n-  virtual Status AppendMultipleMasked(PyObject* seq, PyObject* mask, int64_t size) = 0;\n-\n-  virtual Status GetResult(std::vector<std::shared_ptr<Array>>* chunks) {\n-    *chunks = chunks_;\n-\n-    // Still some accumulated data in the builder. If there are no chunks, we\n-    // always call Finish to deal with the edge case where a size-0 sequence\n-    // was converted with a specific output type, like array([], type=t)\n-    if (chunks_.size() == 0 || builder_->length() > 0) {\n-      std::shared_ptr<Array> last_chunk;\n-      RETURN_NOT_OK(builder_->Finish(&last_chunk));\n-      chunks->emplace_back(std::move(last_chunk));\n-    }\n-    return Status::OK();\n-  }\n-\n-  ArrayBuilder* builder() const { return builder_; }\n-\n- protected:\n-  ArrayBuilder* builder_;\n-  bool unfinished_builder_;\n-  std::vector<std::shared_ptr<Array>> chunks_;\n-};\n-\n-enum class NullCoding : char { NONE_ONLY, PANDAS_SENTINELS };\n-\n-template <NullCoding kind>\n-struct NullChecker {};\n-\n-template <>\n-struct NullChecker<NullCoding::NONE_ONLY> {\n-  static inline bool Check(PyObject* obj) { return obj == Py_None; }\n-};\n-\n-template <>\n-struct NullChecker<NullCoding::PANDAS_SENTINELS> {\n-  static inline bool Check(PyObject* obj) { return internal::PandasObjectIsNull(obj); }\n-};\n-\n-// ----------------------------------------------------------------------\n-// Helper templates to append PyObject* to builder for each target conversion\n-// type\n-\n-template <typename Type, typename Enable = void>\n-struct Unbox {};\n-\n-template <typename Type>\n-struct Unbox<Type, enable_if_integer<Type>> {\n-  using BuilderType = typename TypeTraits<Type>::BuilderType;\n-  static inline Status Append(BuilderType* builder, PyObject* obj) {\n-    typename Type::c_type value;\n-    RETURN_NOT_OK(internal::CIntFromPython(obj, &value));\n-    return builder->Append(value);\n-  }\n-};\n-\n-template <>\n-struct Unbox<HalfFloatType> {\n-  static inline Status Append(HalfFloatBuilder* builder, PyObject* obj) {\n-    npy_half val;\n-    RETURN_NOT_OK(PyFloat_AsHalf(obj, &val));\n-    return builder->Append(val);\n-  }\n-};\n-\n-template <>\n-struct Unbox<FloatType> {\n-  static inline Status Append(FloatBuilder* builder, PyObject* obj) {\n-    if (internal::PyFloatScalar_Check(obj)) {\n-      float val = static_cast<float>(PyFloat_AsDouble(obj));\n-      RETURN_IF_PYERROR();\n-      return builder->Append(val);\n-    } else if (internal::PyIntScalar_Check(obj)) {\n-      float val = 0;\n-      RETURN_NOT_OK(internal::IntegerScalarToFloat32Safe(obj, &val));\n-      return builder->Append(val);\n-    } else {\n-      return internal::InvalidValue(obj, \"tried to convert to float32\");\n-    }\n-  }\n-};\n-\n-template <>\n-struct Unbox<DoubleType> {\n-  static inline Status Append(DoubleBuilder* builder, PyObject* obj) {\n-    if (PyFloat_Check(obj)) {\n-      double val = PyFloat_AS_DOUBLE(obj);\n-      return builder->Append(val);\n-    } else if (internal::PyFloatScalar_Check(obj)) {\n-      // Other kinds of float-y things\n-      double val = PyFloat_AsDouble(obj);\n-      RETURN_IF_PYERROR();\n-      return builder->Append(val);\n-    } else if (internal::PyIntScalar_Check(obj)) {\n-      double val = 0;\n-      RETURN_NOT_OK(internal::IntegerScalarToDoubleSafe(obj, &val));\n-      return builder->Append(val);\n-    } else {\n-      return internal::InvalidValue(obj, \"tried to convert to double\");\n-    }\n-  }\n-};\n-\n-// We use CRTP to avoid virtual calls to the AppendItem(), AppendNull(), and\n-// IsNull() on the hot path\n-template <typename Type, class Derived,\n-          NullCoding null_coding = NullCoding::PANDAS_SENTINELS>\n-class TypedConverter : public SeqConverter {\n- public:\n-  using BuilderType = typename TypeTraits<Type>::BuilderType;\n-\n-  Status Init(ArrayBuilder* builder) override {\n-    builder_ = builder;\n-    DCHECK_NE(builder_, nullptr);\n-    typed_builder_ = checked_cast<BuilderType*>(builder);\n-    return Status::OK();\n-  }\n-\n-  bool CheckNull(PyObject* obj) const { return NullChecker<null_coding>::Check(obj); }\n-\n-  // Append a missing item (default implementation)\n-  Status AppendNull() { return this->typed_builder_->AppendNull(); }\n-\n-  // This is overridden in several subclasses, but if an Unbox implementation\n-  // is defined, it will be used here\n-  Status AppendItem(PyObject* obj) { return Unbox<Type>::Append(typed_builder_, obj); }\n-\n-  Status AppendSingle(PyObject* obj) {\n-    auto self = checked_cast<Derived*>(this);\n-    return CheckNull(obj) ? self->AppendNull() : self->AppendItem(obj);\n-  }\n-\n-  Status AppendSingleVirtual(PyObject* obj) override { return AppendSingle(obj); }\n-\n-  Status AppendMultiple(PyObject* obj, int64_t size) override {\n-    /// Ensure we've allocated enough space\n-    RETURN_NOT_OK(this->typed_builder_->Reserve(size));\n-    // Iterate over the items adding each one\n-    auto self = checked_cast<Derived*>(this);\n-    return internal::VisitSequence(obj,\n-                                   [self](PyObject* item, bool* keep_going /* unused */) {\n-                                     return self->AppendSingle(item);\n-                                   });\n-  }\n-\n-  Status AppendMultipleMasked(PyObject* obj, PyObject* mask, int64_t size) override {\n-    /// Ensure we've allocated enough space\n-    RETURN_NOT_OK(this->typed_builder_->Reserve(size));\n-    // Iterate over the items adding each one\n-    auto self = checked_cast<Derived*>(this);\n-    return internal::VisitSequenceMasked(\n-        obj, mask, [self](PyObject* item, bool is_masked, bool* keep_going /* unused */) {\n-          if (is_masked) {\n-            return self->AppendNull();\n-          } else {\n-            // This will also apply the null-checking convention in the event\n-            // that the value is not masked\n-            return self->AppendSingle(item);\n-          }\n-        });\n-  }\n-\n- protected:\n-  BuilderType* typed_builder_;\n-};\n-\n-// ----------------------------------------------------------------------\n-// Sequence converter for null type\n-\n-class NullConverter : public TypedConverter<NullType, NullConverter> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    return internal::InvalidValue(obj, \"converting to null type\");\n-  }\n-};\n-\n-// ----------------------------------------------------------------------\n-// Sequence converter for boolean type\n-\n-class BoolConverter : public TypedConverter<BooleanType, BoolConverter> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    if (obj == Py_True) {\n-      return typed_builder_->Append(true);\n-    } else if (obj == Py_False) {\n-      return typed_builder_->Append(false);\n-    } else {\n-      return internal::InvalidValue(obj, \"tried to convert to boolean\");\n-    }\n-  }\n-};\n-\n-// ----------------------------------------------------------------------\n-// Sequence converter template for numeric (integer and floating point) types\n-\n-template <typename Type, NullCoding null_coding>\n-class NumericConverter\n-    : public TypedConverter<Type, NumericConverter<Type, null_coding>, null_coding> {};\n-\n-// ----------------------------------------------------------------------\n-// Sequence converters for temporal types\n-\n-class Date32Converter : public TypedConverter<Date32Type, Date32Converter> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    int32_t t;\n-    if (PyDate_Check(obj)) {\n-      auto pydate = reinterpret_cast<PyDateTime_Date*>(obj);\n-      t = static_cast<int32_t>(PyDate_to_s(pydate));\n-    } else {\n-      RETURN_NOT_OK(internal::CIntFromPython(obj, &t, \"Integer too large for date32\"));\n-    }\n-    return typed_builder_->Append(t);\n-  }\n-};\n-\n-class Date64Converter : public TypedConverter<Date64Type, Date64Converter> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    int64_t t;\n-    if (PyDate_Check(obj)) {\n-      auto pydate = reinterpret_cast<PyDateTime_Date*>(obj);\n-      t = PyDate_to_ms(pydate);\n-    } else {\n-      RETURN_NOT_OK(internal::CIntFromPython(obj, &t, \"Integer too large for date64\"));\n-    }\n-    return typed_builder_->Append(t);\n-  }\n-};\n-\n-class TimeConverter : public TypedConverter<Time64Type, TimeConverter> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    if (PyTime_Check(obj)) {\n-      // datetime.time stores microsecond resolution\n-      return typed_builder_->Append(PyTime_to_us(obj));\n-    } else {\n-      return internal::InvalidValue(obj, \"converting to time64\");\n-    }\n-  }\n-};\n-\n-class TimestampConverter : public TypedConverter<TimestampType, TimestampConverter> {\n- public:\n-  explicit TimestampConverter(TimeUnit::type unit) : unit_(unit) {}\n-\n-  Status AppendItem(PyObject* obj) {\n-    int64_t t;\n-    if (PyDateTime_Check(obj)) {\n-      auto pydatetime = reinterpret_cast<PyDateTime_DateTime*>(obj);\n-\n-      switch (unit_) {\n-        case TimeUnit::SECOND:\n-          t = PyDateTime_to_s(pydatetime);\n-          break;\n-        case TimeUnit::MILLI:\n-          t = PyDateTime_to_ms(pydatetime);\n-          break;\n-        case TimeUnit::MICRO:\n-          t = PyDateTime_to_us(pydatetime);\n-          break;\n-        case TimeUnit::NANO:\n-          t = PyDateTime_to_ns(pydatetime);\n-          break;\n-        default:\n-          return Status::UnknownError(\"Invalid time unit\");\n-      }\n-    } else if (PyArray_CheckAnyScalarExact(obj)) {\n-      // numpy.datetime64\n-      std::shared_ptr<DataType> type;\n-      RETURN_NOT_OK(NumPyDtypeToArrow(PyArray_DescrFromScalar(obj), &type));\n-      if (type->id() != Type::TIMESTAMP) {\n-        std::ostringstream ss;\n-        ss << \"Expected np.datetime64 but got: \";\n-        ss << type->ToString();\n-        return Status::Invalid(ss.str());\n-      }\n-      const TimestampType& ttype = checked_cast<const TimestampType&>(*type);\n-      if (unit_ != ttype.unit()) {\n-        return Status::NotImplemented(\n-            \"Cannot convert NumPy datetime64 objects with differing unit\");\n-      }\n-\n-      t = reinterpret_cast<PyDatetimeScalarObject*>(obj)->obval;\n-    } else {\n-      RETURN_NOT_OK(internal::CIntFromPython(obj, &t));\n-    }\n-    return typed_builder_->Append(t);\n-  }\n-\n- private:\n-  TimeUnit::type unit_;\n-};\n-\n-// ----------------------------------------------------------------------\n-// Sequence converters for Binary, FixedSizeBinary, String\n-\n-namespace detail {\n-\n-template <typename BuilderType, typename AppendFunc>\n-inline Status AppendPyString(BuilderType* builder, const PyBytesView& view, bool* is_full,\n-                             AppendFunc&& append_func) {\n-  int32_t length;\n-  RETURN_NOT_OK(internal::CastSize(view.size, &length));\n-  // Did we reach the builder size limit?\n-  if (ARROW_PREDICT_FALSE(builder->value_data_length() + length > kBinaryMemoryLimit)) {\n-    *is_full = true;\n-    return Status::OK();\n-  }\n-  RETURN_NOT_OK(append_func(view.bytes, length));\n-  *is_full = false;\n-  return Status::OK();\n-}\n-\n-inline Status BuilderAppend(BinaryBuilder* builder, PyObject* obj, bool* is_full) {\n-  PyBytesView view;\n-  RETURN_NOT_OK(view.FromString(obj));\n-  return AppendPyString(builder, view, is_full,\n-                        [&builder](const char* bytes, int32_t length) {\n-                          return builder->Append(bytes, length);\n-                        });\n-}\n-\n-inline Status BuilderAppend(FixedSizeBinaryBuilder* builder, PyObject* obj,\n-                            bool* is_full) {\n-  PyBytesView view;\n-  RETURN_NOT_OK(view.FromString(obj));\n-  const auto expected_length =\n-      checked_cast<const FixedSizeBinaryType&>(*builder->type()).byte_width();\n-  if (ARROW_PREDICT_FALSE(view.size != expected_length)) {\n-    std::stringstream ss;\n-    ss << \"expected to be length \" << expected_length << \" was \" << view.size;\n-    return internal::InvalidValue(obj, ss.str());\n-  }\n-\n-  return AppendPyString(\n-      builder, view, is_full,\n-      [&builder](const char* bytes, int32_t length) { return builder->Append(bytes); });\n-}\n-\n-}  // namespace detail\n-\n-template <typename Type>\n-class BinaryLikeConverter : public TypedConverter<Type, BinaryLikeConverter<Type>> {\n- public:\n-  Status AppendItem(PyObject* obj) {\n-    // Accessing members of the templated base requires using this-> here\n-    bool is_full = false;\n-    RETURN_NOT_OK(detail::BuilderAppend(this->typed_builder_, obj, &is_full));\n-\n-    // Exceeded capacity of builder\n-    if (ARROW_PREDICT_FALSE(is_full)) {\n-      std::shared_ptr<Array> chunk;\n-      RETURN_NOT_OK(this->typed_builder_->Finish(&chunk));\n-      this->chunks_.emplace_back(std::move(chunk));\n-\n-      // Append the item now that the builder has been reset\n-      return detail::BuilderAppend(this->typed_builder_, obj, &is_full);\n-    }\n-    return Status::OK();\n-  }\n-};\n-\n-class BytesConverter : public BinaryLikeConverter<BinaryType> {};\n-\n-class FixedWidthBytesConverter : public BinaryLikeConverter<FixedSizeBinaryType> {};\n-\n-// For String/UTF8, if strict_conversions enabled, we reject any non-UTF8,\n-// otherwise we allow but return results as BinaryArray\n-template <bool STRICT>\n-class StringConverter : public TypedConverter<StringType, StringConverter<STRICT>> {\n- public:\n-  StringConverter() : binary_count_(0) {}\n-\n-  Status Append(PyObject* obj, bool* is_full) {\n-    if (STRICT) {\n-      // Force output to be unicode / utf8 and validate that any binary values\n-      // are utf8\n-      bool is_utf8 = false;\n-      RETURN_NOT_OK(string_view_.FromString(obj, &is_utf8));\n-      if (!is_utf8) {\n-        return internal::InvalidValue(obj, \"was not a utf8 string\");\n-      }\n-    } else {\n-      // Non-strict conversion; keep track of whether values are unicode or\n-      // bytes; if any bytes are observe, the result will be bytes\n-      if (PyUnicode_Check(obj)) {\n-        RETURN_NOT_OK(string_view_.FromUnicode(obj));\n-      } else {\n-        // If not unicode or bytes, FromBinary will error\n-        RETURN_NOT_OK(string_view_.FromBinary(obj));\n-        ++binary_count_;\n-      }\n-    }\n-\n-    return detail::AppendPyString(this->typed_builder_, string_view_, is_full,\n-                                  [this](const char* bytes, int32_t length) {\n-                                    return this->typed_builder_->Append(bytes, length);\n-                                  });\n-  }\n-\n-  Status AppendItem(PyObject* obj) {\n-    bool is_full = false;\n-    RETURN_NOT_OK(Append(obj, &is_full));\n-\n-    // Exceeded capacity of builder\n-    if (ARROW_PREDICT_FALSE(is_full)) {\n-      std::shared_ptr<Array> chunk;\n-      RETURN_NOT_OK(this->typed_builder_->Finish(&chunk));\n-      this->chunks_.emplace_back(std::move(chunk));\n-\n-      // Append the item now that the builder has been reset\n-      RETURN_NOT_OK(Append(obj, &is_full));\n-    }\n-    return Status::OK();\n-  }\n-\n-  virtual Status GetResult(std::vector<std::shared_ptr<Array>>* out) {\n-    RETURN_NOT_OK(SeqConverter::GetResult(out));\n-\n-    // If we saw any non-unicode, cast results to BinaryArray\n-    if (binary_count_) {\n-      // We should have bailed out earlier\n-      DCHECK(!STRICT);\n-\n-      for (size_t i = 0; i < out->size(); ++i) {\n-        auto binary_data = (*out)[i]->data()->Copy();\n-        binary_data->type = ::arrow::binary();\n-        (*out)[i] = std::make_shared<BinaryArray>(binary_data);\n-      }\n-    }\n-    return Status::OK();\n-  }\n-\n- private:\n-  // Create a single instance of PyBytesView here to prevent unnecessary object\n-  // creation/destruction\n-  PyBytesView string_view_;\n-\n-  int64_t binary_count_;\n-};\n-\n-// ----------------------------------------------------------------------\n-// Convert lists (NumPy arrays containing lists or ndarrays as values)\n-\n-class ListConverter : public TypedConverter<ListType, ListConverter> {\n- public:\n-  explicit ListConverter(bool from_pandas, bool strict_conversions)\n-      : from_pandas_(from_pandas), strict_conversions_(strict_conversions) {}\n-\n-  Status Init(ArrayBuilder* builder) {\n-    builder_ = builder;\n-    typed_builder_ = checked_cast<ListBuilder*>(builder);\n-\n-    value_type_ = checked_cast<const ListType&>(*builder->type()).value_type();\n-    RETURN_NOT_OK(\n-        GetConverter(value_type_, from_pandas_, strict_conversions_, &value_converter_));\n-    return value_converter_->Init(typed_builder_->value_builder());\n-  }\n-\n-  template <int NUMPY_TYPE, typename Type>\n-  Status AppendNdarrayTypedItem(PyArrayObject* arr);\n-  Status AppendNdarrayItem(PyObject* arr);\n-\n-  Status AppendItem(PyObject* obj) {\n-    RETURN_NOT_OK(typed_builder_->Append());\n-    if (PyArray_Check(obj)) {\n-      return AppendNdarrayItem(obj);\n-    }\n-    const auto list_size = static_cast<int64_t>(PySequence_Size(obj));\n-    if (ARROW_PREDICT_FALSE(list_size == -1)) {\n-      RETURN_IF_PYERROR();\n-    }\n-    return value_converter_->AppendMultiple(obj, list_size);\n-  }\n-\n-  // virtual Status GetResult(std::vector<std::shared_ptr<Array>>* chunks) {\n-  //   // TODO: Handle chunked children\n-  //   return SeqConverter::GetResult(chunks);\n-  // }\n-\n- protected:\n-  std::shared_ptr<DataType> value_type_;\n-  std::unique_ptr<SeqConverter> value_converter_;\n-  bool from_pandas_;\n-  bool strict_conversions_;\n-};\n-\n-template <int NUMPY_TYPE, typename Type>\n-Status ListConverter::AppendNdarrayTypedItem(PyArrayObject* arr) {\n-  using traits = internal::npy_traits<NUMPY_TYPE>;\n-  using T = typename traits::value_type;\n-  using ValueBuilderType = typename TypeTraits<Type>::BuilderType;\n-\n-  const bool null_sentinels_possible = (from_pandas_ && traits::supports_nulls);\n-\n-  auto child_builder = checked_cast<ValueBuilderType*>(value_converter_->builder());\n-\n-  // TODO(wesm): Vector append when not strided\n-  Ndarray1DIndexer<T> values(arr);\n-  if (null_sentinels_possible) {\n-    for (int64_t i = 0; i < values.size(); ++i) {\n-      if (traits::isnull(values[i])) {\n-        RETURN_NOT_OK(child_builder->AppendNull());\n-      } else {\n-        RETURN_NOT_OK(child_builder->Append(values[i]));\n-      }\n-    }\n-  } else {\n-    for (int64_t i = 0; i < values.size(); ++i) {\n-      RETURN_NOT_OK(child_builder->Append(values[i]));\n-    }\n-  }\n-  return Status::OK();\n-}\n-\n-// If the value type does not match the expected NumPy dtype, then fall through\n-// to a slower PySequence-based path\n-#define LIST_FAST_CASE(TYPE, NUMPY_TYPE, ArrowType)               \\\n-  case Type::TYPE: {                                              \\\n-    if (PyArray_DESCR(arr)->type_num != NUMPY_TYPE) {             \\\n-      return value_converter_->AppendMultiple(obj, value_length); \\\n-    }                                                             \\\n-    return AppendNdarrayTypedItem<NUMPY_TYPE, ArrowType>(arr);    \\\n-  }\n-\n-// Use internal::VisitSequence, fast for NPY_OBJECT but slower otherwise\n-#define LIST_SLOW_CASE(TYPE)                                    \\\n-  case Type::TYPE: {                                            \\\n-    return value_converter_->AppendMultiple(obj, value_length); \\\n-  }\n-\n-Status ListConverter::AppendNdarrayItem(PyObject* obj) {\n-  PyArrayObject* arr = reinterpret_cast<PyArrayObject*>(obj);\n-\n-  if (PyArray_NDIM(arr) != 1) {\n-    return Status::Invalid(\"Can only convert 1-dimensional array values\");\n-  }\n-\n-  const int64_t value_length = PyArray_SIZE(arr);\n-\n-  switch (value_type_->id()) {\n-    LIST_SLOW_CASE(NA)\n-    LIST_FAST_CASE(UINT8, NPY_UINT8, UInt8Type)\n-    LIST_FAST_CASE(INT8, NPY_INT8, Int8Type)\n-    LIST_FAST_CASE(UINT16, NPY_UINT16, UInt16Type)\n-    LIST_FAST_CASE(INT16, NPY_INT16, Int16Type)\n-    LIST_FAST_CASE(UINT32, NPY_UINT32, UInt32Type)\n-    LIST_FAST_CASE(INT32, NPY_INT32, Int32Type)\n-    LIST_FAST_CASE(UINT64, NPY_UINT64, UInt64Type)\n-    LIST_FAST_CASE(INT64, NPY_INT64, Int64Type)\n-    LIST_SLOW_CASE(DATE32)\n-    LIST_SLOW_CASE(DATE64)\n-    LIST_SLOW_CASE(TIME64)\n-    LIST_FAST_CASE(TIMESTAMP, NPY_DATETIME, TimestampType)\n-    LIST_FAST_CASE(HALF_FLOAT, NPY_FLOAT16, HalfFloatType)\n-    LIST_FAST_CASE(FLOAT, NPY_FLOAT, FloatType)\n-    LIST_FAST_CASE(DOUBLE, NPY_DOUBLE, DoubleType)\n-    LIST_SLOW_CASE(BINARY)\n-    LIST_SLOW_CASE(FIXED_SIZE_BINARY)\n-    LIST_SLOW_CASE(STRING)\n-    case Type::LIST: {\n-      return value_converter_->AppendSingleVirtual(obj);\n-    }\n-    default: {\n-      std::stringstream ss;\n-      ss << \"Unknown list item type: \";\n-      ss << value_type_->ToString();\n-      return Status::TypeError(ss.str());\n-    }\n-  }\n-}\n-\n-// ----------------------------------------------------------------------\n-// Convert structs\n-\n-class StructConverter : public TypedConverter<StructType, StructConverter> {\n- public:\n-  explicit StructConverter(bool from_pandas, bool strict_conversions)\n-      : from_pandas_(from_pandas), strict_conversions_(strict_conversions) {}\n-\n-  Status Init(ArrayBuilder* builder) {\n-    builder_ = builder;\n-    typed_builder_ = checked_cast<StructBuilder*>(builder);\n-    const auto& struct_type = checked_cast<const StructType&>(*builder->type());\n-\n-    num_fields_ = typed_builder_->num_fields();\n-    DCHECK_EQ(num_fields_, struct_type.num_children());\n-\n-    field_name_list_.reset(PyList_New(num_fields_));\n-    RETURN_IF_PYERROR();\n-\n-    // Initialize the child converters and field names\n-    for (int i = 0; i < num_fields_; i++) {\n-      const std::string& field_name(struct_type.child(i)->name());\n-      std::shared_ptr<DataType> field_type(struct_type.child(i)->type());\n-\n-      std::unique_ptr<SeqConverter> value_converter;\n-      RETURN_NOT_OK(\n-          GetConverter(field_type, from_pandas_, strict_conversions_, &value_converter));\n-      RETURN_NOT_OK(value_converter->Init(typed_builder_->field_builder(i)));\n-      value_converters_.push_back(std::move(value_converter));\n-\n-      // Store the field name as a PyObject, for dict matching\n-      PyObject* nameobj =\n-          PyUnicode_FromStringAndSize(field_name.c_str(), field_name.size());\n-      RETURN_IF_PYERROR();\n-      PyList_SET_ITEM(field_name_list_.obj(), i, nameobj);\n-    }\n-\n-    return Status::OK();\n-  }\n-\n-  Status AppendItem(PyObject* obj) {\n-    RETURN_NOT_OK(typed_builder_->Append());\n-    // Note heterogenous sequences are not allowed\n-    if (ARROW_PREDICT_FALSE(source_kind_ == UNKNOWN)) {\n-      if (PyDict_Check(obj)) {\n-        source_kind_ = DICTS;\n-      } else if (PyTuple_Check(obj)) {\n-        source_kind_ = TUPLES;\n-      }\n-    }\n-    if (PyDict_Check(obj) && source_kind_ == DICTS) {\n-      return AppendDictItem(obj);\n-    } else if (PyTuple_Check(obj) && source_kind_ == TUPLES) {\n-      return AppendTupleItem(obj);\n-    } else {\n-      return Status::TypeError(\"Expected sequence of dicts or tuples for struct type\");\n-    }\n-  }\n-\n-  // Append a missing item\n-  Status AppendNull() {\n-    RETURN_NOT_OK(typed_builder_->AppendNull());\n-    // Need to also insert a missing item on all child builders\n-    // (compare with ListConverter)\n-    for (int i = 0; i < num_fields_; i++) {\n-      RETURN_NOT_OK(value_converters_[i]->AppendSingleVirtual(Py_None));\n-    }\n-    return Status::OK();\n-  }\n-\n- protected:\n-  Status AppendDictItem(PyObject* obj) {\n-    // NOTE we're ignoring any extraneous dict items\n-    for (int i = 0; i < num_fields_; i++) {\n-      PyObject* nameobj = PyList_GET_ITEM(field_name_list_.obj(), i);\n-      PyObject* valueobj = PyDict_GetItem(obj, nameobj);  // borrowed\n-      RETURN_IF_PYERROR();\n-      RETURN_NOT_OK(\n-          value_converters_[i]->AppendSingleVirtual(valueobj ? valueobj : Py_None));\n-    }\n-    return Status::OK();\n-  }\n-\n-  Status AppendTupleItem(PyObject* obj) {\n-    if (PyTuple_GET_SIZE(obj) != num_fields_) {\n-      return Status::Invalid(\"Tuple size must be equal to number of struct fields\");\n-    }\n-    for (int i = 0; i < num_fields_; i++) {\n-      PyObject* valueobj = PyTuple_GET_ITEM(obj, i);\n-      RETURN_NOT_OK(value_converters_[i]->AppendSingleVirtual(valueobj));\n-    }\n-    return Status::OK();\n-  }\n-\n-  std::vector<std::unique_ptr<SeqConverter>> value_converters_;\n-  OwnedRef field_name_list_;\n-  int num_fields_;\n-  // Whether we're converting from a sequence of dicts or tuples\n-  enum { UNKNOWN, DICTS, TUPLES } source_kind_ = UNKNOWN;\n-  bool from_pandas_;\n-  bool strict_conversions_;\n-};\n-\n-class DecimalConverter : public TypedConverter<arrow::Decimal128Type, DecimalConverter> {\n- public:\n-  using BASE = TypedConverter<arrow::Decimal128Type, DecimalConverter>;\n-\n-  Status Init(ArrayBuilder* builder) override {\n-    RETURN_NOT_OK(BASE::Init(builder));\n-    decimal_type_ = checked_cast<const DecimalType*>(typed_builder_->type().get());\n-    return Status::OK();\n-  }\n-\n-  Status AppendItem(PyObject* obj) {\n-    if (internal::PyDecimal_Check(obj)) {\n-      Decimal128 value;\n-      RETURN_NOT_OK(internal::DecimalFromPythonDecimal(obj, *decimal_type_, &value));\n-      return typed_builder_->Append(value);\n-    } else {\n-      // PyObject_IsInstance could error and set an exception\n-      RETURN_IF_PYERROR();\n-      return internal::InvalidValue(obj, \"converting to Decimal128\");\n-    }\n-  }\n-\n- private:\n-  const DecimalType* decimal_type_;\n-};\n-\n-#define NUMERIC_CONVERTER(TYPE_ENUM, TYPE)                           \\\n-  case Type::TYPE_ENUM:                                              \\\n-    if (from_pandas) {                                               \\\n-      *out = std::unique_ptr<SeqConverter>(                          \\\n-          new NumericConverter<TYPE, NullCoding::PANDAS_SENTINELS>); \\\n-    } else {                                                         \\\n-      *out = std::unique_ptr<SeqConverter>(                          \\\n-          new NumericConverter<TYPE, NullCoding::NONE_ONLY>);        \\\n-    }                                                                \\\n-    break;\n-\n-#define SIMPLE_CONVERTER_CASE(TYPE_ENUM, TYPE_CLASS)      \\\n-  case Type::TYPE_ENUM:                                   \\\n-    *out = std::unique_ptr<SeqConverter>(new TYPE_CLASS); \\\n-    break;\n-\n-// Dynamic constructor for sequence converters\n-Status GetConverter(const std::shared_ptr<DataType>& type, bool from_pandas,\n-                    bool strict_conversions, std::unique_ptr<SeqConverter>* out) {\n-  switch (type->id()) {\n-    SIMPLE_CONVERTER_CASE(NA, NullConverter);\n-    SIMPLE_CONVERTER_CASE(BOOL, BoolConverter);\n-    NUMERIC_CONVERTER(INT8, Int8Type);\n-    NUMERIC_CONVERTER(INT16, Int16Type);\n-    NUMERIC_CONVERTER(INT32, Int32Type);\n-    NUMERIC_CONVERTER(INT64, Int64Type);\n-    NUMERIC_CONVERTER(UINT8, UInt8Type);\n-    NUMERIC_CONVERTER(UINT16, UInt16Type);\n-    NUMERIC_CONVERTER(UINT32, UInt32Type);\n-    NUMERIC_CONVERTER(UINT64, UInt64Type);\n-    SIMPLE_CONVERTER_CASE(DATE32, Date32Converter);\n-    SIMPLE_CONVERTER_CASE(DATE64, Date64Converter);\n-    NUMERIC_CONVERTER(HALF_FLOAT, HalfFloatType);\n-    NUMERIC_CONVERTER(FLOAT, FloatType);\n-    NUMERIC_CONVERTER(DOUBLE, DoubleType);\n-    case Type::STRING:\n-      if (strict_conversions) {\n-        *out = std::unique_ptr<SeqConverter>(new StringConverter<true>());\n-      } else {\n-        *out = std::unique_ptr<SeqConverter>(new StringConverter<false>());\n-      }\n-      break;\n-      SIMPLE_CONVERTER_CASE(BINARY, BytesConverter);\n-      SIMPLE_CONVERTER_CASE(FIXED_SIZE_BINARY, FixedWidthBytesConverter);\n-    case Type::TIMESTAMP: {\n-      *out = std::unique_ptr<SeqConverter>(\n-          new TimestampConverter(checked_cast<const TimestampType&>(*type).unit()));\n-      break;\n-    }\n-    case Type::TIME32: {\n-      return Status::NotImplemented(\"No sequence converter for time32 available\");\n-    }\n-      SIMPLE_CONVERTER_CASE(TIME64, TimeConverter);\n-      SIMPLE_CONVERTER_CASE(DECIMAL, DecimalConverter);\n-    case Type::LIST:\n-      *out = std::unique_ptr<SeqConverter>(\n-          new ListConverter(from_pandas, strict_conversions));\n-      break;\n-    case Type::STRUCT:\n-      *out = std::unique_ptr<SeqConverter>(\n-          new StructConverter(from_pandas, strict_conversions));\n-      break;\n-    default:\n-      std::stringstream ss;\n-      ss << \"Sequence converter for type \" << type->ToString() << \" not implemented\";\n-      return Status::NotImplemented(ss.str());\n-  }\n-  return Status::OK();\n-}\n-\n-// ----------------------------------------------------------------------\n-\n-// Convert *obj* to a sequence if necessary\n-// Fill *size* to its length.  If >= 0 on entry, *size* is an upper size\n-// bound that may lead to truncation.\n-Status ConvertToSequenceAndInferSize(PyObject* obj, PyObject** seq, int64_t* size) {\n-  if (PySequence_Check(obj)) {\n-    // obj is already a sequence\n-    int64_t real_size = static_cast<int64_t>(PySequence_Size(obj));\n-    if (*size < 0) {\n-      *size = real_size;\n-    } else {\n-      *size = std::min(real_size, *size);\n-    }\n-    Py_INCREF(obj);\n-    *seq = obj;\n-  } else if (*size < 0) {\n-    // unknown size, exhaust iterator\n-    *seq = PySequence_List(obj);\n-    RETURN_IF_PYERROR();\n-    *size = static_cast<int64_t>(PyList_GET_SIZE(*seq));\n-  } else {\n-    // size is known but iterator could be infinite\n-    Py_ssize_t i, n = *size;\n-    PyObject* iter = PyObject_GetIter(obj);\n-    RETURN_IF_PYERROR();\n-    OwnedRef iter_ref(iter);\n-    PyObject* lst = PyList_New(n);\n-    RETURN_IF_PYERROR();\n-    for (i = 0; i < n; i++) {\n-      PyObject* item = PyIter_Next(iter);\n-      if (!item) break;\n-      PyList_SET_ITEM(lst, i, item);\n-    }\n-    // Shrink list if len(iterator) < size\n-    if (i < n && PyList_SetSlice(lst, i, n, NULL)) {\n-      Py_DECREF(lst);\n-      return Status::UnknownError(\"failed to resize list\");\n-    }\n-    *seq = lst;\n-    *size = std::min<int64_t>(i, *size);\n-  }\n-  return Status::OK();\n-}\n-\n-Status ConvertPySequence(PyObject* sequence_source, PyObject* mask,\n-                         const PyConversionOptions& options,\n-                         std::shared_ptr<ChunkedArray>* out) {\n-  PyAcquireGIL lock;\n-\n-  PyDateTime_IMPORT;\n-\n-  PyObject* seq;\n-  OwnedRef tmp_seq_nanny;\n-\n-  std::shared_ptr<DataType> real_type;\n-\n-  int64_t size = options.size;\n-  RETURN_NOT_OK(ConvertToSequenceAndInferSize(sequence_source, &seq, &size));\n-  tmp_seq_nanny.reset(seq);\n-\n-  // In some cases, type inference may be \"loose\", like strings. If the user\n-  // passed pa.string(), then we will error if we encounter any non-UTF8\n-  // value. If not, then we will allow the result to be a BinaryArray\n-  bool strict_conversions = false;\n-\n-  if (options.type == nullptr) {\n-    RETURN_NOT_OK(InferArrowType(seq, &real_type));\n-  } else {\n-    real_type = options.type;\n-    strict_conversions = true;\n-  }\n-  DCHECK_GE(size, 0);\n-\n-  // Handle NA / NullType case\n-  if (real_type->id() == Type::NA) {\n-    ArrayVector chunks = {std::make_shared<NullArray>(size)};\n-    *out = std::make_shared<ChunkedArray>(chunks);\n-    return Status::OK();\n-  }\n-\n-  // Create the sequence converter, initialize with the builder\n-  std::unique_ptr<SeqConverter> converter;\n-  RETURN_NOT_OK(\n-      GetConverter(real_type, options.from_pandas, strict_conversions, &converter));\n-\n-  // Create ArrayBuilder for type, then pass into the SeqConverter\n-  // instance. The reason this is created here rather than in GetConverter is\n-  // because of nested types (child SeqConverter objects need the child\n-  // builders created by MakeBuilder)\n-  std::unique_ptr<ArrayBuilder> type_builder;\n-  RETURN_NOT_OK(MakeBuilder(options.pool, real_type, &type_builder));\n-  RETURN_NOT_OK(converter->Init(type_builder.get()));\n-\n-  // Convert values\n-  if (mask != nullptr && mask != Py_None) {\n-    RETURN_NOT_OK(converter->AppendMultipleMasked(seq, mask, size));\n-  } else {\n-    RETURN_NOT_OK(converter->AppendMultiple(seq, size));\n-  }\n-\n-  // Retrieve result. Conversion may yield one or more array values\n-  std::vector<std::shared_ptr<Array>> chunks;\n-  RETURN_NOT_OK(converter->GetResult(&chunks));\n-\n-  *out = std::make_shared<ChunkedArray>(chunks);\n-  return Status::OK();\n-}\n-\n-Status ConvertPySequence(PyObject* obj, const PyConversionOptions& options,\n-                         std::shared_ptr<ChunkedArray>* out) {\n-  return ConvertPySequence(obj, nullptr, options, out);\n-}\n-\n-}  // namespace py\n-}  // namespace arrow\ndiff --git a/cpp/src/arrow/python/builtin_convert.h b/cpp/src/arrow/python/builtin_convert.h\ndeleted file mode 100644\nindex d133089f97..0000000000\n--- a/cpp/src/arrow/python/builtin_convert.h\n+++ /dev/null\n@@ -1,83 +0,0 @@\n-// Licensed to the Apache Software Foundation (ASF) under one\n-// or more contributor license agreements.  See the NOTICE file\n-// distributed with this work for additional information\n-// regarding copyright ownership.  The ASF licenses this file\n-// to you under the Apache License, Version 2.0 (the\n-// \"License\"); you may not use this file except in compliance\n-// with the License.  You may obtain a copy of the License at\n-//\n-//   http://www.apache.org/licenses/LICENSE-2.0\n-//\n-// Unless required by applicable law or agreed to in writing,\n-// software distributed under the License is distributed on an\n-// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n-// KIND, either express or implied.  See the License for the\n-// specific language governing permissions and limitations\n-// under the License.\n-\n-// Functions for converting between CPython built-in data structures and Arrow\n-// data structures\n-\n-#ifndef ARROW_PYTHON_ADAPTERS_BUILTIN_H\n-#define ARROW_PYTHON_ADAPTERS_BUILTIN_H\n-\n-#include \"arrow/python/platform.h\"\n-\n-#include <cstdint>\n-#include <memory>\n-\n-#include \"arrow/type.h\"\n-#include \"arrow/util/macros.h\"\n-#include \"arrow/util/visibility.h\"\n-\n-#include \"arrow/python/common.h\"\n-\n-namespace arrow {\n-\n-class Array;\n-class Status;\n-\n-namespace py {\n-\n-struct PyConversionOptions {\n-  PyConversionOptions() : type(NULLPTR), size(-1), pool(NULLPTR), from_pandas(false) {}\n-\n-  PyConversionOptions(const std::shared_ptr<DataType>& type, int64_t size,\n-                      MemoryPool* pool, bool from_pandas)\n-      : type(type), size(size), pool(default_memory_pool()), from_pandas(from_pandas) {}\n-\n-  // Set to null if to be inferred\n-  std::shared_ptr<DataType> type;\n-\n-  // Default is -1: infer from data\n-  int64_t size;\n-\n-  // Memory pool to use for allocations\n-  MemoryPool* pool;\n-\n-  // Default false\n-  bool from_pandas;\n-};\n-\n-/// \\brief Convert sequence (list, generator, NumPy array with dtype object) of\n-/// Python objects.\n-/// \\param[in] obj the sequence to convert\n-/// \\param[in] mask a NumPy array of true/false values to indicate whether\n-/// values in the sequence are null (true) or not null (false). This parameter\n-/// may be null\n-/// \\param[in] options various conversion options\n-/// \\param[out] out a ChunkedArray containing one or more chunks\n-/// \\return Status\n-ARROW_EXPORT\n-Status ConvertPySequence(PyObject* obj, PyObject* mask,\n-                         const PyConversionOptions& options,\n-                         std::shared_ptr<ChunkedArray>* out);\n-\n-ARROW_EXPORT\n-Status ConvertPySequence(PyObject* obj, const PyConversionOptions& options,\n-                         std::shared_ptr<ChunkedArray>* out);\n-\n-}  // namespace py\n-}  // namespace arrow\n-\n-#endif  // ARROW_PYTHON_ADAPTERS_BUILTIN_H\ndiff --git a/cpp/src/arrow/python/decimal.h b/cpp/src/arrow/python/decimal.h\nindex 41d821f4ef..fada81daa9 100644\n--- a/cpp/src/arrow/python/decimal.h\n+++ b/cpp/src/arrow/python/decimal.h\n@@ -28,6 +28,8 @@ class Decimal128;\n \n namespace py {\n \n+class OwnedRef;\n+\n //\n // Python Decimal support\n //\ndiff --git a/cpp/src/arrow/python/arrow_to_python.cc b/cpp/src/arrow/python/deserialize.cc\nsimilarity index 99%\nrename from cpp/src/arrow/python/arrow_to_python.cc\nrename to cpp/src/arrow/python/deserialize.cc\nindex 6af43c4ef2..3dbc18f7bb 100644\n--- a/cpp/src/arrow/python/arrow_to_python.cc\n+++ b/cpp/src/arrow/python/deserialize.cc\n@@ -15,7 +15,7 @@\n // specific language governing permissions and limitations\n // under the License.\n \n-#include \"arrow/python/arrow_to_python.h\"\n+#include \"arrow/python/deserialize.h\"\n \n #include \"arrow/python/numpy_interop.h\"\n \n@@ -40,7 +40,7 @@\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/numpy_convert.h\"\n #include \"arrow/python/pyarrow.h\"\n-#include \"arrow/python/python_to_arrow.h\"\n+#include \"arrow/python/serialize.h\"\n #include \"arrow/python/util/datetime.h\"\n \n namespace arrow {\ndiff --git a/cpp/src/arrow/python/arrow_to_python.h b/cpp/src/arrow/python/deserialize.h\nsimilarity index 98%\nrename from cpp/src/arrow/python/arrow_to_python.h\nrename to cpp/src/arrow/python/deserialize.h\nindex 0d0ec5e2da..23e5902b20 100644\n--- a/cpp/src/arrow/python/arrow_to_python.h\n+++ b/cpp/src/arrow/python/deserialize.h\n@@ -22,7 +22,7 @@\n #include <memory>\n #include <vector>\n \n-#include \"arrow/python/python_to_arrow.h\"\n+#include \"arrow/python/serialize.h\"\n #include \"arrow/status.h\"\n #include \"arrow/util/visibility.h\"\n \ndiff --git a/cpp/src/arrow/python/numpy_to_arrow.cc b/cpp/src/arrow/python/numpy_to_arrow.cc\nindex 0a4dcc1aa4..502afc7376 100644\n--- a/cpp/src/arrow/python/numpy_to_arrow.cc\n+++ b/cpp/src/arrow/python/numpy_to_arrow.cc\n@@ -46,13 +46,13 @@\n #include \"arrow/compute/context.h\"\n #include \"arrow/compute/kernels/cast.h\"\n \n-#include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/common.h\"\n #include \"arrow/python/config.h\"\n #include \"arrow/python/helpers.h\"\n #include \"arrow/python/iterators.h\"\n #include \"arrow/python/numpy-internal.h\"\n #include \"arrow/python/numpy_convert.h\"\n+#include \"arrow/python/python_to_arrow.h\"\n #include \"arrow/python/type_traits.h\"\n #include \"arrow/python/util/datetime.h\"\n \ndiff --git a/cpp/src/arrow/python/python-test.cc b/cpp/src/arrow/python/python-test.cc\nindex 0bc4eb1694..70bbe0085a 100644\n--- a/cpp/src/arrow/python/python-test.cc\n+++ b/cpp/src/arrow/python/python-test.cc\n@@ -27,9 +27,9 @@\n #include \"arrow/test-util.h\"\n \n #include \"arrow/python/arrow_to_pandas.h\"\n-#include \"arrow/python/builtin_convert.h\"\n #include \"arrow/python/decimal.h\"\n #include \"arrow/python/helpers.h\"\n+#include \"arrow/python/python_to_arrow.h\"\n #include \"arrow/util/checked_cast.h\"\n \n namespace arrow {\ndiff --git a/cpp/src/arrow/python/python_to_arrow.cc b/cpp/src/arrow/python/python_to_arrow.cc\nindex 0b4f71c206..3a676960eb 100644\n--- a/cpp/src/arrow/python/python_to_arrow.cc\n+++ b/cpp/src/arrow/python/python_to_arrow.cc\n@@ -18,811 +18,953 @@\n #include \"arrow/python/python_to_arrow.h\"\n #include \"arrow/python/numpy_interop.h\"\n \n-#include <cstdint>\n+#include <datetime.h>\n+\n+#include <algorithm>\n #include <limits>\n-#include <memory>\n+#include <map>\n #include <sstream>\n #include <string>\n+#include <utility>\n #include <vector>\n \n-#include <numpy/arrayobject.h>\n-#include <numpy/arrayscalars.h>\n-\n #include \"arrow/array.h\"\n #include \"arrow/builder.h\"\n-#include \"arrow/io/interfaces.h\"\n-#include \"arrow/io/memory.h\"\n-#include \"arrow/ipc/writer.h\"\n-#include \"arrow/memory_pool.h\"\n-#include \"arrow/record_batch.h\"\n-#include \"arrow/tensor.h\"\n+#include \"arrow/status.h\"\n+#include \"arrow/table.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/type_traits.h\"\n+#include \"arrow/util/checked_cast.h\"\n+#include \"arrow/util/decimal.h\"\n #include \"arrow/util/logging.h\"\n \n-#include \"arrow/python/common.h\"\n+#include \"arrow/python/decimal.h\"\n #include \"arrow/python/helpers.h\"\n+#include \"arrow/python/inference.h\"\n #include \"arrow/python/iterators.h\"\n #include \"arrow/python/numpy_convert.h\"\n-#include \"arrow/python/platform.h\"\n-#include \"arrow/python/pyarrow.h\"\n+#include \"arrow/python/type_traits.h\"\n #include \"arrow/python/util/datetime.h\"\n \n-constexpr int32_t kMaxRecursionDepth = 100;\n-\n namespace arrow {\n namespace py {\n \n-/// A Sequence is a heterogeneous collections of elements. It can contain\n-/// scalar Python types, lists, tuples, dictionaries and tensors.\n-class SequenceBuilder {\n+// ----------------------------------------------------------------------\n+// Sequence converter base and CRTP \"middle\" subclasses\n+\n+class SeqConverter;\n+\n+// Forward-declare converter factory\n+Status GetConverter(const std::shared_ptr<DataType>& type, bool from_pandas,\n+                    bool strict_conversions, std::unique_ptr<SeqConverter>* out);\n+\n+// Marshal Python sequence (list, tuple, etc.) to Arrow array\n+class SeqConverter {\n  public:\n-  explicit SequenceBuilder(MemoryPool* pool ARROW_MEMORY_POOL_DEFAULT)\n-      : pool_(pool),\n-        types_(::arrow::int8(), pool),\n-        offsets_(::arrow::int32(), pool),\n-        nones_(pool),\n-        bools_(::arrow::boolean(), pool),\n-        ints_(::arrow::int64(), pool),\n-        py2_ints_(::arrow::int64(), pool),\n-        bytes_(::arrow::binary(), pool),\n-        strings_(pool),\n-        half_floats_(::arrow::float16(), pool),\n-        floats_(::arrow::float32(), pool),\n-        doubles_(::arrow::float64(), pool),\n-        date64s_(::arrow::date64(), pool),\n-        tensor_indices_(::arrow::int32(), pool),\n-        buffer_indices_(::arrow::int32(), pool),\n-        list_offsets_({0}),\n-        tuple_offsets_({0}),\n-        dict_offsets_({0}),\n-        set_offsets_({0}) {}\n-\n-  /// Appending a none to the sequence\n-  Status AppendNone() {\n-    RETURN_NOT_OK(offsets_.Append(0));\n-    RETURN_NOT_OK(types_.Append(0));\n-    return nones_.AppendNull();\n-  }\n-\n-  Status Update(int64_t offset, int8_t* tag) {\n-    if (*tag == -1) {\n-      *tag = num_tags_++;\n+  virtual ~SeqConverter() = default;\n+\n+  // Initialize the sequence converter with an ArrayBuilder created\n+  // externally. The reason for this interface is that we have\n+  // arrow::MakeBuilder which also creates child builders for nested types, so\n+  // we have to pass in the child builders to child SeqConverter in the case of\n+  // converting Python objects to Arrow nested types\n+  virtual Status Init(ArrayBuilder* builder) = 0;\n+\n+  // Append a single (non-sequence) Python datum to the underlying builder,\n+  // virtual function\n+  virtual Status AppendSingleVirtual(PyObject* obj) = 0;\n+\n+  // Append the contents of a Python sequence to the underlying builder,\n+  // virtual version\n+  virtual Status AppendMultiple(PyObject* seq, int64_t size) = 0;\n+\n+  // Append the contents of a Python sequence to the underlying builder,\n+  // virtual version\n+  virtual Status AppendMultipleMasked(PyObject* seq, PyObject* mask, int64_t size) = 0;\n+\n+  virtual Status GetResult(std::vector<std::shared_ptr<Array>>* chunks) {\n+    *chunks = chunks_;\n+\n+    // Still some accumulated data in the builder. If there are no chunks, we\n+    // always call Finish to deal with the edge case where a size-0 sequence\n+    // was converted with a specific output type, like array([], type=t)\n+    if (chunks_.size() == 0 || builder_->length() > 0) {\n+      std::shared_ptr<Array> last_chunk;\n+      RETURN_NOT_OK(builder_->Finish(&last_chunk));\n+      chunks->emplace_back(std::move(last_chunk));\n     }\n-    int32_t offset32;\n-    RETURN_NOT_OK(internal::CastSize(offset, &offset32));\n-    RETURN_NOT_OK(offsets_.Append(offset32));\n-    RETURN_NOT_OK(types_.Append(*tag));\n-    return nones_.Append(true);\n+    return Status::OK();\n   }\n \n-  template <typename BuilderType, typename T>\n-  Status AppendPrimitive(const T val, int8_t* tag, BuilderType* out) {\n-    RETURN_NOT_OK(Update(out->length(), tag));\n-    return out->Append(val);\n+  ArrayBuilder* builder() const { return builder_; }\n+\n+ protected:\n+  ArrayBuilder* builder_;\n+  bool unfinished_builder_;\n+  std::vector<std::shared_ptr<Array>> chunks_;\n+};\n+\n+enum class NullCoding : char { NONE_ONLY, PANDAS_SENTINELS };\n+\n+template <NullCoding kind>\n+struct NullChecker {};\n+\n+template <>\n+struct NullChecker<NullCoding::NONE_ONLY> {\n+  static inline bool Check(PyObject* obj) { return obj == Py_None; }\n+};\n+\n+template <>\n+struct NullChecker<NullCoding::PANDAS_SENTINELS> {\n+  static inline bool Check(PyObject* obj) { return internal::PandasObjectIsNull(obj); }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Helper templates to append PyObject* to builder for each target conversion\n+// type\n+\n+template <typename Type, typename Enable = void>\n+struct Unbox {};\n+\n+template <typename Type>\n+struct Unbox<Type, enable_if_integer<Type>> {\n+  using BuilderType = typename TypeTraits<Type>::BuilderType;\n+  static inline Status Append(BuilderType* builder, PyObject* obj) {\n+    typename Type::c_type value;\n+    RETURN_NOT_OK(internal::CIntFromPython(obj, &value));\n+    return builder->Append(value);\n   }\n+};\n \n-  /// Appending a boolean to the sequence\n-  Status AppendBool(const bool data) {\n-    return AppendPrimitive(data, &bool_tag_, &bools_);\n+template <>\n+struct Unbox<HalfFloatType> {\n+  static inline Status Append(HalfFloatBuilder* builder, PyObject* obj) {\n+    npy_half val;\n+    RETURN_NOT_OK(PyFloat_AsHalf(obj, &val));\n+    return builder->Append(val);\n   }\n+};\n \n-  /// Appending a python 2 int64_t to the sequence\n-  Status AppendPy2Int64(const int64_t data) {\n-    return AppendPrimitive(data, &py2_int_tag_, &py2_ints_);\n+template <>\n+struct Unbox<FloatType> {\n+  static inline Status Append(FloatBuilder* builder, PyObject* obj) {\n+    if (internal::PyFloatScalar_Check(obj)) {\n+      float val = static_cast<float>(PyFloat_AsDouble(obj));\n+      RETURN_IF_PYERROR();\n+      return builder->Append(val);\n+    } else if (internal::PyIntScalar_Check(obj)) {\n+      float val = 0;\n+      RETURN_NOT_OK(internal::IntegerScalarToFloat32Safe(obj, &val));\n+      return builder->Append(val);\n+    } else {\n+      return internal::InvalidValue(obj, \"tried to convert to float32\");\n+    }\n   }\n+};\n \n-  /// Appending an int64_t to the sequence\n-  Status AppendInt64(const int64_t data) {\n-    return AppendPrimitive(data, &int_tag_, &ints_);\n+template <>\n+struct Unbox<DoubleType> {\n+  static inline Status Append(DoubleBuilder* builder, PyObject* obj) {\n+    if (PyFloat_Check(obj)) {\n+      double val = PyFloat_AS_DOUBLE(obj);\n+      return builder->Append(val);\n+    } else if (internal::PyFloatScalar_Check(obj)) {\n+      // Other kinds of float-y things\n+      double val = PyFloat_AsDouble(obj);\n+      RETURN_IF_PYERROR();\n+      return builder->Append(val);\n+    } else if (internal::PyIntScalar_Check(obj)) {\n+      double val = 0;\n+      RETURN_NOT_OK(internal::IntegerScalarToDoubleSafe(obj, &val));\n+      return builder->Append(val);\n+    } else {\n+      return internal::InvalidValue(obj, \"tried to convert to double\");\n+    }\n   }\n+};\n \n-  /// Appending an uint64_t to the sequence\n-  Status AppendUInt64(const uint64_t data) {\n-    // TODO(wesm): Bounds check\n-    return AppendPrimitive(static_cast<int64_t>(data), &int_tag_, &ints_);\n+// We use CRTP to avoid virtual calls to the AppendItem(), AppendNull(), and\n+// IsNull() on the hot path\n+template <typename Type, class Derived,\n+          NullCoding null_coding = NullCoding::PANDAS_SENTINELS>\n+class TypedConverter : public SeqConverter {\n+ public:\n+  using BuilderType = typename TypeTraits<Type>::BuilderType;\n+\n+  Status Init(ArrayBuilder* builder) override {\n+    builder_ = builder;\n+    DCHECK_NE(builder_, nullptr);\n+    typed_builder_ = checked_cast<BuilderType*>(builder);\n+    return Status::OK();\n   }\n \n-  /// Append a list of bytes to the sequence\n-  Status AppendBytes(const uint8_t* data, int32_t length) {\n-    RETURN_NOT_OK(Update(bytes_.length(), &bytes_tag_));\n-    return bytes_.Append(data, length);\n+  bool CheckNull(PyObject* obj) const { return NullChecker<null_coding>::Check(obj); }\n+\n+  // Append a missing item (default implementation)\n+  Status AppendNull() { return this->typed_builder_->AppendNull(); }\n+\n+  // This is overridden in several subclasses, but if an Unbox implementation\n+  // is defined, it will be used here\n+  Status AppendItem(PyObject* obj) { return Unbox<Type>::Append(typed_builder_, obj); }\n+\n+  Status AppendSingle(PyObject* obj) {\n+    auto self = checked_cast<Derived*>(this);\n+    return CheckNull(obj) ? self->AppendNull() : self->AppendItem(obj);\n   }\n \n-  /// Appending a string to the sequence\n-  Status AppendString(const char* data, int32_t length) {\n-    RETURN_NOT_OK(Update(strings_.length(), &string_tag_));\n-    return strings_.Append(data, length);\n+  Status AppendSingleVirtual(PyObject* obj) override { return AppendSingle(obj); }\n+\n+  Status AppendMultiple(PyObject* obj, int64_t size) override {\n+    /// Ensure we've allocated enough space\n+    RETURN_NOT_OK(this->typed_builder_->Reserve(size));\n+    // Iterate over the items adding each one\n+    auto self = checked_cast<Derived*>(this);\n+    return internal::VisitSequence(obj,\n+                                   [self](PyObject* item, bool* keep_going /* unused */) {\n+                                     return self->AppendSingle(item);\n+                                   });\n   }\n \n-  /// Appending a half_float to the sequence\n-  Status AppendHalfFloat(const npy_half data) {\n-    return AppendPrimitive(data, &half_float_tag_, &half_floats_);\n+  Status AppendMultipleMasked(PyObject* obj, PyObject* mask, int64_t size) override {\n+    /// Ensure we've allocated enough space\n+    RETURN_NOT_OK(this->typed_builder_->Reserve(size));\n+    // Iterate over the items adding each one\n+    auto self = checked_cast<Derived*>(this);\n+    return internal::VisitSequenceMasked(\n+        obj, mask, [self](PyObject* item, bool is_masked, bool* keep_going /* unused */) {\n+          if (is_masked) {\n+            return self->AppendNull();\n+          } else {\n+            // This will also apply the null-checking convention in the event\n+            // that the value is not masked\n+            return self->AppendSingle(item);\n+          }\n+        });\n   }\n \n-  /// Appending a float to the sequence\n-  Status AppendFloat(const float data) {\n-    return AppendPrimitive(data, &float_tag_, &floats_);\n+ protected:\n+  BuilderType* typed_builder_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Sequence converter for null type\n+\n+class NullConverter : public TypedConverter<NullType, NullConverter> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    return internal::InvalidValue(obj, \"converting to null type\");\n   }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Sequence converter for boolean type\n \n-  /// Appending a double to the sequence\n-  Status AppendDouble(const double data) {\n-    return AppendPrimitive(data, &double_tag_, &doubles_);\n+class BoolConverter : public TypedConverter<BooleanType, BoolConverter> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    if (obj == Py_True) {\n+      return typed_builder_->Append(true);\n+    } else if (obj == Py_False) {\n+      return typed_builder_->Append(false);\n+    } else {\n+      return internal::InvalidValue(obj, \"tried to convert to boolean\");\n+    }\n   }\n+};\n+\n+// ----------------------------------------------------------------------\n+// Sequence converter template for numeric (integer and floating point) types\n \n-  /// Appending a Date64 timestamp to the sequence\n-  Status AppendDate64(const int64_t timestamp) {\n-    return AppendPrimitive(timestamp, &date64_tag_, &date64s_);\n+template <typename Type, NullCoding null_coding>\n+class NumericConverter\n+    : public TypedConverter<Type, NumericConverter<Type, null_coding>, null_coding> {};\n+\n+// ----------------------------------------------------------------------\n+// Sequence converters for temporal types\n+\n+class Date32Converter : public TypedConverter<Date32Type, Date32Converter> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    int32_t t;\n+    if (PyDate_Check(obj)) {\n+      auto pydate = reinterpret_cast<PyDateTime_Date*>(obj);\n+      t = static_cast<int32_t>(PyDate_to_s(pydate));\n+    } else {\n+      RETURN_NOT_OK(internal::CIntFromPython(obj, &t, \"Integer too large for date32\"));\n+    }\n+    return typed_builder_->Append(t);\n   }\n+};\n \n-  /// Appending a tensor to the sequence\n-  ///\n-  /// \\param tensor_index Index of the tensor in the object.\n-  Status AppendTensor(const int32_t tensor_index) {\n-    RETURN_NOT_OK(Update(tensor_indices_.length(), &tensor_tag_));\n-    return tensor_indices_.Append(tensor_index);\n+class Date64Converter : public TypedConverter<Date64Type, Date64Converter> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    int64_t t;\n+    if (PyDate_Check(obj)) {\n+      auto pydate = reinterpret_cast<PyDateTime_Date*>(obj);\n+      t = PyDate_to_ms(pydate);\n+    } else {\n+      RETURN_NOT_OK(internal::CIntFromPython(obj, &t, \"Integer too large for date64\"));\n+    }\n+    return typed_builder_->Append(t);\n   }\n+};\n \n-  /// Appending a buffer to the sequence\n-  ///\n-  /// \\param buffer_index Indes of the buffer in the object.\n-  Status AppendBuffer(const int32_t buffer_index) {\n-    RETURN_NOT_OK(Update(buffer_indices_.length(), &buffer_tag_));\n-    return buffer_indices_.Append(buffer_index);\n+class TimeConverter : public TypedConverter<Time64Type, TimeConverter> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    if (PyTime_Check(obj)) {\n+      // datetime.time stores microsecond resolution\n+      return typed_builder_->Append(PyTime_to_us(obj));\n+    } else {\n+      return internal::InvalidValue(obj, \"converting to time64\");\n+    }\n   }\n+};\n \n-  /// Add a sublist to the sequence. The data contained in the sublist will be\n-  /// specified in the \"Finish\" method.\n-  ///\n-  /// To construct l = [[11, 22], 33, [44, 55]] you would for example run\n-  /// list = ListBuilder();\n-  /// list.AppendList(2);\n-  /// list.Append(33);\n-  /// list.AppendList(2);\n-  /// list.Finish([11, 22, 44, 55]);\n-  /// list.Finish();\n+class TimestampConverter : public TypedConverter<TimestampType, TimestampConverter> {\n+ public:\n+  explicit TimestampConverter(TimeUnit::type unit) : unit_(unit) {}\n+\n+  Status AppendItem(PyObject* obj) {\n+    int64_t t;\n+    if (PyDateTime_Check(obj)) {\n+      auto pydatetime = reinterpret_cast<PyDateTime_DateTime*>(obj);\n+\n+      switch (unit_) {\n+        case TimeUnit::SECOND:\n+          t = PyDateTime_to_s(pydatetime);\n+          break;\n+        case TimeUnit::MILLI:\n+          t = PyDateTime_to_ms(pydatetime);\n+          break;\n+        case TimeUnit::MICRO:\n+          t = PyDateTime_to_us(pydatetime);\n+          break;\n+        case TimeUnit::NANO:\n+          t = PyDateTime_to_ns(pydatetime);\n+          break;\n+        default:\n+          return Status::UnknownError(\"Invalid time unit\");\n+      }\n+    } else if (PyArray_CheckAnyScalarExact(obj)) {\n+      // numpy.datetime64\n+      std::shared_ptr<DataType> type;\n+      RETURN_NOT_OK(NumPyDtypeToArrow(PyArray_DescrFromScalar(obj), &type));\n+      if (type->id() != Type::TIMESTAMP) {\n+        std::ostringstream ss;\n+        ss << \"Expected np.datetime64 but got: \";\n+        ss << type->ToString();\n+        return Status::Invalid(ss.str());\n+      }\n+      const TimestampType& ttype = checked_cast<const TimestampType&>(*type);\n+      if (unit_ != ttype.unit()) {\n+        return Status::NotImplemented(\n+            \"Cannot convert NumPy datetime64 objects with differing unit\");\n+      }\n \n-  /// \\param size\n-  /// The size of the sublist\n-  Status AppendList(Py_ssize_t size) {\n-    int32_t offset;\n-    RETURN_NOT_OK(internal::CastSize(list_offsets_.back() + size, &offset));\n-    RETURN_NOT_OK(Update(list_offsets_.size() - 1, &list_tag_));\n-    list_offsets_.push_back(offset);\n-    return Status::OK();\n+      t = reinterpret_cast<PyDatetimeScalarObject*>(obj)->obval;\n+    } else {\n+      RETURN_NOT_OK(internal::CIntFromPython(obj, &t));\n+    }\n+    return typed_builder_->Append(t);\n   }\n \n-  Status AppendTuple(Py_ssize_t size) {\n-    int32_t offset;\n-    RETURN_NOT_OK(internal::CastSize(tuple_offsets_.back() + size, &offset));\n-    RETURN_NOT_OK(Update(tuple_offsets_.size() - 1, &tuple_tag_));\n-    tuple_offsets_.push_back(offset);\n-    return Status::OK();\n-  }\n+ private:\n+  TimeUnit::type unit_;\n+};\n+\n+// ----------------------------------------------------------------------\n+// Sequence converters for Binary, FixedSizeBinary, String\n \n-  Status AppendDict(Py_ssize_t size) {\n-    int32_t offset;\n-    RETURN_NOT_OK(internal::CastSize(dict_offsets_.back() + size, &offset));\n-    RETURN_NOT_OK(Update(dict_offsets_.size() - 1, &dict_tag_));\n-    dict_offsets_.push_back(offset);\n+namespace detail {\n+\n+template <typename BuilderType, typename AppendFunc>\n+inline Status AppendPyString(BuilderType* builder, const PyBytesView& view, bool* is_full,\n+                             AppendFunc&& append_func) {\n+  int32_t length;\n+  RETURN_NOT_OK(internal::CastSize(view.size, &length));\n+  // Did we reach the builder size limit?\n+  if (ARROW_PREDICT_FALSE(builder->value_data_length() + length > kBinaryMemoryLimit)) {\n+    *is_full = true;\n     return Status::OK();\n   }\n+  RETURN_NOT_OK(append_func(view.bytes, length));\n+  *is_full = false;\n+  return Status::OK();\n+}\n \n-  Status AppendSet(Py_ssize_t size) {\n-    int32_t offset;\n-    RETURN_NOT_OK(internal::CastSize(set_offsets_.back() + size, &offset));\n-    RETURN_NOT_OK(Update(set_offsets_.size() - 1, &set_tag_));\n-    set_offsets_.push_back(offset);\n-    return Status::OK();\n+inline Status BuilderAppend(BinaryBuilder* builder, PyObject* obj, bool* is_full) {\n+  PyBytesView view;\n+  RETURN_NOT_OK(view.FromString(obj));\n+  return AppendPyString(builder, view, is_full,\n+                        [&builder](const char* bytes, int32_t length) {\n+                          return builder->Append(bytes, length);\n+                        });\n+}\n+\n+inline Status BuilderAppend(FixedSizeBinaryBuilder* builder, PyObject* obj,\n+                            bool* is_full) {\n+  PyBytesView view;\n+  RETURN_NOT_OK(view.FromString(obj));\n+  const auto expected_length =\n+      checked_cast<const FixedSizeBinaryType&>(*builder->type()).byte_width();\n+  if (ARROW_PREDICT_FALSE(view.size != expected_length)) {\n+    std::stringstream ss;\n+    ss << \"expected to be length \" << expected_length << \" was \" << view.size;\n+    return internal::InvalidValue(obj, ss.str());\n   }\n \n-  template <typename BuilderType>\n-  Status AddElement(const int8_t tag, BuilderType* out, const std::string& name = \"\") {\n-    if (tag != -1) {\n-      fields_[tag] = ::arrow::field(name, out->type());\n-      RETURN_NOT_OK(out->Finish(&children_[tag]));\n-      RETURN_NOT_OK(nones_.Append(true));\n-      type_ids_.push_back(tag);\n+  return AppendPyString(\n+      builder, view, is_full,\n+      [&builder](const char* bytes, int32_t length) { return builder->Append(bytes); });\n+}\n+\n+}  // namespace detail\n+\n+template <typename Type>\n+class BinaryLikeConverter : public TypedConverter<Type, BinaryLikeConverter<Type>> {\n+ public:\n+  Status AppendItem(PyObject* obj) {\n+    // Accessing members of the templated base requires using this-> here\n+    bool is_full = false;\n+    RETURN_NOT_OK(detail::BuilderAppend(this->typed_builder_, obj, &is_full));\n+\n+    // Exceeded capacity of builder\n+    if (ARROW_PREDICT_FALSE(is_full)) {\n+      std::shared_ptr<Array> chunk;\n+      RETURN_NOT_OK(this->typed_builder_->Finish(&chunk));\n+      this->chunks_.emplace_back(std::move(chunk));\n+\n+      // Append the item now that the builder has been reset\n+      return detail::BuilderAppend(this->typed_builder_, obj, &is_full);\n     }\n     return Status::OK();\n   }\n+};\n \n-  Status AddSubsequence(int8_t tag, const Array* data,\n-                        const std::vector<int32_t>& offsets, const std::string& name) {\n-    if (data != nullptr) {\n-      DCHECK(data->length() == offsets.back());\n-      std::shared_ptr<Array> offset_array;\n-      Int32Builder builder(::arrow::int32(), pool_);\n-      RETURN_NOT_OK(builder.AppendValues(offsets.data(), offsets.size()));\n-      RETURN_NOT_OK(builder.Finish(&offset_array));\n-      std::shared_ptr<Array> list_array;\n-      RETURN_NOT_OK(ListArray::FromArrays(*offset_array, *data, pool_, &list_array));\n-      auto field = ::arrow::field(name, list_array->type());\n-      auto type = ::arrow::struct_({field});\n-      fields_[tag] = ::arrow::field(\"\", type);\n-      children_[tag] = std::shared_ptr<StructArray>(\n-          new StructArray(type, list_array->length(), {list_array}));\n-      RETURN_NOT_OK(nones_.Append(true));\n-      type_ids_.push_back(tag);\n+class BytesConverter : public BinaryLikeConverter<BinaryType> {};\n+\n+class FixedWidthBytesConverter : public BinaryLikeConverter<FixedSizeBinaryType> {};\n+\n+// For String/UTF8, if strict_conversions enabled, we reject any non-UTF8,\n+// otherwise we allow but return results as BinaryArray\n+template <bool STRICT>\n+class StringConverter : public TypedConverter<StringType, StringConverter<STRICT>> {\n+ public:\n+  StringConverter() : binary_count_(0) {}\n+\n+  Status Append(PyObject* obj, bool* is_full) {\n+    if (STRICT) {\n+      // Force output to be unicode / utf8 and validate that any binary values\n+      // are utf8\n+      bool is_utf8 = false;\n+      RETURN_NOT_OK(string_view_.FromString(obj, &is_utf8));\n+      if (!is_utf8) {\n+        return internal::InvalidValue(obj, \"was not a utf8 string\");\n+      }\n     } else {\n-      DCHECK_EQ(offsets.size(), 1);\n+      // Non-strict conversion; keep track of whether values are unicode or\n+      // bytes; if any bytes are observe, the result will be bytes\n+      if (PyUnicode_Check(obj)) {\n+        RETURN_NOT_OK(string_view_.FromUnicode(obj));\n+      } else {\n+        // If not unicode or bytes, FromBinary will error\n+        RETURN_NOT_OK(string_view_.FromBinary(obj));\n+        ++binary_count_;\n+      }\n+    }\n+\n+    return detail::AppendPyString(this->typed_builder_, string_view_, is_full,\n+                                  [this](const char* bytes, int32_t length) {\n+                                    return this->typed_builder_->Append(bytes, length);\n+                                  });\n+  }\n+\n+  Status AppendItem(PyObject* obj) {\n+    bool is_full = false;\n+    RETURN_NOT_OK(Append(obj, &is_full));\n+\n+    // Exceeded capacity of builder\n+    if (ARROW_PREDICT_FALSE(is_full)) {\n+      std::shared_ptr<Array> chunk;\n+      RETURN_NOT_OK(this->typed_builder_->Finish(&chunk));\n+      this->chunks_.emplace_back(std::move(chunk));\n+\n+      // Append the item now that the builder has been reset\n+      RETURN_NOT_OK(Append(obj, &is_full));\n     }\n     return Status::OK();\n   }\n \n-  /// Finish building the sequence and return the result.\n-  /// Input arrays may be nullptr\n-  Status Finish(const Array* list_data, const Array* tuple_data, const Array* dict_data,\n-                const Array* set_data, std::shared_ptr<Array>* out) {\n-    fields_.resize(num_tags_);\n-    children_.resize(num_tags_);\n-\n-    RETURN_NOT_OK(AddElement(bool_tag_, &bools_));\n-    RETURN_NOT_OK(AddElement(int_tag_, &ints_));\n-    RETURN_NOT_OK(AddElement(py2_int_tag_, &py2_ints_, \"py2_int\"));\n-    RETURN_NOT_OK(AddElement(string_tag_, &strings_));\n-    RETURN_NOT_OK(AddElement(bytes_tag_, &bytes_));\n-    RETURN_NOT_OK(AddElement(half_float_tag_, &half_floats_));\n-    RETURN_NOT_OK(AddElement(float_tag_, &floats_));\n-    RETURN_NOT_OK(AddElement(double_tag_, &doubles_));\n-    RETURN_NOT_OK(AddElement(date64_tag_, &date64s_));\n-    RETURN_NOT_OK(AddElement(tensor_tag_, &tensor_indices_, \"tensor\"));\n-    RETURN_NOT_OK(AddElement(buffer_tag_, &buffer_indices_, \"buffer\"));\n-\n-    RETURN_NOT_OK(AddSubsequence(list_tag_, list_data, list_offsets_, \"list\"));\n-    RETURN_NOT_OK(AddSubsequence(tuple_tag_, tuple_data, tuple_offsets_, \"tuple\"));\n-    RETURN_NOT_OK(AddSubsequence(dict_tag_, dict_data, dict_offsets_, \"dict\"));\n-    RETURN_NOT_OK(AddSubsequence(set_tag_, set_data, set_offsets_, \"set\"));\n-\n-    std::shared_ptr<Array> types_array;\n-    RETURN_NOT_OK(types_.Finish(&types_array));\n-    const auto& types = checked_cast<const Int8Array&>(*types_array);\n-\n-    std::shared_ptr<Array> offsets_array;\n-    RETURN_NOT_OK(offsets_.Finish(&offsets_array));\n-    const auto& offsets = checked_cast<const Int32Array&>(*offsets_array);\n-\n-    std::shared_ptr<Array> nones_array;\n-    RETURN_NOT_OK(nones_.Finish(&nones_array));\n-    const auto& nones = checked_cast<const BooleanArray&>(*nones_array);\n-\n-    auto type = ::arrow::union_(fields_, type_ids_, UnionMode::DENSE);\n-    out->reset(new UnionArray(type, types.length(), children_, types.values(),\n-                              offsets.values(), nones.null_bitmap(), nones.null_count()));\n+  virtual Status GetResult(std::vector<std::shared_ptr<Array>>* out) {\n+    RETURN_NOT_OK(SeqConverter::GetResult(out));\n+\n+    // If we saw any non-unicode, cast results to BinaryArray\n+    if (binary_count_) {\n+      // We should have bailed out earlier\n+      DCHECK(!STRICT);\n+\n+      for (size_t i = 0; i < out->size(); ++i) {\n+        auto binary_data = (*out)[i]->data()->Copy();\n+        binary_data->type = ::arrow::binary();\n+        (*out)[i] = std::make_shared<BinaryArray>(binary_data);\n+      }\n+    }\n     return Status::OK();\n   }\n \n  private:\n-  MemoryPool* pool_;\n-\n-  Int8Builder types_;\n-  Int32Builder offsets_;\n-\n-  BooleanBuilder nones_;\n-  BooleanBuilder bools_;\n-  Int64Builder ints_;\n-  Int64Builder py2_ints_;\n-  BinaryBuilder bytes_;\n-  StringBuilder strings_;\n-  HalfFloatBuilder half_floats_;\n-  FloatBuilder floats_;\n-  DoubleBuilder doubles_;\n-  Date64Builder date64s_;\n-\n-  Int32Builder tensor_indices_;\n-  Int32Builder buffer_indices_;\n-\n-  std::vector<int32_t> list_offsets_;\n-  std::vector<int32_t> tuple_offsets_;\n-  std::vector<int32_t> dict_offsets_;\n-  std::vector<int32_t> set_offsets_;\n-\n-  // Tags for members of the sequence. If they are set to -1 it means\n-  // they are not used and will not part be of the metadata when we call\n-  // SequenceBuilder::Finish. If a member with one of the tags is added,\n-  // the associated variable gets a unique index starting from 0. This\n-  // happens in the UPDATE macro in sequence.cc.\n-  int8_t bool_tag_ = -1;\n-  int8_t int_tag_ = -1;\n-  int8_t py2_int_tag_ = -1;\n-  int8_t string_tag_ = -1;\n-  int8_t bytes_tag_ = -1;\n-  int8_t half_float_tag_ = -1;\n-  int8_t float_tag_ = -1;\n-  int8_t double_tag_ = -1;\n-  int8_t date64_tag_ = -1;\n-\n-  int8_t tensor_tag_ = -1;\n-  int8_t buffer_tag_ = -1;\n-  int8_t list_tag_ = -1;\n-  int8_t tuple_tag_ = -1;\n-  int8_t dict_tag_ = -1;\n-  int8_t set_tag_ = -1;\n-\n-  int8_t num_tags_ = 0;\n-\n-  // Members for the output union constructed in Finish\n-  std::vector<std::shared_ptr<Field>> fields_;\n-  std::vector<std::shared_ptr<Array>> children_;\n-  std::vector<uint8_t> type_ids_;\n+  // Create a single instance of PyBytesView here to prevent unnecessary object\n+  // creation/destruction\n+  PyBytesView string_view_;\n+\n+  int64_t binary_count_;\n };\n \n-/// Constructing dictionaries of key/value pairs. Sequences of\n-/// keys and values are built separately using a pair of\n-/// SequenceBuilders. The resulting Arrow representation\n-/// can be obtained via the Finish method.\n-class DictBuilder {\n+// ----------------------------------------------------------------------\n+// Convert lists (NumPy arrays containing lists or ndarrays as values)\n+\n+class ListConverter : public TypedConverter<ListType, ListConverter> {\n  public:\n-  explicit DictBuilder(MemoryPool* pool = nullptr) : keys_(pool), vals_(pool) {}\n-\n-  /// Builder for the keys of the dictionary\n-  SequenceBuilder& keys() { return keys_; }\n-  /// Builder for the values of the dictionary\n-  SequenceBuilder& vals() { return vals_; }\n-\n-  /// Construct an Arrow StructArray representing the dictionary.\n-  /// Contains a field \"keys\" for the keys and \"vals\" for the values.\n-  /// \\param val_list_data\n-  ///    List containing the data from nested lists in the value\n-  ///   list of the dictionary\n-  ///\n-  /// \\param val_dict_data\n-  ///   List containing the data from nested dictionaries in the\n-  ///   value list of the dictionary\n-  Status Finish(const Array* key_tuple_data, const Array* key_dict_data,\n-                const Array* val_list_data, const Array* val_tuple_data,\n-                const Array* val_dict_data, const Array* val_set_data,\n-                std::shared_ptr<Array>* out) {\n-    // lists and sets can't be keys of dicts in Python, that is why for\n-    // the keys we do not need to collect sublists\n-    std::shared_ptr<Array> keys, vals;\n-    RETURN_NOT_OK(keys_.Finish(nullptr, key_tuple_data, key_dict_data, nullptr, &keys));\n+  explicit ListConverter(bool from_pandas, bool strict_conversions)\n+      : from_pandas_(from_pandas), strict_conversions_(strict_conversions) {}\n+\n+  Status Init(ArrayBuilder* builder) {\n+    builder_ = builder;\n+    typed_builder_ = checked_cast<ListBuilder*>(builder);\n+\n+    value_type_ = checked_cast<const ListType&>(*builder->type()).value_type();\n     RETURN_NOT_OK(\n-        vals_.Finish(val_list_data, val_tuple_data, val_dict_data, val_set_data, &vals));\n-    auto keys_field = std::make_shared<Field>(\"keys\", keys->type());\n-    auto vals_field = std::make_shared<Field>(\"vals\", vals->type());\n-    auto type = std::make_shared<StructType>(\n-        std::vector<std::shared_ptr<Field>>({keys_field, vals_field}));\n-    std::vector<std::shared_ptr<Array>> field_arrays({keys, vals});\n-    DCHECK(keys->length() == vals->length());\n-    out->reset(new StructArray(type, keys->length(), field_arrays));\n-    return Status::OK();\n+        GetConverter(value_type_, from_pandas_, strict_conversions_, &value_converter_));\n+    return value_converter_->Init(typed_builder_->value_builder());\n   }\n \n- private:\n-  SequenceBuilder keys_;\n-  SequenceBuilder vals_;\n-};\n+  template <int NUMPY_TYPE, typename Type>\n+  Status AppendNdarrayTypedItem(PyArrayObject* arr);\n+  Status AppendNdarrayItem(PyObject* arr);\n \n-Status CallCustomCallback(PyObject* context, PyObject* method_name, PyObject* elem,\n-                          PyObject** result) {\n-  *result = NULL;\n-  if (context == Py_None) {\n-    std::stringstream ss;\n-    ss << \"error while calling callback on \" << internal::PyObject_StdStringRepr(elem)\n-       << \": handler not registered\";\n-    return Status::SerializationError(ss.str());\n-  } else {\n-    *result = PyObject_CallMethodObjArgs(context, method_name, elem, NULL);\n-    return PassPyError();\n+  Status AppendItem(PyObject* obj) {\n+    RETURN_NOT_OK(typed_builder_->Append());\n+    if (PyArray_Check(obj)) {\n+      return AppendNdarrayItem(obj);\n+    }\n+    const auto list_size = static_cast<int64_t>(PySequence_Size(obj));\n+    if (ARROW_PREDICT_FALSE(list_size == -1)) {\n+      RETURN_IF_PYERROR();\n+    }\n+    return value_converter_->AppendMultiple(obj, list_size);\n   }\n-  return Status::OK();\n-}\n \n-Status CallSerializeCallback(PyObject* context, PyObject* value,\n-                             PyObject** serialized_object) {\n-  OwnedRef method_name(PyUnicode_FromString(\"_serialize_callback\"));\n-  RETURN_NOT_OK(CallCustomCallback(context, method_name.obj(), value, serialized_object));\n-  if (!PyDict_Check(*serialized_object)) {\n-    return Status::TypeError(\"serialization callback must return a valid dictionary\");\n-  }\n-  return Status::OK();\n-}\n+  // virtual Status GetResult(std::vector<std::shared_ptr<Array>>* chunks) {\n+  //   // TODO: Handle chunked children\n+  //   return SeqConverter::GetResult(chunks);\n+  // }\n \n-Status CallDeserializeCallback(PyObject* context, PyObject* value,\n-                               PyObject** deserialized_object) {\n-  OwnedRef method_name(PyUnicode_FromString(\"_deserialize_callback\"));\n-  return CallCustomCallback(context, method_name.obj(), value, deserialized_object);\n-}\n+ protected:\n+  std::shared_ptr<DataType> value_type_;\n+  std::unique_ptr<SeqConverter> value_converter_;\n+  bool from_pandas_;\n+  bool strict_conversions_;\n+};\n \n-Status SerializeDict(PyObject* context, std::vector<PyObject*> dicts,\n-                     int32_t recursion_depth, std::shared_ptr<Array>* out,\n-                     SerializedPyObject* blobs_out);\n-\n-Status SerializeArray(PyObject* context, PyArrayObject* array, SequenceBuilder* builder,\n-                      std::vector<PyObject*>* subdicts, SerializedPyObject* blobs_out);\n-\n-Status SerializeSequences(PyObject* context, std::vector<PyObject*> sequences,\n-                          int32_t recursion_depth, std::shared_ptr<Array>* out,\n-                          SerializedPyObject* blobs_out);\n-\n-Status AppendScalar(PyObject* obj, SequenceBuilder* builder) {\n-  if (PyArray_IsScalar(obj, Bool)) {\n-    return builder->AppendBool(reinterpret_cast<PyBoolScalarObject*>(obj)->obval != 0);\n-  } else if (PyArray_IsScalar(obj, Half)) {\n-    return builder->AppendHalfFloat(reinterpret_cast<PyHalfScalarObject*>(obj)->obval);\n-  } else if (PyArray_IsScalar(obj, Float)) {\n-    return builder->AppendFloat(reinterpret_cast<PyFloatScalarObject*>(obj)->obval);\n-  } else if (PyArray_IsScalar(obj, Double)) {\n-    return builder->AppendDouble(reinterpret_cast<PyDoubleScalarObject*>(obj)->obval);\n-  }\n-  int64_t value = 0;\n-  if (PyArray_IsScalar(obj, Byte)) {\n-    value = reinterpret_cast<PyByteScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, UByte)) {\n-    value = reinterpret_cast<PyUByteScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, Short)) {\n-    value = reinterpret_cast<PyShortScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, UShort)) {\n-    value = reinterpret_cast<PyUShortScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, Int)) {\n-    value = reinterpret_cast<PyIntScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, UInt)) {\n-    value = reinterpret_cast<PyUIntScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, Long)) {\n-    value = reinterpret_cast<PyLongScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, ULong)) {\n-    value = reinterpret_cast<PyULongScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, LongLong)) {\n-    value = reinterpret_cast<PyLongLongScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, Int64)) {\n-    value = reinterpret_cast<PyInt64ScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, ULongLong)) {\n-    value = reinterpret_cast<PyULongLongScalarObject*>(obj)->obval;\n-  } else if (PyArray_IsScalar(obj, UInt64)) {\n-    value = reinterpret_cast<PyUInt64ScalarObject*>(obj)->obval;\n-  } else {\n-    DCHECK(false) << \"scalar type not recognized\";\n-  }\n-  return builder->AppendInt64(value);\n-}\n+template <int NUMPY_TYPE, typename Type>\n+Status ListConverter::AppendNdarrayTypedItem(PyArrayObject* arr) {\n+  using traits = internal::npy_traits<NUMPY_TYPE>;\n+  using T = typename traits::value_type;\n+  using ValueBuilderType = typename TypeTraits<Type>::BuilderType;\n \n-Status Append(PyObject* context, PyObject* elem, SequenceBuilder* builder,\n-              std::vector<PyObject*>* sublists, std::vector<PyObject*>* subtuples,\n-              std::vector<PyObject*>* subdicts, std::vector<PyObject*>* subsets,\n-              SerializedPyObject* blobs_out) {\n-  // The bool case must precede the int case (PyInt_Check passes for bools)\n-  if (PyBool_Check(elem)) {\n-    RETURN_NOT_OK(builder->AppendBool(elem == Py_True));\n-  } else if (PyArray_DescrFromScalar(elem)->type_num == NPY_HALF) {\n-    npy_half halffloat = reinterpret_cast<PyHalfScalarObject*>(elem)->obval;\n-    RETURN_NOT_OK(builder->AppendHalfFloat(halffloat));\n-  } else if (PyFloat_Check(elem)) {\n-    RETURN_NOT_OK(builder->AppendDouble(PyFloat_AS_DOUBLE(elem)));\n-  } else if (PyLong_Check(elem)) {\n-    int overflow = 0;\n-    int64_t data = PyLong_AsLongLongAndOverflow(elem, &overflow);\n-    if (!overflow) {\n-      RETURN_NOT_OK(builder->AppendInt64(data));\n-    } else {\n-      // Attempt to serialize the object using the custom callback.\n-      PyObject* serialized_object;\n-      // The reference count of serialized_object will be decremented in SerializeDict\n-      RETURN_NOT_OK(CallSerializeCallback(context, elem, &serialized_object));\n-      RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n-      subdicts->push_back(serialized_object);\n+  const bool null_sentinels_possible = (from_pandas_ && traits::supports_nulls);\n+\n+  auto child_builder = checked_cast<ValueBuilderType*>(value_converter_->builder());\n+\n+  // TODO(wesm): Vector append when not strided\n+  Ndarray1DIndexer<T> values(arr);\n+  if (null_sentinels_possible) {\n+    for (int64_t i = 0; i < values.size(); ++i) {\n+      if (traits::isnull(values[i])) {\n+        RETURN_NOT_OK(child_builder->AppendNull());\n+      } else {\n+        RETURN_NOT_OK(child_builder->Append(values[i]));\n+      }\n     }\n-#if PY_MAJOR_VERSION < 3\n-  } else if (PyInt_Check(elem)) {\n-    RETURN_NOT_OK(builder->AppendPy2Int64(static_cast<int64_t>(PyInt_AS_LONG(elem))));\n-#endif\n-  } else if (PyBytes_Check(elem)) {\n-    auto data = reinterpret_cast<uint8_t*>(PyBytes_AS_STRING(elem));\n-    int32_t size;\n-    RETURN_NOT_OK(internal::CastSize(PyBytes_GET_SIZE(elem), &size));\n-    RETURN_NOT_OK(builder->AppendBytes(data, size));\n-  } else if (PyUnicode_Check(elem)) {\n-    PyBytesView view;\n-    RETURN_NOT_OK(view.FromString(elem));\n-    int32_t size;\n-    RETURN_NOT_OK(internal::CastSize(view.size, &size));\n-    RETURN_NOT_OK(builder->AppendString(view.bytes, size));\n-  } else if (PyList_CheckExact(elem)) {\n-    RETURN_NOT_OK(builder->AppendList(PyList_Size(elem)));\n-    sublists->push_back(elem);\n-  } else if (PyDict_CheckExact(elem)) {\n-    RETURN_NOT_OK(builder->AppendDict(PyDict_Size(elem)));\n-    subdicts->push_back(elem);\n-  } else if (PyTuple_CheckExact(elem)) {\n-    RETURN_NOT_OK(builder->AppendTuple(PyTuple_Size(elem)));\n-    subtuples->push_back(elem);\n-  } else if (PySet_Check(elem)) {\n-    RETURN_NOT_OK(builder->AppendSet(PySet_Size(elem)));\n-    subsets->push_back(elem);\n-  } else if (PyArray_IsScalar(elem, Generic)) {\n-    RETURN_NOT_OK(AppendScalar(elem, builder));\n-  } else if (PyArray_CheckExact(elem)) {\n-    RETURN_NOT_OK(SerializeArray(context, reinterpret_cast<PyArrayObject*>(elem), builder,\n-                                 subdicts, blobs_out));\n-  } else if (elem == Py_None) {\n-    RETURN_NOT_OK(builder->AppendNone());\n-  } else if (PyDateTime_Check(elem)) {\n-    PyDateTime_DateTime* datetime = reinterpret_cast<PyDateTime_DateTime*>(elem);\n-    RETURN_NOT_OK(builder->AppendDate64(PyDateTime_to_us(datetime)));\n-  } else if (is_buffer(elem)) {\n-    RETURN_NOT_OK(builder->AppendBuffer(static_cast<int32_t>(blobs_out->buffers.size())));\n-    std::shared_ptr<Buffer> buffer;\n-    RETURN_NOT_OK(unwrap_buffer(elem, &buffer));\n-    blobs_out->buffers.push_back(buffer);\n   } else {\n-    // Attempt to serialize the object using the custom callback.\n-    PyObject* serialized_object;\n-    // The reference count of serialized_object will be decremented in SerializeDict\n-    RETURN_NOT_OK(CallSerializeCallback(context, elem, &serialized_object));\n-    RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n-    subdicts->push_back(serialized_object);\n+    for (int64_t i = 0; i < values.size(); ++i) {\n+      RETURN_NOT_OK(child_builder->Append(values[i]));\n+    }\n   }\n   return Status::OK();\n }\n \n-Status SerializeArray(PyObject* context, PyArrayObject* array, SequenceBuilder* builder,\n-                      std::vector<PyObject*>* subdicts, SerializedPyObject* blobs_out) {\n-  int dtype = PyArray_TYPE(array);\n-  switch (dtype) {\n-    case NPY_UINT8:\n-    case NPY_INT8:\n-    case NPY_UINT16:\n-    case NPY_INT16:\n-    case NPY_UINT32:\n-    case NPY_INT32:\n-    case NPY_UINT64:\n-    case NPY_INT64:\n-    case NPY_HALF:\n-    case NPY_FLOAT:\n-    case NPY_DOUBLE: {\n-      RETURN_NOT_OK(\n-          builder->AppendTensor(static_cast<int32_t>(blobs_out->tensors.size())));\n-      std::shared_ptr<Tensor> tensor;\n-      RETURN_NOT_OK(NdarrayToTensor(default_memory_pool(),\n-                                    reinterpret_cast<PyObject*>(array), &tensor));\n-      blobs_out->tensors.push_back(tensor);\n-    } break;\n+// If the value type does not match the expected NumPy dtype, then fall through\n+// to a slower PySequence-based path\n+#define LIST_FAST_CASE(TYPE, NUMPY_TYPE, ArrowType)               \\\n+  case Type::TYPE: {                                              \\\n+    if (PyArray_DESCR(arr)->type_num != NUMPY_TYPE) {             \\\n+      return value_converter_->AppendMultiple(obj, value_length); \\\n+    }                                                             \\\n+    return AppendNdarrayTypedItem<NUMPY_TYPE, ArrowType>(arr);    \\\n+  }\n+\n+// Use internal::VisitSequence, fast for NPY_OBJECT but slower otherwise\n+#define LIST_SLOW_CASE(TYPE)                                    \\\n+  case Type::TYPE: {                                            \\\n+    return value_converter_->AppendMultiple(obj, value_length); \\\n+  }\n+\n+Status ListConverter::AppendNdarrayItem(PyObject* obj) {\n+  PyArrayObject* arr = reinterpret_cast<PyArrayObject*>(obj);\n+\n+  if (PyArray_NDIM(arr) != 1) {\n+    return Status::Invalid(\"Can only convert 1-dimensional array values\");\n+  }\n+\n+  const int64_t value_length = PyArray_SIZE(arr);\n+\n+  switch (value_type_->id()) {\n+    LIST_SLOW_CASE(NA)\n+    LIST_FAST_CASE(UINT8, NPY_UINT8, UInt8Type)\n+    LIST_FAST_CASE(INT8, NPY_INT8, Int8Type)\n+    LIST_FAST_CASE(UINT16, NPY_UINT16, UInt16Type)\n+    LIST_FAST_CASE(INT16, NPY_INT16, Int16Type)\n+    LIST_FAST_CASE(UINT32, NPY_UINT32, UInt32Type)\n+    LIST_FAST_CASE(INT32, NPY_INT32, Int32Type)\n+    LIST_FAST_CASE(UINT64, NPY_UINT64, UInt64Type)\n+    LIST_FAST_CASE(INT64, NPY_INT64, Int64Type)\n+    LIST_SLOW_CASE(DATE32)\n+    LIST_SLOW_CASE(DATE64)\n+    LIST_SLOW_CASE(TIME64)\n+    LIST_FAST_CASE(TIMESTAMP, NPY_DATETIME, TimestampType)\n+    LIST_FAST_CASE(HALF_FLOAT, NPY_FLOAT16, HalfFloatType)\n+    LIST_FAST_CASE(FLOAT, NPY_FLOAT, FloatType)\n+    LIST_FAST_CASE(DOUBLE, NPY_DOUBLE, DoubleType)\n+    LIST_SLOW_CASE(BINARY)\n+    LIST_SLOW_CASE(FIXED_SIZE_BINARY)\n+    LIST_SLOW_CASE(STRING)\n+    case Type::LIST: {\n+      return value_converter_->AppendSingleVirtual(obj);\n+    }\n     default: {\n-      PyObject* serialized_object;\n-      // The reference count of serialized_object will be decremented in SerializeDict\n-      RETURN_NOT_OK(CallSerializeCallback(context, reinterpret_cast<PyObject*>(array),\n-                                          &serialized_object));\n-      RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n-      subdicts->push_back(serialized_object);\n+      std::stringstream ss;\n+      ss << \"Unknown list item type: \";\n+      ss << value_type_->ToString();\n+      return Status::TypeError(ss.str());\n     }\n   }\n-  return Status::OK();\n }\n \n-Status SerializeSequences(PyObject* context, std::vector<PyObject*> sequences,\n-                          int32_t recursion_depth, std::shared_ptr<Array>* out,\n-                          SerializedPyObject* blobs_out) {\n-  DCHECK(out);\n-  if (recursion_depth >= kMaxRecursionDepth) {\n-    return Status::NotImplemented(\n-        \"This object exceeds the maximum recursion depth. It may contain itself \"\n-        \"recursively.\");\n-  }\n-  SequenceBuilder builder;\n-  std::vector<PyObject*> sublists, subtuples, subdicts, subsets;\n-  for (const auto& sequence : sequences) {\n-    RETURN_NOT_OK(internal::VisitIterable(\n-        sequence, [&](PyObject* obj, bool* keep_going /* unused */) {\n-          return Append(context, obj, &builder, &sublists, &subtuples, &subdicts,\n-                        &subsets, blobs_out);\n-        }));\n-  }\n-  std::shared_ptr<Array> list;\n-  if (sublists.size() > 0) {\n-    RETURN_NOT_OK(\n-        SerializeSequences(context, sublists, recursion_depth + 1, &list, blobs_out));\n-  }\n-  std::shared_ptr<Array> tuple;\n-  if (subtuples.size() > 0) {\n-    RETURN_NOT_OK(\n-        SerializeSequences(context, subtuples, recursion_depth + 1, &tuple, blobs_out));\n-  }\n-  std::shared_ptr<Array> dict;\n-  if (subdicts.size() > 0) {\n-    RETURN_NOT_OK(\n-        SerializeDict(context, subdicts, recursion_depth + 1, &dict, blobs_out));\n-  }\n-  std::shared_ptr<Array> set;\n-  if (subsets.size() > 0) {\n-    RETURN_NOT_OK(\n-        SerializeSequences(context, subsets, recursion_depth + 1, &set, blobs_out));\n-  }\n-  return builder.Finish(list.get(), tuple.get(), dict.get(), set.get(), out);\n-}\n+// ----------------------------------------------------------------------\n+// Convert structs\n+\n+class StructConverter : public TypedConverter<StructType, StructConverter> {\n+ public:\n+  explicit StructConverter(bool from_pandas, bool strict_conversions)\n+      : from_pandas_(from_pandas), strict_conversions_(strict_conversions) {}\n+\n+  Status Init(ArrayBuilder* builder) {\n+    builder_ = builder;\n+    typed_builder_ = checked_cast<StructBuilder*>(builder);\n+    const auto& struct_type = checked_cast<const StructType&>(*builder->type());\n+\n+    num_fields_ = typed_builder_->num_fields();\n+    DCHECK_EQ(num_fields_, struct_type.num_children());\n+\n+    field_name_list_.reset(PyList_New(num_fields_));\n+    RETURN_IF_PYERROR();\n+\n+    // Initialize the child converters and field names\n+    for (int i = 0; i < num_fields_; i++) {\n+      const std::string& field_name(struct_type.child(i)->name());\n+      std::shared_ptr<DataType> field_type(struct_type.child(i)->type());\n+\n+      std::unique_ptr<SeqConverter> value_converter;\n+      RETURN_NOT_OK(\n+          GetConverter(field_type, from_pandas_, strict_conversions_, &value_converter));\n+      RETURN_NOT_OK(value_converter->Init(typed_builder_->field_builder(i)));\n+      value_converters_.push_back(std::move(value_converter));\n \n-Status SerializeDict(PyObject* context, std::vector<PyObject*> dicts,\n-                     int32_t recursion_depth, std::shared_ptr<Array>* out,\n-                     SerializedPyObject* blobs_out) {\n-  DictBuilder result;\n-  if (recursion_depth >= kMaxRecursionDepth) {\n-    return Status::NotImplemented(\n-        \"This object exceeds the maximum recursion depth. It may contain itself \"\n-        \"recursively.\");\n-  }\n-  std::vector<PyObject*> key_tuples, key_dicts, val_lists, val_tuples, val_dicts,\n-      val_sets, dummy;\n-  for (const auto& dict : dicts) {\n-    PyObject* key;\n-    PyObject* value;\n-    Py_ssize_t pos = 0;\n-    while (PyDict_Next(dict, &pos, &key, &value)) {\n-      RETURN_NOT_OK(Append(context, key, &result.keys(), &dummy, &key_tuples, &key_dicts,\n-                           &dummy, blobs_out));\n-      DCHECK_EQ(dummy.size(), 0);\n-      RETURN_NOT_OK(Append(context, value, &result.vals(), &val_lists, &val_tuples,\n-                           &val_dicts, &val_sets, blobs_out));\n+      // Store the field name as a PyObject, for dict matching\n+      PyObject* nameobj =\n+          PyUnicode_FromStringAndSize(field_name.c_str(), field_name.size());\n+      RETURN_IF_PYERROR();\n+      PyList_SET_ITEM(field_name_list_.obj(), i, nameobj);\n     }\n+\n+    return Status::OK();\n   }\n-  std::shared_ptr<Array> key_tuples_arr;\n-  if (key_tuples.size() > 0) {\n-    RETURN_NOT_OK(SerializeSequences(context, key_tuples, recursion_depth + 1,\n-                                     &key_tuples_arr, blobs_out));\n-  }\n-  std::shared_ptr<Array> key_dicts_arr;\n-  if (key_dicts.size() > 0) {\n-    RETURN_NOT_OK(SerializeDict(context, key_dicts, recursion_depth + 1, &key_dicts_arr,\n-                                blobs_out));\n-  }\n-  std::shared_ptr<Array> val_list_arr;\n-  if (val_lists.size() > 0) {\n-    RETURN_NOT_OK(SerializeSequences(context, val_lists, recursion_depth + 1,\n-                                     &val_list_arr, blobs_out));\n-  }\n-  std::shared_ptr<Array> val_tuples_arr;\n-  if (val_tuples.size() > 0) {\n-    RETURN_NOT_OK(SerializeSequences(context, val_tuples, recursion_depth + 1,\n-                                     &val_tuples_arr, blobs_out));\n-  }\n-  std::shared_ptr<Array> val_dict_arr;\n-  if (val_dicts.size() > 0) {\n-    RETURN_NOT_OK(\n-        SerializeDict(context, val_dicts, recursion_depth + 1, &val_dict_arr, blobs_out));\n-  }\n-  std::shared_ptr<Array> val_set_arr;\n-  if (val_sets.size() > 0) {\n-    RETURN_NOT_OK(SerializeSequences(context, val_sets, recursion_depth + 1, &val_set_arr,\n-                                     blobs_out));\n-  }\n-  RETURN_NOT_OK(result.Finish(key_tuples_arr.get(), key_dicts_arr.get(),\n-                              val_list_arr.get(), val_tuples_arr.get(),\n-                              val_dict_arr.get(), val_set_arr.get(), out));\n-\n-  // This block is used to decrement the reference counts of the results\n-  // returned by the serialization callback, which is called in SerializeArray,\n-  // in DeserializeDict and in Append\n-  static PyObject* py_type = PyUnicode_FromString(\"_pytype_\");\n-  for (const auto& dict : dicts) {\n-    if (PyDict_Contains(dict, py_type)) {\n-      // If the dictionary contains the key \"_pytype_\", then the user has to\n-      // have registered a callback.\n-      if (context == Py_None) {\n-        return Status::Invalid(\"No serialization callback set\");\n+\n+  Status AppendItem(PyObject* obj) {\n+    RETURN_NOT_OK(typed_builder_->Append());\n+    // Note heterogenous sequences are not allowed\n+    if (ARROW_PREDICT_FALSE(source_kind_ == UNKNOWN)) {\n+      if (PyDict_Check(obj)) {\n+        source_kind_ = DICTS;\n+      } else if (PyTuple_Check(obj)) {\n+        source_kind_ = TUPLES;\n       }\n-      Py_XDECREF(dict);\n+    }\n+    if (PyDict_Check(obj) && source_kind_ == DICTS) {\n+      return AppendDictItem(obj);\n+    } else if (PyTuple_Check(obj) && source_kind_ == TUPLES) {\n+      return AppendTupleItem(obj);\n+    } else {\n+      return Status::TypeError(\"Expected sequence of dicts or tuples for struct type\");\n     }\n   }\n \n-  return Status::OK();\n-}\n-\n-std::shared_ptr<RecordBatch> MakeBatch(std::shared_ptr<Array> data) {\n-  auto field = std::make_shared<Field>(\"list\", data->type());\n-  auto schema = ::arrow::schema({field});\n-  return RecordBatch::Make(schema, data->length(), {data});\n-}\n+  // Append a missing item\n+  Status AppendNull() {\n+    RETURN_NOT_OK(typed_builder_->AppendNull());\n+    // Need to also insert a missing item on all child builders\n+    // (compare with ListConverter)\n+    for (int i = 0; i < num_fields_; i++) {\n+      RETURN_NOT_OK(value_converters_[i]->AppendSingleVirtual(Py_None));\n+    }\n+    return Status::OK();\n+  }\n \n-Status SerializeObject(PyObject* context, PyObject* sequence, SerializedPyObject* out) {\n-  PyAcquireGIL lock;\n-  PyDateTime_IMPORT;\n-  import_pyarrow();\n-  std::vector<PyObject*> sequences = {sequence};\n-  std::shared_ptr<Array> array;\n-  RETURN_NOT_OK(SerializeSequences(context, sequences, 0, &array, out));\n-  out->batch = MakeBatch(array);\n-  return Status::OK();\n-}\n+ protected:\n+  Status AppendDictItem(PyObject* obj) {\n+    // NOTE we're ignoring any extraneous dict items\n+    for (int i = 0; i < num_fields_; i++) {\n+      PyObject* nameobj = PyList_GET_ITEM(field_name_list_.obj(), i);\n+      PyObject* valueobj = PyDict_GetItem(obj, nameobj);  // borrowed\n+      RETURN_IF_PYERROR();\n+      RETURN_NOT_OK(\n+          value_converters_[i]->AppendSingleVirtual(valueobj ? valueobj : Py_None));\n+    }\n+    return Status::OK();\n+  }\n \n-Status SerializeTensor(std::shared_ptr<Tensor> tensor, SerializedPyObject* out) {\n-  std::shared_ptr<Array> array;\n-  SequenceBuilder builder;\n-  RETURN_NOT_OK(builder.AppendTensor(static_cast<int32_t>(out->tensors.size())));\n-  out->tensors.push_back(tensor);\n-  RETURN_NOT_OK(builder.Finish(nullptr, nullptr, nullptr, nullptr, &array));\n-  out->batch = MakeBatch(array);\n-  return Status::OK();\n-}\n+  Status AppendTupleItem(PyObject* obj) {\n+    if (PyTuple_GET_SIZE(obj) != num_fields_) {\n+      return Status::Invalid(\"Tuple size must be equal to number of struct fields\");\n+    }\n+    for (int i = 0; i < num_fields_; i++) {\n+      PyObject* valueobj = PyTuple_GET_ITEM(obj, i);\n+      RETURN_NOT_OK(value_converters_[i]->AppendSingleVirtual(valueobj));\n+    }\n+    return Status::OK();\n+  }\n \n-Status WriteTensorHeader(std::shared_ptr<DataType> dtype,\n-                         const std::vector<int64_t>& shape, int64_t tensor_num_bytes,\n-                         io::OutputStream* dst) {\n-  auto empty_tensor = std::make_shared<Tensor>(\n-      dtype, std::make_shared<Buffer>(nullptr, tensor_num_bytes), shape);\n-  SerializedPyObject serialized_tensor;\n-  RETURN_NOT_OK(SerializeTensor(empty_tensor, &serialized_tensor));\n-  return serialized_tensor.WriteTo(dst);\n-}\n+  std::vector<std::unique_ptr<SeqConverter>> value_converters_;\n+  OwnedRef field_name_list_;\n+  int num_fields_;\n+  // Whether we're converting from a sequence of dicts or tuples\n+  enum { UNKNOWN, DICTS, TUPLES } source_kind_ = UNKNOWN;\n+  bool from_pandas_;\n+  bool strict_conversions_;\n+};\n \n-Status SerializedPyObject::WriteTo(io::OutputStream* dst) {\n-  int32_t num_tensors = static_cast<int32_t>(this->tensors.size());\n-  int32_t num_buffers = static_cast<int32_t>(this->buffers.size());\n-  RETURN_NOT_OK(\n-      dst->Write(reinterpret_cast<const uint8_t*>(&num_tensors), sizeof(int32_t)));\n-  RETURN_NOT_OK(\n-      dst->Write(reinterpret_cast<const uint8_t*>(&num_buffers), sizeof(int32_t)));\n-  RETURN_NOT_OK(ipc::WriteRecordBatchStream({this->batch}, dst));\n+class DecimalConverter : public TypedConverter<arrow::Decimal128Type, DecimalConverter> {\n+ public:\n+  using BASE = TypedConverter<arrow::Decimal128Type, DecimalConverter>;\n \n-  int32_t metadata_length;\n-  int64_t body_length;\n-  for (const auto& tensor : this->tensors) {\n-    RETURN_NOT_OK(ipc::WriteTensor(*tensor, dst, &metadata_length, &body_length));\n+  Status Init(ArrayBuilder* builder) override {\n+    RETURN_NOT_OK(BASE::Init(builder));\n+    decimal_type_ = checked_cast<const DecimalType*>(typed_builder_->type().get());\n+    return Status::OK();\n   }\n \n-  for (const auto& buffer : this->buffers) {\n-    int64_t size = buffer->size();\n-    RETURN_NOT_OK(dst->Write(reinterpret_cast<const uint8_t*>(&size), sizeof(int64_t)));\n-    RETURN_NOT_OK(dst->Write(buffer->data(), size));\n+  Status AppendItem(PyObject* obj) {\n+    if (internal::PyDecimal_Check(obj)) {\n+      Decimal128 value;\n+      RETURN_NOT_OK(internal::DecimalFromPythonDecimal(obj, *decimal_type_, &value));\n+      return typed_builder_->Append(value);\n+    } else {\n+      // PyObject_IsInstance could error and set an exception\n+      RETURN_IF_PYERROR();\n+      return internal::InvalidValue(obj, \"converting to Decimal128\");\n+    }\n   }\n \n+ private:\n+  const DecimalType* decimal_type_;\n+};\n+\n+#define NUMERIC_CONVERTER(TYPE_ENUM, TYPE)                           \\\n+  case Type::TYPE_ENUM:                                              \\\n+    if (from_pandas) {                                               \\\n+      *out = std::unique_ptr<SeqConverter>(                          \\\n+          new NumericConverter<TYPE, NullCoding::PANDAS_SENTINELS>); \\\n+    } else {                                                         \\\n+      *out = std::unique_ptr<SeqConverter>(                          \\\n+          new NumericConverter<TYPE, NullCoding::NONE_ONLY>);        \\\n+    }                                                                \\\n+    break;\n+\n+#define SIMPLE_CONVERTER_CASE(TYPE_ENUM, TYPE_CLASS)      \\\n+  case Type::TYPE_ENUM:                                   \\\n+    *out = std::unique_ptr<SeqConverter>(new TYPE_CLASS); \\\n+    break;\n+\n+// Dynamic constructor for sequence converters\n+Status GetConverter(const std::shared_ptr<DataType>& type, bool from_pandas,\n+                    bool strict_conversions, std::unique_ptr<SeqConverter>* out) {\n+  switch (type->id()) {\n+    SIMPLE_CONVERTER_CASE(NA, NullConverter);\n+    SIMPLE_CONVERTER_CASE(BOOL, BoolConverter);\n+    NUMERIC_CONVERTER(INT8, Int8Type);\n+    NUMERIC_CONVERTER(INT16, Int16Type);\n+    NUMERIC_CONVERTER(INT32, Int32Type);\n+    NUMERIC_CONVERTER(INT64, Int64Type);\n+    NUMERIC_CONVERTER(UINT8, UInt8Type);\n+    NUMERIC_CONVERTER(UINT16, UInt16Type);\n+    NUMERIC_CONVERTER(UINT32, UInt32Type);\n+    NUMERIC_CONVERTER(UINT64, UInt64Type);\n+    SIMPLE_CONVERTER_CASE(DATE32, Date32Converter);\n+    SIMPLE_CONVERTER_CASE(DATE64, Date64Converter);\n+    NUMERIC_CONVERTER(HALF_FLOAT, HalfFloatType);\n+    NUMERIC_CONVERTER(FLOAT, FloatType);\n+    NUMERIC_CONVERTER(DOUBLE, DoubleType);\n+    case Type::STRING:\n+      if (strict_conversions) {\n+        *out = std::unique_ptr<SeqConverter>(new StringConverter<true>());\n+      } else {\n+        *out = std::unique_ptr<SeqConverter>(new StringConverter<false>());\n+      }\n+      break;\n+      SIMPLE_CONVERTER_CASE(BINARY, BytesConverter);\n+      SIMPLE_CONVERTER_CASE(FIXED_SIZE_BINARY, FixedWidthBytesConverter);\n+    case Type::TIMESTAMP: {\n+      *out = std::unique_ptr<SeqConverter>(\n+          new TimestampConverter(checked_cast<const TimestampType&>(*type).unit()));\n+      break;\n+    }\n+    case Type::TIME32: {\n+      return Status::NotImplemented(\"No sequence converter for time32 available\");\n+    }\n+      SIMPLE_CONVERTER_CASE(TIME64, TimeConverter);\n+      SIMPLE_CONVERTER_CASE(DECIMAL, DecimalConverter);\n+    case Type::LIST:\n+      *out = std::unique_ptr<SeqConverter>(\n+          new ListConverter(from_pandas, strict_conversions));\n+      break;\n+    case Type::STRUCT:\n+      *out = std::unique_ptr<SeqConverter>(\n+          new StructConverter(from_pandas, strict_conversions));\n+      break;\n+    default:\n+      std::stringstream ss;\n+      ss << \"Sequence converter for type \" << type->ToString() << \" not implemented\";\n+      return Status::NotImplemented(ss.str());\n+  }\n   return Status::OK();\n }\n \n-Status SerializedPyObject::GetComponents(MemoryPool* memory_pool, PyObject** out) {\n-  PyAcquireGIL py_gil;\n-\n-  OwnedRef result(PyDict_New());\n-  PyObject* buffers = PyList_New(0);\n+// ----------------------------------------------------------------------\n+\n+// Convert *obj* to a sequence if necessary\n+// Fill *size* to its length.  If >= 0 on entry, *size* is an upper size\n+// bound that may lead to truncation.\n+Status ConvertToSequenceAndInferSize(PyObject* obj, PyObject** seq, int64_t* size) {\n+  if (PySequence_Check(obj)) {\n+    // obj is already a sequence\n+    int64_t real_size = static_cast<int64_t>(PySequence_Size(obj));\n+    if (*size < 0) {\n+      *size = real_size;\n+    } else {\n+      *size = std::min(real_size, *size);\n+    }\n+    Py_INCREF(obj);\n+    *seq = obj;\n+  } else if (*size < 0) {\n+    // unknown size, exhaust iterator\n+    *seq = PySequence_List(obj);\n+    RETURN_IF_PYERROR();\n+    *size = static_cast<int64_t>(PyList_GET_SIZE(*seq));\n+  } else {\n+    // size is known but iterator could be infinite\n+    Py_ssize_t i, n = *size;\n+    PyObject* iter = PyObject_GetIter(obj);\n+    RETURN_IF_PYERROR();\n+    OwnedRef iter_ref(iter);\n+    PyObject* lst = PyList_New(n);\n+    RETURN_IF_PYERROR();\n+    for (i = 0; i < n; i++) {\n+      PyObject* item = PyIter_Next(iter);\n+      if (!item) break;\n+      PyList_SET_ITEM(lst, i, item);\n+    }\n+    // Shrink list if len(iterator) < size\n+    if (i < n && PyList_SetSlice(lst, i, n, NULL)) {\n+      Py_DECREF(lst);\n+      return Status::UnknownError(\"failed to resize list\");\n+    }\n+    *seq = lst;\n+    *size = std::min<int64_t>(i, *size);\n+  }\n+  return Status::OK();\n+}\n \n-  // TODO(wesm): Not sure how pedantic we need to be about checking the return\n-  // values of these functions. There are other places where we do not check\n-  // PyDict_SetItem/SetItemString return value, but these failures would be\n-  // quite esoteric\n-  PyDict_SetItemString(result.obj(), \"num_tensors\",\n-                       PyLong_FromSize_t(this->tensors.size()));\n-  PyDict_SetItemString(result.obj(), \"num_buffers\",\n-                       PyLong_FromSize_t(this->buffers.size()));\n-  PyDict_SetItemString(result.obj(), \"data\", buffers);\n-  RETURN_IF_PYERROR();\n+Status ConvertPySequence(PyObject* sequence_source, PyObject* mask,\n+                         const PyConversionOptions& options,\n+                         std::shared_ptr<ChunkedArray>* out) {\n+  PyAcquireGIL lock;\n \n-  Py_DECREF(buffers);\n+  PyDateTime_IMPORT;\n \n-  auto PushBuffer = [&buffers](const std::shared_ptr<Buffer>& buffer) {\n-    PyObject* wrapped_buffer = wrap_buffer(buffer);\n-    RETURN_IF_PYERROR();\n-    if (PyList_Append(buffers, wrapped_buffer) < 0) {\n-      Py_DECREF(wrapped_buffer);\n-      RETURN_IF_PYERROR();\n-    }\n-    Py_DECREF(wrapped_buffer);\n-    return Status::OK();\n-  };\n+  PyObject* seq;\n+  OwnedRef tmp_seq_nanny;\n \n-  constexpr int64_t kInitialCapacity = 1024;\n+  std::shared_ptr<DataType> real_type;\n \n-  // Write the record batch describing the object structure\n-  std::shared_ptr<io::BufferOutputStream> stream;\n-  std::shared_ptr<Buffer> buffer;\n+  int64_t size = options.size;\n+  RETURN_NOT_OK(ConvertToSequenceAndInferSize(sequence_source, &seq, &size));\n+  tmp_seq_nanny.reset(seq);\n \n-  py_gil.release();\n-  RETURN_NOT_OK(io::BufferOutputStream::Create(kInitialCapacity, memory_pool, &stream));\n-  RETURN_NOT_OK(ipc::WriteRecordBatchStream({this->batch}, stream.get()));\n-  RETURN_NOT_OK(stream->Finish(&buffer));\n-  py_gil.acquire();\n+  // In some cases, type inference may be \"loose\", like strings. If the user\n+  // passed pa.string(), then we will error if we encounter any non-UTF8\n+  // value. If not, then we will allow the result to be a BinaryArray\n+  bool strict_conversions = false;\n \n-  RETURN_NOT_OK(PushBuffer(buffer));\n+  if (options.type == nullptr) {\n+    RETURN_NOT_OK(InferArrowType(seq, &real_type));\n+  } else {\n+    real_type = options.type;\n+    strict_conversions = true;\n+  }\n+  DCHECK_GE(size, 0);\n \n-  // For each tensor, get a metadata buffer and a buffer for the body\n-  for (const auto& tensor : this->tensors) {\n-    std::unique_ptr<ipc::Message> message;\n-    RETURN_NOT_OK(ipc::GetTensorMessage(*tensor, memory_pool, &message));\n-    RETURN_NOT_OK(PushBuffer(message->metadata()));\n-    RETURN_NOT_OK(PushBuffer(message->body()));\n+  // Handle NA / NullType case\n+  if (real_type->id() == Type::NA) {\n+    ArrayVector chunks = {std::make_shared<NullArray>(size)};\n+    *out = std::make_shared<ChunkedArray>(chunks);\n+    return Status::OK();\n   }\n \n-  for (const auto& buf : this->buffers) {\n-    RETURN_NOT_OK(PushBuffer(buf));\n+  // Create the sequence converter, initialize with the builder\n+  std::unique_ptr<SeqConverter> converter;\n+  RETURN_NOT_OK(\n+      GetConverter(real_type, options.from_pandas, strict_conversions, &converter));\n+\n+  // Create ArrayBuilder for type, then pass into the SeqConverter\n+  // instance. The reason this is created here rather than in GetConverter is\n+  // because of nested types (child SeqConverter objects need the child\n+  // builders created by MakeBuilder)\n+  std::unique_ptr<ArrayBuilder> type_builder;\n+  RETURN_NOT_OK(MakeBuilder(options.pool, real_type, &type_builder));\n+  RETURN_NOT_OK(converter->Init(type_builder.get()));\n+\n+  // Convert values\n+  if (mask != nullptr && mask != Py_None) {\n+    RETURN_NOT_OK(converter->AppendMultipleMasked(seq, mask, size));\n+  } else {\n+    RETURN_NOT_OK(converter->AppendMultiple(seq, size));\n   }\n \n-  *out = result.detach();\n+  // Retrieve result. Conversion may yield one or more array values\n+  std::vector<std::shared_ptr<Array>> chunks;\n+  RETURN_NOT_OK(converter->GetResult(&chunks));\n+\n+  *out = std::make_shared<ChunkedArray>(chunks);\n   return Status::OK();\n }\n \n+Status ConvertPySequence(PyObject* obj, const PyConversionOptions& options,\n+                         std::shared_ptr<ChunkedArray>* out) {\n+  return ConvertPySequence(obj, nullptr, options, out);\n+}\n+\n }  // namespace py\n }  // namespace arrow\ndiff --git a/cpp/src/arrow/python/python_to_arrow.h b/cpp/src/arrow/python/python_to_arrow.h\nindex d0bc44ad0a..d133089f97 100644\n--- a/cpp/src/arrow/python/python_to_arrow.h\n+++ b/cpp/src/arrow/python/python_to_arrow.h\n@@ -15,99 +15,69 @@\n // specific language governing permissions and limitations\n // under the License.\n \n-#ifndef ARROW_PYTHON_PYTHON_TO_ARROW_H\n-#define ARROW_PYTHON_PYTHON_TO_ARROW_H\n+// Functions for converting between CPython built-in data structures and Arrow\n+// data structures\n \n+#ifndef ARROW_PYTHON_ADAPTERS_BUILTIN_H\n+#define ARROW_PYTHON_ADAPTERS_BUILTIN_H\n+\n+#include \"arrow/python/platform.h\"\n+\n+#include <cstdint>\n #include <memory>\n-#include <vector>\n \n-#include \"arrow/status.h\"\n+#include \"arrow/type.h\"\n+#include \"arrow/util/macros.h\"\n #include \"arrow/util/visibility.h\"\n \n-// Forward declaring PyObject, see\n-// https://mail.python.org/pipermail/python-dev/2003-August/037601.html\n-#ifndef PyObject_HEAD\n-struct _object;\n-typedef _object PyObject;\n-#endif\n+#include \"arrow/python/common.h\"\n \n namespace arrow {\n \n-class Buffer;\n-class DataType;\n-class MemoryPool;\n-class RecordBatch;\n-class Tensor;\n+class Array;\n+class Status;\n \n-namespace io {\n+namespace py {\n \n-class OutputStream;\n+struct PyConversionOptions {\n+  PyConversionOptions() : type(NULLPTR), size(-1), pool(NULLPTR), from_pandas(false) {}\n \n-}  // namespace io\n+  PyConversionOptions(const std::shared_ptr<DataType>& type, int64_t size,\n+                      MemoryPool* pool, bool from_pandas)\n+      : type(type), size(size), pool(default_memory_pool()), from_pandas(from_pandas) {}\n \n-namespace py {\n+  // Set to null if to be inferred\n+  std::shared_ptr<DataType> type;\n \n-struct ARROW_EXPORT SerializedPyObject {\n-  std::shared_ptr<RecordBatch> batch;\n-  std::vector<std::shared_ptr<Tensor>> tensors;\n-  std::vector<std::shared_ptr<Buffer>> buffers;\n-\n-  /// \\brief Write serialized Python object to OutputStream\n-  /// \\param[in,out] dst an OutputStream\n-  /// \\return Status\n-  Status WriteTo(io::OutputStream* dst);\n-\n-  /// \\brief Convert SerializedPyObject to a dict containing the message\n-  /// components as Buffer instances with minimal memory allocation\n-  ///\n-  /// {\n-  ///   'num_tensors': N,\n-  ///   'num_buffers': K,\n-  ///   'data': [Buffer]\n-  /// }\n-  ///\n-  /// Each tensor is written as two buffers, one for the metadata and one for\n-  /// the body. Therefore, the number of buffers in 'data' is 2 * N + K + 1,\n-  /// with the first buffer containing the serialized record batch containing\n-  /// the UnionArray that describes the whole object\n-  Status GetComponents(MemoryPool* pool, PyObject** out);\n-};\n+  // Default is -1: infer from data\n+  int64_t size;\n \n-/// \\brief Serialize Python sequence as a SerializedPyObject.\n-/// \\param[in] context Serialization context which contains custom serialization\n-/// and deserialization callbacks. Can be any Python object with a\n-/// _serialize_callback method for serialization and a _deserialize_callback\n-/// method for deserialization. If context is None, no custom serialization\n-/// will be attempted.\n-/// \\param[in] sequence A Python sequence object to serialize to Arrow data\n-/// structures\n-/// \\param[out] out The serialized representation\n-/// \\return Status\n-///\n-/// Release GIL before calling\n-ARROW_EXPORT\n-Status SerializeObject(PyObject* context, PyObject* sequence, SerializedPyObject* out);\n+  // Memory pool to use for allocations\n+  MemoryPool* pool;\n+\n+  // Default false\n+  bool from_pandas;\n+};\n \n-/// \\brief Serialize an Arrow Tensor as a SerializedPyObject.\n-/// \\param[in] tensor Tensor to be serialized\n-/// \\param[out] out The serialized representation\n+/// \\brief Convert sequence (list, generator, NumPy array with dtype object) of\n+/// Python objects.\n+/// \\param[in] obj the sequence to convert\n+/// \\param[in] mask a NumPy array of true/false values to indicate whether\n+/// values in the sequence are null (true) or not null (false). This parameter\n+/// may be null\n+/// \\param[in] options various conversion options\n+/// \\param[out] out a ChunkedArray containing one or more chunks\n /// \\return Status\n ARROW_EXPORT\n-Status SerializeTensor(std::shared_ptr<Tensor> tensor, py::SerializedPyObject* out);\n+Status ConvertPySequence(PyObject* obj, PyObject* mask,\n+                         const PyConversionOptions& options,\n+                         std::shared_ptr<ChunkedArray>* out);\n \n-/// \\brief Write the Tensor metadata header to an OutputStream.\n-/// \\param[in] dtype DataType of the Tensor\n-/// \\param[in] shape The shape of the tensor\n-/// \\param[in] tensor_num_bytes The lengh of the Tensor data in bytes\n-/// \\param[in] dst The OutputStream to write the Tensor header to\n-/// \\return Status\n ARROW_EXPORT\n-Status WriteTensorHeader(std::shared_ptr<DataType> dtype,\n-                         const std::vector<int64_t>& shape, int64_t tensor_num_bytes,\n-                         io::OutputStream* dst);\n+Status ConvertPySequence(PyObject* obj, const PyConversionOptions& options,\n+                         std::shared_ptr<ChunkedArray>* out);\n \n }  // namespace py\n-\n }  // namespace arrow\n \n-#endif  // ARROW_PYTHON_PYTHON_TO_ARROW_H\n+#endif  // ARROW_PYTHON_ADAPTERS_BUILTIN_H\ndiff --git a/cpp/src/arrow/python/serialize.cc b/cpp/src/arrow/python/serialize.cc\nnew file mode 100644\nindex 0000000000..3d4267bad0\n--- /dev/null\n+++ b/cpp/src/arrow/python/serialize.cc\n@@ -0,0 +1,828 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#include \"arrow/python/serialize.h\"\n+#include \"arrow/python/numpy_interop.h\"\n+\n+#include <cstdint>\n+#include <limits>\n+#include <memory>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+#include <numpy/arrayobject.h>\n+#include <numpy/arrayscalars.h>\n+\n+#include \"arrow/array.h\"\n+#include \"arrow/builder.h\"\n+#include \"arrow/io/interfaces.h\"\n+#include \"arrow/io/memory.h\"\n+#include \"arrow/ipc/writer.h\"\n+#include \"arrow/memory_pool.h\"\n+#include \"arrow/record_batch.h\"\n+#include \"arrow/tensor.h\"\n+#include \"arrow/util/logging.h\"\n+\n+#include \"arrow/python/common.h\"\n+#include \"arrow/python/helpers.h\"\n+#include \"arrow/python/iterators.h\"\n+#include \"arrow/python/numpy_convert.h\"\n+#include \"arrow/python/platform.h\"\n+#include \"arrow/python/pyarrow.h\"\n+#include \"arrow/python/util/datetime.h\"\n+\n+constexpr int32_t kMaxRecursionDepth = 100;\n+\n+namespace arrow {\n+namespace py {\n+\n+/// A Sequence is a heterogeneous collections of elements. It can contain\n+/// scalar Python types, lists, tuples, dictionaries and tensors.\n+class SequenceBuilder {\n+ public:\n+  explicit SequenceBuilder(MemoryPool* pool ARROW_MEMORY_POOL_DEFAULT)\n+      : pool_(pool),\n+        types_(::arrow::int8(), pool),\n+        offsets_(::arrow::int32(), pool),\n+        nones_(pool),\n+        bools_(::arrow::boolean(), pool),\n+        ints_(::arrow::int64(), pool),\n+        py2_ints_(::arrow::int64(), pool),\n+        bytes_(::arrow::binary(), pool),\n+        strings_(pool),\n+        half_floats_(::arrow::float16(), pool),\n+        floats_(::arrow::float32(), pool),\n+        doubles_(::arrow::float64(), pool),\n+        date64s_(::arrow::date64(), pool),\n+        tensor_indices_(::arrow::int32(), pool),\n+        buffer_indices_(::arrow::int32(), pool),\n+        list_offsets_({0}),\n+        tuple_offsets_({0}),\n+        dict_offsets_({0}),\n+        set_offsets_({0}) {}\n+\n+  /// Appending a none to the sequence\n+  Status AppendNone() {\n+    RETURN_NOT_OK(offsets_.Append(0));\n+    RETURN_NOT_OK(types_.Append(0));\n+    return nones_.AppendNull();\n+  }\n+\n+  Status Update(int64_t offset, int8_t* tag) {\n+    if (*tag == -1) {\n+      *tag = num_tags_++;\n+    }\n+    int32_t offset32;\n+    RETURN_NOT_OK(internal::CastSize(offset, &offset32));\n+    RETURN_NOT_OK(offsets_.Append(offset32));\n+    RETURN_NOT_OK(types_.Append(*tag));\n+    return nones_.Append(true);\n+  }\n+\n+  template <typename BuilderType, typename T>\n+  Status AppendPrimitive(const T val, int8_t* tag, BuilderType* out) {\n+    RETURN_NOT_OK(Update(out->length(), tag));\n+    return out->Append(val);\n+  }\n+\n+  /// Appending a boolean to the sequence\n+  Status AppendBool(const bool data) {\n+    return AppendPrimitive(data, &bool_tag_, &bools_);\n+  }\n+\n+  /// Appending a python 2 int64_t to the sequence\n+  Status AppendPy2Int64(const int64_t data) {\n+    return AppendPrimitive(data, &py2_int_tag_, &py2_ints_);\n+  }\n+\n+  /// Appending an int64_t to the sequence\n+  Status AppendInt64(const int64_t data) {\n+    return AppendPrimitive(data, &int_tag_, &ints_);\n+  }\n+\n+  /// Appending an uint64_t to the sequence\n+  Status AppendUInt64(const uint64_t data) {\n+    // TODO(wesm): Bounds check\n+    return AppendPrimitive(static_cast<int64_t>(data), &int_tag_, &ints_);\n+  }\n+\n+  /// Append a list of bytes to the sequence\n+  Status AppendBytes(const uint8_t* data, int32_t length) {\n+    RETURN_NOT_OK(Update(bytes_.length(), &bytes_tag_));\n+    return bytes_.Append(data, length);\n+  }\n+\n+  /// Appending a string to the sequence\n+  Status AppendString(const char* data, int32_t length) {\n+    RETURN_NOT_OK(Update(strings_.length(), &string_tag_));\n+    return strings_.Append(data, length);\n+  }\n+\n+  /// Appending a half_float to the sequence\n+  Status AppendHalfFloat(const npy_half data) {\n+    return AppendPrimitive(data, &half_float_tag_, &half_floats_);\n+  }\n+\n+  /// Appending a float to the sequence\n+  Status AppendFloat(const float data) {\n+    return AppendPrimitive(data, &float_tag_, &floats_);\n+  }\n+\n+  /// Appending a double to the sequence\n+  Status AppendDouble(const double data) {\n+    return AppendPrimitive(data, &double_tag_, &doubles_);\n+  }\n+\n+  /// Appending a Date64 timestamp to the sequence\n+  Status AppendDate64(const int64_t timestamp) {\n+    return AppendPrimitive(timestamp, &date64_tag_, &date64s_);\n+  }\n+\n+  /// Appending a tensor to the sequence\n+  ///\n+  /// \\param tensor_index Index of the tensor in the object.\n+  Status AppendTensor(const int32_t tensor_index) {\n+    RETURN_NOT_OK(Update(tensor_indices_.length(), &tensor_tag_));\n+    return tensor_indices_.Append(tensor_index);\n+  }\n+\n+  /// Appending a buffer to the sequence\n+  ///\n+  /// \\param buffer_index Indes of the buffer in the object.\n+  Status AppendBuffer(const int32_t buffer_index) {\n+    RETURN_NOT_OK(Update(buffer_indices_.length(), &buffer_tag_));\n+    return buffer_indices_.Append(buffer_index);\n+  }\n+\n+  /// Add a sublist to the sequence. The data contained in the sublist will be\n+  /// specified in the \"Finish\" method.\n+  ///\n+  /// To construct l = [[11, 22], 33, [44, 55]] you would for example run\n+  /// list = ListBuilder();\n+  /// list.AppendList(2);\n+  /// list.Append(33);\n+  /// list.AppendList(2);\n+  /// list.Finish([11, 22, 44, 55]);\n+  /// list.Finish();\n+\n+  /// \\param size\n+  /// The size of the sublist\n+  Status AppendList(Py_ssize_t size) {\n+    int32_t offset;\n+    RETURN_NOT_OK(internal::CastSize(list_offsets_.back() + size, &offset));\n+    RETURN_NOT_OK(Update(list_offsets_.size() - 1, &list_tag_));\n+    list_offsets_.push_back(offset);\n+    return Status::OK();\n+  }\n+\n+  Status AppendTuple(Py_ssize_t size) {\n+    int32_t offset;\n+    RETURN_NOT_OK(internal::CastSize(tuple_offsets_.back() + size, &offset));\n+    RETURN_NOT_OK(Update(tuple_offsets_.size() - 1, &tuple_tag_));\n+    tuple_offsets_.push_back(offset);\n+    return Status::OK();\n+  }\n+\n+  Status AppendDict(Py_ssize_t size) {\n+    int32_t offset;\n+    RETURN_NOT_OK(internal::CastSize(dict_offsets_.back() + size, &offset));\n+    RETURN_NOT_OK(Update(dict_offsets_.size() - 1, &dict_tag_));\n+    dict_offsets_.push_back(offset);\n+    return Status::OK();\n+  }\n+\n+  Status AppendSet(Py_ssize_t size) {\n+    int32_t offset;\n+    RETURN_NOT_OK(internal::CastSize(set_offsets_.back() + size, &offset));\n+    RETURN_NOT_OK(Update(set_offsets_.size() - 1, &set_tag_));\n+    set_offsets_.push_back(offset);\n+    return Status::OK();\n+  }\n+\n+  template <typename BuilderType>\n+  Status AddElement(const int8_t tag, BuilderType* out, const std::string& name = \"\") {\n+    if (tag != -1) {\n+      fields_[tag] = ::arrow::field(name, out->type());\n+      RETURN_NOT_OK(out->Finish(&children_[tag]));\n+      RETURN_NOT_OK(nones_.Append(true));\n+      type_ids_.push_back(tag);\n+    }\n+    return Status::OK();\n+  }\n+\n+  Status AddSubsequence(int8_t tag, const Array* data,\n+                        const std::vector<int32_t>& offsets, const std::string& name) {\n+    if (data != nullptr) {\n+      DCHECK(data->length() == offsets.back());\n+      std::shared_ptr<Array> offset_array;\n+      Int32Builder builder(::arrow::int32(), pool_);\n+      RETURN_NOT_OK(builder.AppendValues(offsets.data(), offsets.size()));\n+      RETURN_NOT_OK(builder.Finish(&offset_array));\n+      std::shared_ptr<Array> list_array;\n+      RETURN_NOT_OK(ListArray::FromArrays(*offset_array, *data, pool_, &list_array));\n+      auto field = ::arrow::field(name, list_array->type());\n+      auto type = ::arrow::struct_({field});\n+      fields_[tag] = ::arrow::field(\"\", type);\n+      children_[tag] = std::shared_ptr<StructArray>(\n+          new StructArray(type, list_array->length(), {list_array}));\n+      RETURN_NOT_OK(nones_.Append(true));\n+      type_ids_.push_back(tag);\n+    } else {\n+      DCHECK_EQ(offsets.size(), 1);\n+    }\n+    return Status::OK();\n+  }\n+\n+  /// Finish building the sequence and return the result.\n+  /// Input arrays may be nullptr\n+  Status Finish(const Array* list_data, const Array* tuple_data, const Array* dict_data,\n+                const Array* set_data, std::shared_ptr<Array>* out) {\n+    fields_.resize(num_tags_);\n+    children_.resize(num_tags_);\n+\n+    RETURN_NOT_OK(AddElement(bool_tag_, &bools_));\n+    RETURN_NOT_OK(AddElement(int_tag_, &ints_));\n+    RETURN_NOT_OK(AddElement(py2_int_tag_, &py2_ints_, \"py2_int\"));\n+    RETURN_NOT_OK(AddElement(string_tag_, &strings_));\n+    RETURN_NOT_OK(AddElement(bytes_tag_, &bytes_));\n+    RETURN_NOT_OK(AddElement(half_float_tag_, &half_floats_));\n+    RETURN_NOT_OK(AddElement(float_tag_, &floats_));\n+    RETURN_NOT_OK(AddElement(double_tag_, &doubles_));\n+    RETURN_NOT_OK(AddElement(date64_tag_, &date64s_));\n+    RETURN_NOT_OK(AddElement(tensor_tag_, &tensor_indices_, \"tensor\"));\n+    RETURN_NOT_OK(AddElement(buffer_tag_, &buffer_indices_, \"buffer\"));\n+\n+    RETURN_NOT_OK(AddSubsequence(list_tag_, list_data, list_offsets_, \"list\"));\n+    RETURN_NOT_OK(AddSubsequence(tuple_tag_, tuple_data, tuple_offsets_, \"tuple\"));\n+    RETURN_NOT_OK(AddSubsequence(dict_tag_, dict_data, dict_offsets_, \"dict\"));\n+    RETURN_NOT_OK(AddSubsequence(set_tag_, set_data, set_offsets_, \"set\"));\n+\n+    std::shared_ptr<Array> types_array;\n+    RETURN_NOT_OK(types_.Finish(&types_array));\n+    const auto& types = checked_cast<const Int8Array&>(*types_array);\n+\n+    std::shared_ptr<Array> offsets_array;\n+    RETURN_NOT_OK(offsets_.Finish(&offsets_array));\n+    const auto& offsets = checked_cast<const Int32Array&>(*offsets_array);\n+\n+    std::shared_ptr<Array> nones_array;\n+    RETURN_NOT_OK(nones_.Finish(&nones_array));\n+    const auto& nones = checked_cast<const BooleanArray&>(*nones_array);\n+\n+    auto type = ::arrow::union_(fields_, type_ids_, UnionMode::DENSE);\n+    out->reset(new UnionArray(type, types.length(), children_, types.values(),\n+                              offsets.values(), nones.null_bitmap(), nones.null_count()));\n+    return Status::OK();\n+  }\n+\n+ private:\n+  MemoryPool* pool_;\n+\n+  Int8Builder types_;\n+  Int32Builder offsets_;\n+\n+  BooleanBuilder nones_;\n+  BooleanBuilder bools_;\n+  Int64Builder ints_;\n+  Int64Builder py2_ints_;\n+  BinaryBuilder bytes_;\n+  StringBuilder strings_;\n+  HalfFloatBuilder half_floats_;\n+  FloatBuilder floats_;\n+  DoubleBuilder doubles_;\n+  Date64Builder date64s_;\n+\n+  Int32Builder tensor_indices_;\n+  Int32Builder buffer_indices_;\n+\n+  std::vector<int32_t> list_offsets_;\n+  std::vector<int32_t> tuple_offsets_;\n+  std::vector<int32_t> dict_offsets_;\n+  std::vector<int32_t> set_offsets_;\n+\n+  // Tags for members of the sequence. If they are set to -1 it means\n+  // they are not used and will not part be of the metadata when we call\n+  // SequenceBuilder::Finish. If a member with one of the tags is added,\n+  // the associated variable gets a unique index starting from 0. This\n+  // happens in the UPDATE macro in sequence.cc.\n+  int8_t bool_tag_ = -1;\n+  int8_t int_tag_ = -1;\n+  int8_t py2_int_tag_ = -1;\n+  int8_t string_tag_ = -1;\n+  int8_t bytes_tag_ = -1;\n+  int8_t half_float_tag_ = -1;\n+  int8_t float_tag_ = -1;\n+  int8_t double_tag_ = -1;\n+  int8_t date64_tag_ = -1;\n+\n+  int8_t tensor_tag_ = -1;\n+  int8_t buffer_tag_ = -1;\n+  int8_t list_tag_ = -1;\n+  int8_t tuple_tag_ = -1;\n+  int8_t dict_tag_ = -1;\n+  int8_t set_tag_ = -1;\n+\n+  int8_t num_tags_ = 0;\n+\n+  // Members for the output union constructed in Finish\n+  std::vector<std::shared_ptr<Field>> fields_;\n+  std::vector<std::shared_ptr<Array>> children_;\n+  std::vector<uint8_t> type_ids_;\n+};\n+\n+/// Constructing dictionaries of key/value pairs. Sequences of\n+/// keys and values are built separately using a pair of\n+/// SequenceBuilders. The resulting Arrow representation\n+/// can be obtained via the Finish method.\n+class DictBuilder {\n+ public:\n+  explicit DictBuilder(MemoryPool* pool = nullptr) : keys_(pool), vals_(pool) {}\n+\n+  /// Builder for the keys of the dictionary\n+  SequenceBuilder& keys() { return keys_; }\n+  /// Builder for the values of the dictionary\n+  SequenceBuilder& vals() { return vals_; }\n+\n+  /// Construct an Arrow StructArray representing the dictionary.\n+  /// Contains a field \"keys\" for the keys and \"vals\" for the values.\n+  /// \\param val_list_data\n+  ///    List containing the data from nested lists in the value\n+  ///   list of the dictionary\n+  ///\n+  /// \\param val_dict_data\n+  ///   List containing the data from nested dictionaries in the\n+  ///   value list of the dictionary\n+  Status Finish(const Array* key_tuple_data, const Array* key_dict_data,\n+                const Array* val_list_data, const Array* val_tuple_data,\n+                const Array* val_dict_data, const Array* val_set_data,\n+                std::shared_ptr<Array>* out) {\n+    // lists and sets can't be keys of dicts in Python, that is why for\n+    // the keys we do not need to collect sublists\n+    std::shared_ptr<Array> keys, vals;\n+    RETURN_NOT_OK(keys_.Finish(nullptr, key_tuple_data, key_dict_data, nullptr, &keys));\n+    RETURN_NOT_OK(\n+        vals_.Finish(val_list_data, val_tuple_data, val_dict_data, val_set_data, &vals));\n+    auto keys_field = std::make_shared<Field>(\"keys\", keys->type());\n+    auto vals_field = std::make_shared<Field>(\"vals\", vals->type());\n+    auto type = std::make_shared<StructType>(\n+        std::vector<std::shared_ptr<Field>>({keys_field, vals_field}));\n+    std::vector<std::shared_ptr<Array>> field_arrays({keys, vals});\n+    DCHECK(keys->length() == vals->length());\n+    out->reset(new StructArray(type, keys->length(), field_arrays));\n+    return Status::OK();\n+  }\n+\n+ private:\n+  SequenceBuilder keys_;\n+  SequenceBuilder vals_;\n+};\n+\n+Status CallCustomCallback(PyObject* context, PyObject* method_name, PyObject* elem,\n+                          PyObject** result) {\n+  *result = NULL;\n+  if (context == Py_None) {\n+    std::stringstream ss;\n+    ss << \"error while calling callback on \" << internal::PyObject_StdStringRepr(elem)\n+       << \": handler not registered\";\n+    return Status::SerializationError(ss.str());\n+  } else {\n+    *result = PyObject_CallMethodObjArgs(context, method_name, elem, NULL);\n+    return PassPyError();\n+  }\n+  return Status::OK();\n+}\n+\n+Status CallSerializeCallback(PyObject* context, PyObject* value,\n+                             PyObject** serialized_object) {\n+  OwnedRef method_name(PyUnicode_FromString(\"_serialize_callback\"));\n+  RETURN_NOT_OK(CallCustomCallback(context, method_name.obj(), value, serialized_object));\n+  if (!PyDict_Check(*serialized_object)) {\n+    return Status::TypeError(\"serialization callback must return a valid dictionary\");\n+  }\n+  return Status::OK();\n+}\n+\n+Status CallDeserializeCallback(PyObject* context, PyObject* value,\n+                               PyObject** deserialized_object) {\n+  OwnedRef method_name(PyUnicode_FromString(\"_deserialize_callback\"));\n+  return CallCustomCallback(context, method_name.obj(), value, deserialized_object);\n+}\n+\n+Status SerializeDict(PyObject* context, std::vector<PyObject*> dicts,\n+                     int32_t recursion_depth, std::shared_ptr<Array>* out,\n+                     SerializedPyObject* blobs_out);\n+\n+Status SerializeArray(PyObject* context, PyArrayObject* array, SequenceBuilder* builder,\n+                      std::vector<PyObject*>* subdicts, SerializedPyObject* blobs_out);\n+\n+Status SerializeSequences(PyObject* context, std::vector<PyObject*> sequences,\n+                          int32_t recursion_depth, std::shared_ptr<Array>* out,\n+                          SerializedPyObject* blobs_out);\n+\n+Status AppendScalar(PyObject* obj, SequenceBuilder* builder) {\n+  if (PyArray_IsScalar(obj, Bool)) {\n+    return builder->AppendBool(reinterpret_cast<PyBoolScalarObject*>(obj)->obval != 0);\n+  } else if (PyArray_IsScalar(obj, Half)) {\n+    return builder->AppendHalfFloat(reinterpret_cast<PyHalfScalarObject*>(obj)->obval);\n+  } else if (PyArray_IsScalar(obj, Float)) {\n+    return builder->AppendFloat(reinterpret_cast<PyFloatScalarObject*>(obj)->obval);\n+  } else if (PyArray_IsScalar(obj, Double)) {\n+    return builder->AppendDouble(reinterpret_cast<PyDoubleScalarObject*>(obj)->obval);\n+  }\n+  int64_t value = 0;\n+  if (PyArray_IsScalar(obj, Byte)) {\n+    value = reinterpret_cast<PyByteScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, UByte)) {\n+    value = reinterpret_cast<PyUByteScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, Short)) {\n+    value = reinterpret_cast<PyShortScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, UShort)) {\n+    value = reinterpret_cast<PyUShortScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, Int)) {\n+    value = reinterpret_cast<PyIntScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, UInt)) {\n+    value = reinterpret_cast<PyUIntScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, Long)) {\n+    value = reinterpret_cast<PyLongScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, ULong)) {\n+    value = reinterpret_cast<PyULongScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, LongLong)) {\n+    value = reinterpret_cast<PyLongLongScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, Int64)) {\n+    value = reinterpret_cast<PyInt64ScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, ULongLong)) {\n+    value = reinterpret_cast<PyULongLongScalarObject*>(obj)->obval;\n+  } else if (PyArray_IsScalar(obj, UInt64)) {\n+    value = reinterpret_cast<PyUInt64ScalarObject*>(obj)->obval;\n+  } else {\n+    DCHECK(false) << \"scalar type not recognized\";\n+  }\n+  return builder->AppendInt64(value);\n+}\n+\n+Status Append(PyObject* context, PyObject* elem, SequenceBuilder* builder,\n+              std::vector<PyObject*>* sublists, std::vector<PyObject*>* subtuples,\n+              std::vector<PyObject*>* subdicts, std::vector<PyObject*>* subsets,\n+              SerializedPyObject* blobs_out) {\n+  // The bool case must precede the int case (PyInt_Check passes for bools)\n+  if (PyBool_Check(elem)) {\n+    RETURN_NOT_OK(builder->AppendBool(elem == Py_True));\n+  } else if (PyArray_DescrFromScalar(elem)->type_num == NPY_HALF) {\n+    npy_half halffloat = reinterpret_cast<PyHalfScalarObject*>(elem)->obval;\n+    RETURN_NOT_OK(builder->AppendHalfFloat(halffloat));\n+  } else if (PyFloat_Check(elem)) {\n+    RETURN_NOT_OK(builder->AppendDouble(PyFloat_AS_DOUBLE(elem)));\n+  } else if (PyLong_Check(elem)) {\n+    int overflow = 0;\n+    int64_t data = PyLong_AsLongLongAndOverflow(elem, &overflow);\n+    if (!overflow) {\n+      RETURN_NOT_OK(builder->AppendInt64(data));\n+    } else {\n+      // Attempt to serialize the object using the custom callback.\n+      PyObject* serialized_object;\n+      // The reference count of serialized_object will be decremented in SerializeDict\n+      RETURN_NOT_OK(CallSerializeCallback(context, elem, &serialized_object));\n+      RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n+      subdicts->push_back(serialized_object);\n+    }\n+#if PY_MAJOR_VERSION < 3\n+  } else if (PyInt_Check(elem)) {\n+    RETURN_NOT_OK(builder->AppendPy2Int64(static_cast<int64_t>(PyInt_AS_LONG(elem))));\n+#endif\n+  } else if (PyBytes_Check(elem)) {\n+    auto data = reinterpret_cast<uint8_t*>(PyBytes_AS_STRING(elem));\n+    int32_t size;\n+    RETURN_NOT_OK(internal::CastSize(PyBytes_GET_SIZE(elem), &size));\n+    RETURN_NOT_OK(builder->AppendBytes(data, size));\n+  } else if (PyUnicode_Check(elem)) {\n+    PyBytesView view;\n+    RETURN_NOT_OK(view.FromString(elem));\n+    int32_t size;\n+    RETURN_NOT_OK(internal::CastSize(view.size, &size));\n+    RETURN_NOT_OK(builder->AppendString(view.bytes, size));\n+  } else if (PyList_CheckExact(elem)) {\n+    RETURN_NOT_OK(builder->AppendList(PyList_Size(elem)));\n+    sublists->push_back(elem);\n+  } else if (PyDict_CheckExact(elem)) {\n+    RETURN_NOT_OK(builder->AppendDict(PyDict_Size(elem)));\n+    subdicts->push_back(elem);\n+  } else if (PyTuple_CheckExact(elem)) {\n+    RETURN_NOT_OK(builder->AppendTuple(PyTuple_Size(elem)));\n+    subtuples->push_back(elem);\n+  } else if (PySet_Check(elem)) {\n+    RETURN_NOT_OK(builder->AppendSet(PySet_Size(elem)));\n+    subsets->push_back(elem);\n+  } else if (PyArray_IsScalar(elem, Generic)) {\n+    RETURN_NOT_OK(AppendScalar(elem, builder));\n+  } else if (PyArray_CheckExact(elem)) {\n+    RETURN_NOT_OK(SerializeArray(context, reinterpret_cast<PyArrayObject*>(elem), builder,\n+                                 subdicts, blobs_out));\n+  } else if (elem == Py_None) {\n+    RETURN_NOT_OK(builder->AppendNone());\n+  } else if (PyDateTime_Check(elem)) {\n+    PyDateTime_DateTime* datetime = reinterpret_cast<PyDateTime_DateTime*>(elem);\n+    RETURN_NOT_OK(builder->AppendDate64(PyDateTime_to_us(datetime)));\n+  } else if (is_buffer(elem)) {\n+    RETURN_NOT_OK(builder->AppendBuffer(static_cast<int32_t>(blobs_out->buffers.size())));\n+    std::shared_ptr<Buffer> buffer;\n+    RETURN_NOT_OK(unwrap_buffer(elem, &buffer));\n+    blobs_out->buffers.push_back(buffer);\n+  } else {\n+    // Attempt to serialize the object using the custom callback.\n+    PyObject* serialized_object;\n+    // The reference count of serialized_object will be decremented in SerializeDict\n+    RETURN_NOT_OK(CallSerializeCallback(context, elem, &serialized_object));\n+    RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n+    subdicts->push_back(serialized_object);\n+  }\n+  return Status::OK();\n+}\n+\n+Status SerializeArray(PyObject* context, PyArrayObject* array, SequenceBuilder* builder,\n+                      std::vector<PyObject*>* subdicts, SerializedPyObject* blobs_out) {\n+  int dtype = PyArray_TYPE(array);\n+  switch (dtype) {\n+    case NPY_UINT8:\n+    case NPY_INT8:\n+    case NPY_UINT16:\n+    case NPY_INT16:\n+    case NPY_UINT32:\n+    case NPY_INT32:\n+    case NPY_UINT64:\n+    case NPY_INT64:\n+    case NPY_HALF:\n+    case NPY_FLOAT:\n+    case NPY_DOUBLE: {\n+      RETURN_NOT_OK(\n+          builder->AppendTensor(static_cast<int32_t>(blobs_out->tensors.size())));\n+      std::shared_ptr<Tensor> tensor;\n+      RETURN_NOT_OK(NdarrayToTensor(default_memory_pool(),\n+                                    reinterpret_cast<PyObject*>(array), &tensor));\n+      blobs_out->tensors.push_back(tensor);\n+    } break;\n+    default: {\n+      PyObject* serialized_object;\n+      // The reference count of serialized_object will be decremented in SerializeDict\n+      RETURN_NOT_OK(CallSerializeCallback(context, reinterpret_cast<PyObject*>(array),\n+                                          &serialized_object));\n+      RETURN_NOT_OK(builder->AppendDict(PyDict_Size(serialized_object)));\n+      subdicts->push_back(serialized_object);\n+    }\n+  }\n+  return Status::OK();\n+}\n+\n+Status SerializeSequences(PyObject* context, std::vector<PyObject*> sequences,\n+                          int32_t recursion_depth, std::shared_ptr<Array>* out,\n+                          SerializedPyObject* blobs_out) {\n+  DCHECK(out);\n+  if (recursion_depth >= kMaxRecursionDepth) {\n+    return Status::NotImplemented(\n+        \"This object exceeds the maximum recursion depth. It may contain itself \"\n+        \"recursively.\");\n+  }\n+  SequenceBuilder builder;\n+  std::vector<PyObject*> sublists, subtuples, subdicts, subsets;\n+  for (const auto& sequence : sequences) {\n+    RETURN_NOT_OK(internal::VisitIterable(\n+        sequence, [&](PyObject* obj, bool* keep_going /* unused */) {\n+          return Append(context, obj, &builder, &sublists, &subtuples, &subdicts,\n+                        &subsets, blobs_out);\n+        }));\n+  }\n+  std::shared_ptr<Array> list;\n+  if (sublists.size() > 0) {\n+    RETURN_NOT_OK(\n+        SerializeSequences(context, sublists, recursion_depth + 1, &list, blobs_out));\n+  }\n+  std::shared_ptr<Array> tuple;\n+  if (subtuples.size() > 0) {\n+    RETURN_NOT_OK(\n+        SerializeSequences(context, subtuples, recursion_depth + 1, &tuple, blobs_out));\n+  }\n+  std::shared_ptr<Array> dict;\n+  if (subdicts.size() > 0) {\n+    RETURN_NOT_OK(\n+        SerializeDict(context, subdicts, recursion_depth + 1, &dict, blobs_out));\n+  }\n+  std::shared_ptr<Array> set;\n+  if (subsets.size() > 0) {\n+    RETURN_NOT_OK(\n+        SerializeSequences(context, subsets, recursion_depth + 1, &set, blobs_out));\n+  }\n+  return builder.Finish(list.get(), tuple.get(), dict.get(), set.get(), out);\n+}\n+\n+Status SerializeDict(PyObject* context, std::vector<PyObject*> dicts,\n+                     int32_t recursion_depth, std::shared_ptr<Array>* out,\n+                     SerializedPyObject* blobs_out) {\n+  DictBuilder result;\n+  if (recursion_depth >= kMaxRecursionDepth) {\n+    return Status::NotImplemented(\n+        \"This object exceeds the maximum recursion depth. It may contain itself \"\n+        \"recursively.\");\n+  }\n+  std::vector<PyObject*> key_tuples, key_dicts, val_lists, val_tuples, val_dicts,\n+      val_sets, dummy;\n+  for (const auto& dict : dicts) {\n+    PyObject* key;\n+    PyObject* value;\n+    Py_ssize_t pos = 0;\n+    while (PyDict_Next(dict, &pos, &key, &value)) {\n+      RETURN_NOT_OK(Append(context, key, &result.keys(), &dummy, &key_tuples, &key_dicts,\n+                           &dummy, blobs_out));\n+      DCHECK_EQ(dummy.size(), 0);\n+      RETURN_NOT_OK(Append(context, value, &result.vals(), &val_lists, &val_tuples,\n+                           &val_dicts, &val_sets, blobs_out));\n+    }\n+  }\n+  std::shared_ptr<Array> key_tuples_arr;\n+  if (key_tuples.size() > 0) {\n+    RETURN_NOT_OK(SerializeSequences(context, key_tuples, recursion_depth + 1,\n+                                     &key_tuples_arr, blobs_out));\n+  }\n+  std::shared_ptr<Array> key_dicts_arr;\n+  if (key_dicts.size() > 0) {\n+    RETURN_NOT_OK(SerializeDict(context, key_dicts, recursion_depth + 1, &key_dicts_arr,\n+                                blobs_out));\n+  }\n+  std::shared_ptr<Array> val_list_arr;\n+  if (val_lists.size() > 0) {\n+    RETURN_NOT_OK(SerializeSequences(context, val_lists, recursion_depth + 1,\n+                                     &val_list_arr, blobs_out));\n+  }\n+  std::shared_ptr<Array> val_tuples_arr;\n+  if (val_tuples.size() > 0) {\n+    RETURN_NOT_OK(SerializeSequences(context, val_tuples, recursion_depth + 1,\n+                                     &val_tuples_arr, blobs_out));\n+  }\n+  std::shared_ptr<Array> val_dict_arr;\n+  if (val_dicts.size() > 0) {\n+    RETURN_NOT_OK(\n+        SerializeDict(context, val_dicts, recursion_depth + 1, &val_dict_arr, blobs_out));\n+  }\n+  std::shared_ptr<Array> val_set_arr;\n+  if (val_sets.size() > 0) {\n+    RETURN_NOT_OK(SerializeSequences(context, val_sets, recursion_depth + 1, &val_set_arr,\n+                                     blobs_out));\n+  }\n+  RETURN_NOT_OK(result.Finish(key_tuples_arr.get(), key_dicts_arr.get(),\n+                              val_list_arr.get(), val_tuples_arr.get(),\n+                              val_dict_arr.get(), val_set_arr.get(), out));\n+\n+  // This block is used to decrement the reference counts of the results\n+  // returned by the serialization callback, which is called in SerializeArray,\n+  // in DeserializeDict and in Append\n+  static PyObject* py_type = PyUnicode_FromString(\"_pytype_\");\n+  for (const auto& dict : dicts) {\n+    if (PyDict_Contains(dict, py_type)) {\n+      // If the dictionary contains the key \"_pytype_\", then the user has to\n+      // have registered a callback.\n+      if (context == Py_None) {\n+        return Status::Invalid(\"No serialization callback set\");\n+      }\n+      Py_XDECREF(dict);\n+    }\n+  }\n+\n+  return Status::OK();\n+}\n+\n+std::shared_ptr<RecordBatch> MakeBatch(std::shared_ptr<Array> data) {\n+  auto field = std::make_shared<Field>(\"list\", data->type());\n+  auto schema = ::arrow::schema({field});\n+  return RecordBatch::Make(schema, data->length(), {data});\n+}\n+\n+Status SerializeObject(PyObject* context, PyObject* sequence, SerializedPyObject* out) {\n+  PyAcquireGIL lock;\n+  PyDateTime_IMPORT;\n+  import_pyarrow();\n+  std::vector<PyObject*> sequences = {sequence};\n+  std::shared_ptr<Array> array;\n+  RETURN_NOT_OK(SerializeSequences(context, sequences, 0, &array, out));\n+  out->batch = MakeBatch(array);\n+  return Status::OK();\n+}\n+\n+Status SerializeTensor(std::shared_ptr<Tensor> tensor, SerializedPyObject* out) {\n+  std::shared_ptr<Array> array;\n+  SequenceBuilder builder;\n+  RETURN_NOT_OK(builder.AppendTensor(static_cast<int32_t>(out->tensors.size())));\n+  out->tensors.push_back(tensor);\n+  RETURN_NOT_OK(builder.Finish(nullptr, nullptr, nullptr, nullptr, &array));\n+  out->batch = MakeBatch(array);\n+  return Status::OK();\n+}\n+\n+Status WriteTensorHeader(std::shared_ptr<DataType> dtype,\n+                         const std::vector<int64_t>& shape, int64_t tensor_num_bytes,\n+                         io::OutputStream* dst) {\n+  auto empty_tensor = std::make_shared<Tensor>(\n+      dtype, std::make_shared<Buffer>(nullptr, tensor_num_bytes), shape);\n+  SerializedPyObject serialized_tensor;\n+  RETURN_NOT_OK(SerializeTensor(empty_tensor, &serialized_tensor));\n+  return serialized_tensor.WriteTo(dst);\n+}\n+\n+Status SerializedPyObject::WriteTo(io::OutputStream* dst) {\n+  int32_t num_tensors = static_cast<int32_t>(this->tensors.size());\n+  int32_t num_buffers = static_cast<int32_t>(this->buffers.size());\n+  RETURN_NOT_OK(\n+      dst->Write(reinterpret_cast<const uint8_t*>(&num_tensors), sizeof(int32_t)));\n+  RETURN_NOT_OK(\n+      dst->Write(reinterpret_cast<const uint8_t*>(&num_buffers), sizeof(int32_t)));\n+  RETURN_NOT_OK(ipc::WriteRecordBatchStream({this->batch}, dst));\n+\n+  int32_t metadata_length;\n+  int64_t body_length;\n+  for (const auto& tensor : this->tensors) {\n+    RETURN_NOT_OK(ipc::WriteTensor(*tensor, dst, &metadata_length, &body_length));\n+  }\n+\n+  for (const auto& buffer : this->buffers) {\n+    int64_t size = buffer->size();\n+    RETURN_NOT_OK(dst->Write(reinterpret_cast<const uint8_t*>(&size), sizeof(int64_t)));\n+    RETURN_NOT_OK(dst->Write(buffer->data(), size));\n+  }\n+\n+  return Status::OK();\n+}\n+\n+Status SerializedPyObject::GetComponents(MemoryPool* memory_pool, PyObject** out) {\n+  PyAcquireGIL py_gil;\n+\n+  OwnedRef result(PyDict_New());\n+  PyObject* buffers = PyList_New(0);\n+\n+  // TODO(wesm): Not sure how pedantic we need to be about checking the return\n+  // values of these functions. There are other places where we do not check\n+  // PyDict_SetItem/SetItemString return value, but these failures would be\n+  // quite esoteric\n+  PyDict_SetItemString(result.obj(), \"num_tensors\",\n+                       PyLong_FromSize_t(this->tensors.size()));\n+  PyDict_SetItemString(result.obj(), \"num_buffers\",\n+                       PyLong_FromSize_t(this->buffers.size()));\n+  PyDict_SetItemString(result.obj(), \"data\", buffers);\n+  RETURN_IF_PYERROR();\n+\n+  Py_DECREF(buffers);\n+\n+  auto PushBuffer = [&buffers](const std::shared_ptr<Buffer>& buffer) {\n+    PyObject* wrapped_buffer = wrap_buffer(buffer);\n+    RETURN_IF_PYERROR();\n+    if (PyList_Append(buffers, wrapped_buffer) < 0) {\n+      Py_DECREF(wrapped_buffer);\n+      RETURN_IF_PYERROR();\n+    }\n+    Py_DECREF(wrapped_buffer);\n+    return Status::OK();\n+  };\n+\n+  constexpr int64_t kInitialCapacity = 1024;\n+\n+  // Write the record batch describing the object structure\n+  std::shared_ptr<io::BufferOutputStream> stream;\n+  std::shared_ptr<Buffer> buffer;\n+\n+  py_gil.release();\n+  RETURN_NOT_OK(io::BufferOutputStream::Create(kInitialCapacity, memory_pool, &stream));\n+  RETURN_NOT_OK(ipc::WriteRecordBatchStream({this->batch}, stream.get()));\n+  RETURN_NOT_OK(stream->Finish(&buffer));\n+  py_gil.acquire();\n+\n+  RETURN_NOT_OK(PushBuffer(buffer));\n+\n+  // For each tensor, get a metadata buffer and a buffer for the body\n+  for (const auto& tensor : this->tensors) {\n+    std::unique_ptr<ipc::Message> message;\n+    RETURN_NOT_OK(ipc::GetTensorMessage(*tensor, memory_pool, &message));\n+    RETURN_NOT_OK(PushBuffer(message->metadata()));\n+    RETURN_NOT_OK(PushBuffer(message->body()));\n+  }\n+\n+  for (const auto& buf : this->buffers) {\n+    RETURN_NOT_OK(PushBuffer(buf));\n+  }\n+\n+  *out = result.detach();\n+  return Status::OK();\n+}\n+\n+}  // namespace py\n+}  // namespace arrow\ndiff --git a/cpp/src/arrow/python/serialize.h b/cpp/src/arrow/python/serialize.h\nnew file mode 100644\nindex 0000000000..d0bc44ad0a\n--- /dev/null\n+++ b/cpp/src/arrow/python/serialize.h\n@@ -0,0 +1,113 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+#ifndef ARROW_PYTHON_PYTHON_TO_ARROW_H\n+#define ARROW_PYTHON_PYTHON_TO_ARROW_H\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"arrow/status.h\"\n+#include \"arrow/util/visibility.h\"\n+\n+// Forward declaring PyObject, see\n+// https://mail.python.org/pipermail/python-dev/2003-August/037601.html\n+#ifndef PyObject_HEAD\n+struct _object;\n+typedef _object PyObject;\n+#endif\n+\n+namespace arrow {\n+\n+class Buffer;\n+class DataType;\n+class MemoryPool;\n+class RecordBatch;\n+class Tensor;\n+\n+namespace io {\n+\n+class OutputStream;\n+\n+}  // namespace io\n+\n+namespace py {\n+\n+struct ARROW_EXPORT SerializedPyObject {\n+  std::shared_ptr<RecordBatch> batch;\n+  std::vector<std::shared_ptr<Tensor>> tensors;\n+  std::vector<std::shared_ptr<Buffer>> buffers;\n+\n+  /// \\brief Write serialized Python object to OutputStream\n+  /// \\param[in,out] dst an OutputStream\n+  /// \\return Status\n+  Status WriteTo(io::OutputStream* dst);\n+\n+  /// \\brief Convert SerializedPyObject to a dict containing the message\n+  /// components as Buffer instances with minimal memory allocation\n+  ///\n+  /// {\n+  ///   'num_tensors': N,\n+  ///   'num_buffers': K,\n+  ///   'data': [Buffer]\n+  /// }\n+  ///\n+  /// Each tensor is written as two buffers, one for the metadata and one for\n+  /// the body. Therefore, the number of buffers in 'data' is 2 * N + K + 1,\n+  /// with the first buffer containing the serialized record batch containing\n+  /// the UnionArray that describes the whole object\n+  Status GetComponents(MemoryPool* pool, PyObject** out);\n+};\n+\n+/// \\brief Serialize Python sequence as a SerializedPyObject.\n+/// \\param[in] context Serialization context which contains custom serialization\n+/// and deserialization callbacks. Can be any Python object with a\n+/// _serialize_callback method for serialization and a _deserialize_callback\n+/// method for deserialization. If context is None, no custom serialization\n+/// will be attempted.\n+/// \\param[in] sequence A Python sequence object to serialize to Arrow data\n+/// structures\n+/// \\param[out] out The serialized representation\n+/// \\return Status\n+///\n+/// Release GIL before calling\n+ARROW_EXPORT\n+Status SerializeObject(PyObject* context, PyObject* sequence, SerializedPyObject* out);\n+\n+/// \\brief Serialize an Arrow Tensor as a SerializedPyObject.\n+/// \\param[in] tensor Tensor to be serialized\n+/// \\param[out] out The serialized representation\n+/// \\return Status\n+ARROW_EXPORT\n+Status SerializeTensor(std::shared_ptr<Tensor> tensor, py::SerializedPyObject* out);\n+\n+/// \\brief Write the Tensor metadata header to an OutputStream.\n+/// \\param[in] dtype DataType of the Tensor\n+/// \\param[in] shape The shape of the tensor\n+/// \\param[in] tensor_num_bytes The lengh of the Tensor data in bytes\n+/// \\param[in] dst The OutputStream to write the Tensor header to\n+/// \\return Status\n+ARROW_EXPORT\n+Status WriteTensorHeader(std::shared_ptr<DataType> dtype,\n+                         const std::vector<int64_t>& shape, int64_t tensor_num_bytes,\n+                         io::OutputStream* dst);\n+\n+}  // namespace py\n+\n+}  // namespace arrow\n+\n+#endif  // ARROW_PYTHON_PYTHON_TO_ARROW_H\ndiff --git a/python/pyarrow/tensorflow/plasma_op.cc b/python/pyarrow/tensorflow/plasma_op.cc\nindex 58ec1df6fd..7b0e80f140 100644\n--- a/python/pyarrow/tensorflow/plasma_op.cc\n+++ b/python/pyarrow/tensorflow/plasma_op.cc\n@@ -31,12 +31,14 @@\n #endif\n \n #include \"arrow/adapters/tensorflow/convert.h\"\n-#include \"arrow/io/memory.h\"\n-#include \"arrow/python/arrow_to_python.h\"\n-#include \"arrow/python/python_to_arrow.h\"\n-#include \"arrow/tensor.h\"\n-#include \"plasma/client.h\"\n+#include \"arrow/api.h\"\n+#include \"arrow/io/api.h\"\n+\n+// These headers do not include Python.h\n+#include \"arrow/python/deserialize.h\"\n+#include \"arrow/python/serialize.h\"\n \n+#include \"plasma/client.h\"\n \n namespace tf = tensorflow;\n \n\n\n \n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2018-08-18T17:45:11.935+0000",
                    "updated": "2018-08-18T17:45:11.935+0000",
                    "started": "2018-08-18T17:45:11.934+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "136003",
                    "issueId": "13176744"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 2400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@536bb349[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5c759ecb[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@30a39b8d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@40d0d894[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@738fa25b[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@293ef168[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@56d6831[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@433f1470[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4d2ceb36[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@13c7e64a[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7d0528e3[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@46a23fa7[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 2400,
        "customfield_12312520": null,
        "customfield_12312521": "Sat Aug 18 17:45:10 UTC 2018",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2018-08-18T17:45:10.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-2971/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2018-08-03T15:56:51.000+0000",
        "updated": "2018-08-18T17:45:11.000+0000",
        "timeoriginalestimate": null,
        "description": "The current builtin_convert.cc would be better named as python_to_arrow.cc or something similar. The current ones ought to be named serialize.cc and deserialize.cc or something similar",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 2400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python] Give more descriptive names to python_to_arrow.cc/arrow_to_python.cc",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13176744/comment/16584855",
                    "id": "16584855",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 2446\n[https://github.com/apache/arrow/pull/2446]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2018-08-18T17:45:10.296+0000",
                    "updated": "2018-08-18T17:45:10.296+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|i3wo3r:",
        "customfield_12314139": null
    }
}