{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13319220",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220",
    "key": "ARROW-9555",
    "fields": {
        "parent": {
            "id": "13316352",
            "key": "ARROW-9423",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13316352",
            "fields": {
                "summary": "[Rust][DataFusion] Add join",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
                    "id": "4",
                    "description": "An improvement or enhancement to an existing feature or task.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
                    "name": "Improvement",
                    "subtask": false,
                    "avatarId": 21140
                }
            }
        },
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12348823",
                "id": "12348823",
                "description": "",
                "name": "3.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-01-25"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12335005",
                "id": "12335005",
                "name": "Rust - DataFusion"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
            "name": "jorgecarleitao",
            "key": "jorgecarleitao",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
            },
            "displayName": "Jorge Leit\u00e3o",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 36000,
            "total": 36000,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 36000,
            "total": 36000,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9555/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 60,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/463109",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao opened a new pull request #7830:\nURL: https://github.com/apache/arrow/pull/7830\n\n\n   This is PR contains a physical plan to execute an inner join. I have not ran any benchmark, this is pure implementation plus some tests.\r\n   \r\n   The gist of the implementation for a given partition is:\r\n   \r\n   ```python\r\n   for left_record in left_records:\r\n        hash_left = build_hash_of_keys(left_record)\r\n        for right_record in right_records:\r\n               hash_right = build_hash_of_keys(right_record)\r\n               indexes = inner_join(hash_left, hash_right)\r\n               yield concat(left_record, right_record)[indexes]\r\n   ```\r\n   \r\n   I.e. inefficient.\r\n   \r\n   The implementation is currently sequential, even though it can be trivially distributed as each RecordBatch is evaluated independently (we still lock the mutex on partition reading, as in other physical plans). Since we have not committed to a distributed computational model, IMO the sequential is enough for now.\r\n   \r\n   This PR is built on top of #7687 and #7796 \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-07-24T19:54:33.932+0000",
                    "updated": "2020-07-24T19:54:33.932+0000",
                    "started": "2020-07-24T19:54:33.932+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "463109",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/463118",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #7830:\nURL: https://github.com/apache/arrow/pull/7830#issuecomment-663710935\n\n\n   https://issues.apache.org/jira/browse/ARROW-9555\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-07-24T20:04:46.775+0000",
                    "updated": "2020-07-24T20:04:46.775+0000",
                    "started": "2020-07-24T20:04:46.775+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "463118",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/471203",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao closed pull request #7830:\nURL: https://github.com/apache/arrow/pull/7830\n\n\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-16T10:59:09.628+0000",
                    "updated": "2020-08-16T10:59:09.628+0000",
                    "started": "2020-08-16T10:59:09.628+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471203",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/471204",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on pull request #7830:\nURL: https://github.com/apache/arrow/pull/7830#issuecomment-674512019\n\n\n   I agree with you @andygrove that we need to revisit the partitioning before tackling this. Closing\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-08-16T10:59:12.636+0000",
                    "updated": "2020-08-16T10:59:12.636+0000",
                    "started": "2020-08-16T10:59:12.636+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "471204",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513815",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao opened a new pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709\n\n\n   This PR is based on top of #8630 and contains a physical node to perform an inner join in DataFusion.\r\n   \r\n   This is still a draft, but IMO the design is here and the two tests already pass.\r\n   \r\n   This is co-authored with @andygrove , that contributed to the design on how to perform this operation in the context of DataFusion (see ARROW-9555 for details).\r\n   \r\n   The API used for the computation of the join at the arrow level is briefly discussed in [this document](https://docs.google.com/document/d/1KKuBvfx7uKi-x-tWOL60R1FNjDW8B790zPAv6yAlYcU/edit).\r\n   \r\n   There is still a lot to work on, but I I though it would be a good time to have a first round of discussions, and also to gauge timings wrt to the 3.0 release.\r\n   \r\n   There are two main issues being addressed in this PR:\r\n   \r\n   * How to we perform the join at the partition level: this pr collects all batches from the left, and then issues a stream per part on the right. Each batch on that stream joins itself with all the ones from the left (N) via a hash. This allow us to only require computing the hash of a row once (first all the left, then one by one on the right).\r\n   \r\n   * How do we build an array from `N (left)` arrays and a set of indices (matching the hash from the right): this is done using the `MutableArrayData` being worked on #8630, which incrementally memcopies slots from each of the N arrays based on the index. This implementation is useful because it works for all array types and does not require casting anything to rust native types (i.e. it operates on `ArrayData`, not specific implementations).\r\n   \r\n   There are still some steps left to have a join in SQL, most notably the whole logical planning, the output_partition logic, the bindings to SQL and DataFrame API, update the optimizers to handle nodes with 2 children,  and a whole battery of tests.\r\n   \r\n   There is also a natural path for the other joins, as it will be a matter of incorporating the work already on PR #8689 that introduces the option to extend the `MutableArrayData` with nulls, the operation required for left and right joins.\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:18:37.159+0000",
                    "updated": "2020-11-18T22:18:37.159+0000",
                    "started": "2020-11-18T22:18:37.159+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513815",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513819",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#issuecomment-729995084\n\n\n   https://issues.apache.org/jira/browse/ARROW-9555\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:23:47.300+0000",
                    "updated": "2020-11-18T22:23:47.300+0000",
                    "started": "2020-11-18T22:23:47.300+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513819",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513821",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526466282\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_utils.rs\n##########\n@@ -0,0 +1,145 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Functionality used both on logical and physical plans\n+\n+use crate::error::{DataFusionError, Result};\n+use arrow::datatypes::{Field, Schema};\n+use std::collections::HashSet;\n+\n+/// All valid types of joins.\n+#[derive(Clone, Debug)]\n+pub enum JoinHow {\n\nReview comment:\n       nit: `JoinType` would be a more standard name\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:29:58.659+0000",
                    "updated": "2020-11-18T22:29:58.659+0000",
                    "started": "2020-11-18T22:29:58.659+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513821",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513823",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526467098\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinHow};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// None represents a null (e.g. in case of a left join, some right indices are null)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n\nReview comment:\n       This is a great start but will limit us to cases where the column names are the same between two tables. As a follow-on we can expand this to support different names e..g `customer.id = orders.customer_id`. \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:31:34.617+0000",
                    "updated": "2020-11-18T22:31:34.617+0000",
                    "started": "2020-11-18T22:31:34.617+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513823",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513832",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526476310\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinHow};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// None represents a null (e.g. in case of a left join, some right indices are null)\n\nReview comment:\n       this comment is out-of-sync.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:51:19.164+0000",
                    "updated": "2020-11-18T22:51:19.164+0000",
                    "started": "2020-11-18T22:51:19.164+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513832",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513833",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526476498\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinHow};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// None represents a null (e.g. in case of a left join, some right indices are null)\n\nReview comment:\n       ```suggestion\r\n   // A pair (left index, right index)\r\n   ```\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-18T22:54:48.138+0000",
                    "updated": "2020-11-18T22:54:48.138+0000",
                    "started": "2020-11-18T22:54:48.138+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513833",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513935",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526608341\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       this is wrong: we need to get all partitions from the left here.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T05:46:06.450+0000",
                    "updated": "2020-11-19T05:46:06.450+0000",
                    "started": "2020-11-19T05:46:06.450+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513935",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513942",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526611739\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       @andygrove , I understand that the idea is to build a vector of RecordBatches from the left once, and then use it on the multiple parts on the right. However, I do not think we have a place to store it atm, as this is information that is required by multiple parts from the right, and our API does not support mutating `self` on `execute`, which means that we can't \"save\" and share this vector across the right partition.\r\n   \r\n    if we perform this operation on `execute`, we need to compute it once per right part. If we perform this operation on `new`, we can no longer plan without hitting the data.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T05:57:00.119+0000",
                    "updated": "2020-11-19T05:57:00.119+0000",
                    "started": "2020-11-19T05:57:00.119+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513942",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513944",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526611739\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       @andygrove , I understand that the idea is to build a vector of RecordBatches from the left once, and then use it on the multiple parts on the right. However, I do not think we have a place to store it atm, as this is information that is required by multiple parts from the right, and our API does not support mutating `self` on `execute`, which means that we can't \"save\" and share this vector across the right partition.\r\n   \r\n    if we perform this operation on `execute`, we need to compute it once per right part. If we perform this operation on `new`, we can no longer plan without hitting the data.\r\n   \r\n   A solution is to repartition both left and right so that each output part only requires 1 part from the left and one part from the right.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T05:59:10.062+0000",
                    "updated": "2020-11-19T05:59:10.062+0000",
                    "started": "2020-11-19T05:59:10.061+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513944",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513966",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526611739\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       @andygrove , I understand that the idea is to build a vector of RecordBatches from the left once, and then use it on the multiple parts on the right. However, I do not think we have a place to store it atm, as this is information that is required by multiple parts from the right, and our API does not support mutating `self` on `execute`, which means that we can't \"save\" and share this vector across the right partition.\r\n   \r\n    if we perform this operation on `execute`, we need to compute it once per right part. If we perform this operation on `new`, we can no longer plan without hitting the data.\r\n   \r\n   A solution is to repartition both left and right so that each output part only requires 1 part from the left and one part from the right.\r\n   \r\n   EDIT: nvmd, we can just merge the left streams into a single stream and we are done.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T06:46:40.127+0000",
                    "updated": "2020-11-19T06:46:40.127+0000",
                    "started": "2020-11-19T06:46:40.126+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513966",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513973",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526611739\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       @andygrove , I understand that the idea is to build a vector of RecordBatches from the left once, and then use it on the multiple parts on the right. However, I do not think we have a place to store it atm, as this is information that is required by multiple parts from the right, and our API does not support mutating `self` on `execute`, which means that we can't \"save\" and share this vector across the right partition.\r\n   \r\n    if we perform this operation on `execute`, we need to compute it once per right part. If we perform this operation on `new`, we can no longer plan without hitting the data.\r\n   \r\n   A solution is to repartition both left and right so that each output part only requires 1 part from the left and one part from the right.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T07:06:16.411+0000",
                    "updated": "2020-11-19T07:06:16.411+0000",
                    "started": "2020-11-19T07:06:16.411+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513973",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/513978",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526608341\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       this is wrong: we need to get all partitions from the left here.\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinType};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Create a new HashJoinExec\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(&left_schema, &right_schema, &on, &join_type)?);\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // build the keys\n+        let stream = self.left.execute(partition).await?;\n\nReview comment:\n       @andygrove , I understand that the idea is to build a vector of RecordBatches from the left once, and then use it on the multiple parts on the right. However, I do not think we have a place to store it atm, as this is information that is required by multiple parts from the right, and our API does not support mutating `self` on `execute`, which means that we can't \"save\" and share this vector across the right partition.\r\n   \r\n    if we perform this operation on `execute`, we need to compute it once per right part. If we perform this operation on `new`, we can no longer plan without hitting the data.\r\n   \r\n   A solution is to repartition both left and right so that each output part only requires 1 part from the left and one part from the right.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T07:37:24.493+0000",
                    "updated": "2020-11-19T07:37:24.493+0000",
                    "started": "2020-11-19T07:37:24.493+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "513978",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/514068",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526834131\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,507 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::{expressions::col, hash_aggregate::create_key};\n+use super::{\n+    hash_utils::{build_join_schema, check_join_is_valid, JoinType},\n+    merge::MergeExec,\n+};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+// Note that while this is currently equal to `Index`, the `JoinIndex` is semantically different\n+// as a left join may issue None indices, in which case\n+type JoinIndex = Option<(usize, usize)>;\n+// Maps [\"on\" value] -> [list of indices with this key's value]\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Tries to create a new [HashJoinExec].\n+    /// # Error\n+    /// This function errors when it is not possible to join the left and right sides on keys `on`.\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(\n+            &left_schema,\n+            &right_schema,\n+            &on,\n+            &join_type,\n+        ));\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // merge all parts into a single stream\n+        // this is currently expensive as we re-compute this for every part from the right\n+        // TODO: Fix this issue: we can't share this state across parts on the right.\n\nReview comment:\n       When I tried to implement this, my approach was to store the materialized left-hand side in an `Arc<Mutex<_>>` and my plan was to use logic like this:\r\n   \r\n   ```rust\r\n   if (partition == 0) {\r\n     // materialize left input and store in Arc<Mutex<_>>\r\n   } else {\r\n    // wait for left input to be materialized\r\n   }\r\n   ```\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T12:25:23.586+0000",
                    "updated": "2020-11-19T12:25:23.586+0000",
                    "started": "2020-11-19T12:25:23.586+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "514068",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/514072",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526837738\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,507 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::{expressions::col, hash_aggregate::create_key};\n+use super::{\n+    hash_utils::{build_join_schema, check_join_is_valid, JoinType},\n+    merge::MergeExec,\n+};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// A pair (left index, right index)\n+// Note that while this is currently equal to `Index`, the `JoinIndex` is semantically different\n+// as a left join may issue None indices, in which case\n+type JoinIndex = Option<(usize, usize)>;\n+// Maps [\"on\" value] -> [list of indices with this key's value]\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n+    /// How the join is performed\n+    join_type: JoinType,\n+    /// The schema once the join is applied\n+    schema: SchemaRef,\n+}\n+\n+impl HashJoinExec {\n+    /// Tries to create a new [HashJoinExec].\n+    /// # Error\n+    /// This function errors when it is not possible to join the left and right sides on keys `on`.\n+    pub fn try_new(\n+        left: Arc<dyn ExecutionPlan>,\n+        right: Arc<dyn ExecutionPlan>,\n+        on: &HashSet<String>,\n+        join_type: &JoinType,\n+    ) -> Result<Self> {\n+        let left_schema = left.schema();\n+        let right_schema = right.schema();\n+        check_join_is_valid(&left_schema, &right_schema, &on)?;\n+\n+        let on = on.iter().map(|s| s.clone()).collect::<HashSet<_>>();\n+\n+        let schema = Arc::new(build_join_schema(\n+            &left_schema,\n+            &right_schema,\n+            &on,\n+            &join_type,\n+        ));\n+\n+        Ok(HashJoinExec {\n+            left,\n+            right,\n+            on: on.clone(),\n+            join_type: join_type.clone(),\n+            schema,\n+        })\n+    }\n+}\n+\n+#[async_trait]\n+impl ExecutionPlan for HashJoinExec {\n+    fn as_any(&self) -> &dyn Any {\n+        self\n+    }\n+\n+    fn schema(&self) -> SchemaRef {\n+        self.schema.clone()\n+    }\n+\n+    fn children(&self) -> Vec<Arc<dyn ExecutionPlan>> {\n+        vec![self.left.clone(), self.right.clone()]\n+    }\n+\n+    fn with_new_children(\n+        &self,\n+        children: Vec<Arc<dyn ExecutionPlan>>,\n+    ) -> Result<Arc<dyn ExecutionPlan>> {\n+        match children.len() {\n+            2 => Ok(Arc::new(HashJoinExec::try_new(\n+                children[0].clone(),\n+                children[1].clone(),\n+                &self.on,\n+                &self.join_type,\n+            )?)),\n+            _ => Err(DataFusionError::Internal(\n+                \"HashJoinExec wrong number of children\".to_string(),\n+            )),\n+        }\n+    }\n+\n+    fn output_partitioning(&self) -> Partitioning {\n+        self.right.output_partitioning()\n+    }\n+\n+    async fn execute(&self, partition: usize) -> Result<SendableRecordBatchStream> {\n+        // merge all parts into a single stream\n+        // this is currently expensive as we re-compute this for every part from the right\n+        // TODO: Fix this issue: we can't share this state across parts on the right.\n\nReview comment:\n       I woulld be fine with us doing this as a follow up and get the expensive version of this functionally correct before we optimize too much.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T12:31:20.591+0000",
                    "updated": "2020-11-19T12:31:20.591+0000",
                    "started": "2020-11-19T12:31:20.591+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "514072",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/514076",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526849347\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_join.rs\n##########\n@@ -0,0 +1,467 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Defines the join plan for executing partitions in parallel and then joining the results\n+//! into a set of partitions.\n+\n+use std::sync::Arc;\n+use std::{\n+    any::Any,\n+    collections::{HashMap, HashSet},\n+};\n+\n+use async_trait::async_trait;\n+use futures::{Stream, StreamExt, TryStreamExt};\n+\n+use arrow::array::{make_array, Array, MutableArrayData};\n+use arrow::datatypes::{Schema, SchemaRef};\n+use arrow::error::Result as ArrowResult;\n+use arrow::record_batch::RecordBatch;\n+\n+use super::hash_utils::{build_join_schema, check_join_is_valid, JoinHow};\n+use super::{expressions::col, hash_aggregate::create_key};\n+use crate::error::{DataFusionError, Result};\n+\n+use super::{\n+    group_scalar::GroupByScalar, ExecutionPlan, Partitioning, RecordBatchStream,\n+    SendableRecordBatchStream,\n+};\n+\n+// An index of (batch, row) uniquely identifying a row in a part.\n+type Index = (usize, usize);\n+// None represents a null (e.g. in case of a left join, some right indices are null)\n+type JoinIndex = (usize, usize);\n+// A mapping \"on\" value -> list of row indexes with this key's value\n+// E.g. [1, 2] -> [(0, 3), (1, 6), (0, 8)] indicates that (column1, column2) = [1, 2] is true\n+// for rows 3 and 8 from batch 0 and row 6 from batch 1.\n+type JoinHashMap = HashMap<Vec<GroupByScalar>, Vec<Index>>;\n+type JoinLeftData = (JoinHashMap, Vec<RecordBatch>);\n+\n+/// join execution plan executes partitions in parallel and combines them into a set of\n+/// partitions.\n+#[derive(Debug)]\n+pub struct HashJoinExec {\n+    /// left side\n+    left: Arc<dyn ExecutionPlan>,\n+    /// right side\n+    right: Arc<dyn ExecutionPlan>,\n+    /// Set of common columns used to join on\n+    on: HashSet<String>,\n\nReview comment:\n       After thinking about this some more, I think it might be better to address this in the current PR. I would suggest either:\r\n   \r\n   ```rust\r\n   left_keys: Vec<String>\r\n   right_keys: Vec<String>,\r\n   ```\r\n   \r\n   or the more general case:\r\n   \r\n   ```rust\r\n   left_keys: Vec<Arc<dyn PhysicalExpr>>\r\n   right_keys: Vec<Arc<dyn PhysicalExpr>>,\r\n   ```\r\n   \r\n   It should be possible to perform an equi-join using any deterministic expression, for example:\r\n   \r\n   ```\r\n   SELECT a.*, b.* FROM a JOIN b on UPPER(a.name) = UPPER(CONCAT(b.first, ' ', b.last))\r\n   ```\r\n   \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T12:40:24.890+0000",
                    "updated": "2020-11-19T12:40:24.890+0000",
                    "started": "2020-11-19T12:40:24.889+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "514076",
                    "issueId": "13319220"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/worklog/514079",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #8709:\nURL: https://github.com/apache/arrow/pull/8709#discussion_r526851354\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/hash_utils.rs\n##########\n@@ -0,0 +1,144 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Functionality used both on logical and physical plans\n+\n+use crate::error::{DataFusionError, Result};\n+use arrow::datatypes::{Field, Schema};\n+use std::collections::HashSet;\n+\n+/// All valid types of joins.\n+#[derive(Clone, Debug)]\n+pub enum JoinType {\n+    /// Inner join\n+    Inner,\n+}\n+\n+/// Checks whether the schemas \"left\" and \"right\" and columns \"on\" represent a valid join.\n+/// They are valid whenever their columns' intersection equals the set `on`\n+pub fn check_join_is_valid(\n+    left: &Schema,\n+    right: &Schema,\n+    on: &HashSet<String>,\n+) -> Result<()> {\n+    let left: HashSet<String> = left.fields().iter().map(|f| f.name().clone()).collect();\n+    let right: HashSet<String> =\n+        right.fields().iter().map(|f| f.name().clone()).collect();\n+\n+    check_join_set_is_valid(&left, &right, &on)\n+}\n+\n+/// Checks whether the sets left, right and on compose a valid join.\n+/// They are valid whenever their intersection equals the set `on`\n+fn check_join_set_is_valid(\n+    left: &HashSet<String>,\n+    right: &HashSet<String>,\n+    on: &HashSet<String>,\n+) -> Result<()> {\n+    if on.len() == 0 {\n+        return Err(DataFusionError::Plan(\n+            \"The 'on' clause of a join cannot be empty\".to_string(),\n+        ));\n+    }\n+\n+    let on_columns = on.iter().collect::<HashSet<_>>();\n+    let common_columns = left.intersection(&right).collect::<HashSet<_>>();\n+    let missing = on_columns\n+        .difference(&common_columns)\n+        .collect::<HashSet<_>>();\n+    if missing.len() > 0 {\n+        return Err(DataFusionError::Plan(format!(\n+                \"The left or right side of the join does not have columns {:?} columns on \\\"on\\\": \\nLeft: {:?}\\nRight: {:?}\\nOn: {:?}\",\n+                missing,\n+                left,\n+                right,\n+                on,\n+            ).to_string()));\n+    };\n+    Ok(())\n+}\n+\n+/// Creates a schema for a join operation.\n+/// The fields \"on\" from the left side are always first\n+pub fn build_join_schema(\n+    left: &Schema,\n+    right: &Schema,\n+    on: &HashSet<String>,\n+    join_type: &JoinType,\n+) -> Schema {\n+    let fields: Vec<Field> = match join_type {\n+        JoinType::Inner => {\n+            // inner: all fields are there\n+\n+            let on_fields = left.fields().iter().filter(|f| on.contains(f.name()));\n+\n+            let left_fields = left.fields().iter().filter(|f| !on.contains(f.name()));\n+\n+            let right_fields = right.fields().iter().filter(|f| !on.contains(f.name()));\n+\n+            // \"on\" are first by construction, then left, then right\n+            on_fields\n+                .chain(left_fields)\n+                .chain(right_fields)\n+                .map(|f| f.clone())\n+                .collect()\n\nReview comment:\n       It would be more standard to have all the left fields followed by all the right fields. Also, the new schema should have fully qualified names that include the table/alias prefix.\r\n   \r\n   For example if we join table 'a' with fields `id` and name` with table 'b' that also has fields `id` and `name`, the output schema would be `a.id`, `a.name`, `b.id`, `b.name`.\r\n   \r\n   It is quite possible that we are lacking support currently for being able to reference these qualified field names, so we will need to deal with that before we can fully integrate this new join operator into DataFusion.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-11-19T12:43:59.921+0000",
                    "updated": "2020-11-19T12:43:59.921+0000",
                    "started": "2020-11-19T12:43:59.921+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "514079",
                    "issueId": "13319220"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 36000,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@2603abf2[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@66950c83[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@447aa78d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@ac54a95[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b73ec54[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@4a82606a[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5b55e078[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@7be48865[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3fb1ca01[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@66696cad[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@124b0638[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@4d3313b4[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 36000,
        "customfield_12312520": null,
        "customfield_12312521": "Sat Nov 21 17:34:39 UTC 2020",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2020-11-21T17:34:39.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-9555/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2020-07-24T19:24:23.000+0000",
        "updated": "2020-11-21T17:34:55.000+0000",
        "timeoriginalestimate": null,
        "description": "Here is an overview of how I think we should implement support for equijoins, at least for the initial implementation.\r\n * Read all batches from the left-side of the join into a single Vec<RecordBatch>\r\n * Create a map something like HashMap<Vec<ScalarValue>, Vec<(usize,usize)>> to map keys to batch/row indices\r\n * Iterate over this Vec<RecordBatch> and create an entry in a hash map, mapping the join keys to the index of the batch and row in the Vec<RecordBatch>\r\n * For each input partition on the right-side of the join, return an output partition that is an iterator/stream that:\r\n ** For each input row, evaluate the join keys\r\n ** Look up those join keys in the hash map\r\n ** If a match is found:\r\n *** For each (batch, row) index create an output row which has the values from both the left and right row and emit it\r\n ** If no match is found:\r\n *** Do not emit a row",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "10h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 36000
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust] [DataFusion] Add inner (hash) equijoin physical plan",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/comment/17232063",
                    "id": "17232063",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "body": "[~jorgecarleitao] fyi I have tried to spec out the implementation that I had in mind since I have not found the time to implement this myself.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "created": "2020-11-14T16:10:19.629+0000",
                    "updated": "2020-11-14T16:10:19.629+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/comment/17232314",
                    "id": "17232314",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
                        "name": "jorgecarleitao",
                        "key": "jorgecarleitao",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
                        },
                        "displayName": "Jorge Leit\u00e3o",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "Thanks [~andygrove]. I consider this node kind of important, so I will give it a go.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
                        "name": "jorgecarleitao",
                        "key": "jorgecarleitao",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
                        },
                        "displayName": "Jorge Leit\u00e3o",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2020-11-15T15:19:32.205+0000",
                    "updated": "2020-11-15T15:19:32.205+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/comment/17233286",
                    "id": "17233286",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
                        "name": "jorgecarleitao",
                        "key": "jorgecarleitao",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
                        },
                        "displayName": "Jorge Leit\u00e3o",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "I have a draft almost ready.\r\n\r\nThe implementation is blocked by #8689. Specifically, I need an API in arrow to create an array from multiple arrays (each array in the new recordBatch will be composed by multiple entries from different arrays).\r\n\r\nThis is related to the work I have been doing on generalizing the filtering operation (as we also need to build an array a single array), `take` (same issue), sort-merge (same, but create an array from two arrays). The join is the extreme case where we need to create an array from N+1 arrays (N batches from the left + 1 from the right).\r\n\r\nI will be working on top of the code in #8689 as that is paramount for me to accomplish this.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=jorgecarleitao",
                        "name": "jorgecarleitao",
                        "key": "jorgecarleitao",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=jorgecarleitao&avatarId=43827",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=jorgecarleitao&avatarId=43827",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=jorgecarleitao&avatarId=43827",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=jorgecarleitao&avatarId=43827"
                        },
                        "displayName": "Jorge Leit\u00e3o",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2020-11-17T06:19:07.335+0000",
                    "updated": "2020-11-17T06:19:07.335+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13319220/comment/17236740",
                    "id": "17236740",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "body": "Issue resolved by pull request 8709\n[https://github.com/apache/arrow/pull/8709]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "created": "2020-11-21T17:34:39.965+0000",
                    "updated": "2020-11-21T17:34:39.965+0000"
                }
            ],
            "maxResults": 4,
            "total": 4,
            "startAt": 0
        },
        "customfield_12311820": "0|z0h6dc:",
        "customfield_12314139": null
    }
}