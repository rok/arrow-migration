{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13216169",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169",
    "key": "ARROW-4589",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343937",
                "id": "12343937",
                "description": "",
                "name": "0.13.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-04-01"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12343858",
                "id": "12343858",
                "description": "",
                "name": "0.12.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-01-20"
            }
        ],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/6",
            "description": "The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/closed.png",
            "name": "Closed",
            "id": "6",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12335005",
                "id": "12335005",
                "name": "Rust - DataFusion"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
            "name": "andygrove",
            "key": "andygrove",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
            },
            "displayName": "Andy Grove",
            "active": true,
            "timeZone": "America/Denver"
        },
        "aggregateprogress": {
            "progress": 8400,
            "total": 8400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 8400,
            "total": 8400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4589/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 16,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199633",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] [WIP] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664\n \n \n   This PR adds the first query optimizer rule, which ensures that we only load the columns we need from the underlying data store.\r\n   \r\n   I plan to complete this over this weekend but am sharing early to give everyone a chance to review and following along.\r\n   \r\n   I will rebase once complete.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-16T18:41:39.375+0000",
                    "updated": "2019-02-16T18:41:39.375+0000",
                    "started": "2019-02-16T18:41:39.374+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199633",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199647",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on issue #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#issuecomment-464384205\n \n \n   @sunchao @paddyhoran @nevi-me This is ready for review\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-16T20:54:09.532+0000",
                    "updated": "2019-02-16T20:54:09.532+0000",
                    "started": "2019-02-16T20:54:09.531+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199647",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199662",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257482057\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   Why is it \"returning a different schema\" - seems the same schema is still returned?\r\n   \r\n   Also, to help me understand, does each plan operator has its own schema and it could be different from the global schema (e.g., the schema of the input source). If so, is the column index the index to the schema of the **current** schema (e.g., column index in the `expr` of `Sort` will point to the `schema` in the `Sort` plan)?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-16T23:57:36.334+0000",
                    "updated": "2019-02-16T23:57:36.334+0000",
                    "started": "2019-02-16T23:57:36.333+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199662",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199663",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257481560\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/optimizer.rs\n ##########\n @@ -0,0 +1,27 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Query optimizer traits\n+\n+use crate::logicalplan::LogicalPlan;\n+use arrow::error::Result;\n+use std::rc::Rc;\n+\n+/// An optimizer rules performs a transformation on a logical plan to produce an optimized logical plan.\n \n Review comment:\n   nit: can we also restrict the comments to be 90 characters?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-16T23:57:36.335+0000",
                    "updated": "2019-02-16T23:57:36.335+0000",
                    "started": "2019-02-16T23:57:36.335+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199663",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199664",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257482218\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n+                // mapping from the original column index to the new column index so that we\n+                // can rewrite expressions as we walk back up the tree\n+\n+                if mapping.len() != 0 {\n+                    return Err(ArrowError::ComputeError(\"illegal state\".to_string()));\n+                }\n+\n+                for i in 0..schema.fields().len() {\n+                    if let Some(n) = projection.iter().position(|v| *v == i) {\n+                        mapping.insert(i, n);\n+                    }\n+                }\n+\n+                // return the table scan with projection\n+                Ok(Rc::new(LogicalPlan::TableScan {\n+                    schema_name: schema_name.to_string(),\n+                    table_name: table_name.to_string(),\n+                    schema: schema.clone(),\n+                    projection: Some(projection),\n+                }))\n+            }\n+        }\n+    }\n+\n+    fn collect_exprs(&self, expr: &Vec<Expr>, accum: &mut HashSet<usize>) {\n+        expr.iter().for_each(|e| self.collect_expr(e, accum));\n+    }\n+\n+    fn collect_expr(&self, expr: &Expr, accum: &mut HashSet<usize>) {\n+        match expr {\n+            Expr::Column(i) => {\n+                accum.insert(*i);\n+            }\n+            Expr::Literal(_) => { /* not needed */ }\n+            Expr::IsNull(e) => self.collect_expr(e, accum),\n+            Expr::IsNotNull(e) => self.collect_expr(e, accum),\n+            Expr::BinaryExpr { left, right, .. } => {\n+                self.collect_expr(left, accum);\n+                self.collect_expr(right, accum);\n+            }\n+            Expr::Cast { expr, .. } => self.collect_expr(expr, accum),\n+            Expr::Sort { expr, .. } => self.collect_expr(expr, accum),\n+            Expr::AggregateFunction { args, .. } => self.collect_exprs(args, accum),\n+            Expr::ScalarFunction { args, .. } => self.collect_exprs(args, accum),\n+        }\n+    }\n+\n+    fn rewrite_exprs(\n+        &self,\n+        expr: &Vec<Expr>,\n+        mapping: &HashMap<usize, usize>,\n+    ) -> Result<Vec<Expr>> {\n+        Ok(expr\n+            .iter()\n+            .map(|e| self.rewrite_expr(e, mapping))\n+            .collect::<Result<Vec<Expr>>>()?)\n+    }\n+\n+    fn rewrite_expr(&self, expr: &Expr, mapping: &HashMap<usize, usize>) -> Result<Expr> {\n+        match expr {\n+            Expr::Column(i) => Ok(Expr::Column(self.new_index(mapping, i)?)),\n+            Expr::Literal(_) => Ok(expr.clone()),\n+            Expr::IsNull(e) => Ok(Expr::IsNull(Rc::new(self.rewrite_expr(e, mapping)?))),\n+            Expr::IsNotNull(e) => {\n+                Ok(Expr::IsNotNull(Rc::new(self.rewrite_expr(e, mapping)?)))\n+            }\n+            Expr::BinaryExpr { left, op, right } => Ok(Expr::BinaryExpr {\n+                left: Rc::new(self.rewrite_expr(left, mapping)?),\n+                op: op.clone(),\n+                right: Rc::new(self.rewrite_expr(right, mapping)?),\n+            }),\n+            Expr::Cast { expr, data_type } => Ok(Expr::Cast {\n+                expr: Rc::new(self.rewrite_expr(expr, mapping)?),\n+                data_type: data_type.clone(),\n+            }),\n+            Expr::Sort { expr, asc } => Ok(Expr::Sort {\n+                expr: Rc::new(self.rewrite_expr(expr, mapping)?),\n+                asc: *asc,\n+            }),\n+            Expr::AggregateFunction {\n+                name,\n+                args,\n+                return_type,\n+            } => Ok(Expr::AggregateFunction {\n+                name: name.to_string(),\n+                args: self.rewrite_exprs(args, mapping)?,\n+                return_type: return_type.clone(),\n+            }),\n+            Expr::ScalarFunction {\n+                name,\n+                args,\n+                return_type,\n+            } => Ok(Expr::ScalarFunction {\n+                name: name.to_string(),\n+                args: self.rewrite_exprs(args, mapping)?,\n+                return_type: return_type.clone(),\n+            }),\n+        }\n+    }\n+\n+    fn new_index(&self, mapping: &HashMap<usize, usize>, i: &usize) -> Result<usize> {\n+        match mapping.get(i) {\n+            Some(j) => Ok(*j),\n+            _ => Err(ArrowError::ComputeError(\n+                \"Internal error computing new column index\".to_string(),\n+            )),\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+\n+    use super::*;\n+    use crate::logicalplan::Expr::*;\n+    use crate::logicalplan::LogicalPlan::*;\n+    use arrow::datatypes::{DataType, Field, Schema};\n+    use std::cell::RefCell;\n+    use std::rc::Rc;\n+    use std::sync::Arc;\n+\n+    #[test]\n+    fn aggregate_no_group_by() {\n+        let table_scan = test_table_scan();\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"MAX(b)\",\n+                DataType::UInt32,\n+                false,\n+            )])),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[]], aggr=[[#0]]\\n  TableScan: test projection=Some([1])\");\n+    }\n+\n+    #[test]\n+    fn aggregate_group_by() {\n+        let table_scan = test_table_scan();\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![Column(2)],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![\n+                Field::new(\"c\", DataType::UInt32, false),\n+                Field::new(\"MAX(b)\", DataType::UInt32, false),\n+            ])),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[#1]], aggr=[[#0]]\\n  TableScan: test projection=Some([1, 2])\");\n+    }\n+\n+    #[test]\n+    fn aggregate_no_group_by_with_selection() {\n+        let table_scan = test_table_scan();\n+\n+        let selection = Selection {\n+            expr: Column(2),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"MAX(b)\",\n+                DataType::UInt32,\n+                false,\n+            )])),\n+            input: Rc::new(selection),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[]], aggr=[[#0]]\\n  Selection: #1\\n    TableScan: test projection=Some([1, 2])\");\n+    }\n+\n+    #[test]\n+    fn cast() {\n+        let table_scan = test_table_scan();\n+\n+        let projection = Projection {\n+            expr: vec![Cast {\n+                expr: Rc::new(Column(2)),\n+                data_type: DataType::Float64,\n+            }],\n+            input: Rc::new(table_scan),\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"CAST(c AS float)\",\n+                DataType::Float64,\n+                false,\n+            )])),\n+        };\n+\n+        assert_optimized_plan_eq(\n+            &projection,\n+            \"Projection: CAST(#0 AS Float64)\\n  TableScan: test projection=Some([2])\",\n+        );\n+    }\n+\n+    fn assert_optimized_plan_eq(plan: &LogicalPlan, expected: &str) {\n+        let optimized_plan = optimize(plan);\n+        let formatted_plan = format!(\"{:?}\", optimized_plan);\n+        assert_eq!(formatted_plan, expected);\n+    }\n+\n+    fn optimize(plan: &LogicalPlan) -> Rc<LogicalPlan> {\n+        let rule: Rc<RefCell<OptimizerRule>> =\n \n Review comment:\n   Why we need `Rc` and `RefCell` here? can we do:\r\n   ```rust\r\n           let mut rule = ProjectionPushDown::new();\r\n           rule.optimize(plan).unwrap()\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-16T23:57:36.368+0000",
                    "updated": "2019-02-16T23:57:36.368+0000",
                    "started": "2019-02-16T23:57:36.367+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199664",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199668",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257482405\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   Looks like you just found a bug. Good catch. It should be returning the schema after the projection has been applied.\r\n   \r\n   Yes each plan operator has its own schema (for its output). In some cases (filter, limit, sort) the schema does not change so they can just delegate to their input relation.\r\n   \r\n   Column indexes are always for the schema of the input relation.\r\n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T00:04:21.883+0000",
                    "updated": "2019-02-17T00:04:21.883+0000",
                    "started": "2019-02-17T00:04:21.882+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199668",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199670",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257482423\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n+                // mapping from the original column index to the new column index so that we\n+                // can rewrite expressions as we walk back up the tree\n+\n+                if mapping.len() != 0 {\n+                    return Err(ArrowError::ComputeError(\"illegal state\".to_string()));\n+                }\n+\n+                for i in 0..schema.fields().len() {\n+                    if let Some(n) = projection.iter().position(|v| *v == i) {\n+                        mapping.insert(i, n);\n+                    }\n+                }\n+\n+                // return the table scan with projection\n+                Ok(Rc::new(LogicalPlan::TableScan {\n+                    schema_name: schema_name.to_string(),\n+                    table_name: table_name.to_string(),\n+                    schema: schema.clone(),\n+                    projection: Some(projection),\n+                }))\n+            }\n+        }\n+    }\n+\n+    fn collect_exprs(&self, expr: &Vec<Expr>, accum: &mut HashSet<usize>) {\n+        expr.iter().for_each(|e| self.collect_expr(e, accum));\n+    }\n+\n+    fn collect_expr(&self, expr: &Expr, accum: &mut HashSet<usize>) {\n+        match expr {\n+            Expr::Column(i) => {\n+                accum.insert(*i);\n+            }\n+            Expr::Literal(_) => { /* not needed */ }\n+            Expr::IsNull(e) => self.collect_expr(e, accum),\n+            Expr::IsNotNull(e) => self.collect_expr(e, accum),\n+            Expr::BinaryExpr { left, right, .. } => {\n+                self.collect_expr(left, accum);\n+                self.collect_expr(right, accum);\n+            }\n+            Expr::Cast { expr, .. } => self.collect_expr(expr, accum),\n+            Expr::Sort { expr, .. } => self.collect_expr(expr, accum),\n+            Expr::AggregateFunction { args, .. } => self.collect_exprs(args, accum),\n+            Expr::ScalarFunction { args, .. } => self.collect_exprs(args, accum),\n+        }\n+    }\n+\n+    fn rewrite_exprs(\n+        &self,\n+        expr: &Vec<Expr>,\n+        mapping: &HashMap<usize, usize>,\n+    ) -> Result<Vec<Expr>> {\n+        Ok(expr\n+            .iter()\n+            .map(|e| self.rewrite_expr(e, mapping))\n+            .collect::<Result<Vec<Expr>>>()?)\n+    }\n+\n+    fn rewrite_expr(&self, expr: &Expr, mapping: &HashMap<usize, usize>) -> Result<Expr> {\n+        match expr {\n+            Expr::Column(i) => Ok(Expr::Column(self.new_index(mapping, i)?)),\n+            Expr::Literal(_) => Ok(expr.clone()),\n+            Expr::IsNull(e) => Ok(Expr::IsNull(Rc::new(self.rewrite_expr(e, mapping)?))),\n+            Expr::IsNotNull(e) => {\n+                Ok(Expr::IsNotNull(Rc::new(self.rewrite_expr(e, mapping)?)))\n+            }\n+            Expr::BinaryExpr { left, op, right } => Ok(Expr::BinaryExpr {\n+                left: Rc::new(self.rewrite_expr(left, mapping)?),\n+                op: op.clone(),\n+                right: Rc::new(self.rewrite_expr(right, mapping)?),\n+            }),\n+            Expr::Cast { expr, data_type } => Ok(Expr::Cast {\n+                expr: Rc::new(self.rewrite_expr(expr, mapping)?),\n+                data_type: data_type.clone(),\n+            }),\n+            Expr::Sort { expr, asc } => Ok(Expr::Sort {\n+                expr: Rc::new(self.rewrite_expr(expr, mapping)?),\n+                asc: *asc,\n+            }),\n+            Expr::AggregateFunction {\n+                name,\n+                args,\n+                return_type,\n+            } => Ok(Expr::AggregateFunction {\n+                name: name.to_string(),\n+                args: self.rewrite_exprs(args, mapping)?,\n+                return_type: return_type.clone(),\n+            }),\n+            Expr::ScalarFunction {\n+                name,\n+                args,\n+                return_type,\n+            } => Ok(Expr::ScalarFunction {\n+                name: name.to_string(),\n+                args: self.rewrite_exprs(args, mapping)?,\n+                return_type: return_type.clone(),\n+            }),\n+        }\n+    }\n+\n+    fn new_index(&self, mapping: &HashMap<usize, usize>, i: &usize) -> Result<usize> {\n+        match mapping.get(i) {\n+            Some(j) => Ok(*j),\n+            _ => Err(ArrowError::ComputeError(\n+                \"Internal error computing new column index\".to_string(),\n+            )),\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+\n+    use super::*;\n+    use crate::logicalplan::Expr::*;\n+    use crate::logicalplan::LogicalPlan::*;\n+    use arrow::datatypes::{DataType, Field, Schema};\n+    use std::cell::RefCell;\n+    use std::rc::Rc;\n+    use std::sync::Arc;\n+\n+    #[test]\n+    fn aggregate_no_group_by() {\n+        let table_scan = test_table_scan();\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"MAX(b)\",\n+                DataType::UInt32,\n+                false,\n+            )])),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[]], aggr=[[#0]]\\n  TableScan: test projection=Some([1])\");\n+    }\n+\n+    #[test]\n+    fn aggregate_group_by() {\n+        let table_scan = test_table_scan();\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![Column(2)],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![\n+                Field::new(\"c\", DataType::UInt32, false),\n+                Field::new(\"MAX(b)\", DataType::UInt32, false),\n+            ])),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[#1]], aggr=[[#0]]\\n  TableScan: test projection=Some([1, 2])\");\n+    }\n+\n+    #[test]\n+    fn aggregate_no_group_by_with_selection() {\n+        let table_scan = test_table_scan();\n+\n+        let selection = Selection {\n+            expr: Column(2),\n+            input: Rc::new(table_scan),\n+        };\n+\n+        let aggregate = Aggregate {\n+            group_expr: vec![],\n+            aggr_expr: vec![Column(1)],\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"MAX(b)\",\n+                DataType::UInt32,\n+                false,\n+            )])),\n+            input: Rc::new(selection),\n+        };\n+\n+        assert_optimized_plan_eq(&aggregate, \"Aggregate: groupBy=[[]], aggr=[[#0]]\\n  Selection: #1\\n    TableScan: test projection=Some([1, 2])\");\n+    }\n+\n+    #[test]\n+    fn cast() {\n+        let table_scan = test_table_scan();\n+\n+        let projection = Projection {\n+            expr: vec![Cast {\n+                expr: Rc::new(Column(2)),\n+                data_type: DataType::Float64,\n+            }],\n+            input: Rc::new(table_scan),\n+            schema: Arc::new(Schema::new(vec![Field::new(\n+                \"CAST(c AS float)\",\n+                DataType::Float64,\n+                false,\n+            )])),\n+        };\n+\n+        assert_optimized_plan_eq(\n+            &projection,\n+            \"Projection: CAST(#0 AS Float64)\\n  TableScan: test projection=Some([2])\",\n+        );\n+    }\n+\n+    fn assert_optimized_plan_eq(plan: &LogicalPlan, expected: &str) {\n+        let optimized_plan = optimize(plan);\n+        let formatted_plan = format!(\"{:?}\", optimized_plan);\n+        assert_eq!(formatted_plan, expected);\n+    }\n+\n+    fn optimize(plan: &LogicalPlan) -> Rc<LogicalPlan> {\n+        let rule: Rc<RefCell<OptimizerRule>> =\n \n Review comment:\n   You are correct. I was just trying to case from `ProjectionPushDown` to `OptimizerRule` since eventually there will be a list of rules to apply. I will simplify this for now though.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T00:05:19.081+0000",
                    "updated": "2019-02-17T00:05:19.081+0000",
                    "started": "2019-02-17T00:05:19.081+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199670",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199674",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257482825\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   I pushed a fix for the bug and added a test \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T00:24:07.156+0000",
                    "updated": "2019-02-17T00:24:07.156+0000",
                    "started": "2019-02-17T00:24:07.155+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199674",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199684",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257483582\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/optimizer.rs\n ##########\n @@ -0,0 +1,27 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Query optimizer traits\n+\n+use crate::logicalplan::LogicalPlan;\n+use arrow::error::Result;\n+use std::rc::Rc;\n+\n+/// An optimizer rules performs a transformation on a logical plan to produce an optimized logical plan.\n \n Review comment:\n   This seems is still over 90 characters.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T01:11:04.918+0000",
                    "updated": "2019-02-17T01:11:04.918+0000",
                    "started": "2019-02-17T01:11:04.918+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199684",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199685",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257483723\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   > Column indexes are always for the schema of the input relation.\r\n   \r\n   Does the schema of the input relation change? I'm still not sure why we need to rewrite the column indexes - can they always point to the complete schema of the input source?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T01:11:04.934+0000",
                    "updated": "2019-02-17T01:11:04.934+0000",
                    "started": "2019-02-17T01:11:04.924+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199685",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199687",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257483789\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/optimizer.rs\n ##########\n @@ -0,0 +1,27 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Query optimizer traits\n+\n+use crate::logicalplan::LogicalPlan;\n+use arrow::error::Result;\n+use std::rc::Rc;\n+\n+/// An optimizer rules performs a transformation on a logical plan to produce an optimized logical plan.\n \n Review comment:\n   I pushed a second commit to fix this\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T01:15:41.184+0000",
                    "updated": "2019-02-17T01:15:41.184+0000",
                    "started": "2019-02-17T01:15:41.184+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199687",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199688",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257483950\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   The logical plan created via the SQL query planner (and the DataFrame API when we have one) does just refer to the original table schema.\r\n   \r\n   The query optimizer transforms the plan and pushes the projection down to the TableScan so that we basically pretend the table only contains the columns we care about. The rest of the plan is then rewritten to be relative to that.\r\n   \r\n   Each operator in the plan is relative to its input and doesn't know about the underlying table schema which could be many levels down, especially once we have joins and subqueries.\r\n   \r\n   \r\n   \r\n   \r\n   \r\n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T01:24:25.871+0000",
                    "updated": "2019-02-17T01:24:25.871+0000",
                    "started": "2019-02-17T01:24:25.870+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199688",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199691",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257484433\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   I guess one of the reasons for doing this, other than having a concise and simple to comprehend plan, is that ultimately these are indexes into RecordBatch instances. \r\n   \r\n   Let's say we have a csv/parquet file with 300 columns and the query only references 12 of them... If we don't do this rewriting then we are going to have to load a RecordBatch with 300 columns where 288 of them are empty arrays / or empty options, or we have to have some special implementation of RecordBatch which does a mapping.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T01:41:31.860+0000",
                    "updated": "2019-02-17T01:41:31.860+0000",
                    "started": "2019-02-17T01:41:31.860+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199691",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199721",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#discussion_r257493238\n \n \n\n ##########\n File path: rust/datafusion/src/optimizer/projection_push_down.rs\n ##########\n @@ -0,0 +1,377 @@\n+// Licensed to the Apache Software Foundation (ASF) under one\n+// or more contributor license agreements.  See the NOTICE file\n+// distributed with this work for additional information\n+// regarding copyright ownership.  The ASF licenses this file\n+// to you under the Apache License, Version 2.0 (the\n+// \"License\"); you may not use this file except in compliance\n+// with the License.  You may obtain a copy of the License at\n+//\n+//   http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing,\n+// software distributed under the License is distributed on an\n+// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+// KIND, either express or implied.  See the License for the\n+// specific language governing permissions and limitations\n+// under the License.\n+\n+//! Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+\n+use crate::logicalplan::Expr;\n+use crate::logicalplan::LogicalPlan;\n+use crate::optimizer::optimizer::OptimizerRule;\n+use arrow::error::{ArrowError, Result};\n+use std::collections::{HashMap, HashSet};\n+use std::rc::Rc;\n+\n+/// Projection Push Down optimizer rule ensures that only referenced columns are loaded into memory\n+pub struct ProjectionPushDown {}\n+\n+impl OptimizerRule for ProjectionPushDown {\n+    fn optimize(&mut self, plan: &LogicalPlan) -> Result<Rc<LogicalPlan>> {\n+        let mut accum: HashSet<usize> = HashSet::new();\n+        let mut mapping: HashMap<usize, usize> = HashMap::new();\n+        self.optimize_plan(plan, &mut accum, &mut mapping)\n+    }\n+}\n+\n+impl ProjectionPushDown {\n+    pub fn new() -> Self {\n+        Self {}\n+    }\n+\n+    fn optimize_plan(\n+        &self,\n+        plan: &LogicalPlan,\n+        accum: &mut HashSet<usize>,\n+        mapping: &mut HashMap<usize, usize>,\n+    ) -> Result<Rc<LogicalPlan>> {\n+        match plan {\n+            LogicalPlan::Projection {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by projection expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite projection expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Projection {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Selection { expr, input } => {\n+                // collect all columns referenced by filter expression\n+                self.collect_expr(expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite filter expression to use new column indexes\n+                let new_expr = self.rewrite_expr(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Selection {\n+                    expr: new_expr,\n+                    input,\n+                }))\n+            }\n+            LogicalPlan::Aggregate {\n+                input,\n+                group_expr,\n+                aggr_expr,\n+                schema,\n+            } => {\n+                // collect all columns referenced by grouping and aggregate expressions\n+                self.collect_exprs(&group_expr, accum);\n+                self.collect_exprs(&aggr_expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite expressions to use new column indexes\n+                let new_group_expr = self.rewrite_exprs(group_expr, mapping)?;\n+                let new_aggr_expr = self.rewrite_exprs(aggr_expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Aggregate {\n+                    input,\n+                    group_expr: new_group_expr,\n+                    aggr_expr: new_aggr_expr,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::Sort {\n+                expr,\n+                input,\n+                schema,\n+            } => {\n+                // collect all columns referenced by sort expressions\n+                self.collect_exprs(&expr, accum);\n+\n+                // push projection down\n+                let input = self.optimize_plan(&input, accum, mapping)?;\n+\n+                // rewrite sort expressions to use new column indexes\n+                let new_expr = self.rewrite_exprs(expr, mapping)?;\n+\n+                Ok(Rc::new(LogicalPlan::Sort {\n+                    expr: new_expr,\n+                    input,\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::EmptyRelation { schema } => {\n+                Ok(Rc::new(LogicalPlan::EmptyRelation {\n+                    schema: schema.clone(),\n+                }))\n+            }\n+            LogicalPlan::TableScan {\n+                schema_name,\n+                table_name,\n+                schema,\n+                ..\n+            } => {\n+                // once we reach the table scan, we can use the accumulated set of column indexes as\n+                // the projection in the table scan\n+                let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\n+                accum.iter().for_each(|i| projection.push(*i));\n+\n+                // sort the projection otherwise we get non-deterministic behavior\n+                projection.sort();\n+\n+                // now that the table scan is returning a different schema we need to create a\n \n Review comment:\n   OK. Makes sense. Thanks.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T08:16:15.302+0000",
                    "updated": "2019-02-17T08:16:15.302+0000",
                    "started": "2019-02-17T08:16:15.301+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199721",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199723",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nevi-me commented on issue #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664#issuecomment-464429859\n \n \n   Hi @andygrove, I'll be able to take a look on Monday GMT morning\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T08:45:23.548+0000",
                    "updated": "2019-02-17T08:45:23.548+0000",
                    "started": "2019-02-17T08:45:23.548+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199723",
                    "issueId": "13216169"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/worklog/199773",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on pull request #3664: ARROW-4589: [Rust] Projection push down query optimizer rule\nURL: https://github.com/apache/arrow/pull/3664\n \n \n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-02-17T14:53:22.715+0000",
                    "updated": "2019-02-17T14:53:22.715+0000",
                    "started": "2019-02-17T14:53:22.715+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "199773",
                    "issueId": "13216169"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 8400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@23570998[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@34098cbd[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@46b079db[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@3e2469d4[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@540e5b86[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@c67306b[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@495cee9d[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@542b44ab[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@72dff7c3[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@470e759b[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@698853e1[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@67885c67[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 8400,
        "customfield_12312520": null,
        "customfield_12312521": "Sun Feb 17 14:55:09 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-02-17T14:55:09.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-4589/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2019-02-16T17:42:18.000+0000",
        "updated": "2019-02-17T14:55:09.000+0000",
        "timeoriginalestimate": null,
        "description": "If I run a query like the following:\r\n{code:java}\r\nSELECT MIN(fare_amount), MAX(fare_amount) FROM tripdata{code}\r\nI see this logical plan:\r\n{code:java}\r\nLogical plan: Aggregate: groupBy=[[]], aggr=[[MIN(#10), MAX(#10)]]\r\n\u00a0 TableScan: tripdata projection=None{code}\r\n\u00a0\r\n\r\nThis means that every column is being loaded into arrays rather than just the two columns that I care about, resulting in terrible performance.",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "2h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 8400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust] [DataFusion] Implement projection push down query optimizer rule",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13216169/comment/16770425",
                    "id": "16770425",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "body": "Merged. The merge script failed with:\r\n{code:java}\r\nPull request #3664 merged!\r\nMerge hash: 3606aa21\r\n\r\nWould you like to update the associated JIRA? (y/n): y\r\nEnter comma-separated fix version(s) [0.13.0]: y\r\nTraceback (most recent call last):\r\n\u00a0 File \"./dev/merge_arrow_pr.py\", line 455, in <module>\r\n\u00a0\u00a0\u00a0 cli()\r\n\u00a0 File \"./dev/merge_arrow_pr.py\", line 449, in cli\r\n\u00a0\u00a0\u00a0 fix_versions_json = [get_version_json(v) for v in issue_fix_versions]\r\n\u00a0 File \"./dev/merge_arrow_pr.py\", line 447, in get_version_json\r\n\u00a0\u00a0\u00a0 return [x for x in versions if x.name == version_str][0].raw\r\nIndexError: list index out of range\r\n{code}",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=andygrove",
                        "name": "andygrove",
                        "key": "andygrove",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=andygrove&avatarId=28239",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=andygrove&avatarId=28239",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=andygrove&avatarId=28239",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=andygrove&avatarId=28239"
                        },
                        "displayName": "Andy Grove",
                        "active": true,
                        "timeZone": "America/Denver"
                    },
                    "created": "2019-02-17T14:55:09.813+0000",
                    "updated": "2019-02-17T14:55:09.813+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|yi120g:",
        "customfield_12314139": null
    }
}