{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13348468",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468",
    "key": "ARROW-11074",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12349493",
                "id": "12349493",
                "description": "",
                "name": "4.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2021-04-26"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yordan-pavlov",
            "name": "yordan-pavlov",
            "key": "yordan-pavlov",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Yordan Pavlov",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12335005",
                "id": "12335005",
                "name": "Rust - DataFusion"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yordan-pavlov",
            "name": "yordan-pavlov",
            "key": "yordan-pavlov",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Yordan Pavlov",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=yordan-pavlov",
            "name": "yordan-pavlov",
            "key": "yordan-pavlov",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "Yordan Pavlov",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 32400,
            "total": 32400,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 32400,
            "total": 32400,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11074/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 54,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/529896",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov opened a new pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064\n\n\n   While profiling a DataFusion query I found that the code spends a lot of time in reading data from parquet files. Predicate / filter push-down is a commonly used performance optimization, where statistics data stored in parquet files (such as min / max values for columns in a parquet row group) is evaluated against query filters to determine which row groups could contain data requested by a query. In this way, by pushing down query filters all the way to the parquet data source, entire row groups or even parquet files can be skipped often resulting in significant performance improvements.\r\n   \r\n   I have been working on an implementation for a few weeks and initial results look promising - with predicate push-down, DataFusion is now faster than Apache Spark (`140ms for DataFusion vs 200ms for Spark`) for the same query against the same parquet files.\r\n   \r\n   My work is based on the following key ideas:\r\n   * predicate-push down is implemented by filtering row group metadata entries to only those which could contain data which could satisfy query filters\r\n   * it's best to reuse the existing code for evaluating physical expressions already implemented in DataFusion\r\n   * filter expressions pushed down to a parquet table are rewritten to use parquet statistics (instead of the actual column data), for example `(column / 2) = 4`  becomes  `(column_min / 2) <= 4 && 4 <= (column_max / 2)` - this is done once for all files in a parquet table\r\n   * for each parquet file, a RecordBatch containing all required statistics columns ( [`column_min`, `column_max`] in the example above) is produced, and the predicate expression from the previous step is evaluated, producing a binary array which is finally used to filter the row groups in each parquet file\r\n   \r\n   This is still work in progress - more tests left to write; I am publishing this now to gather feedback.\r\n   \r\n   @andygrove let me know what you think\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-12-31T21:54:34.633+0000",
                    "updated": "2020-12-31T21:54:34.633+0000",
                    "started": "2020-12-31T21:54:34.632+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "529896",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/529899",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#issuecomment-753214950\n\n\n   https://issues.apache.org/jira/browse/ARROW-11074\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2020-12-31T22:01:28.970+0000",
                    "updated": "2020-12-31T22:01:28.970+0000",
                    "started": "2020-12-31T22:01:28.970+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "529899",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530392",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io commented on pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#issuecomment-753605727\n\n\n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=h1) Report\n   > Merging [#9064](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=desc) (5b2b6f2) into [master](https://codecov.io/gh/apache/arrow/commit/dfef236f7587e4168ac1e07bd09e42d9373beb70?el=desc) (dfef236) will **decrease** coverage by `0.35%`.\n   > The diff coverage is `22.74%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/9064/graphs/tree.svg?width=650&height=150&src=pr&token=LpTCFbqVT1)](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #9064      +/-   ##\n   ==========================================\n   - Coverage   82.60%   82.25%   -0.36%     \n   ==========================================\n     Files         204      204              \n     Lines       50189    50478     +289     \n   ==========================================\n   + Hits        41459    41520      +61     \n   - Misses       8730     8958     +228     \n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [rust/parquet/src/file/serialized\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9maWxlL3NlcmlhbGl6ZWRfcmVhZGVyLnJz) | `93.42% <0.00%> (-2.19%)` | :arrow_down: |\n   | [rust/datafusion/src/physical\\_plan/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9waHlzaWNhbF9wbGFuL3BhcnF1ZXQucnM=) | `45.73% <22.30%> (-39.15%)` | :arrow_down: |\n   | [rust/datafusion/src/datasource/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9kYXRhc291cmNlL3BhcnF1ZXQucnM=) | `91.54% <36.36%> (-4.67%)` | :arrow_down: |\n   | [rust/parquet/src/arrow/array\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9hcnJvdy9hcnJheV9yZWFkZXIucnM=) | `75.37% <100.00%> (+0.02%)` | :arrow_up: |\n   | [rust/arrow/src/array/array\\_primitive.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvYXJyYXlfcHJpbWl0aXZlLnJz) | `92.82% <0.00%> (+0.49%)` | :arrow_up: |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=footer). Last update [dfef236...5b2b6f2](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T11:53:09.300+0000",
                    "updated": "2021-01-03T11:53:09.300+0000",
                    "started": "2021-01-03T11:53:09.300+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530392",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530397",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "Dandandan commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r550998137\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n\nReview comment:\n       I think we should add a `Copy` to the `Operator` enum so we can dereference `op`.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T12:17:01.404+0000",
                    "updated": "2021-01-03T12:17:01.404+0000",
                    "started": "2021-01-03T12:17:01.404+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530397",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530399",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "Dandandan commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r550998581\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // (input < input) => (predicate)\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // (input <= input) => (predicate)\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n\nReview comment:\n       This can use `Copy` too so functions can use `statistics_type: StatisticsType` \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T12:21:31.135+0000",
                    "updated": "2021-01-03T12:21:31.135+0000",
                    "started": "2021-01-03T12:21:31.135+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530399",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530410",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551006563\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n\nReview comment:\n       nit: probably this doesn't need to be a `pub` struct given that it seems to be tied to the parquet scan implementation\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n\nReview comment:\n       these comments are quite helpful. Thank you\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // (input < input) => (predicate)\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // (input <= input) => (predicate)\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n\nReview comment:\n       I wonder if you could use `NullArray` here instead: https://github.com/apache/arrow/blob//rust/arrow/src/array/null.rs\n\n##########\nFile path: rust/parquet/src/file/serialized_reader.rs\n##########\n@@ -137,6 +137,22 @@ impl<R: 'static + ChunkReader> SerializedFileReader<R> {\n             metadata,\n         })\n     }\n+\n+    pub fn filter_row_groups(\n\nReview comment:\n       this is a fancy way to filter out the row groups -- it is probably worth adding documentation here.\r\n   \r\n   I don't know if there are assumptions in the parquet reader code that the row group metadata matches what was read from the file or not\r\n   \r\n   I suggest you consider filtering the row groups at the DataFusion (aka skip them in the datafusion physical operator) level rather than in the parquet reader level and avoid that potential problem completely. \n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n\nReview comment:\n       I suggest copying the (nicely written) summary of your algorithm from this PR' description somewhere into this file\r\n   \r\n   It is probably good to mention the assumptions of this predicate expression -- which I think is that it will return `true` if a rowgroup *may* contain rows that match the predicate, and will return `false` if and only if all rows in the row group can *not* match the predicate.\r\n   \r\n   The idea of creating arrays of `(col1_min, col1_max, col2_min, col2_max ...)` is clever (and could likely be applied to sources other than parquet files). \n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n\nReview comment:\n       stylistically, you might be able to hoist out the repeated calls to \r\n   \r\n   ```\r\n   rewrite_column_expr(\r\n                       expr_builder.column_expr(),\r\n                       expr_builder.column_name(),\r\n                       max_column_name.as_str()\r\n   ```\r\n   \r\n   and \r\n   \r\n   ```\r\n   rewrite_column_expr(\r\n                       expr_builder.column_expr(),\r\n                       expr_builder.column_name(),\r\n                       min_column_name.as_str()\r\n   ```\r\n   \r\n    by evaluating them once before the `match` expression:\r\n   \r\n   ```\r\n   let min_col_expr = rewrite_column_expr(\r\n                       expr_builder.column_expr(),\r\n                       expr_builder.column_name(),\r\n                       min_column_name.as_str());\r\n   \r\n   let max_col_expr = rewrite_column_expr(\r\n                       expr_builder.column_expr(),\r\n                       expr_builder.column_name(),\r\n                       max_column_name.as_str())\r\n   ```\r\n   \r\n   But they way you have it works well too\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T13:52:58.373+0000",
                    "updated": "2021-01-03T13:52:58.373+0000",
                    "started": "2021-01-03T13:52:58.373+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530410",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530442",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551040214\n\n\n\n##########\nFile path: rust/datafusion/src/datasource/parquet.rs\n##########\n@@ -62,17 +64,37 @@ impl TableProvider for ParquetTable {\n         self.schema.clone()\n     }\n \n+    fn supports_filter_pushdown(\n+        &self,\n+        _filter: &Expr,\n+    ) -> Result<TableProviderFilterPushDown> {\n+        Ok(TableProviderFilterPushDown::Inexact)\n+    }\n+\n     /// Scan the file(s), using the provided projection, and return one BatchIterator per\n     /// partition.\n     fn scan(\n         &self,\n         projection: &Option<Vec<usize>>,\n         batch_size: usize,\n-        _filters: &[Expr],\n+        filters: &[Expr],\n     ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let predicate = if filters.is_empty() {\n+            None\n+        } else {\n+            Some(\n+                filters\n\nReview comment:\n       Could you add a comment explaining the logic here? It isn't immediately obvious to me.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T18:40:12.211+0000",
                    "updated": "2021-01-03T18:40:12.211+0000",
                    "started": "2021-01-03T18:40:12.211+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530442",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530443",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "andygrove commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551040361\n\n\n\n##########\nFile path: rust/datafusion/src/datasource/parquet.rs\n##########\n@@ -62,17 +64,37 @@ impl TableProvider for ParquetTable {\n         self.schema.clone()\n     }\n \n+    fn supports_filter_pushdown(\n+        &self,\n+        _filter: &Expr,\n+    ) -> Result<TableProviderFilterPushDown> {\n+        Ok(TableProviderFilterPushDown::Inexact)\n+    }\n+\n     /// Scan the file(s), using the provided projection, and return one BatchIterator per\n     /// partition.\n     fn scan(\n         &self,\n         projection: &Option<Vec<usize>>,\n         batch_size: usize,\n-        _filters: &[Expr],\n+        filters: &[Expr],\n     ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let predicate = if filters.is_empty() {\n+            None\n+        } else {\n+            Some(\n+                filters\n\nReview comment:\n       Immediately after posting that comment I see how it works now, but I think a comment would still be helpful\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T18:41:55.488+0000",
                    "updated": "2021-01-03T18:41:55.488+0000",
                    "started": "2021-01-03T18:41:55.488+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530443",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530479",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io edited a comment on pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#issuecomment-753605727\n\n\n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=h1) Report\n   > Merging [#9064](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=desc) (a03ccf9) into [master](https://codecov.io/gh/apache/arrow/commit/dfef236f7587e4168ac1e07bd09e42d9373beb70?el=desc) (dfef236) will **decrease** coverage by `0.34%`.\n   > The diff coverage is `26.19%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/9064/graphs/tree.svg?width=650&height=150&src=pr&token=LpTCFbqVT1)](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #9064      +/-   ##\n   ==========================================\n   - Coverage   82.60%   82.25%   -0.35%     \n   ==========================================\n     Files         204      204              \n     Lines       50189    50478     +289     \n   ==========================================\n   + Hits        41459    41522      +63     \n   - Misses       8730     8956     +226     \n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [rust/parquet/src/file/serialized\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9maWxlL3NlcmlhbGl6ZWRfcmVhZGVyLnJz) | `93.42% <0.00%> (-2.19%)` | :arrow_down: |\n   | [rust/datafusion/src/physical\\_plan/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9waHlzaWNhbF9wbGFuL3BhcnF1ZXQucnM=) | `45.73% <22.30%> (-39.15%)` | :arrow_down: |\n   | [rust/datafusion/src/datasource/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9kYXRhc291cmNlL3BhcnF1ZXQucnM=) | `91.54% <36.36%> (-4.67%)` | :arrow_down: |\n   | [rust/parquet/src/arrow/array\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9hcnJvdy9hcnJheV9yZWFkZXIucnM=) | `75.71% <100.00%> (+0.36%)` | :arrow_up: |\n   | [rust/arrow/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvcmVjb3JkX2JhdGNoLnJz) | `73.83% <0.00%> (-2.24%)` | :arrow_down: |\n   | [rust/arrow/src/array/array\\_struct.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvYXJyYXlfc3RydWN0LnJz) | `88.43% <0.00%> (-0.18%)` | :arrow_down: |\n   | [rust/arrow/src/array/equal/mod.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvZXF1YWwvbW9kLnJz) | `92.69% <0.00%> (+0.37%)` | :arrow_up: |\n   | [rust/arrow/src/array/array\\_primitive.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvYXJyYXlfcHJpbWl0aXZlLnJz) | `92.82% <0.00%> (+0.49%)` | :arrow_up: |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=footer). Last update [dfef236...a03ccf9](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:22:59.072+0000",
                    "updated": "2021-01-03T22:22:59.072+0000",
                    "started": "2021-01-03T22:22:59.071+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530479",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530480",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551062412\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n\nReview comment:\n       this particular comment should be `// column = literal => (min, max) = literal => min <= literal && literal <= max` :), but yes, it does require some thinking so I thought it would be good to add these comments to help with the process\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:27:42.520+0000",
                    "updated": "2021-01-03T22:27:42.520+0000",
                    "started": "2021-01-03T22:27:42.520+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530480",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530481",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551062848\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // (input < input) => (predicate)\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // (input <= input) => (predicate)\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n\nReview comment:\n       you are right - I have implemented the `Copy` trait for `StatisticsType` - this makes the code a bit cleaner as it's not necessary to use references all the time\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:33:16.099+0000",
                    "updated": "2021-01-03T22:33:16.099+0000",
                    "started": "2021-01-03T22:33:16.099+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530481",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530482",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551063325\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n\nReview comment:\n       I don't feel strongly about this either way, but at the moment it has to be public because it is used as a parameter in `pub ParquetExec::new`\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:37:19.326+0000",
                    "updated": "2021-01-03T22:37:19.326+0000",
                    "started": "2021-01-03T22:37:19.325+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530482",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530484",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551063977\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n\nReview comment:\n       done\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:45:05.998+0000",
                    "updated": "2021-01-03T22:45:05.998+0000",
                    "started": "2021-01-03T22:45:05.998+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530484",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530486",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551064489\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // (input < input) => (predicate)\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // (input <= input) => (predicate)\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n\nReview comment:\n       that's what I thought at first, and then realized that `NullArray` returns data type of `DataType::Null`, which doesn't work when the statistics record batch is created as it checks that types from the schema fields and from arrays are the same; that's why I wrote the `build_null_array` function\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:49:32.185+0000",
                    "updated": "2021-01-03T22:49:32.185+0000",
                    "started": "2021-01-03T22:49:32.185+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530486",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530487",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "yordan-pavlov commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551064584\n\n\n\n##########\nFile path: rust/datafusion/src/datasource/parquet.rs\n##########\n@@ -62,17 +64,37 @@ impl TableProvider for ParquetTable {\n         self.schema.clone()\n     }\n \n+    fn supports_filter_pushdown(\n+        &self,\n+        _filter: &Expr,\n+    ) -> Result<TableProviderFilterPushDown> {\n+        Ok(TableProviderFilterPushDown::Inexact)\n+    }\n+\n     /// Scan the file(s), using the provided projection, and return one BatchIterator per\n     /// partition.\n     fn scan(\n         &self,\n         projection: &Option<Vec<usize>>,\n         batch_size: usize,\n-        _filters: &[Expr],\n+        filters: &[Expr],\n     ) -> Result<Arc<dyn ExecutionPlan>> {\n+        let predicate = if filters.is_empty() {\n+            None\n+        } else {\n+            Some(\n+                filters\n\nReview comment:\n       \ud83d\udc4d \n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T22:50:45.424+0000",
                    "updated": "2021-01-03T22:50:45.424+0000",
                    "started": "2021-01-03T22:50:45.423+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530487",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530489",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "codecov-io edited a comment on pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#issuecomment-753605727\n\n\n   # [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=h1) Report\n   > Merging [#9064](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=desc) (5db2c6f) into [master](https://codecov.io/gh/apache/arrow/commit/dfef236f7587e4168ac1e07bd09e42d9373beb70?el=desc) (dfef236) will **decrease** coverage by `0.35%`.\n   > The diff coverage is `26.11%`.\n   \n   [![Impacted file tree graph](https://codecov.io/gh/apache/arrow/pull/9064/graphs/tree.svg?width=650&height=150&src=pr&token=LpTCFbqVT1)](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree)\n   \n   ```diff\n   @@            Coverage Diff             @@\n   ##           master    #9064      +/-   ##\n   ==========================================\n   - Coverage   82.60%   82.25%   -0.36%     \n   ==========================================\n     Files         204      204              \n     Lines       50189    50479     +290     \n   ==========================================\n   + Hits        41459    41521      +62     \n   - Misses       8730     8958     +228     \n   ```\n   \n   \n   | [Impacted Files](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=tree) | Coverage \u0394 | |\n   |---|---|---|\n   | [rust/datafusion/src/logical\\_plan/operators.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9sb2dpY2FsX3BsYW4vb3BlcmF0b3JzLnJz) | `75.00% <\u00f8> (\u00f8)` | |\n   | [rust/parquet/src/file/serialized\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9maWxlL3NlcmlhbGl6ZWRfcmVhZGVyLnJz) | `93.42% <0.00%> (-2.19%)` | :arrow_down: |\n   | [rust/datafusion/src/physical\\_plan/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9waHlzaWNhbF9wbGFuL3BhcnF1ZXQucnM=) | `45.63% <22.22%> (-39.25%)` | :arrow_down: |\n   | [rust/datafusion/src/datasource/parquet.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9kYXRhZnVzaW9uL3NyYy9kYXRhc291cmNlL3BhcnF1ZXQucnM=) | `91.54% <36.36%> (-4.67%)` | :arrow_down: |\n   | [rust/parquet/src/arrow/array\\_reader.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9hcnJvdy9hcnJheV9yZWFkZXIucnM=) | `75.71% <100.00%> (+0.36%)` | :arrow_up: |\n   | [rust/arrow/src/record\\_batch.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvcmVjb3JkX2JhdGNoLnJz) | `73.83% <0.00%> (-2.24%)` | :arrow_down: |\n   | [rust/parquet/src/encodings/encoding.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9wYXJxdWV0L3NyYy9lbmNvZGluZ3MvZW5jb2RpbmcucnM=) | `95.24% <0.00%> (-0.20%)` | :arrow_down: |\n   | [rust/arrow/src/array/array\\_struct.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvYXJyYXlfc3RydWN0LnJz) | `88.43% <0.00%> (-0.18%)` | :arrow_down: |\n   | [rust/arrow/src/array/equal/mod.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvZXF1YWwvbW9kLnJz) | `92.69% <0.00%> (+0.37%)` | :arrow_up: |\n   | [rust/arrow/src/array/array\\_primitive.rs](https://codecov.io/gh/apache/arrow/pull/9064/diff?src=pr&el=tree#diff-cnVzdC9hcnJvdy9zcmMvYXJyYXkvYXJyYXlfcHJpbWl0aXZlLnJz) | `92.82% <0.00%> (+0.49%)` | :arrow_up: |\n   \n   ------\n   \n   [Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=continue).\n   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)\n   > `\u0394 = absolute <relative> (impact)`, `\u00f8 = not affected`, `? = missing data`\n   > Powered by [Codecov](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=footer). Last update [dfef236...5db2c6f](https://codecov.io/gh/apache/arrow/pull/9064?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).\n   \n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-03T23:05:52.674+0000",
                    "updated": "2021-01-03T23:05:52.674+0000",
                    "started": "2021-01-03T23:05:52.673+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530489",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530796",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "jorgecarleitao commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551156780\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,477 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n\nReview comment:\n       I would error or `panic!` here or before that, or validate that the predicate is a boolean array.\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,477 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                *statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op;\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op,\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == &statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, *op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == Operator::And || op == Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => (min, max) = literal => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Copy, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n+    let mut null_bitmap_builder = BooleanBufferBuilder::new(length);\n+    null_bitmap_builder.append_n(length, false);\n+    let array_data = ArrayData::builder(data_type.clone())\n+        .len(length)\n+        .null_bit_buffer(null_bitmap_builder.finish())\n+        .build();\n+    make_array(array_data)\n+}\n+\n+fn build_statistics_array(\n\nReview comment:\n       I would have split this in N functions, one per array type (via generics), and write `build_statistics_array` as simply `match data_type { each implementation }`.\r\n   \r\n   This would follow the convention in other places and reduces the risk of mistakes, particularly in matching datatypes.\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,477 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                *statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op;\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op,\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == &statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, *op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == Operator::And || op == Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => (min, max) = literal => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Copy, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n+    let mut null_bitmap_builder = BooleanBufferBuilder::new(length);\n+    null_bitmap_builder.append_n(length, false);\n+    let array_data = ArrayData::builder(data_type.clone())\n+        .len(length)\n+        .null_bit_buffer(null_bitmap_builder.finish())\n+        .build();\n+    make_array(array_data)\n+}\n+\n+fn build_statistics_array(\n+    statistics: &[Option<&ParquetStatistics>],\n+    statistics_type: StatisticsType,\n+    data_type: &DataType,\n+) -> ArrayRef {\n+    let statistics_count = statistics.len();\n+    // this should be handled by the condition below\n+    // if statistics_count < 1 {\n+    //     return build_null_array(data_type, 0);\n+    // }\n+\n+    let first_group_stats = statistics.iter().find(|s| s.is_some());\n+    let first_group_stats = if let Some(Some(statistics)) = first_group_stats {\n+        // found first row group with statistics defined\n+        statistics\n+    } else {\n+        // no row group has statistics defined\n+        return build_null_array(data_type, statistics_count);\n+    };\n+\n+    let (data_size, arrow_type) = match first_group_stats {\n+        ParquetStatistics::Int32(_) => (std::mem::size_of::<i32>(), DataType::Int32),\n+        ParquetStatistics::Int64(_) => (std::mem::size_of::<i64>(), DataType::Int64),\n+        ParquetStatistics::Float(_) => (std::mem::size_of::<f32>(), DataType::Float32),\n+        ParquetStatistics::Double(_) => (std::mem::size_of::<f64>(), DataType::Float64),\n+        ParquetStatistics::ByteArray(_) if data_type == &DataType::Utf8 => {\n+            (0, DataType::Utf8)\n+        }\n+        _ => {\n+            // type of statistics not supported\n+            return build_null_array(data_type, statistics_count);\n+        }\n+    };\n+\n+    let statistics = statistics.iter().map(|s| {\n+        s.filter(|s| s.has_min_max_set())\n+            .map(|s| match statistics_type {\n+                StatisticsType::Min => s.min_bytes(),\n+                StatisticsType::Max => s.max_bytes(),\n+            })\n+    });\n+\n+    if arrow_type == DataType::Utf8 {\n+        let data_size = statistics\n+            .clone()\n+            .map(|x| x.map(|b| b.len()).unwrap_or(0))\n+            .sum();\n+        let mut builder =\n+            arrow::array::StringBuilder::with_capacity(statistics_count, data_size);\n+        let string_statistics =\n+            statistics.map(|x| x.and_then(|bytes| std::str::from_utf8(bytes).ok()));\n+        for maybe_string in string_statistics {\n+            match maybe_string {\n+                Some(string_value) => builder.append_value(string_value).unwrap(),\n+                None => builder.append_null().unwrap(),\n+            };\n+        }\n+        return Arc::new(builder.finish());\n+    }\n+\n+    let mut data_buffer = MutableBuffer::new(statistics_count * data_size);\n+    let mut bitmap_builder = BooleanBufferBuilder::new(statistics_count);\n+    let mut null_count = 0;\n+    for s in statistics {\n+        if let Some(stat_data) = s {\n+            bitmap_builder.append(true);\n+            data_buffer.extend_from_slice(stat_data);\n+        } else {\n+            bitmap_builder.append(false);\n+            data_buffer.resize(data_buffer.len() + data_size);\n+            null_count += 1;\n+        }\n+    }\n+\n+    let mut builder = ArrayData::builder(arrow_type)\n+        .len(statistics_count)\n+        .add_buffer(data_buffer.into());\n+    if null_count > 0 {\n+        builder = builder.null_bit_buffer(bitmap_builder.finish());\n+    }\n+    let array_data = builder.build();\n+    let statistics_array = make_array(array_data);\n+    if statistics_array.data_type() == data_type {\n+        return statistics_array;\n+    }\n\nReview comment:\n       This is only valid for primitive types. In general, I would recommend using `PrimitiveArray<T>::from_iter`, `BooleanArray::from_iter` and `StringArray::from_iter`.  Using `MutableBuffer` in this high level is prone to errors. E.g. if we add a filter for boolean types (e.g. eq and neq), this does not panic but the array is not valid (as the size is measured in bits, not bytes).\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,477 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                *statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op;\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op,\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == &statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, *op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == Operator::And || op == Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => (min, max) = literal => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Copy, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n+    let mut null_bitmap_builder = BooleanBufferBuilder::new(length);\n+    null_bitmap_builder.append_n(length, false);\n+    let array_data = ArrayData::builder(data_type.clone())\n+        .len(length)\n+        .null_bit_buffer(null_bitmap_builder.finish())\n+        .build();\n+    make_array(array_data)\n+}\n+\n+fn build_statistics_array(\n+    statistics: &[Option<&ParquetStatistics>],\n+    statistics_type: StatisticsType,\n+    data_type: &DataType,\n+) -> ArrayRef {\n+    let statistics_count = statistics.len();\n+    // this should be handled by the condition below\n+    // if statistics_count < 1 {\n+    //     return build_null_array(data_type, 0);\n+    // }\n+\n+    let first_group_stats = statistics.iter().find(|s| s.is_some());\n+    let first_group_stats = if let Some(Some(statistics)) = first_group_stats {\n+        // found first row group with statistics defined\n+        statistics\n+    } else {\n+        // no row group has statistics defined\n+        return build_null_array(data_type, statistics_count);\n+    };\n+\n+    let (data_size, arrow_type) = match first_group_stats {\n+        ParquetStatistics::Int32(_) => (std::mem::size_of::<i32>(), DataType::Int32),\n+        ParquetStatistics::Int64(_) => (std::mem::size_of::<i64>(), DataType::Int64),\n+        ParquetStatistics::Float(_) => (std::mem::size_of::<f32>(), DataType::Float32),\n+        ParquetStatistics::Double(_) => (std::mem::size_of::<f64>(), DataType::Float64),\n+        ParquetStatistics::ByteArray(_) if data_type == &DataType::Utf8 => {\n+            (0, DataType::Utf8)\n+        }\n+        _ => {\n+            // type of statistics not supported\n+            return build_null_array(data_type, statistics_count);\n+        }\n+    };\n+\n+    let statistics = statistics.iter().map(|s| {\n+        s.filter(|s| s.has_min_max_set())\n+            .map(|s| match statistics_type {\n+                StatisticsType::Min => s.min_bytes(),\n+                StatisticsType::Max => s.max_bytes(),\n+            })\n+    });\n+\n+    if arrow_type == DataType::Utf8 {\n\nReview comment:\n       I would use a match, as this is a bit brittle against matching specific datatypes.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-04T18:09:57.600+0000",
                    "updated": "2021-01-04T18:09:57.600+0000",
                    "started": "2021-01-04T18:09:57.600+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530796",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530832",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551525125\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n+    parquet_schema: Schema,\n+    predicate_expr: Arc<dyn PhysicalExpr>,\n+    stat_column_req: Vec<(String, StatisticsType, Field)>,\n+}\n+\n+impl PredicateExpressionBuilder {\n+    /// Try to create a new instance of PredicateExpressionBuilder\n+    pub fn try_new(expr: &Expr, parquet_schema: Schema) -> Result<Self> {\n+        // build predicate expression once\n+        let mut stat_column_req = Vec::<(String, StatisticsType, Field)>::new();\n+        let predicate_expr =\n+            build_predicate_expression(expr, &parquet_schema, &mut stat_column_req)?;\n+\n+        Ok(Self {\n+            parquet_schema,\n+            predicate_expr,\n+            stat_column_req,\n+        })\n+    }\n+\n+    /// Generate a predicate function used to filter row group metadata\n+    pub fn build_row_group_predicate(\n+        &self,\n+        row_group_metadata: &[RowGroupMetaData],\n+    ) -> Box<dyn Fn(&RowGroupMetaData, usize) -> bool> {\n+        // build statistics record batch\n+        let predicate_result = build_row_group_record_batch(\n+            row_group_metadata,\n+            &self.parquet_schema,\n+            &self.stat_column_req,\n+        )\n+        .and_then(|statistics_batch| {\n+            // execute predicate expression\n+            self.predicate_expr.evaluate(&statistics_batch)\n+        })\n+        .and_then(|v| match v {\n+            ColumnarValue::Array(array) => Ok(array),\n+            ColumnarValue::Scalar(_) => Err(DataFusionError::Plan(\n+                \"predicate expression didn't return an array\".to_string(),\n+            )),\n+        });\n+\n+        let predicate_array = match predicate_result {\n+            Ok(array) => array,\n+            _ => return Box::new(|_r, _i| true),\n+        };\n+\n+        let predicate_array = predicate_array.as_any().downcast_ref::<BooleanArray>();\n+        match predicate_array {\n+            // return row group predicate function\n+            Some(array) => {\n+                let predicate_values =\n+                    array.iter().map(|x| x.unwrap_or(false)).collect::<Vec<_>>();\n+                Box::new(move |_, i| predicate_values[i])\n+            }\n+            // predicate result is not a BooleanArray\n+            _ => Box::new(|_r, _i| true),\n+        }\n+    }\n+}\n+\n+fn build_row_group_record_batch(\n+    row_groups: &[RowGroupMetaData],\n+    parquet_schema: &Schema,\n+    stat_column_req: &Vec<(String, StatisticsType, Field)>,\n+) -> Result<RecordBatch> {\n+    let mut fields = Vec::<Field>::new();\n+    let mut arrays = Vec::<ArrayRef>::new();\n+    for (column_name, statistics_type, stat_field) in stat_column_req {\n+        if let Some((column_index, _)) = parquet_schema.column_with_name(column_name) {\n+            let statistics = row_groups\n+                .iter()\n+                .map(|g| g.column(column_index).statistics())\n+                .collect::<Vec<_>>();\n+            let array = build_statistics_array(\n+                &statistics,\n+                statistics_type,\n+                stat_field.data_type(),\n+            );\n+            fields.push(stat_field.clone());\n+            arrays.push(array);\n+        }\n+    }\n+    let schema = Arc::new(Schema::new(fields));\n+    RecordBatch::try_new(schema, arrays)\n+        .map_err(|err| DataFusionError::Plan(err.to_string()))\n+}\n+\n+struct PhysicalExpressionBuilder<'a> {\n+    column_name: String,\n+    column_expr: &'a Expr,\n+    scalar_expr: &'a Expr,\n+    parquet_field: &'a Field,\n+    statistics_fields: Vec<Field>,\n+    stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    reverse_operator: bool,\n+}\n+\n+impl<'a> PhysicalExpressionBuilder<'a> {\n+    fn try_new(\n+        left: &'a Expr,\n+        right: &'a Expr,\n+        parquet_schema: &'a Schema,\n+        stat_column_req: &'a mut Vec<(String, StatisticsType, Field)>,\n+    ) -> Result<Self> {\n+        // find column name; input could be a more complicated expression\n+        let mut left_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(left, &mut left_columns)?;\n+        let mut right_columns = HashSet::<String>::new();\n+        utils::expr_to_column_names(right, &mut right_columns)?;\n+        let (column_expr, scalar_expr, column_names, reverse_operator) =\n+            match (left_columns.len(), right_columns.len()) {\n+                (1, 0) => (left, right, left_columns, false),\n+                (0, 1) => (right, left, right_columns, true),\n+                _ => {\n+                    // if more than one column used in expression - not supported\n+                    return Err(DataFusionError::Plan(\n+                        \"Multi-column expressions are not currently supported\"\n+                            .to_string(),\n+                    ));\n+                }\n+            };\n+        let column_name = column_names.iter().next().unwrap().clone();\n+        let field = match parquet_schema.column_with_name(&column_name) {\n+            Some((_, f)) => f,\n+            _ => {\n+                // field not found in parquet schema\n+                return Err(DataFusionError::Plan(\n+                    \"Field not found in parquet schema\".to_string(),\n+                ));\n+            }\n+        };\n+\n+        Ok(Self {\n+            column_name,\n+            column_expr,\n+            scalar_expr,\n+            parquet_field: field,\n+            statistics_fields: Vec::new(),\n+            stat_column_req,\n+            reverse_operator,\n+        })\n+    }\n+\n+    fn correct_operator(&self, op: &Operator) -> Operator {\n+        if !self.reverse_operator {\n+            return op.clone();\n+        }\n+\n+        match op {\n+            Operator::Lt => Operator::Gt,\n+            Operator::Gt => Operator::Lt,\n+            Operator::LtEq => Operator::GtEq,\n+            Operator::GtEq => Operator::LtEq,\n+            _ => op.clone(),\n+        }\n+    }\n+\n+    fn column_expr(&self) -> &Expr {\n+        self.column_expr\n+    }\n+\n+    fn scalar_expr(&self) -> &Expr {\n+        self.scalar_expr\n+    }\n+\n+    fn column_name(&self) -> &String {\n+        &self.column_name\n+    }\n+\n+    fn is_stat_column_missing(&self, statistics_type: &StatisticsType) -> bool {\n+        self.stat_column_req\n+            .iter()\n+            .filter(|(c, t, _f)| c == &self.column_name && t == statistics_type)\n+            .count()\n+            == 0\n+    }\n+\n+    fn add_min_column(&mut self) -> String {\n+        let min_field = Field::new(\n+            format!(\"{}_min\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let min_column_name = min_field.name().clone();\n+        self.statistics_fields.push(min_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Min) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Min,\n+                min_field,\n+            ));\n+        }\n+        min_column_name\n+    }\n+\n+    fn add_max_column(&mut self) -> String {\n+        let max_field = Field::new(\n+            format!(\"{}_max\", self.column_name).as_str(),\n+            self.parquet_field.data_type().clone(),\n+            self.parquet_field.is_nullable(),\n+        );\n+        let max_column_name = max_field.name().clone();\n+        self.statistics_fields.push(max_field.clone());\n+        if self.is_stat_column_missing(&StatisticsType::Max) {\n+            // only add statistics column if not previously added\n+            self.stat_column_req.push((\n+                self.column_name.to_string(),\n+                StatisticsType::Max,\n+                max_field,\n+            ));\n+        }\n+        max_column_name\n+    }\n+\n+    fn build(&self, statistics_expr: &Expr) -> Result<Arc<dyn PhysicalExpr>> {\n+        let execution_context_state = ExecutionContextState {\n+            datasources: HashMap::new(),\n+            scalar_functions: HashMap::new(),\n+            var_provider: HashMap::new(),\n+            aggregate_functions: HashMap::new(),\n+            config: ExecutionConfig::new(),\n+        };\n+        let schema = Schema::new(self.statistics_fields.clone());\n+        DefaultPhysicalPlanner::default().create_physical_expr(\n+            statistics_expr,\n+            &schema,\n+            &execution_context_state,\n+        )\n+    }\n+}\n+\n+/// Translate logical filter expression into parquet statistics physical filter expression\n+fn build_predicate_expression(\n+    expr: &Expr,\n+    parquet_schema: &Schema,\n+    stat_column_req: &mut Vec<(String, StatisticsType, Field)>,\n+) -> Result<Arc<dyn PhysicalExpr>> {\n+    // predicate expression can only be a binary expression\n+    let (left, op, right) = match expr {\n+        Expr::BinaryExpr { left, op, right } => (left, op, right),\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+\n+    if op == &Operator::And || op == &Operator::Or {\n+        let left_expr =\n+            build_predicate_expression(left, parquet_schema, stat_column_req)?;\n+        let right_expr =\n+            build_predicate_expression(right, parquet_schema, stat_column_req)?;\n+        let stat_fields = stat_column_req\n+            .iter()\n+            .map(|(_, _, f)| f.clone())\n+            .collect::<Vec<_>>();\n+        let stat_schema = Schema::new(stat_fields);\n+        let result = expressions::binary(left_expr, op.clone(), right_expr, &stat_schema);\n+        return result;\n+    }\n+\n+    let mut expr_builder = match PhysicalExpressionBuilder::try_new(\n+        left,\n+        right,\n+        parquet_schema,\n+        stat_column_req,\n+    ) {\n+        Ok(builder) => builder,\n+        // allow partial failure in predicate expression generation\n+        // this can still produce a useful predicate when multiple conditions are joined using AND\n+        Err(_) => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    let corrected_op = expr_builder.correct_operator(op);\n+    let statistics_expr = match corrected_op {\n+        Operator::Eq => {\n+            let min_column_name = expr_builder.add_min_column();\n+            let max_column_name = expr_builder.add_max_column();\n+            // column = literal => column = (min, max) => min <= literal && literal <= max\n+            // (column / 2) = 4 => (column_min / 2) <= 4 && 4 <= (column_max / 2)\n+            expr_builder\n+                .scalar_expr()\n+                .gt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    min_column_name.as_str(),\n+                )?)\n+                .and(expr_builder.scalar_expr().lt_eq(rewrite_column_expr(\n+                    expr_builder.column_expr(),\n+                    expr_builder.column_name(),\n+                    max_column_name.as_str(),\n+                )?))\n+        }\n+        Operator::Gt => {\n+            let max_column_name = expr_builder.add_max_column();\n+            // column > literal => (min, max) > literal => max > literal\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::GtEq => {\n+            // column >= literal => (min, max) >= literal => max >= literal\n+            let max_column_name = expr_builder.add_max_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                max_column_name.as_str(),\n+            )?\n+            .gt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::Lt => {\n+            // (input < input) => (predicate)\n+            // column < literal => (min, max) < literal => min < literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt(expr_builder.scalar_expr().clone())\n+        }\n+        Operator::LtEq => {\n+            // (input <= input) => (predicate)\n+            // column <= literal => (min, max) <= literal => min <= literal\n+            let min_column_name = expr_builder.add_min_column();\n+            rewrite_column_expr(\n+                expr_builder.column_expr(),\n+                expr_builder.column_name(),\n+                min_column_name.as_str(),\n+            )?\n+            .lt_eq(expr_builder.scalar_expr().clone())\n+        }\n+        // other expressions are not supported\n+        _ => {\n+            return Ok(expressions::lit(ScalarValue::Boolean(Some(true))));\n+        }\n+    };\n+    expr_builder.build(&statistics_expr)\n+}\n+\n+/// replaces a column with an old name with a new name in an expression\n+fn rewrite_column_expr(\n+    expr: &Expr,\n+    column_old_name: &str,\n+    column_new_name: &str,\n+) -> Result<Expr> {\n+    let expressions = utils::expr_sub_expressions(&expr)?;\n+    let expressions = expressions\n+        .iter()\n+        .map(|e| rewrite_column_expr(e, column_old_name, column_new_name))\n+        .collect::<Result<Vec<_>>>()?;\n+\n+    if let Expr::Column(name) = expr {\n+        if name == column_old_name {\n+            return Ok(Expr::Column(column_new_name.to_string()));\n+        }\n+    }\n+    utils::rewrite_expression(&expr, &expressions)\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n+enum StatisticsType {\n+    Min,\n+    Max,\n+}\n+\n+fn build_null_array(data_type: &DataType, length: usize) -> ArrayRef {\n\nReview comment:\n       makes sense\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-04T19:37:35.218+0000",
                    "updated": "2021-01-04T19:37:35.218+0000",
                    "started": "2021-01-04T19:37:35.218+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530832",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530833",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551525987\n\n\n\n##########\nFile path: rust/datafusion/src/physical_plan/parquet.rs\n##########\n@@ -209,6 +251,479 @@ impl ParquetExec {\n     }\n }\n \n+#[derive(Debug, Clone)]\n+/// Predicate builder used for generating of predicate functions, used to filter row group metadata\n+pub struct PredicateExpressionBuilder {\n\nReview comment:\n       I don't feel strongly either way either -- no need to change it\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-04T19:39:20.210+0000",
                    "updated": "2021-01-04T19:39:20.210+0000",
                    "started": "2021-01-04T19:39:20.210+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530833",
                    "issueId": "13348468"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/worklog/530884",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "sunchao commented on a change in pull request #9064:\nURL: https://github.com/apache/arrow/pull/9064#discussion_r551558307\n\n\n\n##########\nFile path: rust/datafusion/src/datasource/parquet.rs\n##########\n@@ -62,17 +64,37 @@ impl TableProvider for ParquetTable {\n         self.schema.clone()\n     }\n \n+    fn supports_filter_pushdown(\n+        &self,\n+        _filter: &Expr,\n+    ) -> Result<TableProviderFilterPushDown> {\n+        Ok(TableProviderFilterPushDown::Inexact)\n+    }\n+\n     /// Scan the file(s), using the provided projection, and return one BatchIterator per\n     /// partition.\n     fn scan(\n         &self,\n         projection: &Option<Vec<usize>>,\n         batch_size: usize,\n-        _filters: &[Expr],\n+        filters: &[Expr],\n\nReview comment:\n       Ideally, I feel we should have a proper filter API defined in data fusion which can be shared among various data sources. On the other hand, the actual filtering logic should be implemented by different data sources / formats, probably via converting the data fusion's filter API to the corresponding ones from the latter. \r\n   \r\n   But this is a very good start and we can probably do them as follow ups (if we don't care much for API changes).\n\n##########\nFile path: rust/parquet/src/file/serialized_reader.rs\n##########\n@@ -137,6 +137,22 @@ impl<R: 'static + ChunkReader> SerializedFileReader<R> {\n             metadata,\n         })\n     }\n+\n+    pub fn filter_row_groups(\n\nReview comment:\n       Yeah I think we can either move this to the application layer (i.e., data fusion), or expose it as a util function from `footer.rs`.\n\n\n\n\n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2021-01-04T20:46:36.095+0000",
                    "updated": "2021-01-04T20:46:36.095+0000",
                    "started": "2021-01-04T20:46:36.095+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "530884",
                    "issueId": "13348468"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 32400,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@41565439[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@51cc83a1[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@c719713[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@3995ccb7[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@12884227[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@42b71c0d[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@42fec247[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@55994edb[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5d0b735[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7ffd94f6[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4ba58db4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@5455ed8d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 32400,
        "customfield_12312520": null,
        "customfield_12312521": "Tue Jan 19 15:51:16 UTC 2021",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2021-01-19T15:51:16.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-11074/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2020-12-30T12:14:07.000+0000",
        "updated": "2021-01-19T16:17:22.000+0000",
        "timeoriginalestimate": null,
        "description": "While profiling a DataFusion query I found that the code spends a lot of time in reading data from parquet files. Predicate / filter push-down is a commonly used performance optimization, where statistics data stored in parquet files (such as min / max values for columns in a parquet row group) is evaluated against query filters to determine which row groups could contain data requested by a query. In this way, by pushing down query filters all the way to the parquet data source, entire row groups or even parquet files can be skipped often resulting in significant performance improvements.\r\n\r\n\u00a0\r\n\r\nI have been working on an implementation for a few weeks and initial results look promising - with predicate push-down, DataFusion is now faster than Apache Spark (140ms for DataFusion vs 200ms for Spark) for the same query against the same parquet files. And I suspect with the latest improvements to the filter kernel, DataFusion performance will be even better.\r\n\r\n\u00a0\r\n\r\nMy work is based on the following key ideas:\r\n * it's best to reuse the existing code for evaluating physical expressions already implemented in DataFusion\r\n * filter expressions pushed down to a parquet table are rewritten to use parquet statistics, for example `(column\u00a0/\u00a02)\u00a0=\u00a04`\u00a0 becomes\u00a0 `(column_min\u00a0/\u00a02)\u00a0<=\u00a04\u00a0&&\u00a04\u00a0<=\u00a0(column_max\u00a0/\u00a02)` - this is done once for all files in a parquet table\r\n * for each parquet file, a RecordBatch containing all required statistics columns is produced, and the predicate expression from the previous step is evaluated, producing a binary array which is finally used to filter the row groups in each parquet file\r\n\r\nNext steps are: integrate this work with latest changes from master branch, publish WIP PR, implement more unit tests\r\n\r\n[~andygrove]\u00a0, [~alamb]\u00a0let me know what you think",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "9h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 32400
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Rust][DataFusion] Implement predicate push-down for parquet tables",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13348468/comment/17267985",
                    "id": "17267985",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
                        "name": "alamb",
                        "key": "alamb",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
                        },
                        "displayName": "Andrew Lamb",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 9064\n[https://github.com/apache/arrow/pull/9064]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
                        "name": "alamb",
                        "key": "alamb",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
                        },
                        "displayName": "Andrew Lamb",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-01-19T15:51:16.869+0000",
                    "updated": "2021-01-19T15:51:16.869+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0lxls:",
        "customfield_12314139": null
    }
}