{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13418918",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918",
    "key": "ARROW-15183",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12351051",
                "id": "12351051",
                "description": "",
                "name": "8.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-05-06"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [
            {
                "id": "12629176",
                "self": "https://issues.apache.org/jira/rest/api/2/issueLink/12629176",
                "type": {
                    "id": "10030",
                    "name": "Reference",
                    "inward": "is related to",
                    "outward": "relates to",
                    "self": "https://issues.apache.org/jira/rest/api/2/issueLinkType/10030"
                },
                "outwardIssue": {
                    "id": "13415935",
                    "key": "ARROW-15019",
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13415935",
                    "fields": {
                        "summary": "[Python] Add bindings for new dataset writing options",
                        "status": {
                            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                            "name": "Resolved",
                            "id": "5",
                            "statusCategory": {
                                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                                "id": 3,
                                "key": "done",
                                "colorName": "green",
                                "name": "Done"
                            }
                        },
                        "priority": {
                            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                            "name": "Major",
                            "id": "3"
                        },
                        "issuetype": {
                            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
                            "id": "7",
                            "description": "The sub-task of the issue",
                            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
                            "name": "Sub-task",
                            "subtask": true,
                            "avatarId": 21146
                        }
                    }
                }
            }
        ],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332532",
                "id": "12332532",
                "name": "Documentation"
            },
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12328936",
                "id": "12328936",
                "name": "Python"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
            "name": "vibhatha",
            "key": "vibhatha",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
            },
            "displayName": "Vibhatha Lakmal Abeykoon",
            "active": true,
            "timeZone": "Etc/UTC"
        },
        "aggregateprogress": {
            "progress": 18600,
            "total": 18600,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 18600,
            "total": 18600,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15183/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 31,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/705983",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha opened a new pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112\n\n\n   This PR includes a minor documentation update for showing how `max_open_files`, `min_rows_per_group` and `max_rows_per_group` parameters can be used in Python dataset API. \r\n   \r\n   The disucssion on the issue: https://issues.apache.org/jira/browse/ARROW-15183\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-10T10:58:38.364+0000",
                    "updated": "2022-01-10T10:58:38.364+0000",
                    "started": "2022-01-10T10:58:38.364+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "705983",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/705985",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#issuecomment-1008757286\n\n\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-10T10:58:54.133+0000",
                    "updated": "2022-01-10T10:58:54.133+0000",
                    "started": "2022-01-10T10:58:54.133+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "705985",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/707177",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r782597375\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -699,3 +699,46 @@ Parquet files:\n \n     # also clean-up custom base directory used in some examples\n     shutil.rmtree(str(base), ignore_errors=True)\n+\n+\n+Configuring files open during a write\n\nReview comment:\n       I think these sections would actually be best placed next to the \"Partitioning performance considerations\" section at https://github.com/apache/arrow/blame/master/docs/source/python/dataset.rst#L578\r\n   \r\n   \n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -699,3 +699,46 @@ Parquet files:\n \n     # also clean-up custom base directory used in some examples\n     shutil.rmtree(str(base), ignore_errors=True)\n+\n+\n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+The number of files opened at during the write time can be set as follows;\n+\n+.. ipython:: python\n+\n+    ds.write_dataset(data=table, base_dir=\"data_dir\", max_open_files=max_open_files)\n+\n+The maximum number of rows per file can be set as follows;\n+\n+.. ipython:: pythoin\n+    ds.write_dataset(record_batch, \"data_dir\", format=\"parquet\",\n\nReview comment:\n       I think that space between the directive and code is required.\r\n   \r\n   ```suggestion\r\n   .. ipython:: python\r\n   \r\n       ds.write_dataset(record_batch, \"data_dir\", format=\"parquet\",\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-11T23:34:15.167+0000",
                    "updated": "2022-01-11T23:34:15.167+0000",
                    "started": "2022-01-11T23:34:15.167+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "707177",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/707335",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#issuecomment-1010765139\n\n\n   @wjones127 thanks for the review, I will update the PR. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-12T08:23:45.260+0000",
                    "updated": "2022-01-12T08:23:45.260+0000",
                    "started": "2022-01-12T08:23:45.260+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "707335",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/710217",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r786377402\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -699,3 +699,46 @@ Parquet files:\n \n     # also clean-up custom base directory used in some examples\n     shutil.rmtree(str(base), ignore_errors=True)\n+\n+\n+Configuring files open during a write\n\nReview comment:\n       I moved the docs to this section. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T02:38:11.851+0000",
                    "updated": "2022-01-18T02:38:11.851+0000",
                    "started": "2022-01-18T02:38:11.850+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710217",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/710218",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r786377629\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -699,3 +699,46 @@ Parquet files:\n \n     # also clean-up custom base directory used in some examples\n     shutil.rmtree(str(base), ignore_errors=True)\n+\n+\n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+The number of files opened at during the write time can be set as follows;\n+\n+.. ipython:: python\n+\n+    ds.write_dataset(data=table, base_dir=\"data_dir\", max_open_files=max_open_files)\n+\n+The maximum number of rows per file can be set as follows;\n+\n+.. ipython:: pythoin\n+    ds.write_dataset(record_batch, \"data_dir\", format=\"parquet\",\n\nReview comment:\n       I replaced this with the short description on the option name and it's functionality ( minor modification to C++ docstring and added here) \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T02:38:54.675+0000",
                    "updated": "2022-01-18T02:38:54.675+0000",
                    "started": "2022-01-18T02:38:54.674+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710218",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/711702",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r788004898\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of files opened with the ``max_rows_per_files`` parameter of\n+:meth:`write_dataset`.\n\nReview comment:\n       ```suggestion\r\n   Set the maximum number of rows written in each file with the ``max_rows_per_files`` parameter of\r\n   :meth:`write_dataset`.\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n\nReview comment:\n       @westonpace does my understand below sound correct? I know it's a little complicated with multi-threading\r\n   \r\n   ```suggestion\r\n   \r\n   To mitigate the many-small-files problem caused by this limit, you can \r\n   also sort your data by the partition columns (assuming it is not already\r\n   sorted). This ensures that files are usually closed after all data for\r\n   their respective partition has been written.\r\n   \r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of files opened with the ``max_rows_per_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one \n+file will be created in each output directory unless files need to be closed to respect \n+``max_open_files``. \n\nReview comment:\n       ```suggestion\r\n   ``max_open_files``. This setting is the primary way to control file size. \r\n   For workloads writing a lot of data files can get very large without a \r\n   row count cap, leading to out-of-memory errors in downstream readers. The \r\n   relationship between row count and file size depends on the dataset schema\r\n   and how well compressed (if at all) the data is. For most applications,\r\n   it's best to keep file sizes below 1GB.\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n\nReview comment:\n       We should probably mention that this setting applies to partitioned datasets:\r\n   \r\n   ```suggestion\r\n   If  ``max_open_files`` is set greater than 0 then this will limit the maximum \r\n   number of files that can be left open. This only applies to writing partitioned\r\n   datasets, where rows are dispatched to the appropriate file depending on their\r\n   partition values. If an attempt is made to open too many  files then the least\r\n   recently used file will be closed.  If this setting is set \r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n\nReview comment:\n       ```suggestion\r\n   Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``.\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n\nReview comment:\n       If we can, let's eliminate \"Modify this value depending on the nature of write operations associated with the usage\" and replace with more specific advice.\r\n   \r\n   ```suggestion\r\n   If your process is concurrently using other file handlers, either with a \r\n   dataset scanner or otherwise, you may hit a system file handler limit. For \r\n   example, if you are scanning a dataset with 300 files and writing out to\r\n   900 files, the total of 1200 files may be over a system limit. (On Linux,\r\n   this might be a \"Too Many Open Files\" error.) You can either reduce this\r\n   ``max_open_files`` setting or increasing your file handler limit on your\r\n   system. The default value is 900 which also allows some number of files\r\n   to be open by the scanner before hitting the default Linux limit of 1024. \r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of files opened with the ``max_rows_per_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one \n+file will be created in each output directory unless files need to be closed to respect \n+``max_open_files``. \n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n\nReview comment:\n       A few points worth discussing:\r\n   \r\n    * Row groups matter for Parquet and Feather/IPC; they affect how data is seen by reader and because of row group statistics can affect file size.\r\n    * Row groups are just batch size for CSV / JSON; the readers aren't affected.\r\n   \r\n   My impression is that we have reasonable default for these values, and users generally won't want to set these. Can you think of examples where we would recommend users adjust these values?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-19T22:06:59.082+0000",
                    "updated": "2022-01-19T22:06:59.082+0000",
                    "started": "2022-01-19T22:06:59.082+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "711702",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/711860",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#issuecomment-1017096397\n\n\n   @wjones127 Nice points. I will work on these ideas. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-20T03:58:05.040+0000",
                    "updated": "2022-01-20T03:58:05.040+0000",
                    "started": "2022-01-20T03:58:05.039+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "711860",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/741437",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r826587938\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n\nReview comment:\n       @wjones127 this is a better description. Let me add it. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-15T05:14:53.840+0000",
                    "updated": "2022-03-15T05:14:53.840+0000",
                    "started": "2022-03-15T05:14:53.839+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "741437",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/741442",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r826595344\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n\nReview comment:\n       This is another overlapping suggestion. I think the previous suggestion contained the expected idea. What do you think @wjones127? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-15T05:34:14.092+0000",
                    "updated": "2022-03-15T05:34:14.092+0000",
                    "started": "2022-03-15T05:34:14.092+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "741442",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/741443",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r826599846\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of files opened with the ``max_rows_per_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one \n+file will be created in each output directory unless files need to be closed to respect \n+``max_open_files``. \n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n\nReview comment:\n       I guess we can think of logging activities where online activities are monitored in windows (window aggregations) and summaries are logged by computing on those aggregated values. So if we assume such a scenario, depending on the accuracy required for the computation (if it is a learning task) and the required performance optimizations (execution time and memory), the users should be able to tune the parameter. This could be an interesting blog article if we can demonstrate it. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-15T05:45:42.753+0000",
                    "updated": "2022-03-15T05:45:42.753+0000",
                    "started": "2022-03-15T05:45:42.753+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "741443",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/741723",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r827096582\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,72 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n\nReview comment:\n       ```suggestion\r\n   important to optimize the writes, such as the number of rows per file and\r\n   the number of files open during write. \r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,60 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. If an attempt is made to open too many \n+files then the least recently used file will be closed.  If this setting is set \n+too low you may end up fragmenting your data into many small files.\n+\n+The default value is 900 which also allows some number of files to be open \n+by the scannerbefore hitting the default Linux limit of 1024. Modify this value \n+depending on the nature of write operations associated with the usage. \n+\n+Another important configuration used in `write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of files opened with the ``max_rows_per_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one \n+file will be created in each output directory unless files need to be closed to respect \n+``max_open_files``. \n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n\nReview comment:\n       Those are good examples.\r\n   \r\n   Could you add a paragraph discussing how `row_groups` affect later reads for Parquet and Feather/IPC, but not CSV or JSON? \n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,72 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, i.e number of rows per file and\n+number of files open during write. \n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one \n+file will be created in each output directory unless files need to be closed to respect \n+``max_open_files``. This setting is the primary way to control file size. \n+For workloads writing a lot of data files can get very large without a \n\nReview comment:\n       ```suggestion\r\n   For workloads writing a lot of data, files can get very large without a\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-15T16:20:48.916+0000",
                    "updated": "2022-03-15T16:20:48.916+0000",
                    "started": "2022-03-15T16:20:48.916+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "741723",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/741957",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#issuecomment-1068587522\n\n\n   @wjones127 I wasn't exactly sure about not committing the changes to the test submodule. I will check this. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-16T00:09:06.070+0000",
                    "updated": "2022-03-16T00:09:06.070+0000",
                    "started": "2022-03-16T00:09:06.070+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "741957",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/744629",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#issuecomment-1073017100\n\n\n   @wjones127 I think this was a mistake from my end. Sorry about the confusion on committing the submodule. \r\n   I corrected it. \n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-19T14:10:13.579+0000",
                    "updated": "2022-03-19T14:10:13.579+0000",
                    "started": "2022-03-19T14:10:13.578+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "744629",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/744635",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "vibhatha commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r830489459\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``max_rows_per_group`` is set greater than 0 then the dataset writer may split \n+up large incoming batches into multiple row groups.  If this value is set then \n+``min_rows_per_group`` should also be set or else you may end up with very small \n+row groups (e.g. if the incoming row group size is just barely larger than this value).\n+In addition row_groups are a factor which impacts write/read of Parquest, Feather and IPC\n+formats. The main purpose of these formats are to provide high performance data structures\n+for I/O operations on larger datasets. The row_group concept allows the write/read operations\n+to be optimized and gather a defined number of rows at once and execute the I/O operation. \n+But row_groups are not integrated to support JSON or CSV formats. \n\nReview comment:\n       @wjones127 I added a small para on row-groups. Is this helpful? \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-19T14:22:13.551+0000",
                    "updated": "2022-03-19T14:22:13.551+0000",
                    "started": "2022-03-19T14:22:13.550+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "744635",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/745158",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r831207882\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``max_rows_per_group`` is set greater than 0 then the dataset writer may split \n+up large incoming batches into multiple row groups.  If this value is set then \n+``min_rows_per_group`` should also be set or else you may end up with very small \n+row groups (e.g. if the incoming row group size is just barely larger than this value).\n+In addition row_groups are a factor which impacts write/read of Parquest, Feather and IPC\n\nReview comment:\n       ```suggestion\r\n   In addition row_groups are a factor which impacts write/read of Parquet, Feather and IPC\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-21T15:00:48.844+0000",
                    "updated": "2022-03-21T15:00:48.844+0000",
                    "started": "2022-03-21T15:00:48.844+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "745158",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/745166",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r831207882\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``max_rows_per_group`` is set greater than 0 then the dataset writer may split \n+up large incoming batches into multiple row groups.  If this value is set then \n+``min_rows_per_group`` should also be set or else you may end up with very small \n+row groups (e.g. if the incoming row group size is just barely larger than this value).\n+In addition row_groups are a factor which impacts write/read of Parquest, Feather and IPC\n\nReview comment:\n       I think Feather and IPC are the same format.\r\n   ```suggestion\r\n   In addition row_groups are a factor which impacts write/read of Parquet and Feather/IPC\r\n   ```\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-21T15:16:03.474+0000",
                    "updated": "2022-03-21T15:16:03.474+0000",
                    "started": "2022-03-21T15:16:03.473+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "745166",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/745179",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r831240137\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``max_rows_per_group`` is set greater than 0 then the dataset writer may split \n+up large incoming batches into multiple row groups.  If this value is set then \n+``min_rows_per_group`` should also be set or else you may end up with very small \n+row groups (e.g. if the incoming row group size is just barely larger than this value).\n+In addition row_groups are a factor which impacts write/read of Parquest, Feather and IPC\n+formats. The main purpose of these formats are to provide high performance data structures\n+for I/O operations on larger datasets. The row_group concept allows the write/read operations\n+to be optimized and gather a defined number of rows at once and execute the I/O operation. \n+But row_groups are not integrated to support JSON or CSV formats. \n\nReview comment:\n       I think it could use a little more direct advice to help users see the symptoms of when they've done something wrong. Here's my suggestion:\r\n   \r\n   > Row groups are build into the Parquet and IPC/Feather formats, but don't affect JSON or CSV. When reading back Parquet and IPC formats in Arrow, the row group boundaries become the record batch boundaries, determining the default batch size of downstream readers. Additionally, row groups in Parquet files have column statistics which can help readers skip irrelevant data but can add size to the file. As an extreme example, if one sets `max_rows_per_group=1` in Parquet, they will have large files because most of the file will be row group statistics. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-21T15:29:46.327+0000",
                    "updated": "2022-03-21T15:29:46.327+0000",
                    "started": "2022-03-21T15:29:46.327+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "745179",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/745542",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r831688034\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n\nReview comment:\n       ```suggestion\r\n   the maximum number of open files allowed during the write.\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n\nReview comment:\n       ```suggestion\r\n   system. The default value is 900 which allows some number of files\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n\nReview comment:\n       Again, this setting isn't really mean to control the maximum number of files opened.\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n\nReview comment:\n       As long as the user is creating multiple (reasonably sized) row groups we shouldn't get out-of-memory errors even if the file is very large.  Also, what evidence do you have for \"For most applications, it's best to keep file sizes below 1GB\"?\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n\nReview comment:\n       ```suggestion\r\n   less than this value if other options such as ``max_open_files`` or \r\n   ``max_rows_per_file`` force smaller row group sizes.\r\n   ```\r\n   \r\n   I think it is an error if `max_rows_per_file` is less than `min_rows_per_group`.\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n\nReview comment:\n       Can you reword this paragraph?  I'm having a hard time understanding the intent.  Also, I'm not sure I recognize the term \"mini-batch\" (at least, not in a way that would apply here).\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n\nReview comment:\n       ```suggestion\r\n   ``max_open_files`` setting or increase the file handler limit on your\r\n   ```\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n\nReview comment:\n       `min_rows_per_group` isn't really intended to control the maximum number of files opened.  This might be confusing since we have `max_open_files`.\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n+\n+Configuring rows per group during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to disk, depending on the volume of data obtained, \n+(in a mini-batch setting where, records are obtained in batch by batch)\n+the volume of data written to disk per each group can be configured. \n+This can be configured using a minimum and maximum parameter. \n+\n+Set the maximum number of files opened with the ``min_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``min_rows_per_group`` is set greater than 0 then this will cause the \n+dataset writer to batch incoming data and only write the row groups to the \n+disk when sufficient rows have accumulated. The final row group size may be \n+less than this value and other options such as ``max_open_files`` or \n+``max_rows_per_file`` lead to smaller row group sizes.\n+\n+Set the maximum number of files opened with the ``max_rows_per_group`` parameter of\n+:meth:`write_dataset`.\n+\n+Note: if ``max_rows_per_group`` is set greater than 0 then the dataset writer may split \n+up large incoming batches into multiple row groups.  If this value is set then \n+``min_rows_per_group`` should also be set or else you may end up with very small \n+row groups (e.g. if the incoming row group size is just barely larger than this value).\n+In addition row_groups are a factor which impacts write/read of Parquet and Feather/IPC\n+formats. The main purpose of these formats are to provide high performance data structures\n+for I/O operations on larger datasets. The row_group concept allows the write/read operations\n+to be optimized and gather a defined number of rows at once and execute the I/O operation. \n+But row_groups are not integrated to support JSON or CSV formats. \n\nReview comment:\n       What happens if the dataset is JSON or CSV and this is set?  Is it an error or is this property ignored?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-22T01:32:55.437+0000",
                    "updated": "2022-03-22T01:32:55.437+0000",
                    "started": "2022-03-22T01:32:55.437+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "745542",
                    "issueId": "13418918"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/worklog/745548",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wjones127 commented on a change in pull request #12112:\nURL: https://github.com/apache/arrow/pull/12112#discussion_r831709424\n\n\n\n##########\nFile path: docs/source/python/dataset.rst\n##########\n@@ -613,6 +613,77 @@ guidelines apply. Row groups can provide parallelism when reading and allow data\n based on statistics, but very small groups can cause metadata to be a significant portion\n of file size. Arrow's file writer provides sensible defaults for group sizing in most cases.\n \n+Configuring files open during a write\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+When writing data to the disk, there are a few parameters that can be \n+important to optimize the writes, such as the number of rows per file and\n+the number of files open during write.\n+\n+Set the maximum number of files opened with the ``max_open_files`` parameter of\n+:meth:`write_dataset`.\n+\n+If  ``max_open_files`` is set greater than 0 then this will limit the maximum \n+number of files that can be left open. This only applies to writing partitioned\n+datasets, where rows are dispatched to the appropriate file depending on their\n+partition values. If an attempt is made to open too many  files then the least\n+recently used file will be closed.  If this setting is set too low you may end\n+up fragmenting your data into many small files.\n+\n+If your process is concurrently using other file handlers, either with a \n+dataset scanner or otherwise, you may hit a system file handler limit. For \n+example, if you are scanning a dataset with 300 files and writing out to\n+900 files, the total of 1200 files may be over a system limit. (On Linux,\n+this might be a \"Too Many Open Files\" error.) You can either reduce this\n+``max_open_files`` setting or increasing your file handler limit on your\n+system. The default value is 900 which also allows some number of files\n+to be open by the scanner before hitting the default Linux limit of 1024. \n+\n+Another important configuration used in :meth:`write_dataset` is ``max_rows_per_file``. \n+\n+Set the maximum number of rows written in each file with the ``max_rows_per_files``\n+parameter of :meth:`write_dataset`.\n+\n+If ``max_rows_per_file`` is set greater than 0 then this will limit how many \n+rows are placed in any single file. Otherwise there will be no limit and one\n+file will be created in each output directory unless files need to be closed to respect\n+``max_open_files``. This setting is the primary way to control file size.\n+For workloads writing a lot of data, files can get very large without a\n+row count cap, leading to out-of-memory errors in downstream readers. The\n+relationship between row count and file size depends on the dataset schema\n+and how well compressed (if at all) the data is. For most applications,\n+it's best to keep file sizes below 1GB.\n\nReview comment:\n       > As long as the user is creating multiple (reasonably sized) row groups we shouldn't get out-of-memory errors even if the file is very large.\r\n   Are we assuming downstream readers are necessarily Arrow? I suggested that based on my experience with Spark, which as I recall, read whole files.\r\n   \r\n   > Also, what evidence do you have for \"For most applications, it's best to keep file sizes below 1GB\"?\r\n   In retrospect, that guidance is a bit low. My previous heuristic target was between 50 MB per file at a minimum and 2 GB as a maximum. That might be more specific to a Spark / S3 context; so maybe not as appropriate here.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: github-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-03-22T02:11:30.039+0000",
                    "updated": "2022-03-22T02:11:30.039+0000",
                    "started": "2022-03-22T02:11:30.039+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "745548",
                    "issueId": "13418918"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 18600,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1c859ae9[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@912b26b[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@299d2ce4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@35005256[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5f15214f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@3bf11572[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7b4be4e4[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@4c6e9b39[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3a07c4a2[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@9ee0c15[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2da6a508[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@15a944d6[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 18600,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Apr 14 02:14:36 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-04-14T02:14:36.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15183/watchers",
            "watchCount": 3,
            "isWatching": false
        },
        "created": "2021-12-22T01:42:48.000+0000",
        "updated": "2022-04-15T03:54:36.000+0000",
        "timeoriginalestimate": null,
        "description": "Recently the write options `max_open_files`, `max_rows_per_file`, `min_rows_per_group` and `max_rows_per_group` were included to the Python bindings. But these are not documented here: [https://arrow.apache.org/docs/python/dataset.html#writing-datasets.]\u00a0",
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5h 10m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 18600
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Python][Docs] Add Missing Dataset Write Options ",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17463794",
                    "id": "17463794",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "The docs are not built immediately after every push. I see there are docstrings already, is there anything else to do here?",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-12-22T13:01:56.503+0000",
                    "updated": "2021-12-22T13:01:56.503+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17463892",
                    "id": "17463892",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=willjones127",
                        "name": "willjones127",
                        "key": "willjones127",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"
                        },
                        "displayName": "Will Jones",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "As far as I know, those parameters don't often need to be changed from their defaults, but there may be some extreme cases where it's worth tuning them. If we know circumstances where we recommend setting those, we can write some guidance in the datasets guide.\r\n\r\nIf we end up getting a lot of questions about this, we might have more information about this. That's how we ended up writing the partitioning guidance for ARROW-15150.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=willjones127",
                        "name": "willjones127",
                        "key": "willjones127",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34058",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34058",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34058",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34058"
                        },
                        "displayName": "Will Jones",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-22T15:14:59.685+0000",
                    "updated": "2021-12-22T15:14:59.685+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17464483",
                    "id": "17464483",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~lidavidm]\u00a0does the rst files generated from the docstrings.\u00a0\r\nI thought we have to separately document the options with examples.\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2021-12-23T11:29:11.691+0000",
                    "updated": "2021-12-23T11:29:11.691+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17464536",
                    "id": "17464536",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Ah, adding an example would be helpful. Not everything needs to have separate prose documentation written but adding these new options to the guide does make sense.",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2021-12-23T12:56:20.308+0000",
                    "updated": "2021-12-23T12:56:20.308+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17506686",
                    "id": "17506686",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "body": "[~willjones127]\u00a0and [~lidavidm] Good points. I will add some information covering this basis.\u00a0",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=vibhatha",
                        "name": "vibhatha",
                        "key": "vibhatha",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34044",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34044",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34044",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34044"
                        },
                        "displayName": "Vibhatha Lakmal Abeykoon",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "created": "2022-03-15T04:23:40.870+0000",
                    "updated": "2022-03-15T04:23:40.870+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13418918/comment/17522014",
                    "id": "17522014",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "body": "Issue resolved by pull request 12112\n[https://github.com/apache/arrow/pull/12112]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=westonpace",
                        "name": "westonpace",
                        "key": "westonpace",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=34045",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=34045",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=34045",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=34045"
                        },
                        "displayName": "Weston Pace",
                        "active": true,
                        "timeZone": "America/Los_Angeles"
                    },
                    "created": "2022-04-14T02:14:36.334+0000",
                    "updated": "2022-04-14T02:14:36.334+0000"
                }
            ],
            "maxResults": 6,
            "total": 6,
            "startAt": 0
        },
        "customfield_12311820": "0|z0xy0o:",
        "customfield_12314139": null
    }
}