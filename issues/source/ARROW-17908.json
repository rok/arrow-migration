{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13484118",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118",
    "key": "ARROW-17908",
    "fields": {
        "parent": {
            "id": "13484117",
            "key": "ARROW-17907",
            "self": "https://issues.apache.org/jira/rest/api/2/issue/13484117",
            "fields": {
                "summary": "[Website] Blog about Arrow <--> Parquet translation and nesting",
                "status": {
                    "self": "https://issues.apache.org/jira/rest/api/2/status/5",
                    "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
                    "name": "Resolved",
                    "id": "5",
                    "statusCategory": {
                        "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                        "id": 3,
                        "key": "done",
                        "colorName": "green",
                        "name": "Done"
                    }
                },
                "priority": {
                    "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
                    "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
                    "name": "Major",
                    "id": "3"
                },
                "issuetype": {
                    "self": "https://issues.apache.org/jira/rest/api/2/issuetype/3",
                    "id": "3",
                    "description": "A task that needs to be done.",
                    "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21148&avatarType=issuetype",
                    "name": "Task",
                    "subtask": false,
                    "avatarId": 21148
                }
            }
        },
        "fixVersions": [],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
            "name": "alamb",
            "key": "alamb",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
            },
            "displayName": "Andrew Lamb",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
            "name": "alamb",
            "key": "alamb",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
            },
            "displayName": "Andrew Lamb",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
            "name": "alamb",
            "key": "alamb",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
            },
            "displayName": "Andrew Lamb",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 7200,
            "total": 7200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 7200,
            "total": 7200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17908/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 12,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813851",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb opened a new pull request, #245:\nURL: https://github.com/apache/arrow-site/pull/245\n\n   See rationale on https://issues.apache.org/jira/browse/ARROW-17907\n\n\n",
                    "created": "2022-10-01T12:04:00.608+0000",
                    "updated": "2022-10-01T12:04:00.608+0000",
                    "started": "2022-10-01T12:04:00.608+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813851",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813852",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#issuecomment-1264343669\n\n   <!--\n     Licensed to the Apache Software Foundation (ASF) under one\n     or more contributor license agreements.  See the NOTICE file\n     distributed with this work for additional information\n     regarding copyright ownership.  The ASF licenses this file\n     to you under the Apache License, Version 2.0 (the\n     \"License\"); you may not use this file except in compliance\n     with the License.  You may obtain a copy of the License at\n   \n       http://www.apache.org/licenses/LICENSE-2.0\n   \n     Unless required by applicable law or agreed to in writing,\n     software distributed under the License is distributed on an\n     \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n     KIND, either express or implied.  See the License for the\n     specific language governing permissions and limitations\n     under the License.\n   -->\n   \n   Thanks for opening a pull request!\n   \n   Could you open an issue for this pull request on JIRA?\n   https://issues.apache.org/jira/browse/ARROW\n   \n   Then could you also rename pull request title in the following format?\n   \n       ARROW-${JIRA_ID}: [${COMPONENT}] ${SUMMARY}\n   \n   See also:\n   \n     * [Other pull requests](https://github.com/apache/arrow-site/pulls/)\n     * [Contribution Guidelines - How to contribute patches](https://arrow.apache.org/docs/developers/contributing.html#how-to-contribute-patches)\n   \n\n\n",
                    "created": "2022-10-01T12:04:14.253+0000",
                    "updated": "2022-10-01T12:04:14.253+0000",
                    "started": "2022-10-01T12:04:14.253+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813852",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813853",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985091819\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n\nReview Comment:\n   ```suggestion\r\n   date: \"2022-10-01 00:00:00\"\r\n   ```\n\n\n\n",
                    "created": "2022-10-01T12:06:22.641+0000",
                    "updated": "2022-10-01T12:06:22.641+0000",
                    "started": "2022-10-01T12:06:22.641+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813853",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813854",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#issuecomment-1264349734\n\n   https://issues.apache.org/jira/browse/ARROW-17908\n\n\n",
                    "created": "2022-10-01T12:24:24.160+0000",
                    "updated": "2022-10-01T12:24:24.160+0000",
                    "started": "2022-10-01T12:24:24.159+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813854",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813861",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "tustvold commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985099116\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, so fast, efficient, and correct translation between them is a key building block.\n\nReview Comment:\n   ```suggestion\r\n   It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\r\n   ```\r\n   \r\n   So fast made me think of \"it's so fast\", as opposed to introducing a subordinate clause\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n\nReview Comment:\n   ```suggestion\r\n   A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\r\n   ```\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, so fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series article we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\u201cColumn1\u201d: 1, \u201cColumn2\u201d: 2}\n+{\u201cColumn1\u201d: 3, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+{\u201cColumn1\u201d: 5, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \u201cthis value is unknown.\u201d\n\nReview Comment:\n   ```suggestion\r\n   Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \u201cthis value is unknown\".\r\n   ```\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, so fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series article we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n\nReview Comment:\n   ```suggestion\r\n   In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\r\n   ```\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, so fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series article we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\u201cColumn1\u201d: 1, \u201cColumn2\u201d: 2}\n+{\u201cColumn1\u201d: 3, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+{\u201cColumn1\u201d: 5, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \u201cthis value is unknown.\u201d\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502   \u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  4  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Validity   Values\n+```\n+\n+In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called definition levels. These will be expanded upon later, but for now a definition level of 1 indicates a valid value, and 0 a null value. Unlike arrow, nulls are not encoded in the list of values\n\nReview Comment:\n   ```suggestion\r\n   In Parquet the validity information is also stored separately from the values, however, instead of being encoded as a validity bitmask it is encoded as a list of 16-bit integers called definition levels. These will be expanded upon in the next post, but for now a definition level of 1 indicates a valid value, and 0 a null value. Unlike arrow, nulls are not encoded in the list of values\r\n   ```\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10--01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic (as we will show), and we noticed a lack of available approachable technical information, and thus want to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, so fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series article we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\u201cColumn1\u201d: 1, \u201cColumn2\u201d: 2}\n+{\u201cColumn1\u201d: 3, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+{\u201cColumn1\u201d: 5, \u201cColumn2\u201d: 4, \u201cColumn3\u201d: 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, and they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n\nReview Comment:\n   ```suggestion\r\n   Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\r\n   ```\n\n\n\n",
                    "created": "2022-10-01T13:34:55.990+0000",
                    "updated": "2022-10-01T13:34:55.990+0000",
                    "started": "2022-10-01T13:34:55.989+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813861",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813895",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#issuecomment-1264486174\n\n   > Looks good to me, thank you for translating my brain dump into something intelligible \ud83d\ude06\r\n   \r\n   Lol ",
                    "created": "2022-10-01T21:31:47.859+0000",
                    "updated": "2022-10-01T21:31:47.859+0000",
                    "started": "2022-10-01T21:31:47.858+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813895",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813925",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#issuecomment-1264613515\n\n   cc @nevi-me  you may be interested in this content and/or have ideas of how to better present it\n\n\n",
                    "created": "2022-10-02T10:59:27.044+0000",
                    "updated": "2022-10-02T10:59:27.044+0000",
                    "started": "2022-10-02T10:59:27.044+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813925",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813944",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "mystic-lama commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985260056\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n\nReview Comment:\n   can we enhance it as \"arrow is in-memory/interchange format\" \n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n\nReview Comment:\n   A link to DataPage information would be great. While reading, got to this point and question came what is DataPage :)\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n\nReview Comment:\n   A link to DataPage information would be great. While reading, got to this point and question came what is DataPage :) Understand that it is parquet concept, so it can be ignored as well\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \"this value is unknown\".\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n\nReview Comment:\n   can add link to https://arrow.apache.org/docs/format/Columnar.html#validity-bitmaps \n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \"this value is unknown\".\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n\nReview Comment:\n   can add link for validity bitmaps https://arrow.apache.org/docs/format/Columnar.html#validity-bitmaps \n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \"this value is unknown\".\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502   \u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  4  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n\nReview Comment:\n   This diagram is giving a perception that bitmap is also same size (like an integer), which I assume is not the case. It's a bitmap with corresponding bit set. I do not have any bright ideas to enhance this, perhaps a note.\n\n\n\n",
                    "created": "2022-10-05T19:55:45.843+0000",
                    "updated": "2022-10-05T19:55:45.843+0000",
                    "started": "2022-10-05T19:55:45.843+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813944",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/813995",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "martin-g commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985480915\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n\nReview Comment:\n   I am not native English speaker but `... has been complete support for ...` does not sound correct to me.\n\n\n\n",
                    "created": "2022-10-05T20:01:04.121+0000",
                    "updated": "2022-10-05T20:01:04.121+0000",
                    "started": "2022-10-05T20:01:04.120+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "813995",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/814014",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "alamb commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985644773\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \"this value is unknown\".\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502   \u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  1  \u2502   \u2502  4  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524   \u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  0  \u2502   \u2502 ??  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2518\n\nReview Comment:\n   In 43860e33b9\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n+```\n+\n+# Nullable Primitive Column\n+\n+Now let us consider the case of a nullable column, where some of the values might have the special sentinel value `NULL` that designates \"this value is unknown\".\n+\n+In Arrow nulls are stored separately from the values in the form of a validity bitmask, with arbitrary data in the corresponding positions in the values buffer.\n\nReview Comment:\n   in 43860e33b9\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n+\n+\n+## Non-Nullable Primitive Column\n+\n+Let us start with the simplest case of a non-nullable list of 32-bit signed integers.\n+\n+In arrow this would be represented as a `PrimitiveArray`, which would store them contiguously in memory\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+Values\n+```\n+\n+Parquet has multiple different encodings that it can use for integers types, the exact details of which are beyond the scope of this post, but broadly speaking it will encode the data as one or more pages containing the integers\n+\n+```text\n+\u250c\u2500\u2500\u2500\u2500\u2500\u2510\n+\u2502  1  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+|  2  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  3  \u2502\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2524\n+\u2502  4  \u2502\n+\u2514\u2500\u2500\u2500\u2500\u2500\u2518\n+ Data\n+\n+ DataPage\n\nReview Comment:\n   Added in 70ecc7ad76\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n+\n+[Apache Arrow](https://arrow.apache.org/) is an open, language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations. [Apache Parquet](https://parquet.apache.org/) is an open, column-oriented data file format designed for very efficient data encoding and retrieval.\n+\n+It is increasingly common for analytic systems to use Arrow to process data stored in Parquet files, and therefore fast, efficient, and correct translation between them is a key building block.\n+\n+Historically analytic processing primarily focused on querying data with a tabular schema, that is one where there are a fixed number of columns, and each row contains a single value for a given column. However, with the increasing adoption of structured document formats such as XML, JSON, etc\u2026, this schema limitation can seem antiquated and unnecessarily limiting.\n+\n+As of version [20.0.0](https://crates.io/crates/arrow/20.0.0), released in August 2022, the Rust implementation is feature complete. Instructions for getting started can be found [here](https://docs.rs/parquet/latest/parquet/arrow/index.html) and feel free to raise any issues on our [bugtracker](https://github.com/apache/arrow-rs/issues).\n+\n+In this series we will explain how Parquet and Arrow represent nested data, highlighting the similarities and differences between them, and giving a flavor of the practicalities of supporting reading and writing between them.\n+\n+## Columnar vs Record-Oriented\n+\n+First, it is necessary to take a step back and discuss the difference between columnar and record-oriented data formats. In a record oriented data format, such as JSON, all the values for a given record are stored contiguously.\n+\n+For example\n+\n+```json\n+{\"Column1\": 1, \"Column2\": 2}\n+{\"Column1\": 3, \"Column2\": 4, \"Column3\": 5}\n+{\"Column1\": 5, \"Column2\": 4, \"Column3\": 5}\n+```\n+\n+In a columnar representation, the data for a given column is instead stored contiguously\n+\n+```text\n+Column1: [1, 3, 5]\n+Column2: [2, 4, 4]\n+Column3: [null, 5, 5]\n+```\n+\n+Aside from potentially yielding better data compression, a columnar layout can dramatically improve performance of certain queries. This is because laying data out contiguously in memory allows both the compiler and CPU to better exploit opportunities to process the data in parallel. The specifics of [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) and [ILP](https://en.wikipedia.org/wiki/Instruction-level_parallelism) are well beyond the scope of this post, but the important takeaway is that processing large blocks of data without intervening conditional branches has substantial performance benefits.\n+\n+\n+## Parquet vs Arrow\n+Parquet and Arrow are complementary technologies, but they make some different design tradeoffs. In particular, Parquet is a storage format designed for maximum space efficiency, whereas Arrow is an in-memory format intended to be operated on by vectorized computational kernels.\n+\n+The major distinction is that arrow provides O(1) random access lookups to any array index, whilst parquet does not. In particular, Parquet uses [dremel record shredding](https://akshays-blog.medium.com/wrapping-head-around-repetition-and-definition-levels-in-dremel-powering-bigquery-c1a33c9695da), [variable length encoding schemes](https://github.com/apache/parquet-format/blob/master/Encodings.md), and [block compression](https://github.com/apache/parquet-format/blob/master/Compression.md) to drastically reduce the data size, but these techniques come at the loss of performant random access lookups.\n+\n+A common pattern that plays to each technologies strengths, is to stream data from a compressed representation, such as parquet, in thousand row batches in the arrow format, process these batches individually, and accumulate the results in a more compressed representation. This benefits from the ability to efficiently perform computations on arrow data, whilst keeping memory requirements in check, and allowing the computation kernels to be agnostic to the encodings of the source and destination.\n+\n+**Arrow is primarily an interchange format, whereas Parquet is a storage format.**\n\nReview Comment:\n   Fixed in 6b14b13d81\n\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n\nReview Comment:\n   I tried to improve the wording in f210634631\n\n\n\n",
                    "created": "2022-10-05T20:02:41.741+0000",
                    "updated": "2022-10-05T20:02:41.741+0000",
                    "started": "2022-10-05T20:02:41.740+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "814014",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/814016",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "tustvold commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985645887\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n\nReview Comment:\n   ```suggestion\r\n   A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been adding complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\r\n   ```\r\n   Perhaps?\n\n\n\n",
                    "created": "2022-10-05T20:02:43.025+0000",
                    "updated": "2022-10-05T20:02:43.025+0000",
                    "started": "2022-10-05T20:02:43.025+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "814016",
                    "issueId": "13484118"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/worklog/814017",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "martin-g commented on code in PR #245:\nURL: https://github.com/apache/arrow-site/pull/245#discussion_r985647779\n\n\n##########\n_posts/2022-10-01-arrow-parquet-encoding-part-1.md:\n##########\n@@ -0,0 +1,151 @@\n+---\n+layout: post\n+title: Arrow and Parquet Part 1: Primitive Types and Nullability\n+date: \"2022-10-01 00:00:00\"\n+author: tustvold, alamb\n+categories: [parquet, arrow]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+## Introduction\n+\n+A long-running project within the [Rust Apache Arrow](https://github.com/apache/arrow-rs) implementation has been complete support for reading and writing arbitrarily nested parquet schema. This is a complex topic, as we will show, and we noticed a lack of available approachable technical information, and thus wanted to share our learnings with the community.\n\nReview Comment:\n   That's better!\r\n   Thanks, @tustvold !\n\n\n\n",
                    "created": "2022-10-05T20:02:44.038+0000",
                    "updated": "2022-10-05T20:02:44.038+0000",
                    "started": "2022-10-05T20:02:44.037+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "814017",
                    "issueId": "13484118"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/7",
            "id": "7",
            "description": "The sub-task of the issue",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21146&avatarType=issuetype",
            "name": "Sub-task",
            "subtask": true,
            "avatarId": 21146
        },
        "timespent": 7200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@1f0b4863[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@8d08ee2[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@4bc96782[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@47e25787[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@7c6e43fe[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@51f4ddac[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@51014567[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@48a6eaf[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@af5ac73[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@7d9b093c[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@266a5195[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@71c2ace1[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 7200,
        "customfield_12312520": null,
        "customfield_12312521": "Wed Oct 05 18:38:42 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-10-05T18:38:31.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-17908/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2022-10-01T11:27:44.000+0000",
        "updated": "2022-10-05T20:02:44.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "2h",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 7200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Website] Arbitrarily Nested Data in Parquet and Arrow: Part 1: Introduction",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13484118/comment/17613103",
                    "id": "17613103",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
                        "name": "alamb",
                        "key": "alamb",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
                        },
                        "displayName": "Andrew Lamb",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Resolved in https://github.com/apache/arrow-site/pull/245",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=alamb",
                        "name": "alamb",
                        "key": "alamb",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=alamb&avatarId=43364",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=alamb&avatarId=43364",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=alamb&avatarId=43364",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=alamb&avatarId=43364"
                        },
                        "displayName": "Andrew Lamb",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2022-10-05T18:38:42.578+0000",
                    "updated": "2022-10-05T18:38:42.578+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z18zwo:",
        "customfield_12314139": null
    }
}