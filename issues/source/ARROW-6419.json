{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13254317",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317",
    "key": "ARROW-6419",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12345978",
                "id": "12345978",
                "description": "",
                "name": "0.15.0",
                "archived": false,
                "released": true,
                "releaseDate": "2019-10-05"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [
            "pull-request-available"
        ],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332543",
                "id": "12332543",
                "name": "Website"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
            "name": "wesm",
            "key": "wesmckinn",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
            },
            "displayName": "Wes McKinney",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 13200,
            "total": 13200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 13200,
            "total": 13200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6419/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 30,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305386",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19\n \n \n   The dates will need to be changed for the actual publication date.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T03:07:36.215+0000",
                    "updated": "2019-09-03T03:07:36.215+0000",
                    "started": "2019-09-03T03:07:36.214+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305386",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305387",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#issuecomment-527286878\n \n \n   cc @hatemhelal @xhochy for any review. \r\n   \r\n   Note that we have dropped BinaryArray read performance in the non-dictionary case. Not sure why that is yet. I opened https://issues.apache.org/jira/browse/ARROW-6417 to investigate\r\n   \r\n   ![20190903_parquet_read_perf](https://user-images.githubusercontent.com/329591/64141564-2b9a4b80-cdce-11e9-94ea-bfcc0dea0b23.png)\r\n   \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T03:08:51.739+0000",
                    "updated": "2019-09-03T03:08:51.739+0000",
                    "started": "2019-09-03T03:08:51.739+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305387",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305388",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#issuecomment-527287458\n \n \n   In light of the mixed performance results the post might need a new title to reframe around the dictionary read improvements\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T03:12:58.172+0000",
                    "updated": "2019-09-03T03:12:58.172+0000",
                    "started": "2019-09-03T03:12:58.172+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305388",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305801",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320433177\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n \n Review comment:\n   This section header probably should go away if you start the performance-improvements section above, where I suggested.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:52.229+0000",
                    "updated": "2019-09-03T19:11:52.229+0000",
                    "started": "2019-09-03T19:11:52.228+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305801",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305802",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320429697\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n+branch. We construct two kinds of Arrow tables with 10 columns each:\n+\n+* \"Low cardinality\" and \"high cardinality\" variants. The \"low cardinality\" case\n+  has 1,000 unique string values of 32-bytes each. The \"high cardinality\" has\n+  100,000 unique values\n+* \"Dense\" (non-dictionary) and \"Dictionary\" variants\n+\n+[Click here is the full benchmark script.][11]\n \n Review comment:\n   ```suggestion\r\n   See the full benchmark script [here][11].\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:52.248+0000",
                    "updated": "2019-09-03T19:11:52.248+0000",
                    "started": "2019-09-03T19:11:52.248+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305802",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305803",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320432915\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n \n Review comment:\n   This seems to start a new section, where you've moved on from \"background\" and are starting to discuss your optimizations.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.363+0000",
                    "updated": "2019-09-03T19:11:53.363+0000",
                    "started": "2019-09-03T19:11:53.363+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305803",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305804",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320432165\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n \n Review comment:\n   Link to that please\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.422+0000",
                    "updated": "2019-09-03T19:11:53.422+0000",
                    "started": "2019-09-03T19:11:53.422+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305804",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305805",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320431254\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n \n Review comment:\n   ```suggestion\r\n   This post reviews the work that was done and shows benchmarks comparing Arrow 0.11.0\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.611+0000",
                    "updated": "2019-09-03T19:11:53.611+0000",
                    "started": "2019-09-03T19:11:53.610+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305805",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305806",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320430931\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n \n Review comment:\n   Use the past tense? (We ran, we constructed, etc.)\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.769+0000",
                    "updated": "2019-09-03T19:11:53.769+0000",
                    "started": "2019-09-03T19:11:53.769+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305806",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305810",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320429352\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n+branch. We construct two kinds of Arrow tables with 10 columns each:\n+\n+* \"Low cardinality\" and \"high cardinality\" variants. The \"low cardinality\" case\n+  has 1,000 unique string values of 32-bytes each. The \"high cardinality\" has\n+  100,000 unique values\n+* \"Dense\" (non-dictionary) and \"Dictionary\" variants\n+\n+[Click here is the full benchmark script.][11]\n+\n+We show both single-threaded and multithreaded read performance. The test\n+machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical\n+cores and 32 virtual cores.\n+\n+First, the writing benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_write_perf.png\"\n+     alt=\"Parquet write benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here we note that writing `arrow::DictionaryArray` is dramatically faster due\n+to the optimizations described above. We have achieved a small improvement in\n+writing dense (non-dictionary) binary arrays.\n+\n+Then, the reading benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_read_perf.png\"\n+     alt=\"Parquet read benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here, similarly reading `DictionaryArray` directly is many times faster.\n+\n+In these benchmarks we note that reading the dense binary data is slower in\n+master branch than in version 0.11.0, so we will need to do some profiling and\n+see what we can do to bring read performance back inline. Optimizing the dense\n+read path has not been too much of a priority relative to the dictionary read\n+path in this work.\n+\n+# Memory Use Improvements\n+\n+In addition to faster performance, reading columns as dictionary-encoded can\n+yield significantly less memory use.\n+\n+In the `dict-random` case above, we found that the master branch uses 405 MB of\n+RAM at peak while loading a 152 MB dataset. In v0.11.1, loading the same\n+Parquet file without the accelerated dictionary support uses 1.94 GB of peak\n+memory while the resulting non-dictionary table occupies 1.01 GB.\n \n Review comment:\n   Isn't there an even bigger difference when compared with 0.14?\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.772+0000",
                    "updated": "2019-09-03T19:11:53.772+0000",
                    "started": "2019-09-03T19:11:53.772+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305810",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305808",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320432044\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n \n Review comment:\n   lowercase \"pandas\" reads odd to start a sentence\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.772+0000",
                    "updated": "2019-09-03T19:11:53.772+0000",
                    "started": "2019-09-03T19:11:53.772+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305808",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305807",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320430467\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n+branch. We construct two kinds of Arrow tables with 10 columns each:\n+\n+* \"Low cardinality\" and \"high cardinality\" variants. The \"low cardinality\" case\n+  has 1,000 unique string values of 32-bytes each. The \"high cardinality\" has\n+  100,000 unique values\n+* \"Dense\" (non-dictionary) and \"Dictionary\" variants\n+\n+[Click here is the full benchmark script.][11]\n+\n+We show both single-threaded and multithreaded read performance. The test\n+machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical\n+cores and 32 virtual cores.\n+\n+First, the writing benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_write_perf.png\"\n+     alt=\"Parquet write benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here we note that writing `arrow::DictionaryArray` is dramatically faster due\n+to the optimizations described above. We have achieved a small improvement in\n+writing dense (non-dictionary) binary arrays.\n+\n+Then, the reading benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_read_perf.png\"\n+     alt=\"Parquet read benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here, similarly reading `DictionaryArray` directly is many times faster.\n+\n+In these benchmarks we note that reading the dense binary data is slower in\n \n Review comment:\n   ```suggestion\r\n   These benchmarks show that reading the dense binary data is slower in the\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.772+0000",
                    "updated": "2019-09-03T19:11:53.772+0000",
                    "started": "2019-09-03T19:11:53.772+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305807",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305809",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320430285\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n+branch. We construct two kinds of Arrow tables with 10 columns each:\n+\n+* \"Low cardinality\" and \"high cardinality\" variants. The \"low cardinality\" case\n+  has 1,000 unique string values of 32-bytes each. The \"high cardinality\" has\n+  100,000 unique values\n+* \"Dense\" (non-dictionary) and \"Dictionary\" variants\n+\n+[Click here is the full benchmark script.][11]\n+\n+We show both single-threaded and multithreaded read performance. The test\n+machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical\n+cores and 32 virtual cores.\n+\n+First, the writing benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_write_perf.png\"\n+     alt=\"Parquet write benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here we note that writing `arrow::DictionaryArray` is dramatically faster due\n \n Review comment:\n   ```suggestion\r\n   Writing `arrow::DictionaryArray` is dramatically faster due\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:53.772+0000",
                    "updated": "2019-09-03T19:11:53.772+0000",
                    "started": "2019-09-03T19:11:53.772+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305809",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305811",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320431812\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n \n Review comment:\n   I'd move this paragraph to after the list of Jiras. This is an odd way to start a \"summary\"\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T19:11:54.477+0000",
                    "updated": "2019-09-03T19:11:54.477+0000",
                    "started": "2019-09-03T19:11:54.476+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305811",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305835",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320453000\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the Parquet\n+specification.\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n+\n+All of these things together have produced some excellent performance results\n+that we will detail below.\n+\n+# Pushing Arrow columnar read and write lower in the Parquet stack\n+\n+The other class of optimizations we implemented was removing an abstraction\n+layer between the low-level Parquet column data encoder and decoder classes and\n+the Arrow columnar data structures. This involves:\n+\n+* Adding `ColumnWriter::WriteArrow` and `Encoder::Put` methods that accept\n+  `arrow::Array` objects directly\n+* Adding `ByteArrayDecoder::DecodeArrow` method to decode binary data directly\n+  into an `arrow::BinaryBuilder`.\n+\n+While the performance improvements from this work are less dramatic than for\n+dictionary-encoded data, they are still meaningful in real-world applications.\n+\n+# Performance Benchmarks\n+\n+We run some benchmarks comparing Arrow 0.11.1 with the current master\n+branch. We construct two kinds of Arrow tables with 10 columns each:\n+\n+* \"Low cardinality\" and \"high cardinality\" variants. The \"low cardinality\" case\n+  has 1,000 unique string values of 32-bytes each. The \"high cardinality\" has\n+  100,000 unique values\n+* \"Dense\" (non-dictionary) and \"Dictionary\" variants\n+\n+[Click here is the full benchmark script.][11]\n+\n+We show both single-threaded and multithreaded read performance. The test\n+machine is an Intel i9-9960X using gcc 8.3.0 (on Ubuntu 18.04) with 16 physical\n+cores and 32 virtual cores.\n+\n+First, the writing benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_write_perf.png\"\n+     alt=\"Parquet write benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here we note that writing `arrow::DictionaryArray` is dramatically faster due\n+to the optimizations described above. We have achieved a small improvement in\n+writing dense (non-dictionary) binary arrays.\n+\n+Then, the reading benchmarks:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903_parquet_read_perf.png\"\n+     alt=\"Parquet read benchmarks\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+Here, similarly reading `DictionaryArray` directly is many times faster.\n+\n+In these benchmarks we note that reading the dense binary data is slower in\n+master branch than in version 0.11.0, so we will need to do some profiling and\n+see what we can do to bring read performance back inline. Optimizing the dense\n+read path has not been too much of a priority relative to the dictionary read\n+path in this work.\n+\n+# Memory Use Improvements\n+\n+In addition to faster performance, reading columns as dictionary-encoded can\n+yield significantly less memory use.\n+\n+In the `dict-random` case above, we found that the master branch uses 405 MB of\n+RAM at peak while loading a 152 MB dataset. In v0.11.1, loading the same\n+Parquet file without the accelerated dictionary support uses 1.94 GB of peak\n+memory while the resulting non-dictionary table occupies 1.01 GB.\n \n Review comment:\n   I'll mention that 0.14.x has a nasty bug in it\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T20:01:37.307+0000",
                    "updated": "2019-09-03T20:01:37.307+0000",
                    "started": "2019-09-03T20:01:37.307+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305835",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305837",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320453511\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n \n Review comment:\n   Lowercase is the official styling of the project name =) \n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T20:03:04.254+0000",
                    "updated": "2019-09-03T20:03:04.254+0000",
                    "started": "2019-09-03T20:03:04.254+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305837",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/305862",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "nealrichardson commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r320463301\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,233 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on string-heavy data coming in Apache Arrow 0.15\"\n+date: \"2019-09-01 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string types, including native support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+We discuss the work that was done and show benchmarks comparing Arrow 0.11.0\n+(released in October, 2018) with the current development version (to be\n+released soon as Arrow 0.15.0).\n+\n+# Summary of work\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n \n Review comment:\n   Yes, I figured, just observing that it looks odd as the start of a sentence, e.e. cummings style.\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-03T20:27:36.009+0000",
                    "updated": "2019-09-03T20:27:36.009+0000",
                    "started": "2019-09-03T20:27:36.009+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "305862",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/306602",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "wesm commented on issue #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#issuecomment-528024557\n \n \n   I'll address the comments and update the benchmark results with ARROW-6417 taken into account so we can publish this ~tomorrow\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-04T18:23:10.092+0000",
                    "updated": "2019-09-04T18:23:10.092+0000",
                    "started": "2019-09-04T18:23:10.091+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "306602",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/307200",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r321242756\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,238 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on dictionary-encoded string data coming in Apache Arrow 0.15\"\n+date: \"2019-09-05 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string data, with new \"native\" support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+This post reviews work that was done and shows benchmarks comparing Arrow\n+0.12.1 with the current development version (to be released soon as Arrow\n+0.15.0).\n+\n+# Summary of work\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n \n Review comment:\n   ```suggestion\r\n   Direct writing of `arrow::DictionaryArray` to Parquet column writers ([ARROW-3246][5])\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-05T14:39:26.366+0000",
                    "updated": "2019-09-05T14:39:26.366+0000",
                    "started": "2019-09-05T14:39:26.365+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "307200",
                    "issueId": "13254317"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/worklog/307201",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "hatemhelal commented on pull request #19: ARROW-6419: [Website] Blog post about Parquet C++ read performance improvements in Arrow 0.15\nURL: https://github.com/apache/arrow-site/pull/19#discussion_r321296528\n \n \n\n ##########\n File path: _posts/2019-09-03-faster-strings-cpp-parquet.md\n ##########\n @@ -0,0 +1,238 @@\n+---\n+layout: post\n+title: \"Faster C++ Apache Parquet performance on dictionary-encoded string data coming in Apache Arrow 0.15\"\n+date: \"2019-09-05 00:00:00 -0600\"\n+author: Wes McKinney\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+We have been implementing a series of optimizations in the Apache Parquet C++\n+internals to improve read and write efficiency (both performance and memory\n+use) for Arrow columnar binary and string data, with new \"native\" support for\n+Arrow's dictionary types. This should have a big impact on users of the C++,\n+MATLAB, Python, R, and Ruby interfaces to Parquet files.\n+\n+This post reviews work that was done and shows benchmarks comparing Arrow\n+0.12.1 with the current development version (to be released soon as Arrow\n+0.15.0).\n+\n+# Summary of work\n+\n+One of the largest and most complex optimizations involves encoding and\n+decoding Parquet files' internal dictionary-encoded data streams to and from\n+Arrow's in-memory dictionary-encoded `DictionaryArray`\n+representation. Dictionary encoding is a compression strategy in Parquet, and\n+there is no formal \"dictionary\" or \"categorical\" type. I will go into more\n+detail about this below.\n+\n+Some of the particular JIRA issues related to this work include:\n+\n+- Vectorize comparators for computing statistics ([PARQUET-1523][1])\n+- Read binary directly data directly into DictionaryBuilder<T>\n+  ([ARROW-3769][2])\n+- Writing Parquet's dictionary indices directly into DictionaryBuilder<T>\n+  ([ARROW-3772][3])\n+- Write dense (non-dictionary) Arrow arrays directly into Parquet data encoders\n+  ([ARROW-6152][4])\n+- Direct writing of arrow::DictionaryArray to Parquet column writers ([ARROW-3246][5])\n+- Supporting changing dictionaries ([ARROW-3144][6])\n+- Internal IO optimizations and improved raw `BYTE_ARRAY` encoding performance\n+  ([ARROW-4398][7])\n+\n+One of the challenges of developing the Parquet C++ library is that we\n+maintain low-level read and write APIs that do not involve the Arrow columnar\n+data structures. So we have had to take care to do Arrow-related optimizations\n+without impacting non-Arrow Parquet users, which includes database systems like\n+Clickhouse and Vertica.\n+\n+# Background: how Parquet files do dictionary encoding\n+\n+Many direct and indirect users of Apache Arrow use dictionary encoding to\n+improve performance and memory use on binary or string data types that include\n+many repeated values. pandas users will know this as the [Categorical type][8]\n+while in R such encoding is known as [`factor`][9]. In the Arrow C++ library\n+and various bindings we have the `DictionaryArray` object for representing such\n+data in memory.\n+\n+For example, an array such as\n+\n+```\n+['apple', 'orange', 'apple', NULL, 'orange', 'orange']\n+```\n+\n+has dictionary-encoded form\n+\n+```\n+dictionary: ['apple', 'orange']\n+indices: [0, 1, 0, NULL, 1, 1]\n+```\n+\n+The [Parquet format uses dictionary encoding][10] to compress data, and it is\n+used for all Parquet data types, not just binary or string data. Parquet\n+further uses bit-packing and run-length encoding (RLE) to compress the\n+dictionary indices, so if you had data like\n+\n+```\n+['apple', 'apple', 'apple', 'apple', 'apple', 'apple', 'orange']\n+```\n+\n+the indices would be encoded like\n+\n+```\n+[rle-run=(6, 0),\n+ bit-packed-run=[1]]\n+```\n+\n+The full details of the rle-bitpacking encoding are found in the [Parquet\n+specification][10].\n+\n+When writing a Parquet file, most implementations will use dictionary encoding\n+to compress a column until the dictionary itself reaches a certain size\n+threshold, usually around 1 megabyte. At this point, the column writer will\n+\"fall back\" to `PLAIN` encoding where values are written end-to-end in \"data\n+pages\" and then usually compressed with Snappy or Gzip. See the following rough\n+diagram:\n+\n+<div align=\"center\">\n+<img src=\"{{ site.base-url }}/img/20190903-parquet-dictionary-column-chunk.png\"\n+     alt=\"Internal ColumnChunk structure\"\n+     width=\"80%\" class=\"img-responsive\">\n+</div>\n+\n+# Faster reading and writing of dictionary-encoded data\n+\n+When reading a Parquet file, the dictionary-encoded portions are usually\n+materialized to their non-dictionary-encoded form, causing binary or string\n+values to be duplicated in memory. So an obvious (but not trivial) optimization\n+is to skip this \"dense\" materialization. There are several issues to deal with:\n+\n+* A Parquet file often contains multiple ColumnChunks for each semantic column,\n+  and the dictionary values may be different in each ColumnChunk\n+* We must gracefully handle the \"fall back\" portion which is not\n+  dictionary-encoded\n+\n+We pursued several avenues to help with this:\n+\n+* Allowing each `arrow::DictionaryArray` to have a different dictionary\n+  (before, the dictionary was part of the `DictionaryType`, which caused\n+  problems)\n+* We enabled the Parquet dictionary indices to be directly written into an\n+  Arrow `DictionaryBuilder` without rehashing the data\n+* When decoding a ColumnChunk, we first append the dictionary values and\n+  indices into an Arrow `DictionaryBuilder`, and when we encounter the \"fall\n+  back\" portion we use a hash table to convert those values to\n+  dictionary-encoded form\n+* We override the \"fall back\" logic when writing a ColumnChunk from an\n+  `arrow::DictionaryArray` so that reading such data back is more efficient\n \n Review comment:\n   Same thing here:\r\n   \r\n   ```suggestion\r\n    `DictionaryArray` so that reading such data back is more efficient\r\n   ```\n \n----------------------------------------------------------------\nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n \nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2019-09-05T14:39:26.930+0000",
                    "updated": "2019-09-05T14:39:26.930+0000",
                    "started": "2019-09-05T14:39:26.930+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "307201",
                    "issueId": "13254317"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/2",
            "id": "2",
            "description": "A new feature of the product, which has yet to be developed.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21141&avatarType=issuetype",
            "name": "New Feature",
            "subtask": false,
            "avatarId": 21141
        },
        "timespent": 13200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@5e25acc5[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2ed1d807[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@475ab678[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@676b5f74[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3dc414d7[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@5a74831c[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@488f075c[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@75a271c6[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@68ddb109[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@16cfb36d[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@26cae30f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@40809a51[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 13200,
        "customfield_12312520": null,
        "customfield_12312521": "Thu Sep 05 17:13:31 UTC 2019",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2019-09-11T01:00:27.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-6419/watchers",
            "watchCount": 2,
            "isWatching": false
        },
        "created": "2019-09-03T03:06:25.000+0000",
        "updated": "2019-09-11T01:00:27.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "3h 40m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 13200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Website] Blog post about Parquet dictionary performance work coming in 0.15.x release",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/comment/16923518",
                    "id": "16923518",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Resolved in https://github.com/apache/arrow-site/commit/ba15c82c3d4447c3af93951477a7b973807dfe04",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-09-05T15:04:48.370+0000",
                    "updated": "2019-09-05T15:04:48.370+0000"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13254317/comment/16923625",
                    "id": "16923625",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "I'm going to rerun the benchmarks after reverting the jemalloc version in ARROW-6417, and only publish then...",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=wesm",
                        "name": "wesm",
                        "key": "wesmckinn",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?ownerId=wesmckinn&avatarId=29931",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&ownerId=wesmckinn&avatarId=29931",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&ownerId=wesmckinn&avatarId=29931",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&ownerId=wesmckinn&avatarId=29931"
                        },
                        "displayName": "Wes McKinney",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2019-09-05T17:13:31.482+0000",
                    "updated": "2019-09-05T17:13:31.482+0000"
                }
            ],
            "maxResults": 2,
            "total": 2,
            "startAt": 0
        },
        "customfield_12311820": "0|z069fk:",
        "customfield_12314139": null
    }
}