{
    "expand": "operations,versionedRepresentations,editmeta,changelog,renderedFields",
    "id": "13422769",
    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769",
    "key": "ARROW-15339",
    "fields": {
        "fixVersions": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/version/12350591",
                "id": "12350591",
                "description": "",
                "name": "7.0.0",
                "archived": false,
                "released": true,
                "releaseDate": "2022-02-03"
            }
        ],
        "resolution": {
            "self": "https://issues.apache.org/jira/rest/api/2/resolution/1",
            "id": "1",
            "description": "A fix for this issue is checked into the tree and tested.",
            "name": "Fixed"
        },
        "customfield_12312322": null,
        "customfield_12312323": null,
        "customfield_12312320": null,
        "customfield_12310420": "9223372036854775807",
        "customfield_12312321": null,
        "customfield_12312328": null,
        "customfield_12312329": null,
        "customfield_12312326": null,
        "customfield_12310300": null,
        "customfield_12312327": null,
        "customfield_12312324": null,
        "customfield_12312720": null,
        "customfield_12312325": null,
        "lastViewed": null,
        "priority": {
            "self": "https://issues.apache.org/jira/rest/api/2/priority/3",
            "iconUrl": "https://issues.apache.org/jira/images/icons/priorities/major.svg",
            "name": "Major",
            "id": "3"
        },
        "labels": [],
        "customfield_12312333": null,
        "customfield_12312334": null,
        "customfield_12313422": "false",
        "customfield_12310310": "0.0",
        "customfield_12312331": null,
        "customfield_12312332": null,
        "aggregatetimeoriginalestimate": null,
        "timeestimate": 0,
        "customfield_12312330": null,
        "versions": [],
        "customfield_12311120": null,
        "customfield_12313826": null,
        "customfield_12312339": null,
        "issuelinks": [],
        "customfield_12313825": null,
        "assignee": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "customfield_12312337": null,
        "customfield_12313823": null,
        "customfield_12312338": null,
        "customfield_12311920": null,
        "customfield_12313822": null,
        "customfield_12312335": null,
        "customfield_12313821": null,
        "customfield_12312336": null,
        "customfield_12313820": null,
        "status": {
            "self": "https://issues.apache.org/jira/rest/api/2/status/5",
            "description": "A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.",
            "iconUrl": "https://issues.apache.org/jira/images/icons/statuses/resolved.png",
            "name": "Resolved",
            "id": "5",
            "statusCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/statuscategory/3",
                "id": 3,
                "key": "done",
                "colorName": "green",
                "name": "Done"
            }
        },
        "components": [
            {
                "self": "https://issues.apache.org/jira/rest/api/2/component/12332543",
                "id": "12332543",
                "name": "Website"
            }
        ],
        "customfield_12312026": null,
        "customfield_12312023": null,
        "customfield_12312024": null,
        "aggregatetimeestimate": 0,
        "customfield_12312022": null,
        "customfield_12310921": null,
        "customfield_12310920": "9223372036854775807",
        "customfield_12312823": null,
        "creator": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "subtasks": [],
        "reporter": {
            "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
            "name": "lidavidm",
            "key": "lidavidm",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
            },
            "displayName": "David Li",
            "active": true,
            "timeZone": "America/New_York"
        },
        "aggregateprogress": {
            "progress": 19200,
            "total": 19200,
            "percent": 100
        },
        "customfield_12313520": null,
        "customfield_12310250": null,
        "progress": {
            "progress": 19200,
            "total": 19200,
            "percent": 100
        },
        "customfield_12313924": null,
        "votes": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15339/votes",
            "votes": 0,
            "hasVoted": false
        },
        "worklog": {
            "startAt": 0,
            "maxResults": 20,
            "total": 32,
            "worklogs": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709160",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "github-actions[bot] commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1013267150\n\n\n   https://issues.apache.org/jira/browse/ARROW-15339\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T16:22:21.354+0000",
                    "updated": "2022-01-14T16:22:21.354+0000",
                    "started": "2022-01-14T16:22:21.353+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709160",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709161",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r784979644\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n+  </figcaption>\n+</figure>\n+\n+Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow.\n+For example, \u201clakehouse\u201d systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation.\n+Additionally, in-memory SQL-based query engines like [DuckDB][duckdb], which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries.\n+\n+## Summary and Acknowledgements\n+\n+<!-- TODO: is there documentation on how to deploy the plugin? Can't find anything on ceph.io -->\n\nReview comment:\n       Does anyone know where on the Ceph website this is documented? Worst case we could link to the integration_skyhook.sh script but I'd prefer to link some docs on how to deploy this.\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       CC @JayjeetAtGithub you mentioned you have an updated version of the CPU usage chart?\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n\nReview comment:\n       CC @JayjeetAtGithub where did this figure come from? I'd like to source it.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T16:23:55.835+0000",
                    "updated": "2022-01-14T16:23:55.835+0000",
                    "started": "2022-01-14T16:23:55.835+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709161",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709163",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1013269149\n\n\n   <details>\r\n   <summary>Rendered</summary>\r\n   \r\n   ![image](https://user-images.githubusercontent.com/327919/149549708-38734d78-edb4-4c91-85ff-790af730d754.png)\r\n   \r\n   </details>\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T16:24:45.705+0000",
                    "updated": "2022-01-14T16:24:45.705+0000",
                    "started": "2022-01-14T16:24:45.705+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709163",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709221",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r785046914\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       [cpu_client.pdf](https://github.com/apache/arrow-site/files/7872314/cpu_client.pdf)\r\n   \r\n   [cpu_storage.pdf](https://github.com/apache/arrow-site/files/7872315/cpu_storage_stacked.pdf)\r\n   \r\n   @lidavidm  Here are the updated plots. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T17:59:13.801+0000",
                    "updated": "2022-01-14T17:59:13.801+0000",
                    "started": "2022-01-14T17:59:13.800+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709221",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709224",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r785052449\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n\nReview comment:\n       @lidavidm AFAIK Carlos created it. @carlosm ?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T18:07:48.899+0000",
                    "updated": "2022-01-14T18:07:48.899+0000",
                    "started": "2022-01-14T18:07:48.899+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709224",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709225",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r785052449\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n\nReview comment:\n       @lidavidm AFAIK Carlos created it. @carlosmalt ?\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T18:08:20.300+0000",
                    "updated": "2022-01-14T18:08:20.300+0000",
                    "started": "2022-01-14T18:08:20.300+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709225",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709226",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r785053483\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n+  </figcaption>\n+</figure>\n+\n+Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow.\n+For example, \u201clakehouse\u201d systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation.\n+Additionally, in-memory SQL-based query engines like [DuckDB][duckdb], which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries.\n+\n+## Summary and Acknowledgements\n+\n+<!-- TODO: is there documentation on how to deploy the plugin? Can't find anything on ceph.io -->\n\nReview comment:\n       @carlosmalt do you know of some official documentation regarding this ? Unfortunately, I am not aware of any.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T18:09:37.100+0000",
                    "updated": "2022-01-14T18:09:37.100+0000",
                    "started": "2022-01-14T18:09:37.100+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709226",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709240",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r785077596\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       Thanks! I'll get those integrated. Is there a citation we should put for those?\r\n   \r\n   (Also they look a little less clear-cut than the previous one - is there info on the benchmark scenario? For instance, I assume the client CPU is still being consumed because there's actual work the client is doing?)\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T18:49:17.247+0000",
                    "updated": "2022-01-14T18:49:17.247+0000",
                    "started": "2022-01-14T18:49:17.246+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709240",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/709262",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1013395615\n\n\n   <details>\r\n   <summary>Re-rendered</summary>\r\n   \r\n   ![image](https://user-images.githubusercontent.com/327919/149573468-ae350404-d42b-4ed4-8f8f-75ed07c88f85.png)\r\n   \r\n   </details>\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-14T19:25:22.941+0000",
                    "updated": "2022-01-14T19:25:22.941+0000",
                    "started": "2022-01-14T19:25:22.941+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "709262",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/710791",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r787076997\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       No, there is no citation for those, those experiments were conducted by us. The client CPU is still getting consumed a little bit, because there is decompression from LZ4 record batches on the client side. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T19:22:48.637+0000",
                    "updated": "2022-01-18T19:22:48.637+0000",
                    "started": "2022-01-18T19:22:48.637+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710791",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/710794",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r787081435\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       Sending back compressed data is actually kind of important in Skyhook, because otherwise we would end up sending back uncompressed Arrow data which is a lot larger than Parquet bytes, and that harms performance adversely.  so, essentially, this is a sort of tradeoff we have to deal with. \n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T19:29:08.967+0000",
                    "updated": "2022-01-18T19:29:08.967+0000",
                    "started": "2022-01-18T19:29:08.967+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710794",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/710801",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r787089900\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n\nReview comment:\n       (I meant to put this on the figure above.) I see it in your blog post as well, so for now, I've cited that post.\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines. Figure sourced from TODO.\n+  </figcaption>\n+</figure>\n+\n+Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow.\n+For example, \u201clakehouse\u201d systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation.\n+Additionally, in-memory SQL-based query engines like [DuckDB][duckdb], which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries.\n+\n+## Summary and Acknowledgements\n+\n+<!-- TODO: is there documentation on how to deploy the plugin? Can't find anything on ceph.io -->\n\nReview comment:\n       I see instructions in your blog post, so for now, I'll link there.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T19:37:41.778+0000",
                    "updated": "2022-01-18T19:37:41.778+0000",
                    "started": "2022-01-18T19:37:41.778+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710801",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/710803",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r787090884\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       Ah, thanks. I've tweaked the wording here, if that looks good.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-18T19:38:14.884+0000",
                    "updated": "2022-01-18T19:38:14.884+0000",
                    "started": "2022-01-18T19:38:14.883+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "710803",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/713234",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "JayjeetAtGithub commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r790159565\n\n\n\n##########\nFile path: _posts/2022-01-21-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-1-21 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<!-- TODO: add the updated figure from Jayjeet -->\n\nReview comment:\n       I see it. Looks good. Thanks.\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-22T16:38:14.632+0000",
                    "updated": "2022-01-22T16:38:14.632+0000",
                    "started": "2022-01-22T16:38:14.632+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "713234",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714030",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drabastomek commented on a change in pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#discussion_r791146692\n\n\n\n##########\nFile path: _posts/2022-01-28-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-01-28 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n\nReview comment:\n       I'd drop 'here': \"While formats like Apache Parquet enable some optimizations, fundamentally, the responsibility is all on the client.\"\n\n##########\nFile path: _posts/2022-01-28-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-01-28 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n\nReview comment:\n       Rewrite? \"After decoding, filtering, and projecting, Ceph sends the Arrow record batches directly to the client,...\"\n\n##########\nFile path: _posts/2022-01-28-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-01-28 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption markdown=\"1\">\n+Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+(Figure sourced from [\u201cSkyhookDM is now a part of Apache Arrow!\u201d][medium].)\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n+For systems like Dask that use the Arrow Datasets API, this means that just by switching to the Skyhook file format, we can speed up dataset scans, reduce the amount of data that needs to be transferred, and free up CPU resources for computations.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-cpu.png\"\n+       alt=\"In benchmarks, Skyhook reduces client CPU usage while minimally impacting storage cluster CPU usage.\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption>\n+    Skyhook frees the client CPU to do useful work, while minimally impacting the work done by the storage machines.\n+    The client still does some work in decompressing the LZ4-compressed record batches sent by Skyhook.\n+    (Note that the storage cluster plot is cumulative.)\n+  </figcaption>\n+</figure>\n+\n+Of course, the ideas behind Skyhook apply to other systems adjacent to and beyond Apache Arrow.\n+For example, \u201clakehouse\u201d systems like Apache Iceberg and Delta Lake also build on distributed storage systems, and can naturally benefit from Skyhook to offload computation.\n+Additionally, in-memory SQL-based query engines like [DuckDB][duckdb], which integrate seamlessly with Apache Arrow, can benefit from Skyhook by offloading portions of SQL queries.\n+\n+## Summary and Acknowledgements\n+\n+Skyhook, available in Arrow 7.0, builds on research into programmable storage systems.\n\nReview comment:\n       Should we use the full version number? 7.0.0?\n\n##########\nFile path: _posts/2022-01-28-skyhook-bringing-computation-to-storage-with-apache-arrow.md\n##########\n@@ -0,0 +1,219 @@\n+---\n+layout: post\n+title: \"Skyhook: Bringing Computation to Storage with Apache Arrow\"\n+date: \"2022-01-28 00:00:00\"\n+author: Jayjeet Chakraborty, Carlos Maltzahn, David Li, Tom Drabas\n+categories: [application]\n+---\n+<!--\n+{% comment %}\n+Licensed to the Apache Software Foundation (ASF) under one or more\n+contributor license agreements.  See the NOTICE file distributed with\n+this work for additional information regarding copyright ownership.\n+The ASF licenses this file to you under the Apache License, Version 2.0\n+(the \"License\"); you may not use this file except in compliance with\n+the License.  You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+{% endcomment %}\n+-->\n+\n+CPUs, memory, storage, and network bandwidth get better every year, but increasingly, they\u2019re improving in different dimensions.\n+Processors are faster, but their memory bandwidth hasn\u2019t kept up; meanwhile, cloud computing has led to storage being separated from applications across a network link.\n+This divergent evolution means we need to rethink where and when we perform computation to best make use of the resources available to us.\n+\n+For example, when querying a dataset on a storage system like Ceph or Amazon S3, all the work of filtering data gets done by the client.\n+Data has to be transferred over the network, and then the client has to spend precious CPU cycles decoding it, only to throw it away in the end due to a filter.\n+While formats like Apache Parquet enable some optimizations here, fundamentally, the responsibility is all on the client.\n+Meanwhile, even though the storage system has its own compute capabilities, it\u2019s relegated to just serving \u201cdumb bytes\u201d.\n+\n+Thanks to the [Center for Research in Open Source Software][cross] (CROSS) at the University of California, Santa Cruz, Apache Arrow 7.0.0 includes Skyhook, an [Arrow Datasets][dataset] extension that solves this problem by using the storage layer to reduce client resource utilization.\n+We\u2019ll examine the developments surrounding Skyhook as well as how Skyhook works.\n+\n+## Introducing Programmable Storage\n+\n+Skyhook is an example of programmable storage: exposing higher-level functionality from storage systems for clients to build upon.\n+This allows us to make better use of existing resources (both hardware and development effort) in such systems, reduces the implementation burden of common operations for each client, and enables such operations to scale with the storage layer.\n+\n+Historically, big data systems like Apache Hadoop have tried to colocate computation and storage for efficiency.\n+More recently, cloud and distributed computing have disaggregated computation and storage for flexibility and scalability, but at a performance cost.\n+Programmable storage strikes a balance between these goals, allowing some operations to be run right next to the data while still keeping data and compute separate at a higher level.\n+\n+In particular, Skyhook builds on [Ceph][ceph], a distributed storage system that scales to exabytes of data while being reliable and flexible.\n+With its Object Class SDK, Ceph enables programmable storage by allowing extensions that define new object types with custom functionality.\n+\n+## Skyhook Architecture\n+\n+Let\u2019s look at how Skyhook applies these ideas.\n+Overall, the idea is simple: the client should be able to ask Ceph to perform basic operations like decoding files, filtering the data, and selecting columns.\n+That way, the work gets done using existing storage cluster resources, which means it\u2019s both adjacent to the data and can scale with the cluster size.\n+Also, this reduces the data transferred over the network, and of course reduces the client workload.\n+\n+On the storage system side, Skyhook uses the Ceph Object Class SDK to define scan operations on data stored in Parquet or Feather format.\n+To implement these operations, Skyhook first implements a file system shim in Ceph\u2019s object storage layer, then uses the existing filtering and projection capabilities of the Arrow Datasets library on top of that shim.\n+\n+Then, Skyhook defines a custom \u201cfile format\u201d in the Arrow Datasets layer.\n+Queries against such files get translated to direct requests to Ceph using those new operations, bypassing the traditional POSIX file system layer.\n+After decoding, filtering, and projection, Ceph directly sends the client Arrow record batches, minimizing CPU overhead for encoding/decoding\u2014another optimization Arrow makes possible.\n+The record batches use Arrow\u2019s compression support to further save bandwidth.\n+\n+<figure>\n+  <img src=\"{{ site.baseurl }}/img/20220121-skyhook-architecture.png\"\n+       alt=\"Skyhook Architecture\"\n+       width=\"100%\" class=\"img-responsive\">\n+  <figcaption markdown=\"1\">\n+Skyhook extends Ceph and Arrow Datasets to push queries down to Ceph, reducing the client workload and network traffic.\n+(Figure sourced from [\u201cSkyhookDM is now a part of Apache Arrow!\u201d][medium].)\n+  </figcaption>\n+</figure>\n+\n+Skyhook also optimizes how Parquet files in particular are stored.\n+Parquet files consist of a series of row groups, which each contain a chunk of the rows in a file.\n+When storing such files, Skyhook either pads or splits them so that each row group is stored as its own Ceph object.\n+By striping or splitting the file in this way, we can parallelize scanning at sub-file granularity across the Ceph nodes for further performance improvements.\n+\n+## Applications\n+\n+In benchmarks, Skyhook has minimal storage-side CPU overhead and virtually eliminates client-side CPU usage.\n+And scaling the storage cluster decreases query latency commensurately.\n\nReview comment:\n       I'd cut the 'And' from the beginning and start the sentence as \"Scaling the storage cluster ...\"\n\n\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-24T21:12:05.323+0000",
                    "updated": "2022-01-24T21:12:05.323+0000",
                    "started": "2022-01-24T21:12:05.323+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714030",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714034",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1020554643\n\n\n   Thanks for the comments @drabastomek, I've made those edits!\r\n   \r\n   @nealrichardson @westonpace any comments here? Should we wait for 7.0.0 before publishing, or just publish now?\r\n   \r\n   <details>\r\n   <summary>Re-re-rendered</summary>\r\n   \r\n   ![image](https://user-images.githubusercontent.com/327919/150866068-938b4b8c-9ec7-407e-858e-ba93d74bac8e.png)\r\n   \r\n   </details>\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-24T21:16:25.014+0000",
                    "updated": "2022-01-24T21:16:25.014+0000",
                    "started": "2022-01-24T21:16:25.013+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714034",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714058",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1020584252\n\n\n   This looks very cool.  One minor comment.  On the CPU usage plots the Y axis is \"Usage (%)\" for both rows of plots.  I am assuming the top row is \"client CPU\" and the bottom row is \"storage CPU\" but I have to guess.\r\n   \r\n   Also, there are lots of colors on those lines in the bottom-right chart.  Do those colors have any meaning?  I'm just guessing it is one line per \"storage node\" or something like that but then that begs the question why some lines are near 0% usage but others have very high usage.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-24T21:52:20.728+0000",
                    "updated": "2022-01-24T21:52:20.728+0000",
                    "started": "2022-01-24T21:52:20.727+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714058",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714062",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "lidavidm commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1020593118\n\n\n   Ah, yes. @JayjeetAtGithub I can manually edit the plots to add this info, but if it's possible to re-generate them it would be much appreciated.\r\n   \r\n   The bottom charts are cumulative, I'm assuming (so it's not 140% usage for \"purple\", it's 140% total usage for all nodes). I tried to note that in the caption but maybe it should be part of the chart itself as well.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-24T22:04:28.895+0000",
                    "updated": "2022-01-24T22:04:28.895+0000",
                    "started": "2022-01-24T22:04:28.895+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714062",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714065",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "drabastomek commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1020596123\n\n\n   I think we might want either describe or apply label to the y-axis of the charts in the left column as well, describing client (top, I assume) and storage (bottom) systems. Adding a legend (to storage charts) might look 'clunky' so perhaps just explanation under the chart that these lines represent individual storage nodes?\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-24T22:08:05.921+0000",
                    "updated": "2022-01-24T22:08:05.921+0000",
                    "started": "2022-01-24T22:08:05.921+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714065",
                    "issueId": "13422769"
                },
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/worklog/714677",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=githubbot",
                        "name": "githubbot",
                        "key": "githubbot",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "ASF GitHub Bot",
                        "active": true,
                        "timeZone": "Etc/UTC"
                    },
                    "comment": "westonpace commented on pull request #179:\nURL: https://github.com/apache/arrow-site/pull/179#issuecomment-1020584252\n\n\n   This looks very cool.  One minor comment.  On the CPU usage plots the Y axis is \"Usage (%)\" for both rows of plots.  I am assuming the top row is \"client CPU\" and the bottom row is \"storage CPU\" but I have to guess.\r\n   \r\n   Also, there are lots of colors on those lines in the bottom-right chart.  Do those colors have any meaning?  I'm just guessing it is one line per \"storage node\" or something like that but then that begs the question why some lines are near 0% usage but others have very high usage.\n\n\n-- \nThis is an automated message from the Apache Git Service.\nTo respond to the message, please log on to GitHub and use the\nURL above to go to the specific comment.\n\nTo unsubscribe, e-mail: commits-unsubscribe@arrow.apache.org\n\nFor queries about this service, please contact Infrastructure at:\nusers@infra.apache.org\n",
                    "created": "2022-01-25T16:38:24.130+0000",
                    "updated": "2022-01-25T16:38:24.130+0000",
                    "started": "2022-01-25T16:38:24.130+0000",
                    "timeSpent": "10m",
                    "timeSpentSeconds": 600,
                    "id": "714677",
                    "issueId": "13422769"
                }
            ]
        },
        "customfield_12313920": null,
        "issuetype": {
            "self": "https://issues.apache.org/jira/rest/api/2/issuetype/4",
            "id": "4",
            "description": "An improvement or enhancement to an existing feature or task.",
            "iconUrl": "https://issues.apache.org/jira/secure/viewavatar?size=xsmall&avatarId=21140&avatarType=issuetype",
            "name": "Improvement",
            "subtask": false,
            "avatarId": 21140
        },
        "timespent": 19200,
        "customfield_12314020": "{summaryBean=com.atlassian.jira.plugin.devstatus.rest.SummaryBean@18346357[summary={pullrequest=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@75f60575[overall=PullRequestOverallBean{stateCount=0, state='OPEN', details=PullRequestOverallDetails{openCount=0, mergedCount=0, declinedCount=0}},byInstanceType={}], build=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@57a330b8[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BuildOverallBean@123b5a94[failedBuildCount=0,successfulBuildCount=0,unknownBuildCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], review=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@3930662f[overall=com.atlassian.jira.plugin.devstatus.summary.beans.ReviewsOverallBean@6319da63[stateCount=0,state=<null>,dueDate=<null>,overDue=false,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], deployment-environment=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@2df10048[overall=com.atlassian.jira.plugin.devstatus.summary.beans.DeploymentOverallBean@65560098[topEnvironments=[],showProjects=false,successfulCount=0,count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], repository=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@5fed0303[overall=com.atlassian.jira.plugin.devstatus.summary.beans.CommitOverallBean@728a005f[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}], branch=com.atlassian.jira.plugin.devstatus.rest.SummaryItemBean@575cad06[overall=com.atlassian.jira.plugin.devstatus.summary.beans.BranchOverallBean@25b25b5[count=0,lastUpdated=<null>,lastUpdatedTimestamp=<null>],byInstanceType={}]},errors=[],configErrors=[]], devSummaryJson={\"cachedValue\":{\"errors\":[],\"configErrors\":[],\"summary\":{\"pullrequest\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":\"OPEN\",\"details\":{\"openCount\":0,\"mergedCount\":0,\"declinedCount\":0,\"total\":0},\"open\":true},\"byInstanceType\":{}},\"build\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"failedBuildCount\":0,\"successfulBuildCount\":0,\"unknownBuildCount\":0},\"byInstanceType\":{}},\"review\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"stateCount\":0,\"state\":null,\"dueDate\":null,\"overDue\":false,\"completed\":false},\"byInstanceType\":{}},\"deployment-environment\":{\"overall\":{\"count\":0,\"lastUpdated\":null,\"topEnvironments\":[],\"showProjects\":false,\"successfulCount\":0},\"byInstanceType\":{}},\"repository\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}},\"branch\":{\"overall\":{\"count\":0,\"lastUpdated\":null},\"byInstanceType\":{}}}},\"isStale\":false}}",
        "customfield_12314141": null,
        "customfield_12314140": null,
        "project": {
            "self": "https://issues.apache.org/jira/rest/api/2/project/12319525",
            "id": "12319525",
            "key": "ARROW",
            "name": "Apache Arrow",
            "projectTypeKey": "software",
            "avatarUrls": {
                "48x48": "https://issues.apache.org/jira/secure/projectavatar?pid=12319525&avatarId=10011",
                "24x24": "https://issues.apache.org/jira/secure/projectavatar?size=small&pid=12319525&avatarId=10011",
                "16x16": "https://issues.apache.org/jira/secure/projectavatar?size=xsmall&pid=12319525&avatarId=10011",
                "32x32": "https://issues.apache.org/jira/secure/projectavatar?size=medium&pid=12319525&avatarId=10011"
            },
            "projectCategory": {
                "self": "https://issues.apache.org/jira/rest/api/2/projectCategory/13960",
                "id": "13960",
                "description": "Apache Arrow",
                "name": "Arrow"
            }
        },
        "aggregatetimespent": 19200,
        "customfield_12312520": null,
        "customfield_12312521": "Mon Jan 31 19:40:23 UTC 2022",
        "customfield_12314422": null,
        "customfield_12314421": null,
        "customfield_12314146": null,
        "customfield_12314420": null,
        "customfield_12314145": null,
        "customfield_12314144": null,
        "customfield_12314143": null,
        "resolutiondate": "2022-01-31T19:40:23.000+0000",
        "workratio": -1,
        "customfield_12312923": null,
        "customfield_12312920": null,
        "customfield_12312921": null,
        "watches": {
            "self": "https://issues.apache.org/jira/rest/api/2/issue/ARROW-15339/watchers",
            "watchCount": 1,
            "isWatching": false
        },
        "created": "2022-01-14T16:20:59.000+0000",
        "updated": "2022-01-31T23:22:52.000+0000",
        "timeoriginalestimate": null,
        "description": null,
        "customfield_10010": null,
        "timetracking": {
            "remainingEstimate": "0h",
            "timeSpent": "5h 20m",
            "remainingEstimateSeconds": 0,
            "timeSpentSeconds": 19200
        },
        "customfield_12314523": null,
        "customfield_12314127": null,
        "customfield_12314522": null,
        "customfield_12314126": null,
        "customfield_12314521": null,
        "customfield_12314125": null,
        "customfield_12314520": null,
        "customfield_12314124": null,
        "attachment": [],
        "customfield_12312340": null,
        "customfield_12314123": null,
        "customfield_12312341": null,
        "customfield_12312220": null,
        "customfield_12314122": null,
        "customfield_12314121": null,
        "customfield_12314120": null,
        "customfield_12314129": null,
        "customfield_12314524": null,
        "customfield_12314128": null,
        "summary": "[Website] Add Skyhook blog post",
        "customfield_12314130": null,
        "customfield_12310291": null,
        "customfield_12310290": null,
        "customfield_12314138": null,
        "customfield_12314137": null,
        "environment": null,
        "customfield_12314136": null,
        "customfield_12314135": null,
        "customfield_12311020": null,
        "customfield_12314134": null,
        "duedate": null,
        "customfield_12314132": null,
        "customfield_12314131": null,
        "comment": {
            "comments": [
                {
                    "self": "https://issues.apache.org/jira/rest/api/2/issue/13422769/comment/17484895",
                    "id": "17484895",
                    "author": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "body": "Issue resolved by pull request 179\n[https://github.com/apache/arrow-site/pull/179]",
                    "updateAuthor": {
                        "self": "https://issues.apache.org/jira/rest/api/2/user?username=lidavidm",
                        "name": "lidavidm",
                        "key": "lidavidm",
                        "avatarUrls": {
                            "48x48": "https://issues.apache.org/jira/secure/useravatar?avatarId=10452",
                            "24x24": "https://issues.apache.org/jira/secure/useravatar?size=small&avatarId=10452",
                            "16x16": "https://issues.apache.org/jira/secure/useravatar?size=xsmall&avatarId=10452",
                            "32x32": "https://issues.apache.org/jira/secure/useravatar?size=medium&avatarId=10452"
                        },
                        "displayName": "David Li",
                        "active": true,
                        "timeZone": "America/New_York"
                    },
                    "created": "2022-01-31T19:40:23.547+0000",
                    "updated": "2022-01-31T19:40:23.547+0000"
                }
            ],
            "maxResults": 1,
            "total": 1,
            "startAt": 0
        },
        "customfield_12311820": "0|z0ylq8:",
        "customfield_12314139": null
    }
}