{
    "issue": {
        "title": "[C++] Increase flatbuffers max_tables parameter in order to read wide tables",
        "body": "***Note**: This issue was originally created as [ARROW-10056](https://issues.apache.org/jira/browse/ARROW-10056). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\npyarrow writes an invalid Feather v2 file, which it can't read afterwards.\r\n\r\n\r\n```java\n\r\n    OSError: Verification of flatbuffer-encoded Footer failed.\r\n```\r\n\r\n\r\nThe following code reproduces the problem for me:\r\n\r\n```python\n\r\nimport pyarrow as pa\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nnbr_regions = 1223024\r\nnbr_motifs = 4891\r\n\r\n# Create (big) dataframe.\r\ndf = pd.DataFrame(\r\n    np.arange(nbr_regions * nbr_motifs, dtype=np.float32).reshape((nbr_regions, nbr_motifs)),\r\n    index=pd.Index(['region' + str(i) for i in range(nbr_regions)], name='regions'),\r\n    columns=pd.Index(['motif' + str(i) for i in range(nbr_motifs)], name='motifs')\r\n)\r\n\r\n# Transpose dataframe\r\ndf_transposed = df.transpose()\r\n\r\n# Write transposed dataframe to Feather v2 format.\r\npf.write_feather(df_transposed, 'df_transposed.feather')\r\n\r\n# Trying to read the transposed dataframe from Feather v2 format, results in this error:\r\ndf_transposed_read = pf.read_feather('df_transposed.feather')\r\n```\r\n\r\n```python\n\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-64-b41ad5157e77> in <module>\r\n----> 1 df_transposed_read = pf.read_feather('df_transposed.feather')\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_feather(source, columns, use_threads, memory_map)\r\n    213     \"\"\"\r\n    214     _check_pandas_version()\r\n--> 215     return (read_table(source, columns=columns, memory_map=memory_map)\r\n    216             .to_pandas(use_threads=use_threads))\r\n    217\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_table(source, columns, memory_map)\r\n    235     \"\"\"\r\n    236     reader = ext.FeatherReader()\r\n--> 237     reader.open(source, use_memory_map=memory_map)\r\n    238\r\n    239     if columns is None:\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.pxi in pyarrow.lib.FeatherReader.open()\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nOSError: Verification of flatbuffer-encoded Footer failed.\r\n```\r\n\r\nLater I discovered that it happens also if the original dataframe is created in the transposed order:\r\n```python\n\r\n# Create (big) dataframe.\r\ndf_without_transpose = pd.DataFrame(\r\n    np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions)),\r\n    index=pd.Index(['motif' + str(i) for i in range(nbr_motifs)], name='motifs'),\r\n    columns=pd.Index(['region' + str(i) for i in range(nbr_regions)], name='regions'),\r\n)\r\n\r\npf.write_feather(df_without_transpose, 'df_without_transpose.feather')\r\n\r\ndf_without_transpose_read = pf.read_feather('df_without_transpose.feather')\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-91-3cdad1d58c35> in <module>\r\n----> 1 df_without_transpose_read = pf.read_feather('df_without_transpose.feather')\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_feather(source, columns, use_threads, memory_map)\r\n    213     \"\"\"\r\n    214     _check_pandas_version()\r\n--> 215     return (read_table(source, columns=columns, memory_map=memory_map)\r\n    216             .to_pandas(use_threads=use_threads))\r\n    217\r\n\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_table(source, columns, memory_map)\r\n    235     \"\"\"\r\n    236     reader = ext.FeatherReader()\r\n--> 237     reader.open(source, use_memory_map=memory_map)\r\n    238\r\n    239     if columns is None:\r\n\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.pxi in pyarrow.lib.FeatherReader.open()\r\n\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nOSError: Verification of flatbuffer-encoded Footer failed.\r\n```\r\n\r\nWriting to Feather v1 format works:\r\n```python\n\r\npf.write_feather(df_transposed, 'df_transposed.v1.feather', version=1)\r\n\r\ndf_transposed_read_v1 = pf.read_feather('df_transposed.v1.feather')\r\n\r\n# Now do the same, but also save the index in the Feather v1 file.\r\ndf_transposed_reset_index = df_transposed.reset_index()\r\n\r\npf.write_feather(df_transposed_reset_index, 'df_transposed_reset_index.v1.feather', version=1)\r\n\r\ndf_transposed_reset_index_read_v1 = pf.read_feather('df_transposed_reset_index.v1.feather')\r\n\r\n# Returns True\r\ndf_transposed_reset_index_read_v1.equals(df_transposed)\r\n```\r\n\r\n",
        "created_at": "2020-09-21T17:16:35.000Z",
        "updated_at": "2021-02-08T18:47:24.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-01-29T20:03:15.000Z"
    },
    "comments": [
        {
            "created_at": "2020-09-21T18:10:49.024Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17199562) by Gert Hulselmans (ghuls):*\nI was able to make a reduced test case.\r\n\r\npyarrow fails to write a Feather v2 file when we have 499999 (or more) columns in our dataframe.\r\n```python\n\r\ndef feather_bisect(nbr_motifs, nbr_regions):\r\n    print(f'shape: ({nbr_motifs}, {nbr_regions})')\r\n    df_bisect = pd.DataFrame(\r\n        np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions)),\r\n        index=pd.Index(['motif' + str(i) for i in range(nbr_motifs)], name='motifs'),\r\n        columns=pd.Index(['region' + str(i) for i in range(nbr_regions)], name='regions'),\r\n    )\r\n\r\n    print('write')\r\n    pf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n\r\n    print('read')\r\n    df_bisect_read = pf.read_feather('df_bisect.feather')\r\n\r\n    df_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\n    print(df_bisect_read.equals(df_bisect))\r\n```\r\n```python\n\r\nIn [159]: feather_bisect(1, 500000-2)\r\nshape: (1, 499998)\r\nwrite\r\nread\r\nTrue\r\n\r\nIn [160]: feather_bisect(1, 500000-1)\r\nshape: (1, 499999)\r\nwrite\r\nread\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-160-ace2792e890e> in <module>\r\n----> 1 feather_bisect(1, 500000-1)\r\n\r\n<ipython-input-157-a5289fbb6df8> in feather_bisect(nbr_motifs, nbr_regions)\r\n     11\r\n     12     print('read')\r\n---> 13     df_bisect_read = pf.read_feather('df_bisect.feather')\r\n     14\r\n     15     df_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_feather(source, columns, use_threads, memory_map)\r\n    213     \"\"\"\r\n    214     _check_pandas_version()\r\n--> 215     return (read_table(source, columns=columns, memory_map=memory_map)\r\n    216             .to_pandas(use_threads=use_threads))\r\n    217\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in read_table(source, columns, memory_map)\r\n    235     \"\"\"\r\n    236     reader = ext.FeatherReader()\r\n--> 237     reader.open(source, use_memory_map=memory_map)\r\n    238\r\n    239     if columns is None:\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.pxi in pyarrow.lib.FeatherReader.open()\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nOSError: Verification of flatbuffer-encoded Footer failed.\r\n```"
        },
        {
            "created_at": "2020-09-22T05:17:26.720Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17199813) by Micah Kornfield (emkornfield@gmail.com):*\nIf I had to guess there is an overflow someplace in the metadata writing that we aren't catching (or maybe a limit on flatbuffers somehow).\u00a0 I'm curious do you run into the problem at a later point if you shorten the column names?"
        },
        {
            "created_at": "2020-09-22T07:14:50.044Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17199885) by Gert Hulselmans (ghuls):*\nIt seems like the length of the column names does not matter:\r\n```python\n\r\ndef feather_bisect_long_names(nbr_motifs, nbr_regions):\r\n    print(f'shape: ({nbr_motifs}, {nbr_regions})')\r\n    df_bisect = pd.DataFrame(\r\n        np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions)),\r\n        index=pd.Index(['motif' * 100 + str(i) for i in range(nbr_motifs)], name='motifs'),\r\n        columns=pd.Index(['region' * 100 + str(i) for i in range(nbr_regions)], name='regions'),\r\n    )\r\n\r\n    print('write')\r\n    pf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n\r\n    print('read')\r\n    df_bisect_read = pf.read_feather('df_bisect.feather')\r\n\r\n    df_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\n    print(df_bisect_read.equals(df_bisect))\r\n\r\ndef feather_bisect_short_names(nbr_motifs, nbr_regions):\r\n    print(f'shape: ({nb_motifs}, {nbr_regions})')\r\n    df_bisect = pd.DataFrame(\r\n        np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions)),\r\n        index=pd.Index(['m' + str(i) for i in range(nbr_motifs)], name='motifs'),\r\n        columns=pd.Index(['r' + str(i) for i in range(nbr_regions)], name='regions'),\r\n    )\r\n\r\n    print('write')\r\n    pf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n\r\n    print('read')\r\n    df_bisect_read = pf.read_feather('df_bisect.feather')\r\n\r\n    df_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\n    print(df_bisect_read.equals(df_bisect))\r\n\r\n\r\ndef feather_bisect_no_names(nbr_motifs, nbr_regions):\r\n    print(f'shape: ({nb_motifs}, {nbr_regions})')\r\n    df_bisect = pd.DataFrame(\r\n        np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions))\r\n    )\r\n\r\n    print('write')\r\n    pf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n\r\n    print('read')\r\n    df_bisect_read = pf.read_feather('df_bisect.feather')\r\n\r\n    df_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\n    print(df_bisect_read.equals(df_bisect))\r\n```\r\nAll work with 499998 but not with 499999 columns.\r\n\r\nI also managed to get another error message (pointing to an overflow) when I constructed very big column names. If the same happens above (wrong calculation of buffer size, but not a negative number), it might explain the corruption of the written Feather v2 file.\r\n```python\n\r\ndef feather_bisect_very_long_names(nbr_motifs, nbr_regions):\r\n    print(f'shape: ({nbr_motifs}, {nbr_regions})')\r\n    df_bisect = pd.DataFrame(\r\n        np.arange(nbr_motifs * nbr_regions, dtype=np.float32).reshape((nbr_motifs, nbr_regions)),\r\n        index=pd.Index(['motif' * 10000 + str(i) for i in range(nbr_motifs)], name='motifs'),\r\n        columns=pd.Index(['region' * 10000 + str(i) for i in range(nbr_regions)], name='regions'),\r\n    )\r\n\r\nprint('write')\r\npf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n\r\nprint('read')\r\ndf_bisect_read = pf.read_feather('df_bisect.feather')\r\n\r\ndf_bisect_read.set_index(pd.Index(['motif0'], name='motifs'), inplace=True)\r\n\r\nprint(df_bisect_read.equals(df_bisect))\r\n\r\nIn [174]: feather_bisect_very_long_names(1, 500000-2)\r\n shape: (1, 499998)\r\n write\r\n ---------------------------------------------------------------------------\r\n ArrowInvalid Traceback (most recent call last)\r\n <ipython-input-174-5a1ef546402a> in <module>\r\n ----> 1 feather_bisect(1, 500000-2)\r\n\r\n<ipython-input-173-dced00174027> in feather_bisect_very_long_names(nbr_motifs, nbr_regions)\r\n 8\r\n 9 print('write')\r\n ---> 10 pf.write_feather(df_bisect, 'df_bisect.feather', compression='uncompressed')\r\n 11\r\n 12 print('read')\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.py in write_feather(df, dest, compression, compression_level, chunksize, version)\r\n 180\r\n 181 try:\r\n --> 182 ext.write_feather(table, dest, compression=compression,\r\n 183 compression_level=compression_level,\r\n 184 chunksize=chunksize, version=version)\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/feather.pxi in pyarrow.lib.write_feather()\r\n\r\n/software/miniconda3/envs/pyarrow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Negative buffer resize: -114399492\r\n```\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-09-23T14:26:33.581Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17200855) by Joris Van den Bossche (jorisvandenbossche):*\nWhile simplifying the example a bit, I noticed that this is actually working:\r\n\r\n```python\n\r\nn_columns = 499999\r\ntable = pa.table([np.random.randn(1) for _ in range(n_columns)], names=['col' + str(i) for i in range(n_columns)])\r\n\r\nfrom pyarrow import feather\r\nfeather.write_feather(table, \"test_wide.feather\")\r\nresult = feather.read_table(\"test_wide.feather\")\r\n```\r\n\r\nSo it seems that it are the \"key-value metadata\" that are causing the issue (coming from the pandas->arrow conversion). So this is failing:\r\n\r\n\r\n```python\n\r\ntable = pa.table(pd.DataFrame(np.random.randn(1, n_columns), columns=['col' + str(i) for i in range(n_columns)]))\r\n\r\nfeather.write_feather(table, \"test_wide_pandas.feather\")\r\nresult = feather.read_table(\"test_wide_pandas.feather\")\r\n```\r\n\r\nbut this is working again (when removing the pandas metadata):\r\n\r\n```python\n\r\ntable2 = table.replace_schema_metadata()\r\n\r\nfeather.write_feather(table2, \"test_wide_pandas2.feather\")\r\nresult = feather.read_table(\"test_wide_pandas2.feather\")\r\n```"
        },
        {
            "created_at": "2020-09-23T14:38:51.312Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17200864) by Joris Van den Bossche (jorisvandenbossche):*\nSo it's clearly not the exact size of the schema metadata itself (otherwise the length of the column names would matter), but at least the presence of a large schema metadata triggers the issue."
        },
        {
            "created_at": "2020-10-20T08:15:48.094Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17217418) by Gert Hulselmans (ghuls):*\nDoes the pandas dataframe contain anything that is needed to easily reconstuct a pandas dataframe from a feather file, or is this metadata relatively useless?"
        },
        {
            "created_at": "2020-10-20T12:53:50.982Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17217586) by Joris Van den Bossche (jorisvandenbossche):*\nThe pandas metadata is required for a faithful roundtrip to cover many corner cases. But if you have a relatively simple dataframe (eg columns with floats), the pandas metadata should not be required. \r\n\r\n> [from ARROW-10344] Is this pandas metadata very useful to have in my case? My feather files just contain one string column (row indices) and for the rest I have just columns of int16, int32, float32 (all other columns have the same type in one feather file).\r\n\r\nFor those cases the pandas metadata shouldn't be very important. The only thing is that with the metadata present, the conversion to pandas will automatically set the string column again as the index. But this is of course something you can easily do yourself as well.\r\n"
        },
        {
            "created_at": "2020-10-20T20:02:52.336Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17217904) by Gert Hulselmans (ghuls):*\nI just tested the code you posted, and indeed this works:\r\n```python\n\r\nn_columns = 499999\r\ntable = pa.table([np.random.randn(1) for _ in range(n_columns)], names=['col' + str(i) for i in range(n_columns)])\r\n\r\nfrom pyarrow import feather\r\nfeather.write_feather(table, \"test_wide.feather\")\r\nresult = feather.read_table(\"test_wide.feather\")\r\n```\r\n\r\nBut with e.g. : \"n_columns = 599999\" it fails again. So it is not specific to pandas to table conversion."
        },
        {
            "created_at": "2020-11-11T16:35:14.964Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17230074) by Joris Van den Bossche (jorisvandenbossche):*\nComing back to this again: there is an inherent limitation of flatbuffers at play, I think. A single FlatBuffer has a max size of 2GB (see eg https://stackoverflow.com/questions/59185516/do-flatbuffers-have-size-limits for an explanation why). \r\n\r\nSo that means we have a theoretical max number of columns we can write to an IPC file (this number then depends on some things, like whether there are custom metadata present (like the pandas metadata), or the exact type (some types have more metadata)). But so even for plain floats with no custom metadata (like your last example) has an inherent max number of columns. \r\n\r\n(note this is only when serializing, the Table itself (in memory) can have more columns. But of course serializing is a critical aspect ..)\r\n\r\nThat said, we should still ensure we don't _write_ such invalid data, though, but already error while writing, I would say."
        },
        {
            "created_at": "2020-11-24T15:04:05.350Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17238201) by Gert Hulselmans (ghuls):*\nI am not sure that I am hitting the 2GB FlatBufffer limit as that would imply that each column (assuming we would have a 1 million columns) would occupy more than 2kb of space.\r\n\r\nIt seems more likely to me that I hit the max_depth = 64 or  max_tables = 1000000 limit\r\n\r\nAccording to https://groups.google.com/g/flatbuffers/c/JtDGnBPx9is, it seems like these parameters are changeable:\r\n```c++\n\r\n    /// To expand the capacity of a single buffer, _max_tables is set to 10000000\r\n    flatbuffers::uoffset_t _max_depth = 64;\r\n    flatbuffers::uoffset_t _max_tables = 10000000;\r\n    flatbuffers::Verifier verifier(buf, bufsize, _max_depth, _max_tables);\r\n    OK = OK && grl::flatbuffer::VerifyKUKAiiwaStatesBuffer(verifier);\r\n```\r\n"
        },
        {
            "created_at": "2020-11-24T15:11:27.445Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17238205) by Gert Hulselmans (ghuls):*\nI assume this line would need changing:\r\n\r\nhttps://github.com/apache/arrow/blob/72a0e96cb8d52502cfa0ee7b6ab01ec559642941/cpp/src/arrow/ipc/reader.cc#L1054"
        },
        {
            "created_at": "2020-12-16T12:22:35.564Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17250281) by Gert Hulselmans (ghuls):*\nI finally managed to compile pyarrow from git (didn't work properly for me a few weeks ago).\r\n\r\nAfter increasing the number of max_tables to 10_000_000 (default flatbuffer value: 1_000_000), writing to a feather file and reading it back works with:\r\n- arrow table with 4999999 (= 10000000 / 2 - 1) columns\n- pandas dataframe with 4999998 (= 10000000 / 2 - 2) columns\n  \n  According to https://groups.google.com/g/flatbuffers/c/JtDGnBPx9is max_tables can even be set to MAX_INT. For my usecase, almost 5 milion rows are enough, but a higher limit probably doesn't hurt.\n  \n  It seems like flatbuffer does not have issues writing feather files with more columns, only the reading of the feather file seems to check the number of columns.\n  \n  It would be great if writing feather files with more columns than 499999 is supported by the arrow library.\n  \n  {code:c++}\n  diff --git a/cpp/src/arrow/ipc/reader.cc b/cpp/src/arrow/ipc/reader.cc\n  index 3d855425c..a3c95ef4b 100644\n\u2014 a/cpp/src/arrow/ipc/reader.cc\r\n+++ b/cpp/src/arrow/ipc/reader.cc\r\n@@ -1051,7 +1051,7 @@ class RecordBatchFileReaderImpl : public RecordBatchFileReader {\r\n         file_->ReadAt(footer_offset_ - footer_length - file_end_size, footer_length));\r\n\r\n     auto data = footer_buffer_->data();\r\n-    flatbuffers::Verifier verifier(data, footer_buffer_->size(), 128);\n  +    flatbuffers::Verifier verifier(data, footer_buffer_->size(), 128, 10000000);\n       if (!flatbuf::VerifyFooterBuffer(verifier)) {\n         return Status::IOError(\"Verification of flatbuffer-encoded Footer failed.\");\n       }\n  ```Java\n  \n  {code:python}\n  import pyarrow as pa\n  import pyarrow.feather as pf\n  import numpy as np\n  \n  n_columns = 4999999\n  \n  print('make table')\n  table = pa.table([np.random.randn(1) for _ in range(n_columns)], names=['col' + str(i) for i in range(n_columns)])\n  \n  print('write feather file')\n  pf.write_feather(table, \"/tmp/test_wide.feather\")\n  del table\n  \n  print('read feather file and verify')\n  result = pf.read_table(\"/tmp/test_wide.feather\")\n  ```\n  \n  ```python\n  \n  import pyarrow as pa\n  import pyarrow.feather as pf\n  import numpy as np\n  import pandas as pd\n  \n  n_columns = 4999998\n  print('make table')\n  table = pa.table(pd.DataFrame(np.random.randn(1, n_columns), columns=['col' + str(i) for i in range(n_columns)]))\n  \n  print('write feather file')\n  pf.write_feather(table, \"/tmp/test_wide.feather\")\n  del table\n  \n  print('read feather file and verify')\n  result = pf.read_table(\"/tmp/test_wide.feather\")\n  ```\n  \u00a0"
        },
        {
            "created_at": "2021-01-12T09:39:55.242Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17263198) by Gert Hulselmans (ghuls):*\n`[~jorisvandenbossche]` Any chance that the default value for number of columns in `flatbuffers::Verifier verifier` will be increased (or can be set by the user) in the future?"
        },
        {
            "created_at": "2021-01-28T15:12:46.261Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17273794) by Gert Hulselmans (ghuls):*\nhttps://github.com/apache/arrow/pull/9349"
        },
        {
            "created_at": "2021-01-28T16:50:43.527Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17273848) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ghuls]` sorry for not reacting here earlier, but thanks a lot for the investigation and PR! "
        },
        {
            "created_at": "2021-01-28T17:42:37.675Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17273893) by Gert Hulselmans (ghuls):*\n`[~jorisvandenbossche]` No problem. I just hope it gets merged soon (so it will be in the next conda release) as I now have to write feather files in v1 format to avoid this problem. As some of this feather files are 200G, the compression option which is possible with Feather v2, is good to have."
        },
        {
            "created_at": "2021-01-29T20:03:15.717Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10056?focusedCommentId=17275298) by Neal Richardson (npr):*\nIssue resolved by pull request 9349\n<https://github.com/apache/arrow/pull/9349>"
        }
    ]
}