{
    "issue": {
        "title": "[Python] Flaky test test_write_dataset_max_open_files",
        "body": "***Note**: This issue was originally created as [ARROW-15438](https://issues.apache.org/jira/browse/ARROW-15438). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFound during 7.0.0 verification\r\n```\n\r\npyarrow/tests/test_dataset.py::test_write_dataset_max_open_files FAILED \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0[ 30%]\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> traceback >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>tempdir = PosixPath('/tmp/pytest-of-root/pytest-1/test_write_dataset_max_open_fi0')\u00a0 \u00a0 def test_write_dataset_max_open_files(tempdir):\r\n\u00a0 \u00a0 \u00a0 \u00a0 directory = tempdir / 'ds'\r\n\u00a0 \u00a0 \u00a0 \u00a0 file_format = \"parquet\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 partition_column_id = 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 column_names = ['c1', 'c2']\r\n\u00a0 \u00a0 \u00a0 \u00a0 record_batch_1 = pa.record_batch(data=[[1, 2, 3, 4, 0, 10],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0['a', 'b', 'c', 'd', 'e', 'a']],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0names=column_names)\r\n\u00a0 \u00a0 \u00a0 \u00a0 record_batch_2 = pa.record_batch(data=[[5, 6, 7, 8, 0, 1],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0['a', 'b', 'c', 'd', 'e', 'c']],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0names=column_names)\r\n\u00a0 \u00a0 \u00a0 \u00a0 record_batch_3 = pa.record_batch(data=[[9, 10, 11, 12, 0, 1],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0['a', 'b', 'c', 'd', 'e', 'd']],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0names=column_names)\r\n\u00a0 \u00a0 \u00a0 \u00a0 record_batch_4 = pa.record_batch(data=[[13, 14, 15, 16, 0, 1],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0['a', 'b', 'c', 'd', 'e', 'b']],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0names=column_names)\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 table = pa.Table.from_batches([record_batch_1, record_batch_2,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0record_batch_3, record_batch_4])\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 partitioning = ds.partitioning(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pa.schema([(column_names[partition_column_id], pa.string())]),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 flavor=\"hive\")\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 data_source_1 = directory / \"default\"\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 ds.write_dataset(data=table, base_dir=data_source_1,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0partitioning=partitioning, format=file_format)\r\n\u00a0 \u00a0\u00a0\r\n# Here we consider the number of unique partitions created when\r\n# partitioning column contains duplicate records.\r\n# \u00a0 Returns: (number_of_files_generated, number_of_partitions)\r\n\u00a0 \u00a0 \u00a0 \u00a0 def _get_compare_pair(data_source, record_batch, file_format, col_id):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_of_files_generated = _get_num_of_files_generated(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 base_directory=data_source, file_format=file_format)\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number_of_partitions = len(pa.compute.unique(record_batch[col_id]))\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return num_of_files_generated, number_of_partitions\r\n\u00a0 \u00a0\u00a0\r\n# CASE 1: when max_open_files=default & max_open_files >= num_of_partitions\r\n# \u00a0 \u00a0 \u00a0 \u00a0 In case of a writing to disk via partitioning based on a\r\n# \u00a0 \u00a0 \u00a0 \u00a0 particular column (considering row labels in that column),\r\n# \u00a0 \u00a0 \u00a0 \u00a0 the number of unique rows must be equal\r\n# \u00a0 \u00a0 \u00a0 \u00a0 to the number of files generated\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 num_of_files_generated, number_of_partitions \\\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = _get_compare_pair(data_source_1, record_batch_1, file_format,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 partition_column_id)\r\n\u00a0 \u00a0 \u00a0 \u00a0 assert num_of_files_generated == number_of_partitions\r\n\u00a0 \u00a0\u00a0\r\n# CASE 2: when max_open_files > 0 & max_open_files < num_of_partitions\r\n# \u00a0 \u00a0 \u00a0 \u00a0 the number of files generated must be greater than the number of\r\n# \u00a0 \u00a0 \u00a0 \u00a0 partitions\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 data_source_2 = directory / \"max_1\"\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 max_open_files = 3\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 ds.write_dataset(data=table, base_dir=data_source_2,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0partitioning=partitioning, format=file_format,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0max_open_files=max_open_files)\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 num_of_files_generated, number_of_partitions \\\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = _get_compare_pair(data_source_2, record_batch_1, file_format,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 partition_column_id)\r\n> \u00a0 \u00a0 \u00a0 assert num_of_files_generated > number_of_partitions\r\nE \u00a0 \u00a0 \u00a0 assert 5 > 5pyarrow/tests/test_dataset.py:3807: AssertionError\r\n \n```",
        "created_at": "2022-01-25T00:04:13.000Z",
        "updated_at": "2022-02-01T09:12:07.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-01-27T10:01:59.000Z"
    },
    "comments": [
        {
            "created_at": "2022-01-25T09:30:48.778Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481678) by Krisztian Szucs (kszucs):*\ncc `[~jorisvandenbossche]`"
        },
        {
            "created_at": "2022-01-25T09:37:41.759Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481682) by Joris Van den Bossche (jorisvandenbossche):*\nThis was added in ARROW-15019 (cc `[~vibhatha]`)"
        },
        {
            "created_at": "2022-01-25T09:41:54.949Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481686) by Vibhatha Lakmal Abeykoon (vibhatha):*\n`[~jorisvandenbossche]` will take a look.\u00a0"
        },
        {
            "created_at": "2022-01-25T11:28:56.433Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481751) by Vibhatha Lakmal Abeykoon (vibhatha):*\nI created a PR\r\n\r\n`[~westonpace]` \u00a0could this be happening due to the small data size?\u00a0\r\nI increased it and lowered the `max_open_files` and tested it.\u00a0\r\nSeems to be working, but can we fix this better. Am I missing something here?\u00a0"
        },
        {
            "created_at": "2022-01-25T13:43:23.497Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481829) by Antoine Pitrou (apitrou):*\nI also see this regularly on my local computer."
        },
        {
            "created_at": "2022-01-25T13:49:37.435Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481833) by Vibhatha Lakmal Abeykoon (vibhatha):*\n`[~apitrou]` \u00a0could it be due to the less data size?\u00a0"
        },
        {
            "created_at": "2022-01-25T13:50:11.122Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481834) by Vibhatha Lakmal Abeykoon (vibhatha):*\nSince the intial data size was too small? What I did was increase it a little bit and reduce the `max_open_files`.\u00a0"
        },
        {
            "created_at": "2022-01-25T14:31:15.757Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481856) by Antoine Pitrou (apitrou):*\nI have no idea. It may be timing-dependent?"
        },
        {
            "created_at": "2022-01-25T14:31:49.118Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481857) by Antoine Pitrou (apitrou):*\nIs parallel writing enabled by default? If so, disabling it would probably make the test more robust."
        },
        {
            "created_at": "2022-01-25T14:48:16.359Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17481868) by Vibhatha Lakmal Abeykoon (vibhatha):*\nYes, most probably. I changed `use_threads`=False and reduced the `max_open_files` = 1 it reduces the low number of file generation to an extent.\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-01-25T20:46:46.731Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17482100) by Weston Pace (westonpace):*\nThreading makes sense.  The WriteNode's InputReceived will be called 4 times (one per batch).  Each call will generate 5 calls to the dataset writer (one per partition in that batch).\r\n\r\nWe'd get more than 5 files if we get something like:\r\n\r\nB1P1, B1P2, B1P3, B1P4, B1P5, B2P1, ..., B4P5\r\n\r\nHowever, we'd only get 5 files if we get something like:\r\n\r\nB1P1, B2P1, B3P1, B4P1, B1P2, ... B4P5\r\n\r\nSince we scan the source in parallel both orders are possible.\r\n\r\n> Is parallel writing enabled by default? If so, disabling it would probably make the test more robust.\r\n\r\nThere is no easy way in the dataset writer to disable parallel writing (the CPU path is completely serial but it submits I/O tasks for each batch so you would need to shrink the I/O thread pool to size 1).\r\n\r\n>  I changed `use_threads`=False and reduced the `max_open_files` = 1 it reduces the low number of file generation to an extent. \r\n\r\nThis will disable parallel scanning which should be enough to prevent the flakiness (unless I am misunderstanding how the error is generated).  I'll try and setup a reproduction."
        },
        {
            "created_at": "2022-01-25T22:57:50.529Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17482134) by Weston Pace (westonpace):*\nI was able to reproduce fairly regularly and confirmed the issue was the order in which the batches were delivered to the dataset writer.  Unfortunately, we were also not completely respecting the `use_threads` option in `write_dataset`.  If `use_threads=False` then we would scan in serial but still send batches into the exec plan in parallel.  I added a new PR that sets `use_threads=False` (based on Vibhatha's PR) and also updates the Write method to be serial.\r\n\r\nI don't know if this bug should block the RC but if we cut another RC it would probably be nice to include."
        },
        {
            "created_at": "2022-01-25T23:27:59.505Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17482140) by Antoine Pitrou (apitrou):*\nThis is for the most part a flaky test, so while the fix is nice to have it shouldn't block the release IMHO."
        },
        {
            "created_at": "2022-01-27T10:01:59.982Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15438?focusedCommentId=17483025) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 12263\n<https://github.com/apache/arrow/pull/12263>"
        }
    ]
}