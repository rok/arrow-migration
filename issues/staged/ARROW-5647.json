{
    "issue": {
        "title": "[Python] Accessing a file from Databricks using pandas read_parquet using the pyarrow engine fails with : Passed non-file path: /mnt/aa/example.parquet ",
        "body": "***Note**: This issue was originally created as [ARROW-5647](https://issues.apache.org/jira/browse/ARROW-5647). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen trying to access a file using a mount point pointing to an Azure blob storage account the code fails with the following error:\r\n<font color=\"#8b0000\">OSError</font>: Passed non-file path: /mnt/aa/example.parquet\r\n<font color=\"#8b0000\">---------------------------------------------------------------------------</font> <font color=\"#8b0000\">OSError</font> Traceback (most recent call last) <font color=\"#006400\"><command-1848295812523966></font> in <font color=\"#4682b4\"><module></font><font color=\"#00008b\">()</font> <font color=\"#006400\">----> 1</font> pddf2 <font color=\"#AA4B00\">=</font> pd<font color=\"#AA4B00\">.</font>read_parquet<font color=\"#AA4B00\">(</font><font color=\"#00008b\">\"/mnt/aa/example.parquet\"</font><font color=\"#AA4B00\">,</font> engine<font color=\"#AA4B00\">=</font><font color=\"#00008b\">'pyarrow'</font><font color=\"#AA4B00\">)</font> <font color=\"#006400\"> 2</font> display<font color=\"#AA4B00\">(</font>pddf2<font color=\"#AA4B00\">)</font> <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pandas/io/parquet.py</font> in <font color=\"#4682b4\">read_parquet</font><font color=\"#00008b\">(path, engine, columns, \\*\\*kwargs)</font> <font color=\"#006400\"> 280</font> <font color=\"#006400\"> 281</font> impl <font color=\"#AA4B00\">=</font> get_engine<font color=\"#AA4B00\">(</font>engine<font color=\"#AA4B00\">)</font> <font color=\"#006400\">--> 282</font> <font color=\"#006400\">return</font> impl<font color=\"#AA4B00\">.</font>read<font color=\"#AA4B00\">(</font>path<font color=\"#AA4B00\">,</font> columns<font color=\"#AA4B00\">=</font>columns<font color=\"#AA4B00\">,</font> <font color=\"#AA4B00\">\\*\\*</font>kwargs<font color=\"#AA4B00\">)</font> <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pandas/io/parquet.py</font> in <font color=\"#4682b4\">read</font><font color=\"#00008b\">(self, path, columns, \\*\\*kwargs)</font> <font color=\"#006400\"> 127</font> kwargs<font color=\"#AA4B00\">[</font><font color=\"#00008b\">'use_pandas_metadata'</font><font color=\"#AA4B00\">]</font> <font color=\"#AA4B00\">=</font> <font color=\"#006400\">True</font> <font color=\"#006400\"> 128</font> result = self.api.parquet.read_table(path, columns=columns, <font color=\"#006400\">--> 129</font><font color=\"#AA4B00\"> \\*\\*kwargs).to_pandas() </font><font color=\"#006400\"> 130</font> <font color=\"#006400\">if</font> should_close<font color=\"#AA4B00\">:</font> <font color=\"#006400\"> 131</font> <font color=\"#006400\">try</font><font color=\"#AA4B00\">:</font> <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py</font> in <font color=\"#4682b4\">read_table</font><font color=\"#00008b\">(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, filesystem)</font> <font color=\"#006400\"> 1150</font> return fs.read_parquet(path, columns=columns, <font color=\"#006400\"> 1151</font> use_threads<font color=\"#AA4B00\">=</font>use_threads<font color=\"#AA4B00\">,</font> metadata<font color=\"#AA4B00\">=</font>metadata<font color=\"#AA4B00\">,</font> <font color=\"#006400\">-> 1152</font><font color=\"#AA4B00\"> use_pandas_metadata=use_pandas_metadata) </font><font color=\"#006400\"> 1153</font> <font color=\"#006400\"> 1154</font> pf <font color=\"#AA4B00\">=</font> ParquetFile<font color=\"#AA4B00\">(</font>source<font color=\"#AA4B00\">,</font> metadata<font color=\"#AA4B00\">=</font>metadata<font color=\"#AA4B00\">)</font> <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pyarrow/filesystem.py</font> in <font color=\"#4682b4\">read_parquet</font><font color=\"#00008b\">(self, path, columns, metadata, schema, use_threads, use_pandas_metadata)</font> <font color=\"#006400\"> 177</font> <font color=\"#006400\">from</font> pyarrow<font color=\"#AA4B00\">.</font>parquet <font color=\"#006400\">import</font> ParquetDataset <font color=\"#006400\"> 178</font> dataset = ParquetDataset(path, schema=schema, metadata=metadata, <font color=\"#006400\">--> 179</font><font color=\"#AA4B00\"> filesystem=self) </font><font color=\"#006400\"> 180</font> return dataset.read(columns=columns, use_threads=use_threads, <font color=\"#006400\"> 181</font> use_pandas_metadata=use_pandas_metadata) <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py</font> in <font color=\"#4682b4\">__init__</font><font color=\"#00008b\">(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, memory_map)</font> <font color=\"#006400\"> 933</font> self<font color=\"#AA4B00\">.</font>metadata_path<font color=\"#AA4B00\">)</font> <font color=\"#AA4B00\">=</font> _make_manifest<font color=\"#AA4B00\">(</font> <font color=\"#006400\"> 934</font> path_or_paths<font color=\"#AA4B00\">,</font> self<font color=\"#AA4B00\">.</font>fs<font color=\"#AA4B00\">,</font> metadata_nthreads<font color=\"#AA4B00\">=</font>metadata_nthreads<font color=\"#AA4B00\">,</font> <font color=\"#006400\">--> 935</font><font color=\"#AA4B00\"> open_file_func=self._open_file_func) </font><font color=\"#006400\"> 936</font> <font color=\"#006400\"> 937</font> <font color=\"#006400\">if</font> self<font color=\"#AA4B00\">.</font>common_metadata_path <font color=\"#006400\">is</font> <font color=\"#006400\">not</font> <font color=\"#006400\">None</font><font color=\"#AA4B00\">:</font> <font color=\"#006400\">/databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py</font> in <font color=\"#4682b4\">_make_manifest</font><font color=\"#00008b\">(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)</font> <font color=\"#006400\"> 1108</font> <font color=\"#006400\">if</font> <font color=\"#006400\">not</font> fs<font color=\"#AA4B00\">.</font>isfile<font color=\"#AA4B00\">(</font>path<font color=\"#AA4B00\">)</font><font color=\"#AA4B00\">:</font> <font color=\"#006400\"> 1109</font> raise IOError('Passed non-file path: \\{0}' <font color=\"#006400\">-> 1110</font><font color=\"#AA4B00\"> .format(path)) </font><font color=\"#006400\"> 1111</font> piece <font color=\"#AA4B00\">=</font> ParquetDatasetPiece<font color=\"#AA4B00\">(</font>path<font color=\"#AA4B00\">,</font> open_file_func<font color=\"#AA4B00\">=</font>open_file_func<font color=\"#AA4B00\">)</font> <font color=\"#006400\"> 1112</font> pieces<font color=\"#AA4B00\">.</font>append<font color=\"#AA4B00\">(</font>piece<font color=\"#AA4B00\">)</font> <font color=\"#8b0000\">OSError</font>: Passed non-file path: /mnt/aa/example.parquet\r\n\u00a0\r\nI am using the following code from a Databricks notebook to reproduce the issue:\r\n<font color=\"#005000\">%sh \r\n</font>\r\n<font color=\"#005000\">sudo apt-get -y install python3-pip\r\n/databricks/python3/bin/pip3 uninstall pandas -y\r\n/databricks/python3/bin/pip3 uninstall numpy -y</font>\r\n<font color=\"#005000\">{color:#b08000}/databricks/python3/bin/pip3 uninstall pyarrow -y</font>{color}\r\n\u00a0\r\n\u00a0\r\n<font color=\"#005000\">{color:#b08000}{color:#b08000}%sh \r\n/databricks/python3/bin/pip3 install numpy==1.14.0\r\n/databricks/python3/bin/pip3 install pandas==0.24.1\r\n/databricks/python3/bin/pip3 install pyarrow==0.13.0</font>{color}{color}\r\n\u00a0\r\n<font color=\"#005000\">{color:#b08000}{color:#b08000}{color:#b08000}dbutils.fs.mount(\r\n\u00a0 source = \"wasbs://<mycontainer>@<mystorageaccount>.blob.core.windows.net\",\r\n\u00a0 mount_point = \"/mnt/aa\",\r\n\u00a0 extra_configs = \\{\"fs.azure.account.key.<mystorageaccount>.blob.core.windows.net\":dbutils.secrets.get(scope = \"storage\", key = \"blob_key\")})</font>{color}{color}{color}\r\n\u00a0\r\n<font color=\"#005000\">{color:#b08000}{color:#b08000}{color:#b08000}pddf2 = pd.read_parquet(\"/mnt/aa/example.parquet\", engine='pyarrow')\r\ndisplay(pddf2)</font>{color}{color}{color}\r\n\u00a0",
        "created_at": "2019-06-18T22:30:50.000Z",
        "updated_at": "2019-07-07T21:57:12.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-07-07T21:57:12.000Z"
    },
    "comments": [
        {
            "created_at": "2019-06-18T22:32:55.114Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16867096) by Wes McKinney (wesm):*\nWhat does `os.path.isfile(\"/mnt/aa/example.parquet\")` return? "
        },
        {
            "created_at": "2019-06-18T22:40:07.261Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16867100) by Simon Lidberg (simonlid):*\nThat actually do return false\r\n\r\n\u00a0\r\n\r\nI can however read the same path using\r\n\r\n\u00a0\r\n\r\ndf2 = spark.read.parquet(\"/mnt/aa/example.parquet\")"
        },
        {
            "created_at": "2019-06-21T22:43:12.646Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16869928) by Robin K\u00e5veland (kaaveland):*\nThis is a curiousity with databricks file system (DBFS), actually. Spark on databricks will automatically access files over the `dbfs:` file system for you, without you even noticing. The native system calls for doing I/O don't do that. You should be able to open the file if you access it on `/dbfs/mnt/aa/example.parquet`. The\u00a0`/dbfs` on the local file system has the same contents as the\u00a0`dbfs:` file system that spark uses."
        },
        {
            "created_at": "2019-06-21T22:45:08.935Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16869930) by Robin K\u00e5veland (kaaveland):*\nNote that the DBFS documentation has warnings for trying to read files that are larger than 2G using local file system I/O, so if you need to do that, you should probably split those files into smaller chunks using spark first."
        },
        {
            "created_at": "2019-06-24T06:55:51.082Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16870857) by Simon Lidberg (simonlid):*\nI tested now with accessing the file using /dbfs/mnt/aa/example.parquet it fails but this time with another error:\r\n\r\nArrowIOError Traceback (most recent call last) <command-4042920808160098> in <module>() ----> 1 pddf2 = pd.read_parquet(\"/dbfs/mnt/aa/example2.parquet\", engine='pyarrow')  2 display(pddf2) /databricks/python/lib/python3.5/site-packages/pandas/io/parquet.py in read_parquet(path, engine, columns, \\*\\*kwargs)  280  281 impl = get_engine(engine) --> 282 return impl.read(path, columns=columns, \\*\\*kwargs) /databricks/python/lib/python3.5/site-packages/pandas/io/parquet.py in read(self, path, columns, \\*\\*kwargs)  127 kwargs['use_pandas_metadata'] = True  128 result = self.api.parquet.read_table(path, columns=columns, --> 129 \\*\\*kwargs).to_pandas()  130 if should_close:  131 try: /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in read_table(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, filesystem)  1150 return fs.read_parquet(path, columns=columns,  1151 use_threads=use_threads, metadata=metadata, -> 1152 use_pandas_metadata=use_pandas_metadata)  1153  1154 pf = ParquetFile(source, metadata=metadata) /databricks/python/lib/python3.5/site-packages/pyarrow/filesystem.py in read_parquet(self, path, columns, metadata, schema, use_threads, use_pandas_metadata)  177 from pyarrow.parquet import ParquetDataset  178 dataset = ParquetDataset(path, schema=schema, metadata=metadata, --> 179 filesystem=self)  180 return dataset.read(columns=columns, use_threads=use_threads,  181 use_pandas_metadata=use_pandas_metadata) /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in __init__(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, memory_map)  956  957 if validate_schema: --> 958 self.validate_schemas()  959  960 if filters is not None: /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in validate_schemas(self)  967 self.schema = self.common_metadata.schema  968 else: --> 969 self.schema = self.pieces[0].get_metadata().schema  970 elif self.schema is None:  971 self.schema = self.metadata.schema /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in get_metadata(self, open_file_func)  500 f = self._open(open_file_func)  501 else: --> 502 f = self.open()  503 return f.metadata  504 /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in open(self)  518 Returns instance of ParquetFile  519 \"\"\" --> 520 reader = self.open_file_func(self.path)  521 if not isinstance(reader, ParquetFile):  522 reader = ParquetFile(reader) /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in open_file(path, meta)  1054 return ParquetFile(path, metadata=meta,  1055 memory_map=self.memory_map, -> 1056 common_metadata=self.common_metadata)  1057 else:  1058 def open_file(path, meta=None): /databricks/python/lib/python3.5/site-packages/pyarrow/parquet.py in __init__(self, source, metadata, common_metadata, memory_map)  128 memory_map=True):  129 self.reader = ParquetReader() --> 130 self.reader.open(source, use_memory_map=memory_map, metadata=metadata)  131 self.common_metadata = common_metadata  132 self._nested_paths_by_prefix = self._build_nested_paths() /databricks/python/lib/python3.5/site-packages/pyarrow/_parquet.cpython-35m-x86_64-linux-gnu.so in pyarrow._parquet.ParquetReader.open() /databricks/python/lib/python3.5/site-packages/pyarrow/lib.cpython-35m-x86_64-linux-gnu.so in pyarrow.lib.check_status() ArrowIOError: Invalid parquet file. Corrupt footer.\r\n\r\nMy entire test code is as follows:\r\n\r\nfrom pyspark.sql import \\*\r\n--Create test data\r\nrow1 = Row(id='1', name='Alpha')\r\nrow2 = Row(id='2', name='Beta')\r\nrow3 = Row(id='3', name='Gamma')\r\nrow4 = Row(id='4', name='Delta')\r\n\r\ndf = spark.createDataFrame([row1, row2, row3, row4])\r\ndisplay(df)\r\n\r\n--Write the data to the mount point\r\ndf.write.parquet(\"/mnt/aa/example2.parquet\")\r\n\r\n--Try reading using pandas read_parquet\r\npddf2 = pd.read_parquet(\"/dbfs/mnt/aa/example.parquet\", engine='pyarrow')\r\ndisplay(pddf2)\u00a0\r\n\r\nThe same file can be read using \r\ndf2 = spark.read.parquet(\"/mnt/aa/example.parquet\")\r\n\r\npddf = df2.toPandas()\r\ntype(pddf)\r\n"
        },
        {
            "created_at": "2019-06-24T08:24:11.317Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16870919) by Robin K\u00e5veland (kaaveland):*\nThis error is a bit misleading from Arrow. I'm guessing you wrote this parquet file with spark?\r\n\r\n\u00a0\r\n\r\nSpark will tend to put non-parquet files in a parquet folder / dataset, files named things like `_started`, `_comitted`, `_SUCCESS` and so on. These aren't valid parquet files, spark uses them for book-keeping reasons. I think it has to do with writing datasets to blob storage, which doesn't support things like atomic directory renames.\r\n\r\nI'm not sure why arrow attempts to read them as parquet files, but you can easily work around this. Here are two things that I do to get around this issue:\r\n\r\n\u00a0\r\n```java\n\r\nimport pyarrow.parquet as pq\r\nimport glob\r\n\r\nsegments = glob.glob('/dbfs/mnt/aa/example.parquet/*.parquet')\r\npdf = pq.ParquetDataset(segments).read().to_pandas()\n```\r\n\u00a0\r\n\r\nThis won't always work, eg. you may have a more deeply nested parquet dataset. In this case I've found no other way around this than deleting the offending files.\r\n\r\n\u00a0\r\n\r\nBut I bet, if you were to check, you'd find that your `example.parquet` is a folder that contains exactly 1 file that ends with `.parquet`, so you could just read that one out."
        },
        {
            "created_at": "2019-06-24T08:27:31.465Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16870942) by Robin K\u00e5veland (kaaveland):*\nThis is a pain point for me. I use spark a lot to join together and filter some pretty big parquet files (100GB etc), where I will end up with a much smaller one, of only 2-3GB in the end.\u00a0 I usually have to fight both spark and arrow for a little while before I can read the smaller file into pandas."
        },
        {
            "created_at": "2019-06-24T15:18:17.731Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16871399) by Wes McKinney (wesm):*\nThe \"_started\" files will be excluded automatically in version 0.14.0. PRs to address other problems you're having would be welcome"
        },
        {
            "created_at": "2019-07-04T10:31:36.811Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5647?focusedCommentId=16878528) by Robin K\u00e5veland (kaaveland):*\n`[~wesmckinn]` \u2013 as far as I can tell, all my \"gripes\" are fixed on master. I think this issue could probably be closed `[~simonlid]` ?"
        }
    ]
}