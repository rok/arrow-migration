{
    "issue": {
        "title": "[Python] Consistent handling of categoricals",
        "body": "***Note**: This issue was originally created as [ARROW-11157](https://issues.apache.org/jira/browse/ARROW-11157). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhat is the current state of categoricals with pyarrow? The `categories` parameter mentioned [in this GitHub](https://github.com/apache/arrow/issues/1688) issue\u00a0does not seem to be accepted in `pd.read_parquet` anymore. I see that read/write of `int` categoricals does not work, though `str` do \u2013 except if the file is written by fastparquet.\r\n\r\nUsing pandas 1.1.5, pyarrow 2.0.0, and fastparquet 0.4.1, I see the following handling of categoricals:\r\n\r\n\u00a0\r\n```java\n\r\nimport os\r\nimport pandas as pd\r\n\r\n\r\nfname = '/tmp/tst'\r\n\r\n\r\ndata = {\r\n\u00a0 \u00a0 'int': pd.Series([0, 1] * 1000, dtype=pd.CategoricalDtype([0,1])),\r\n\u00a0 \u00a0 'str': pd.Series(['foo', 'bar'] * 1000, dtype=pd.CategoricalDtype(['foo', 'bar'])),\r\n}\r\ndf = pd.DataFrame(data)\r\n\r\n\r\nfor write in ['fastparquet', 'pyarrow']:\r\n\u00a0 \u00a0 for read in ['fastparquet', 'pyarrow']:\r\n\u00a0 \u00a0 \u00a0 \u00a0 if os.path.exists(fname):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 os.remove(fname)\r\n\u00a0 \u00a0 \u00a0 \u00a0 df.to_parquet(fname, engine=write, compression=None)\r\n\u00a0 \u00a0 \u00a0 \u00a0 df_read = pd.read_parquet(fname, engine=read)\r\n\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 print()\r\n\u00a0 \u00a0 \u00a0 \u00a0 print('write:', write, 'read:', read)\r\n\u00a0 \u00a0 \u00a0 \u00a0 for t in data.keys():\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(t, df[t].dtype == df_read[t].dtype)\n```\r\n\u00a0\r\n\r\n\u00a0\r\n```\n\r\nwrite: fastparquet read: fastparquet\r\nint True\r\nstr True\r\nwrite: fastparquet read: pyarrow\r\nint False\r\nstr False\r\nwrite: pyarrow read: fastparquet\r\nint True\r\nstr True\r\nwrite: pyarrow read: pyarrow\r\nint False\r\nstr True\r\n```",
        "created_at": "2021-01-07T01:24:41.000Z",
        "updated_at": "2022-08-22T02:26:58.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-01-07T19:31:29.687Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11157?focusedCommentId=17260751) by Wes McKinney (wesm):*\nThis looks buggy to me. Less consideration has been given to non-string categorical data since it appears less frequently in practice. \r\n\r\ncc `[~jorisvandenbossche]`"
        },
        {
            "created_at": "2021-01-25T15:29:32.156Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11157?focusedCommentId=17271370) by Joris Van den Bossche (jorisvandenbossche):*\nLooking a bit into this. Converting the pandas DataFrame to pyarrow Table still preserves the categorical (dictionary) data type:\r\n\r\n```Java\n\r\nIn [55]: table = pa.table(df)\r\n\r\nIn [56]: table\r\nOut[56]: \r\npyarrow.Table\r\nint: dictionary<values=int64, indices=int8, ordered=0>\r\nstr: dictionary<values=string, indices=int8, ordered=0>\r\n```\r\n\r\nbut when reading, we restore the dictionary type for the string column but not for the int column (as you already observed):\r\n\r\n```Java\n\r\nIn [58]: pq.write_table(table, \"test_categoricals.parquet\")\r\n\r\nIn [59]: pq.write_table?\r\n\r\nIn [60]: pq.read_table(\"test_categoricals.parquet\")\r\nOut[60]: \r\npyarrow.Table\r\nint: int64\r\nstr: dictionary<values=string, indices=int32, ordered=0>\r\n```\r\n\r\nWe store the original Arrow schema metadata, so we can restore the original types (since parquet doesn't have the concept of a separate dictionary/categorical type, only uses that as compression). The code where this is done lives here: https://github.com/apache/arrow/blob/b3b62412140182a0ac7b4f585fac27c8c57f1662/cpp/src/parquet/arrow/schema.cc#L874-L884 \r\nThe `IsDictionaryReadSupported` in the if-check is basically checking if the data type is binary or string, so we only restore the dictionary type for those data types and not for integer (and thus when finally converting to pandas, you also don't see a categorical data type for the integer column). \r\n\r\nI am not familiar enough with this part of the codebase to really know the reason for this, but I suppose it is related to ARROW-6140. For string/binary values, we can read the parquet data directly into an Arrow DictionaryArray, while this is not yet supported for other types. So if we would currently want to restore the integer columns also as dictionary type, that would basically mean to do a dictionary encoding after the fact of the materialized integer column (as long as ARROW-6140 is not implemented). \r\n\r\nIt is also documented that the `read_dictionary` keyword is (for this reason) only valid for string/bytes columns: https://arrow.apache.org/docs/python/parquet.html#reading-types-as-dictionaryarray\r\n\r\nNow, the above is the actual parquet reading. If we know, based on the pandas dtypes information we store in the metadata, that the original column was a categorical column, we could also still preserve the categorical dtype on conversion from pyarrow -> pandas (but that's something we currently also don't do for eg string columns)."
        },
        {
            "created_at": "2021-01-25T15:37:57.020Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11157?focusedCommentId=17271372) by Joris Van den Bossche (jorisvandenbossche):*\nAdditional answer specifically about this:\r\n\r\n>  The `categories` parameter mentioned in this GitHub issue does not seem to be accepted in `pd.read_parquet` anymore\r\n\r\nIt certainly still works in the `to_pandas` method:\r\n\r\n```Java\n\r\nIn [89]: pa.table({'int': [1, 2]}).to_pandas().dtypes\r\nOut[89]: \r\nint    int64\r\ndtype: object\r\n\r\nIn [90]: pa.table({'int': [1, 2]}).to_pandas(categories=['int']).dtypes\r\nOut[90]: \r\nint    category\r\ndtype: object\r\n```\r\n\r\nIt might be that this worked at some point directly in the `pd.read_parquet` function, but indeed not at the moment. The main problem here is that additional keywords (`kwargs`) can be passed either to parquet.read_table or to Table.to_pandas. And right now they are passed to `read_table`. We should probably have a way to specify keywords for `to_pandas` as well. \r\n\r\nAs a workaround, you can read with pyarrow and do the conversion to pandas manually. So basically instead of `pd.parquet(..)` you can do `pyarrow.parquet.read_table(..).to_pandas(..)`."
        },
        {
            "created_at": "2022-08-22T02:26:58.911Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11157?focusedCommentId=17582693) by A (Ark-kun):*\nI need the following to work:\u00a0\r\n\r\nGiven a parquet file, automatically load it and convert to Pandas DataFrame with correct categorical columns (which are required by XGBoost for example).\r\n\r\nThe suggested workarounds do not work since this has to be done programmatically without prior knowledge of any schema - based on the parquet file alone."
        }
    ]
}