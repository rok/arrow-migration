{
    "issue": {
        "title": "[Python] Better error message when user connects to HDFS cluster with wrong port",
        "body": "***Note**: This issue was originally created as [ARROW-3957](https://issues.apache.org/jira/browse/ARROW-3957). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm trying to connect to HDFS using libhdfs and Kerberos.\r\n\r\nI have JAVA_HOME and HADOOP_HOME set and\u00a0`pyarrow.hdfs.connect`\u00a0sets CLASSPATH correctly.\r\n\r\nMy connect call looks like:\r\n\r\n`import pyarrow.hdfs`\r\n\r\n`c = pyarrow.hdfs.connect(host='MYHOST', port=42424,`\r\n\r\n`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0user='ME', kerb_ticket=\"/tmp/krb5cc_498970\")`\r\n\r\nThis doesn't error but the resulting connection can't do anything. They either error like this:\r\n\r\n`ArrowIOError: HDFS list directory failed, errno: 255 (Unknown error 255)\u00a0`\r\n\r\nOr swallow errors (e.g.\u00a0`exists`\u00a0returning\u00a0`False`).\r\n\r\nNote that\u00a0`connect`\u00a0errors if the host is wrong but doesn't error if the port, user, or kerb_ticket are wrong. I have no idea how to debug this, because no useful errors.\r\n\r\nNote that I\u00a0_can_\u00a0connect using the hdfs Python package. (Of course, that doesn't provide the API I need to read Parquet files.).\r\n\r\nAny help would be appreciated greatly.",
        "created_at": "2018-12-07T22:42:41.000Z",
        "updated_at": "2018-12-18T18:25:06.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-12-18T18:24:54.000Z"
    },
    "comments": [
        {
            "created_at": "2018-12-10T15:33:20.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16714931) by Wes McKinney (wesm):*\nWell, this is a bit problematic because libhdfs is supposed to return NULL when the connection fails:\r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/thirdparty/hadoop/include/hdfs.h#L232\r\n\r\nYou can see we check for null here\r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/src/arrow/io/hdfs.cc#L346\r\n\r\nI'm not sure what we can do here if libhdfs is failing to work as advertised. Can you open an issue with Apache Hadoop about this?"
        },
        {
            "created_at": "2018-12-10T21:33:41.206Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16715586) by Jim Fulton (j1m):*\nI finally figured out what stupid thing I was doing. I was specifying a webhdfs port. :/\r\n\r\n\u00a0\r\n\r\nIf you want to chalk this off to user error, I'll understand. :)"
        },
        {
            "created_at": "2018-12-10T21:35:37.587Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16715589) by Wes McKinney (wesm):*\nWell, it's obviously not good! Seems like libhdfs should return a reasonable error in this case"
        },
        {
            "created_at": "2018-12-10T21:38:23.605Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16715591) by Jim Fulton (j1m):*\nA contributing factor was that I was using a Jupyter notebook, which hid some output.\r\n\r\n\u00a0\r\n\r\nWhen I ran outside of a notebook, I could see a Java traceback featuring:\r\n\r\n`java.io.IOException: Failed on local exception: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length`\r\n\r\n\u00a0\r\n\r\nI also tried the hdfs command-line tool and saw the same error, so I knew I was screwing up consistently. ;)\r\n\r\n\u00a0\r\n\r\nEventually, I realized I was using the wrong protocol.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2018-12-10T21:38:49.129Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16715595) by Jim Fulton (j1m):*\nYes, I'll open a Hadoop ticket."
        },
        {
            "created_at": "2018-12-18T01:47:36.630Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16723579) by Wes McKinney (wesm):*\nI was able to reproduce the issue by connecting to a live HDFS cluster with the wrong port\r\n\r\n```Java\n\r\nIn [1]: import pyarrow as pa\r\nco\r\nIn [2]: con = pa.hdfs.connect('localhost', port=50070)\r\nSLF4J: Class path contains multiple SLF4J bindings.\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\r\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\r\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\r\n2018-12-17 19:45:45,570 WARN  util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n\r\nIn [3]: con.ls('/')\r\nhdfsListDirectory(/): FileSystem#listStatus error:\r\njava.io.IOException: Failed on local exception: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.; Host Details : local host is: \"badgerpad16/127.0.1.1\"; destination host is: \"localhost\":50070; \r\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1474)\r\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1401)\r\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\r\n\tat com.sun.proxy.$Proxy9.getListing(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:554)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\r\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\r\n\tat com.sun.proxy.$Proxy10.getListing(Unknown Source)\r\n\tat org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1958)\r\n\tat org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1941)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:693)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:105)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:755)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:751)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:751)\r\nCaused by: com.google.protobuf.InvalidProtocolBufferException: Protocol message end-group tag did not match expected tag.\r\n\tat com.google.protobuf.InvalidProtocolBufferException.invalidEndTag(InvalidProtocolBufferException.java:94)\r\n\tat com.google.protobuf.CodedInputStream.checkLastTagWas(CodedInputStream.java:124)\r\n\tat com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:202)\r\n\tat com.google.protobuf.AbstractParser.parsePartialDelimitedFrom(AbstractParser.java:241)\r\n\tat com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:253)\r\n\tat com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:259)\r\n\tat com.google.protobuf.AbstractParser.parseDelimitedFrom(AbstractParser.java:49)\r\n\tat org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto.parseDelimitedFrom(RpcHeaderProtos.java:3167)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1074)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:968)\r\n---------------------------------------------------------------------------\r\nArrowIOError                              Traceback (most recent call last)\r\n<ipython-input-3-cf288d1dd1d6> in <module>()\r\n----> 1 con.ls('/')\r\n\r\n~/code/arrow/python/pyarrow/hdfs.py in ls(self, path, detail)\r\n     99         result : list of dicts (detail=True) or strings (detail=False)\r\n    100         \"\"\"\r\n--> 101         return super(HadoopFileSystem, self).ls(path, detail)\r\n    102 \r\n    103     def walk(self, top_path):\r\n\r\n~/code/arrow/python/pyarrow/io-hdfs.pxi in pyarrow.lib.HadoopFileSystem.ls()\r\n    270 \r\n    271         with nogil:\r\n--> 272             check_status(self.client.get()\r\n    273                          .ListDirectory(c_path, &listing))\r\n    274 \r\n\r\n~/code/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n     81             raise ArrowInvalid(message)\r\n     82         elif status.IsIOError():\r\n---> 83             raise ArrowIOError(message)\r\n     84         elif status.IsOutOfMemory():\r\n     85             raise ArrowMemoryError(message)\r\n\r\nArrowIOError: HDFS list directory failed, errno: 255 (Unknown error 255)\r\n```\r\n\r\nIt might be worth suggesting that the port is wrong when the user gets errno 255. "
        },
        {
            "created_at": "2018-12-18T03:42:36.165Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16723634) by Wes McKinney (wesm):*\nI did this in https://github.com/apache/arrow/pull/3209\r\n\r\nThe error message now looks like\r\n\r\n```Java\n\r\nArrowIOError: HDFS list directory of / failed, errno: 255 (Unknown error 255) Please check that you are connecting to the correct HDFS RPC port\r\n```"
        },
        {
            "created_at": "2018-12-18T18:24:54.129Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3957?focusedCommentId=16724336) by Wes McKinney (wesm):*\nResolved in https://github.com/apache/arrow/commit/758bd557584107cb336cbc3422744dacd93978af"
        }
    ]
}