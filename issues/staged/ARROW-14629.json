{
    "issue": {
        "title": "[Release][Python] Parquet test fails on AlmaLinux8",
        "body": "***Note**: This issue was originally created as [ARROW-14629](https://issues.apache.org/jira/browse/ARROW-14629). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen running verification tests on AlmaLinux 8, Parquet test fails\r\n\r\nMain steps to reproduce\r\n\r\n```bash\n\r\ndnf -y update\r\ndnf clean all\r\ndnf -y install \\\r\n  dnf-plugins-core \\\r\n  yum-utils\r\ndnf config-manager --set-enabled powertools\r\ndnf -y update\r\ndnf -y module disable ruby\r\ndnf -y module enable ruby:2.7\r\ndnf -y groupinstall \"Development Tools\"\r\ndnf -y install \\\r\n  epel-release \\\r\n  ninja-build \\\r\n  libcurl-devel \\\r\n  python3-pip \\\r\n  python3-devel \\\r\n  cmake \\\r\n  git \\\r\n  ncurses-devel \\\r\n  gobject-introspection-devel \\\r\n  libffi-devel \\\r\n  openssl-devel \\\r\n  maven \\\r\n  java-1.8.0-openjdk-devel \\\r\n  wget \\\r\n  readline-devel \\\r\n  gdbm-devel \\\r\n  ruby-devel \\\r\n  llvm-toolset \\\r\n  llvm-devel\r\ndnf -y update\r\nalias pip=pip3\r\nalternatives --set python /usr/bin/python3\r\nln -s /usr/bin/pip3 /usr/bin/pip\r\ngit clone https://github.com/apache/arrow/\r\npip install -r arrow/python/requirements-build.txt \\\r\n     -r arrow/python/requirements-test.txt\r\ncd arrow\r\nmkdir dist\r\nexport ARROW_HOME=$(pwd)/dist\r\nexport LD_LIBRARY_PATH=$(pwd)/dist/lib:$LD_LIBRARY_PATH\r\ncd cpp\r\nmkdir build\r\ncmake -DCMAKE_INSTALL_PREFIX=$ARROW_HOME \\\r\n      -DCMAKE_INSTALL_LIBDIR=lib \\\r\n      -DARROW_WITH_BZ2=ON \\\r\n      -DARROW_WITH_ZLIB=ON \\\r\n      -DARROW_WITH_ZSTD=ON \\\r\n      -DARROW_WITH_LZ4=ON \\\r\n      -DARROW_WITH_SNAPPY=ON \\\r\n      -DARROW_WITH_BROTLI=ON \\\r\n      -DARROW_PARQUET=ON \\\r\n      -DARROW_PYTHON=ON \\\r\n      -DARROW_BUILD_TESTS=ON \\\r\n      ..\r\nmake -j4\r\nmake install\r\ncd ..\r\ncd ..\r\ncd python\r\nexport PYARROW_WITH_PARQUET=1\r\npython setup.py build_ext --inplace\r\nexport PYARROW_TEST_PARQUET=ON\r\n python -m pytest -r s --pyargs pyarrow\r\n```\r\n\r\nResulting error:\r\n\r\n```bash\n\r\n============================================ FAILURES =============================================\r\n________________________________ test_permutation_of_column_order _________________________________\r\n\r\nsource = '/tmp/pytest-of-root/pytest-9/test_permutation_of_column_ord0/dataset_column_order_permutation'\r\ncolumns = None, use_threads = True, metadata = None, use_pandas_metadata = False\r\nmemory_map = False, read_dictionary = None\r\nfilesystem = <pyarrow._fs.LocalFileSystem object at 0x7f70875b7e30>, filters = None\r\nbuffer_size = 0, partitioning = 'hive', use_legacy_dataset = False, ignore_prefixes = None\r\npre_buffer = True, coerce_int96_timestamp_unit = None\r\n\r\n    def read_table(source, columns=None, use_threads=True, metadata=None,\r\n                   use_pandas_metadata=False, memory_map=False,\r\n                   read_dictionary=None, filesystem=None, filters=None,\r\n                   buffer_size=0, partitioning=\"hive\", use_legacy_dataset=False,\r\n                   ignore_prefixes=None, pre_buffer=True,\r\n                   coerce_int96_timestamp_unit=None):\r\n        if not use_legacy_dataset:\r\n            if metadata is not None:\r\n                raise ValueError(\r\n                    \"The 'metadata' keyword is no longer supported with the new \"\r\n                    \"datasets-based implementation. Specify \"\r\n                    \"'use_legacy_dataset=True' to temporarily recover the old \"\r\n                    \"behaviour.\"\r\n                )\r\n            try:\r\n                dataset = _ParquetDatasetV2(\r\n                    source,\r\n                    filesystem=filesystem,\r\n                    partitioning=partitioning,\r\n                    memory_map=memory_map,\r\n                    read_dictionary=read_dictionary,\r\n                    buffer_size=buffer_size,\r\n                    filters=filters,\r\n                    ignore_prefixes=ignore_prefixes,\r\n                    pre_buffer=pre_buffer,\r\n>                   coerce_int96_timestamp_unit=coerce_int96_timestamp_unit\r\n                )\r\n\r\npyarrow/parquet.py:1960: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <pyarrow.parquet._ParquetDatasetV2 object at 0x7f7087556da0>\r\npath_or_paths = '/tmp/pytest-of-root/pytest-9/test_permutation_of_column_ord0/dataset_column_order_permutation'\r\nfilesystem = None, filters = None, partitioning = 'hive', read_dictionary = None, buffer_size = 0\r\nmemory_map = False, ignore_prefixes = None, pre_buffer = True, coerce_int96_timestamp_unit = None\r\nkwargs = {}\r\n\r\n    def __init__(self, path_or_paths, filesystem=None, filters=None,\r\n                 partitioning=\"hive\", read_dictionary=None, buffer_size=None,\r\n                 memory_map=False, ignore_prefixes=None, pre_buffer=True,\r\n                 coerce_int96_timestamp_unit=None, **kwargs):\r\n>       import pyarrow.dataset as ds\r\n\r\npyarrow/parquet.py:1680: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n    \"\"\"Dataset is currently unstable. APIs subject to change without notice.\"\"\"\r\n    \r\n    import pyarrow as pa\r\n    from pyarrow.util import _is_iterable, _stringify_path, _is_path_like\r\n    \r\n>   from pyarrow._dataset import (  # noqa\r\n        CsvFileFormat,\r\n        CsvFragmentScanOptions,\r\n        Expression,\r\n        Dataset,\r\n        DatasetFactory,\r\n        DirectoryPartitioning,\r\n        FileFormat,\r\n        FileFragment,\r\n        FileSystemDataset,\r\n        FileSystemDatasetFactory,\r\n        FileSystemFactoryOptions,\r\n        FileWriteOptions,\r\n        Fragment,\r\n        HivePartitioning,\r\n        IpcFileFormat,\r\n        IpcFileWriteOptions,\r\n        InMemoryDataset,\r\n        ParquetDatasetFactory,\r\n        ParquetFactoryOptions,\r\n        ParquetFileFormat,\r\n        ParquetFileFragment,\r\n        ParquetFileWriteOptions,\r\n        ParquetFragmentScanOptions,\r\n        ParquetReadOptions,\r\n        Partitioning,\r\n        PartitioningFactory,\r\n        RowGroupInfo,\r\n        Scanner,\r\n        TaggedRecordBatch,\r\n        UnionDataset,\r\n        UnionDatasetFactory,\r\n        _get_partition_keys,\r\n        _filesystemdataset_write,\r\n    )\r\nE   ModuleNotFoundError: No module named 'pyarrow._dataset'\r\n\r\npyarrow/dataset.py:23: ModuleNotFoundError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\ntempdir = PosixPath('/tmp/pytest-of-root/pytest-9/test_permutation_of_column_ord0')\r\n\r\n    def test_permutation_of_column_order(tempdir):\r\n# ARROW-2366\r\n        case = tempdir / \"dataset_column_order_permutation\"\r\n        case.mkdir(exist_ok=True)\r\n    \r\n        data1 = pa.table([[1, 2, 3], [.1, .2, .3]], names=['a', 'b'])\r\n        pq.write_table(data1, case / \"data1.parquet\")\r\n    \r\n        data2 = pa.table([[.4, .5, .6], [4, 5, 6]], names=['b', 'a'])\r\n        pq.write_table(data2, case / \"data2.parquet\")\r\n    \r\n>       table = pq.read_table(str(case))\r\n\r\npyarrow/tests/parquet/test_basic.py:645: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\npyarrow/parquet.py:1977: in read_table\r\n    source = filesystem.open_input_file(path)\r\npyarrow/_fs.pyx:588: in pyarrow._fs.FileSystem.open_input_file\r\n    in_handle = GetResultValue(self.fs.OpenInputFile(pathstr))\r\npyarrow/error.pxi:143: in pyarrow.lib.pyarrow_internal_check_status\r\n    return check_status(status)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\n>   raise IOError(message)\r\nE   OSError: Cannot open for reading: path '/tmp/pytest-of-root/pytest-9/test_permutation_of_column_ord0/dataset_column_order_permutation' is a directory\r\n\r\npyarrow/error.pxi:114: OSError\r\n\r\n```\r\n",
        "created_at": "2021-11-08T13:30:17.000Z",
        "updated_at": "2021-11-10T01:56:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-11-08T20:20:17.000Z"
    },
    "comments": [
        {
            "created_at": "2021-11-08T19:07:43.093Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14629?focusedCommentId=17440682) by Joris Van den Bossche (jorisvandenbossche):*\nAh, we forgot to add a `dataset` marker in this case. (it's easy to forget since we don't have a CI build that does _not_ include dataset at the moment ..)"
        },
        {
            "created_at": "2021-11-08T20:20:17.674Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14629?focusedCommentId=17440734) by Joris Van den Bossche (jorisvandenbossche):*\nIssue resolved by pull request 11643\n<https://github.com/apache/arrow/pull/11643>"
        },
        {
            "created_at": "2021-11-10T01:56:36.778Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14629?focusedCommentId=17441459) by Kouhei Sutou (kou):*\nThe verification steps verify the master not 6.0.1.\r\nSo this isn't needed for 6.0.1."
        }
    ]
}