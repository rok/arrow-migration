{
    "issue": {
        "title": "[Python] hdfs fails to connect to for HDFS 3.x cluster",
        "body": "***Note**: This issue was originally created as [ARROW-9019](https://issues.apache.org/jira/browse/ARROW-9019). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm trying to use the pyarrow hdfs connector with Hadoop 3.1.3 and I get an error that looks like a protobuf or jar mismatch problem with Hadoop. The same code works on a Hadoop 2.9 cluster.\r\n\u00a0\r\nI'm wondering if there is something special I need to do or if pyarrow doesn't support Hadoop 3.x yet?\r\nNote I tried with pyarrow 0.15.1, 0.16.0, and 0.17.1.\r\n\u00a0\r\n\u00a0 \u00a0 import pyarrow as pa\r\n\u00a0 \u00a0 hdfs_kwargs = dict(host=\"namenodehost\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port=9000,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 user=\"tgraves\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 driver='libhdfs',\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kerb_ticket=None,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 extra_conf=None)\r\n\u00a0 \u00a0 fs = pa.hdfs.connect(\\*\\*hdfs_kwargs)\r\n\u00a0 \u00a0 res = fs.exists(\"/user/tgraves\")\r\n\u00a0\r\nError that I get on Hadoop 3.x is:\r\n\u00a0\r\ndfsExists: invokeMethod((Lorg/apache/hadoop/fs/Path;)Z) error:\r\nClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.shaded.com.google.protobuf.Messagejava.lang.ClassCastException: org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto cannot be cast to org.apache.hadoop.shaded.com.google.protobuf.Message\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:904)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1661)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1577)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1574)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1589)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1683)",
        "created_at": "2020-06-02T16:45:33.000Z",
        "updated_at": "2022-07-12T14:04:50.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-06-14T18:28:52.601Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17135258) by Andy (McArdle):*\nThink it is related to library version.\u00a0 I had similar issue - resolved by running export LD_LIBRARY_PATH=/opt/anaconda/3.7.1/lib/:$LD_LIBRARY_PATH prior to executing the code (you'll need to use your own python distro lib path).\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-06-15T14:58:36.958Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17135940) by Thomas Graves (tgraves):*\ncan you give more details on what was missing? \u00a0I used the exact same setup and it worked with Hadoop 2.9.\u00a0"
        },
        {
            "created_at": "2020-06-17T22:32:42.203Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17138880) by Andy (McArdle):*\nSorry, went back and retried and realized I had a different error related to kerberos and the 'libkrb5support.so' (we have HDP 3.1.5 with kerberos).\r\n\r\nI'm able to do (with pyarrow 0.13.0 and anaconda disto 3.7.1):\r\n\r\nimport pyarrow as pa\r\n fs = pa.hdfs.connect() # <- here I use the defaults\r\n res = fs.exists(\"/user/<myuserid>\")\r\n\r\n\u00a0\r\n\r\nas long as I make sure I'm pointing at the correct libkrb5support library.\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-30T00:19:31.236Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17256212) by Bradley Miro (bradmiro):*\nHello! I'm on the GCP Dataproc team and was wondering if there's been any progress or workarounds for this? I am attempting to support a Horovod + Dataproc integration but this keeps popping up as a blocker to finishing the integration. Any help would be appreciated :)\u00a0"
        },
        {
            "created_at": "2021-01-11T19:03:09.489Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17262875) by Thomas Graves (tgraves):*\nping on this again, any information or ideas on working around or fixing?"
        },
        {
            "created_at": "2021-01-11T19:19:13.446Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17262883) by Bradley Miro (bradmiro):*\nHey `[~tgraves]`,\r\n\r\nThis ended up working once I did this:\r\n```java\n\r\nexport CLASSPATH=$(hadoop classpath --glob)\n```\r\n(assuming the Hadoop binary is in your PATH)"
        },
        {
            "created_at": "2021-03-11T21:24:51.393Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17299881) by Thomas Graves (tgraves):*\n`[~bradmiro]` \u00a0I don't really understand how that fixes the issue, the hadoop classpath is already included when a container launchs on yarn, in this case I launched Spark on yarn and the hadoop classpath should already be there.\u00a0 Now the only thing I can think of is if this caused the order of things in the classpath to change"
        },
        {
            "created_at": "2021-03-11T21:55:24.089Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17299894) by Thomas Graves (tgraves):*\nNote I was able to finally test this and on dataproc at least setting the classpath did work around the issue.\u00a0 It must be a jar file order issue.\u00a0 In this case though I set it and manually started pyspark."
        },
        {
            "created_at": "2021-06-22T12:41:23.118Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17367291) by Antoine Pitrou (apitrou):*\ncc `[~icook]`"
        },
        {
            "created_at": "2021-06-23T15:04:36.850Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17368262) by Ian Cook (icook):*\nSince this can be solved by setting `CLASSPATH`\u00a0as described in the above comments, perhaps we should review the code in [`python/pyarrow/hdfs.py`](https://github.com/apache/arrow/blob/master/python/pyarrow/hdfs.py)\u00a0which automatically sets `CLASSPATH`\u00a0to check for any faulty logic there.\r\n\r\nFYI, according to the comments in that file, `pyarrow.hdfs.connect` is deprecated and `pyarrow.fs.HadoopFileSystem` should be used instead. I'm not sure if that has any bearing on this issue. (Update: see the related issue\u00a0[ARROW-13141](https://issues.apache.org/jira/browse/ARROW-13141))"
        },
        {
            "created_at": "2022-07-12T14:04:50.254Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9019?focusedCommentId=17565780) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}