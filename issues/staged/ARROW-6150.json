{
    "issue": {
        "title": "[Python] Intermittent HDFS error",
        "body": "***Note**: This issue was originally created as [ARROW-6150](https://issues.apache.org/jira/browse/ARROW-6150). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm running a Dask-YARN job that dumps a results dictionary into HDFS (code shown in traceback below) using PyArrow's HDFS IO library. However, the job intermittently runs into the error shown below, not every run, only sometimes. I'm unable to determine the root cause of this issue.\r\n\r\n\u00a0\r\n\r\n` File \"/extractor.py\", line 87, in __call__ json.dump(results_dict, fp=_UTF8Encoder(f), indent=4) File \"pyarrow/io.pxi\", line 72, in pyarrow.lib.NativeFile.__exit__ File \"pyarrow/io.pxi\", line 130, in pyarrow.lib.NativeFile.close File \"pyarrow/error.pxi\", line 87, in pyarrow.lib.check_status pyarrow.lib.ArrowIOError: HDFS CloseFile failed, errno: 255 (Unknown error 255) Please check that you are connecting to the correct HDFS RPC port`",
        "created_at": "2019-08-06T18:19:52.000Z",
        "updated_at": "2019-08-08T14:47:15.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-08-08T14:47:15.000Z"
    },
    "comments": [
        {
            "created_at": "2019-08-06T18:59:55.876Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6150?focusedCommentId=16901386) by Wes McKinney (wesm):*\nOur use of libhdfs is pretty straightforward, so the issue seems unlikely to be caused by a bug in the Arrow implementation. I've seen other reports of the errno 255, they might give some clue about what could be wrong with the job. If you find anything out (or a way to reliably reproduce) let us know"
        },
        {
            "created_at": "2019-08-06T19:08:59.737Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6150?focusedCommentId=16901394) by Saurabh Bajaj (sbajaj):*\n`[~wesmckinn]` Thanks for your response!\u00a0\r\n\r\nI found\u00a0https://issues.apache.org/jira/browse/ARROW-3957\u00a0and the PR that address it:\u00a0<https://github.com/apache/arrow/commit/758bd557584107cb336cbc3422744dacd93978af>.\u00a0\r\n\r\nSeems like the cause of the issue is an incorrect port? The default to\u00a0`pa.hdfs.connect()`\u00a0is\u00a0`port=0`. What would be the correct port to use?"
        },
        {
            "created_at": "2019-08-06T19:12:30.066Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6150?focusedCommentId=16901397) by Saurabh Bajaj (sbajaj):*\nI tried setting port=8020 in pa.hdfs.connect(), but same intermittent errors.\u00a0"
        },
        {
            "created_at": "2019-08-06T19:14:28.866Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6150?focusedCommentId=16901399) by Wes McKinney (wesm):*\nThe RPC port depends on your Hadoop configuration. You can generally find it in the HDFS web ui. `pyarrow.hdfs.connect` will also try to use the defaults in `core-site.xml` (I think) to connect"
        },
        {
            "created_at": "2019-08-08T14:33:46.579Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6150?focusedCommentId=16903033) by Saurabh Bajaj (sbajaj):*\nTurns out this was being cause by duplication of computation of \"dask.get\" tasks on Delayed objects, hence a user error. Closing this ticket.\u00a0"
        }
    ]
}