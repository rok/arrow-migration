{
    "issue": {
        "title": "[C++] Support hash-join on larger than memory datasets",
        "body": "***Note**: This issue was originally created as [ARROW-16389](https://issues.apache.org/jira/browse/ARROW-16389). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe current implementation of the hash-join node current queues in memory the hashtable, the entire build side input, and the entire probe side input (e.g. the entire dataset).  This means the current implementation will run out of memory and crash if the input dataset is larger than the memory on the system.\r\n\r\nBy spilling to disk when memory starts to fill up we can allow the hash-join node to process datasets larger than the available memory on the machine.",
        "created_at": "2022-04-28T01:57:56.000Z",
        "updated_at": "2022-09-15T21:00:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-05-12T01:39:04.985Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16389?focusedCommentId=17535807) by Weston Pace (westonpace):*\nAttaching a design for this created by `[~michalno]`: https://docs.google.com/document/d/1hPxmfJYmWne_OyX3-CqXXyvDBoimi00M0WbS6bXcGYw/edit?usp=sharing"
        }
    ]
}