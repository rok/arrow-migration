{
    "issue": {
        "title": "[Python] PyArrow: write_dataset - Could not open CSV input source",
        "body": "***Note**: This issue was originally created as [ARROW-16810](https://issues.apache.org/jira/browse/ARROW-16810). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi Arrow Community!\u00a0\r\n\r\nHappy Friday! I am a new user to Arrow, specifically using pyarrow. However, I am very excited about the project.\u00a0\r\n\r\nI am experiencing issues with the '{**}write_dataset'{**} function from the '{**}dataset{**}' module. Please forgive me, if this is a known issue. However, I have searched the GitHub 'Issues', as well as Stack Overflow and I have not identified a similar issue.\u00a0\r\n\r\nI have a directory that contains 90 CSV files (essentially one CSV for each day between 2021-01-01 and 2021-03-31).\u00a0 My objective was to read all the CSV files into a dataset and write the dataset to a single Parquet file format. Unfortunately, some of the CSV files contained nulls in some columns, which presented some issues which were resolved by specifying DataTypes with the following Stack Overflow solution:\r\n\r\n[How do I specify a dtype for all columns when reading a CSV file with pyarrow?](https://stackoverflow.com/questions/71533197/how-do-i-specify-a-dtype-for-all-columns-when-reading-a-csv-file-with-pyarrow)\r\n\r\nThe following code works on the first pass.\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.csv as csv\r\nimport pyarrow.dataset as ds\r\nimport re\r\n```\r\n```python\n\r\npa.__version__\r\n'8.0.0'\r\n```\r\n```python\n\r\ncolumn_types = {}\r\ncsv_path = '/home/user/csv_files'\r\nfield_re_pattern = \"value_*\"\r\n\r\n# Open a dataset with the 'csv_path' path and 'csv' file format\r\n# and assign to 'dataset1'\r\ndataset1 = ds.dataset(csv_path, format='csv')\r\n\r\n# Loop through each field in the 'dataset1' schema,\r\n# match the 'field_re_pattern' regex pattern in the field name,\r\n# and assign 'int64' DataType to the field.name in the 'column_types'\r\n# dictionary \r\nfor field in (field for field in dataset1.schema \\\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if re.match(field_re_pattern, field.name)):\r\n\u00a0 \u00a0 \u00a0 \u00a0 column_types[field.name] = pa.int64()\r\n\r\n# Creates options for CSV data using the 'column_types' dictionary\r\n# This returns a <class 'pyarrow._csv.ConvertOptions'>\r\nconvert_options = csv.ConvertOptions(column_types=column_types)\r\n\r\n# Creates FileFormat for CSV using the 'convert_options' \r\n# This returns a <class 'pyarrow._dataset.CsvFileFormat'>\r\ncustom_csv_format = ds.CsvFileFormat(convert_options=convert_options)\r\n\r\n# Open a a dataset with the 'csv_path' path, instead of using the \r\n# 'csv' file format, use the 'custom_csv_format' and assign to \r\n# 'dataset2'\r\ndataset2 = ds.dataset(csv_path, format=custom_csv_format)\r\n\r\n# Write the 'dataset2' to the 'csv_path' base directory in the \r\n# 'parquet' format, and overwrite/ignore if the file exists\r\nds.write_dataset(dataset2, base_dir=csv_path, format='parquet', existing_data_behavior='overwrite_or_ignore')\r\n```\r\nAs previously stated, on first pass, the code works and creates a single parquet file (part-0.parquet) with the correct data, row count, and schema.\r\n\r\nHowever, if the code is run again, the following error is encountered:\r\n```python\n\r\nArrowInvalid: Could not open CSV input source '/home/user/csv_files/part-0.parquet': Invalid: CSV parse error: Row #2: Expected 4 columns, got 1: \f\u00106NQJR\u0005\f\u0014JV02XW\u0011$\f0Y8V\tp\t\u0018A\u0005$\u0018A18CEBS\u0005\f\u0014305DEM\u00110\u001030TTW\u0005\u0018\t\ufffd5\u0005\f\u0018HZ50GCV\u0005\f\u0014JV1CSV\r\n```\r\nMy interpretation of the error is that on the second pass the 'dataset2' variable now includes the 'part-0.parquet' file (which can be confirmed with the `dataset2.files` output showing the file) and the CSV reader is attempting to parse/read the parquet file.\r\n\r\nIf this is the case, is there an argument to ignore the parquet file and only evaluate the CSV files? Also, if a dataset object has a format of 'csv' or 'pyarrow._dataset.CsvFileFormat' associated with it would be nice to evaluate only CSV files and not all file types in the path. If that is not the current behavior.\r\n\r\nIf this is not the case, any ideas on the cause or solution?\r\n\r\nAny assistance would be greatly appreciated.\r\n\r\nThank you and have a great day!",
        "created_at": "2022-06-10T20:45:38.000Z",
        "updated_at": "2022-06-16T19:57:43.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-06-11T03:53:37.010Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16810?focusedCommentId=17553021) by Yibo Cai (yibocai):*\nI think pyarrow.dataset.dataset(dir, format='xxx') will read all files under the dir and try to parse them as format 'xxx'.\r\ncc `[~jorisvandenbossche]` for comments."
        },
        {
            "created_at": "2022-06-11T04:40:53.968Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16810?focusedCommentId=17553023) by Earle Lyons (earlev4):*\nThanks so much for the response, `[~yibocai]`! \r\n\r\nI sincerely appreciate the response and information. Your response makes sense and seems to align with the behavior. \r\n\r\nI suppose I could add code to remove any parquet files before pyarrow.dataset(path, format=custom_csv_format).\r\n\r\nHowever, an argument to read only specific file types would be very helpful.\r\n\r\nFor example...\r\n1. pyarrow.dataset(path, format=custom_csv_format, filetype='csv')\n1. pyarrow.dataset(path, format=custom_csv_format, fileext='.csv')\n1. pyarrow.dataset(path/\\*.csv, format=custom_csv_format)\n   \n   Per your comment I am CCing `[~jorisvandenbossche]`.\n   \n   Thanks again! :)\u00a0"
        },
        {
            "created_at": "2022-06-16T19:09:43.989Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16810?focusedCommentId=17555262) by Weston Pace (westonpace):*\nI think the most common thing we see for this kind of repartitioning is to write the output files to a new directory.  That way a future discovery won't see both files.\r\n\r\nThe `dataset()` function can also accept a list of files.  So you should be able to use the python `glob` module to do your own discovery and then pass the list of files to pyarrow instead of a directory.  I slightly prefer this method I think since there are many different ways to list and exclude files and we don't offer much value in doing this versus doing it in python.\r\n\r\nThat being said, we did add some support for glob parsing in the C++ lib to handle Substrait (which can pass in paths like \\***/**.csv) so it might not be too hard to add support for option #3 that you supplied."
        },
        {
            "created_at": "2022-06-16T19:57:43.475Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16810?focusedCommentId=17555275) by Earle Lyons (earlev4):*\nHi `[~westonpace]`!\u00a0\r\n\r\nGood day to you! Thanks so much for the response and very helpful information. If I recall correctly, I tried to output the files to a new subdirectory (i.e. '/home/user/csv_files/pq_files') and the parquet file was discovered, but I did not try a new directory (i.e. '/home/user/data/pq_files').\u00a0\r\n\r\nI agree, passing a list of files is probably the best method given the available options. To your point, there are benefits and flexibility with including/excluding files using a list.\r\n\r\nIn the future, it would be wonderful if paths with wildcards and supported format extensions (i.e. /\\*.csv) could be handled in the dataset.dataset 'source' parameter.\r\n\r\nThanks again! \u00a0:)"
        }
    ]
}