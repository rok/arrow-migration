{
    "issue": {
        "title": "pa_table.to_pydict() crashed\u00a0, Check failed: (off) <= (length) Slice offset greater than array length",
        "body": "***Note**: This issue was originally created as [ARROW-13427](https://issues.apache.org/jira/browse/ARROW-13427). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm not sure if this issue is related to\u00a0\r\n\r\nhttps://issues.apache.org/jira/browse/ARROW-10054\r\n\r\n```Java\n\r\n[2021-07-22 02:43:17,457 INFO get_bucket_trig_feat_raw2.py:100] read batch 0\r\n/arrow/cpp/src/arrow/array/data.cc:94: Check failed: (off) <= (length) Slice offset greater than array length\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(+0x554d58)[0x7f491040bd58]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZN5arrow4util8ArrowLogD1Ev+0xdd)[0x7f491040c5ad]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZNK5arrow9ArrayData5SliceEll+0x3c5)[0x7f491054c1f5]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZNK5arrow5Array5SliceEll+0x18)[0x7f491055e708]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZN5arrow8internal23ScalarFromArraySlotImpl5VisitINS_8ListTypeEEENS_6StatusERKNS_13BaseListArrayIT_EE+0x45)[0x7f4910582ab5]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZN5arrow16VisitArrayInlineINS_8internal23ScalarFromArraySlotImplEEENS_6StatusERKNS_5ArrayEPT_+0xce)[0x7f49105904fe]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZNO5arrow8internal23ScalarFromArraySlotImpl6FinishEv+0x11b)[0x7f49105917fb]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/libarrow.so.200(_ZNK5arrow5Array9GetScalarEl+0x35)[0x7f4910569ce5]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x189e28)[0x7f4914042e28]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0xbab9c)[0x7f4913f73b9c]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0xb5368)[0x7f4913f6e368]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0xc709f)[0x7f4913f8009f]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x11785e)[0x7f4913fd085e]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x12256b)[0x7f4913fdb56b]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x13398c)[0x7f4913fec98c]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x12256b)[0x7f4913fdb56b]\r\n/usr/local/lib64/python3.6/site-packages/pyarrow/lib.cpython-36m-x86_64-linux-gnu.so(+0x146823)[0x7f4913fff823]\r\n/lib64/libpython3.6m.so.1.0(_PyCFunction_FastCallDict+0x31a)[0x7f492888f6ba]\r\n/lib64/libpython3.6m.so.1.0(+0x167a50)[0x7f492888fa50]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x16818f)[0x7f492889018f]\r\n/lib64/libpython3.6m.so.1.0(+0x139f22)[0x7f4928861f22]\r\n/lib64/libpython3.6m.so.1.0(+0x13741a)[0x7f492885f41a]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x7b6)[0x7f49288b8806]\r\n/lib64/libpython3.6m.so.1.0(+0x142a3a)[0x7f492886aa3a]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x142a3a)[0x7f492886aa3a]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(PyEval_EvalCodeEx+0x337)[0x7f492889a397]\r\n/lib64/libpython3.6m.so.1.0(PyEval_EvalCode+0x1b)[0x7f492889b0eb]\r\n/lib64/libpython3.6m.so.1.0(+0x212d00)[0x7f492893ad00]\r\n/lib64/libpython3.6m.so.1.0(_PyCFunction_FastCallDict+0x92)[0x7f492888f432]\r\n/lib64/libpython3.6m.so.1.0(+0x167a50)[0x7f492888fa50]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(+0x114497)[0x7f492883c497]\r\n/lib64/libpython3.6m.so.1.0(+0x142bf0)[0x7f492886abf0]\r\n/lib64/libpython3.6m.so.1.0(+0x167b36)[0x7f492888fb36]\r\n/lib64/libpython3.6m.so.1.0(_PyEval_EvalFrameDefault+0x474)[0x7f49288b84c4]\r\n/lib64/libpython3.6m.so.1.0(PyEval_EvalCodeEx+0x337)[0x7f492889a397]\r\n/lib64/libpython3.6m.so.1.0(+0x173153)[0x7f492889b153]\r\n/lib64/libpython3.6m.so.1.0(PyObject_Call+0x47)[0x7f492883dfb7]\r\n/lib64/libpython3.6m.so.1.0(+0x213f31)[0x7f492893bf31]\r\n/lib64/libpython3.6m.so.1.0(Py_Main+0x2f0)[0x7f492893c360]\r\n/opt/python/bin/python3.6(main+0x116)[0x55c723ec4b96]\r\n/lib64/libc.so.6(__libc_start_main+0xf3)[0x7f49279df6a3]\r\n/opt/python/bin/python3.6(_start+0x2e)[0x55c723ec4d1e]\r\n21/07/22 02:43:17,631 ERROR Executor: Exception in task 298.0 in stage 5.0 (TID 303)\r\norg.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:536)\r\n at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:525)\r\n at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:643)\r\n at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\r\n at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\r\n at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\r\n at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n at org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:472)\r\n at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:475)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.EOFException\r\n at java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n ... 17 more\r\n```",
        "created_at": "2021-07-22T06:16:50.000Z",
        "updated_at": "2021-07-22T18:14:03.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-07-22T18:14:03.000Z"
    },
    "comments": [
        {
            "created_at": "2021-07-22T14:51:33.742Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13427?focusedCommentId=17385571) by Antoine Pitrou (apitrou):*\nCan you try with PyArrow 4.0.1? 2.0.0 is a bit old already."
        },
        {
            "created_at": "2021-07-22T18:02:03.734Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13427?focusedCommentId=17385693) by Jim Gan (jgan2012):*\nI switched to use PyArrrow 4.0.1 now and will see if such an error occurs again.\r\n\r\n\u00a0\r\n\r\nThe crash in to_pydict() occurred after encountering a corrupted arrow file (read size was different from actual size)\r\n\r\n\u00a0\r\n\r\nIt would be great that such crash could be prevented because it killed the long running pyspark job(pipeline).\u00a0 In my case it is OK to skip or ignore the corrupted arrow files.\r\n\r\n\u00a0\r\n\r\nI added pa_table.validate() in front of to_pydict() which seems to avoid the crash. My pipeline finished successfully with the change.\r\n\r\n\u00a0\r\n```java\n\r\ntry:\r\n   for batch_id, batch in enumerate(arrow_stream):\r\n        pa_table = pa.Table.from_batches([batch]).select(self.schema)\r\n# add validate() that seems to avoid the crash\r\n        pa_table.validate()\r\n        pv_dict = pa_table.to_pydict()\r\n        \r\nexcept Exception as e:\r\n   logging.error(\"file: {} got exception: {}\".format(file, str(e)))\r\n```\r\n\u00a0\r\n\r\nThe error messages in the log:\r\n\r\n(1) got exception: Expected to be able to read 11431400 bytes for message body, got 10389614\r\n\r\n(2) /arrow/cpp/src/arrow/array/data.cc:94: Check failed: (off) <= (length) Slice offset greater than array length"
        },
        {
            "created_at": "2021-07-22T18:13:19.412Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13427?focusedCommentId=17385702) by Antoine Pitrou (apitrou):*\nIndeed, if you may have invalid / corrupt data, calling the `validate()` method is the way to check for it. This is not done by default because validation has a non-zero cost (also, full validation using `validate(full=True)`, while safer, may be more costly even)."
        },
        {
            "created_at": "2021-07-22T18:14:03.526Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13427?focusedCommentId=17385703) by Antoine Pitrou (apitrou):*\nClosing as not a bug, since this is caused by reading an invalid data file. The `validate()` method is able to catch and report the issue."
        }
    ]
}