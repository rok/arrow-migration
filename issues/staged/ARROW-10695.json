{
    "issue": {
        "title": "[C++][Dataset] Allow to use a UUID in the basename_template when writing a dataset",
        "body": "***Note**: This issue was originally created as [ARROW-10695](https://issues.apache.org/jira/browse/ARROW-10695). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently we allow the user to specify a `basename_template`, and this can include a `\"\\{i\\}\"` part to replace it with an automatically incremented integer (so each generated file written to a single partition is unique):\r\n\r\nhttps://github.com/apache/arrow/blob/master/python/pyarrow/dataset.py#L713-L717\r\n\r\nIt _might_ be useful to also have the ability to use a UUID, to ensure the file is unique in general (not only for a single write) and to mimic the behaviour of the old `write_to_dataset` implementation.\r\n\r\nFor example, we could look for a `\"\\{uuid\\}\"` in the template string, and if present replace it for each file with a new UUID.",
        "created_at": "2020-11-23T10:39:51.000Z",
        "updated_at": "2021-09-16T16:26:52.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-06-23T16:06:11.000Z"
    },
    "comments": [
        {
            "created_at": "2020-12-04T10:17:31.336Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17243918) by Joris Van den Bossche (jorisvandenbossche):*\ncc `[~bkietz]` What do you think about this?"
        },
        {
            "created_at": "2020-12-04T18:52:46.417Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17244232) by Lance Dacey (ldacey):*\nFYI, I think this might be necessary for some use cases. For example, I have Airflow extract data from dozens of APIs in parallel and write to the same target partitioned dataset (partitioned based on the Airflow scheduled date, so all files belong in the same batch folder) - this causes the part-0.parquet file to be overwritten each time which results in lost data instead of there being dozens of files.\r\n\r\nFor the meantime, I added the code below. I need to keep the \\{i} it seems or I get an error:\r\n```python\n\r\n            if self.create_uuid_filename:\r\n                basename_template = guid() + \"-{i}.parquet\"\r\n            else:\r\n                basename_template = \"part-{i}.parquet\"\r\n```\r\nguid() is imported from pyarrow.utils"
        },
        {
            "created_at": "2020-12-17T19:30:55.858Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17251308) by Ben Kietzman (bkietz):*\nThis doesn't seem worthwhile to me; if uniqueness across parallel writes is necessary then each write can incorporate a guid or other particle into its basename_template. Writes to \"part-etl-{i}.parquet\" will not collide with writes to \"part-now-{i}.parquet\""
        },
        {
            "created_at": "2021-02-17T17:38:56.278Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17286019) by Antoine Pitrou (apitrou):*\nI'm not sure I understand the proposal. In any case, there's nothing magical about UUIDs. Plain random strings would work as well without being dependent on specific characteristics that UUIDs have."
        },
        {
            "created_at": "2021-02-17T18:01:57.461Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17286036) by Lance Dacey (ldacey):*\nPerhaps this has changed, but I was running into issues when writing to a dataset in parallel. \r\n\r\nFor example, I use Airflow to extract data from 6 different servers in parallel (separate tasks are used to download data from each source \"extract_cms_1\", \"extract_cms_2\") using turbodbc which fetches the data in pyarrow tables --> this data is written to Azure Blob using ds.write_dataset()\r\n\r\nI noticed that the part-{i} names were clashing when this happened. part-0 would be replaced a few times for example, and it seemed random or hinted at race conditions. I have another Airflow DAG which is downloading from 74 different REST APIs as well (the downloads can happen simultaneously but the source and credentials used are different per account).\r\n\r\nAdding the guid() to the filenames solved that issue for me. \r\n\r\nIs there a separate issue open for the partition_filename_cb to be added to ds.write_dataset()? I have been using that feature to \"repartition\" Dataset A with many small files into Dataset B with one file per partition (larger physical file, less fragments)."
        },
        {
            "created_at": "2021-02-22T16:28:26.247Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17288463) by Ben Kietzman (bkietz):*\n`[~ldacey]` could you provide a minimal example which reproduces the collision?"
        },
        {
            "created_at": "2021-03-23T12:07:41.022Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17306996) by Lance Dacey (ldacey):*\nSorry, did not see a notification for this. Hm - I am not sure how to provide a minimal example easily. The issue is when multiple machines are writing to the same dataset at the same time into the same partition. \r\n\r\nFor example, machine A downloads data from server 1 and saves it to the dataset at the same time as machine B downloading data and saving data from server 2.\r\n\r\nMy workaround for now was to  ensure that the basename_template is a unique value. Initially, I was using a UUID filename as the basename_template, but I need to be able to use fs.glob() to get a list of all of the fragments which were just written to process them in downstream tasks. Unfortunately, there is no metadata_collector for ds.write_dataset() yet."
        },
        {
            "created_at": "2021-04-13T11:52:34.974Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320103) by Joris Van den Bossche (jorisvandenbossche):*\nFor me it's clear that you can get collisions because we use identical names when writing. Even aside the \"multiple machines writing in parallel at the same time\", you can also \"reproduce\" it by simply writing twice to the same directory. Dummy example:\r\n\r\n```python\n\r\n>>> table1 = pa.table({'a': [1, 2, 3]})\r\n>>> ds.write_dataset(table1, \"test_dataset_overwrite\", format=\"parquet\")\r\n>>> table2 = pa.table({'a': [4, 5, 6]})\r\n>>> ds.write_dataset(table2, \"test_dataset_overwrite\", format=\"parquet\")\r\n>>> ds.dataset(\"test_dataset_overwrite/\", format=\"parquet\").to_table().to_pandas()\r\n   a\r\n0  4\r\n1  5\r\n2  6\r\n```\r\n\r\nWith the current API, for the above dummy example it's of course obvious that it has overwritten the first one. But in the end, it's the same issue as what `[~ldacey]` tries to explain with the multiple machines writing in parallel case.\r\n\r\nIf you want to avoid such overwriting of existing files, you need to ensure that the writer uses unique file names, instead of using the default, fixed `part-\\{0\\}.ext`. The eventual question is then:\r\n\r\n- Do we leave this as the responsibility of the user (as `[~ldacey]` did with eg `basename_template = guid() + \"-\\{i\\}.parquet\"`\n- Or do we want to provide this as a bit of convenience that you can obtain unique file names by allowing the `\\{uuid\\`} in the template string?\n  \n  Since the first option is also quite straightforward, it's maybe good enough to leave this as the responsibility of the user (and we could add an example to do this with python in the docs). \n  Unless we would want to make this the default, though, to include a uuid in the base template (as I think eg spark does, and what we actually did in the legacy implementation of `parquet.write_to_dataset`).\n  \n  The topic here is also closely related to the issue of \"overwriting\" vs \"appending\" a dataset (eg ARROW-7706, although that's for the legacy ParquetDataset implementation)"
        },
        {
            "created_at": "2021-04-13T12:00:59.230Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320112) by Joris Van den Bossche (jorisvandenbossche):*\n> Is there a separate issue open for the partition_filename_cb to be added to ds.write_dataset()? I have been using that feature to \"repartition\" Dataset A with many small files into Dataset B with one file per partition (larger physical file, less fragments).\r\n\r\n`[~ldacey]` Could you open an issue for it, describing the use case for which you needed `partition_filename_cb` ?  \r\nIt's not added in the new dataset implementation (it's also harder to implement since it would need a Python callback from C++), so it would be good to first see what are the use cases and if that needs such a callback or if it can be solved differently. \r\n\r\n> Unfortunately, there is no metadata_collector for ds.write_dataset() yet.\r\n\r\nSame for this one, it would be interesting to describe your use case for it in a new JIRA.\r\n"
        },
        {
            "created_at": "2021-04-13T12:03:30.755Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320115) by Antoine Pitrou (apitrou):*\nI think this should definitely be the responsibility of the user. This means the user chooses whichever random generation scheme they prefer. Some users may prefer to use a timestamp, etc."
        },
        {
            "created_at": "2021-04-13T12:14:31.506Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320130) by Antoine Pitrou (apitrou):*\nSide note: perhaps we could still implement an option to raise an error instead of overwriting existing files."
        },
        {
            "created_at": "2021-04-13T12:29:39.185Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320136) by Joris Van den Bossche (jorisvandenbossche):*\n> Side note: perhaps we could still implement an option to raise an error instead of overwriting existing files.\r\n\r\nIndeed, I was just writing a new JIRA for that: ARROW-12358\r\n\r\n> I think this should definitely be the responsibility of the user. This means the user chooses whichever random generation scheme they prefer. Some users may prefer to use a timestamp, etc.\r\n\r\nI am personally certainly fine with leaving it to the user (in the end, it's quite straightforward with the filename template, and indeed the user can put anything they want in it). \r\nI think possible reasons to potentially special case uuid is because that's 1) what we actually used before, and 2) it's what (I think) spark does. \r\n"
        },
        {
            "created_at": "2021-04-13T13:08:18.568Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320167) by Lance Dacey (ldacey):*\nI have been creating my own basename_template with either a uuid or a name with the task+timestamp of when the data was processed and it has worked well. I like that approach better than the uuid filename actually. \r\n\r\nI think the remaining issue with the default part-\\{i\\} template is that it can also be a bit inconsistent when writing data in loops. Say I am processing a directory of files one by one in a loop and I partition the data on the \"date\" column. A lot of the files will just overwrite the part-0.parquet file, but you might also see part-11.parquet or another random filename. I suppose the surprising part is that write_dataset() does not always append new random files nor does it **always** overwrite what is there. This does not impact me now that I customize the basename_template though, but I think an \"append\" or \"replace\" flag would make a lot of sense\r\n\r\nI'll open another issue with my use case for metadata_collector and partition_filename_cb which I am using heavily"
        },
        {
            "created_at": "2021-04-13T13:35:09.592Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320183) by Joris Van den Bossche (jorisvandenbossche):*\n> I think the remaining issue with the default part-\\{i\\} template is that it can also be a bit inconsistent when writing data in loops. ... but I think an \"append\" or \"replace\" flag would make a lot of sense\r\n\r\nYes, agreed. Let's keep that in ARROW-12358 (feel free to add that comment there)\r\n\r\n> I'll open another issue with my use case for metadata_collector and partition_filename_cb which I am using heavily\r\n\r\nThanks!"
        },
        {
            "created_at": "2021-04-13T14:00:53.192Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320192) by Lance Dacey (ldacey):*\n`[~jorisvandenbossche]` \r\npartition_filename_cb: https://issues.apache.org/jira/browse/ARROW-12358\r\nmetadata_collector: https://issues.apache.org/jira/browse/ARROW-12365"
        },
        {
            "created_at": "2021-06-23T16:06:11.864Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17368315) by Antoine Pitrou (apitrou):*\nOk, closing this issue, then."
        }
    ]
}