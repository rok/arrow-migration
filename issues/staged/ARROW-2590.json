{
    "issue": {
        "title": "[Python] Pyspark python_udf serialization error on grouped map (Amazon EMR)",
        "body": "***Note**: This issue was originally created as [ARROW-2590](https://issues.apache.org/jira/browse/ARROW-2590). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI am writing a python_udf grouped map aggregation on Spark 2.3.0 in Amazon EMR. When I try to run any aggregation, I get the following Python stack trace:\r\n> `18/05/16 14:08:56 ERROR Utils: Aborting task`\n> ` org.apache.spark.api.python.PythonException: Traceback (most recent call last):`\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/worker.py\", line 229, in m`}}\n> ` ain`\n> ` \\{{ process()`}}\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/worker.py\", line 224, in p`}}\n> ` rocess`\n> ` \\{{ serializer.dump_stream(func(split_index, iterator), outfile)`}}\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/serializers.py\", line 261,`}}\n> ` \\{{ in dump_stream`}}\n> ` \\{{ batch = _create_batch(series, self._timezone)`}}\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/serializers.py\", line 239,`}}\n> ` \\{{ in _create_batch`}}\n> ` {{ arrs = [create_array(s, t) for s, t in series]`}}\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/serializers.py\", line 239,`}}\n> ` \\{{ in <listcomp>`}}\n> ` {{ arrs = [create_array(s, t) for s, t in series]`}}\n> ` \\{{ File \"/mnt/yarn/usercache/hadoop/appcache/application_1526400761989_0068/container_1526400761989_0068_01_000002/pyspark.zip/pyspark/serializers.py\", line 237, in create_array`}}\n> ` \\{{ return pa.Array.from_pandas(s, mask=mask, type=t)`}}\n> ` \\{{ File \"array.pxi\", line 372, in pyarrow.lib.Array.from_pandas`}}\n> ` \\{{ File \"array.pxi\", line 177, in pyarrow.lib.array`}}\n> ` \\{{ File \"array.pxi\", line 77, in pyarrow.lib._ndarray_to_array`}}\n> ` \\{{ File \"error.pxi\", line 98, in pyarrow.lib.check_status`}}\n> ` pyarrow.lib.ArrowException: Unknown error: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)`\r\nTo be clear, this happens when I run any aggregation, including the identity aggregation (return the Pandas DataFrame that was passed in). I do not get this error when I return an empty DataFrame, so it seems to be a symptom of the serialization of the Pandas DataFrame back to Spark.\r\n\r\nI have observed this behavior with the following versions:\r\n \\* Spark 2.3.0\r\n \\* PyArrow 0.9.0 (also 0.8.0)\r\n \\* Pandas 0.22.0 (also 0.22.1)\r\n \\* Numpy 1.14.1\r\n\r\nHere is some sample code:\r\n> `@func.pandas_udf(SCHEMA, func.PandasUDFType.GROUPED_MAP)`\r\n> `def aggregation(df):`\r\n> `\u00a0 \u00a0 return df`\r\n> `df.groupBy('a').apply(aggregation) # get error`",
        "created_at": "2018-05-16T14:21:28.000Z",
        "updated_at": "2022-08-27T14:42:05.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-04-22T17:55:29.000Z"
    },
    "comments": [
        {
            "created_at": "2018-05-22T17:51:51.342Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16484358) by Lee Patrick (theletterd):*\nRunning into the same exact issue \u2013 any updates or workarounds for this?"
        },
        {
            "created_at": "2018-08-06T15:23:08.927Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16570351) by Ross Cohen (BossColo):*\nThis is from a mismatch between your schema and what's in your dataframe. I've had this error before as well."
        },
        {
            "created_at": "2019-02-25T18:21:10.724Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16777173) by Stanislav Stolpovskiy (aberonxp):*\nRoss, thank you for your comment!\r\n\r\n\u00a0\r\n\r\nI have the same error:\r\n```java\n\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 253, in main\r\n\r\n\u00a0 \u00a0 process()\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 248, in process\r\n\r\n\u00a0 \u00a0 serializer.dump_stream(func(split_index, iterator), outfile)\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\r\n\r\n\u00a0 \u00a0 batch = _create_batch(series, self._timezone)\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 246, in _create_batch\r\n\r\n\u00a0 \u00a0 arrs = [create_array(s, t) for s, t in series]\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 246, in <listcomp>\r\n\r\n\u00a0 \u00a0 arrs = [create_array(s, t) for s, t in series]\r\n\r\n\u00a0 File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 244, in create_array\r\n\r\n\u00a0 \u00a0 return pa.Array.from_pandas(s, mask=mask, type=t)\r\n\r\n\u00a0 File \"pyarrow/array.pxi\", line 531, in pyarrow.lib.Array.from_pandas\r\n\r\n\u00a0 File \"pyarrow/array.pxi\", line 171, in pyarrow.lib.array\r\n\r\n\u00a0 File \"pyarrow/array.pxi\", line 80, in pyarrow.lib._ndarray_to_array\r\n\r\n\u00a0 File \"pyarrow/error.pxi\", line 81, in pyarrow.lib.check_status\r\n\r\npyarrow.lib.ArrowInvalid: 'utf-32-le' codec can't decode bytes in position 0-3: code point not in range(0x110000)```\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nAfter rechecking all fields in returned output dataframe\u00a0I have found that field type is not equal with schema provided in UDF method.\u00a0\r\n\r\nAfter changing type issue gone.\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2019-02-27T09:59:07.018Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16779080) by Uwe Korn (uwe):*\nCan someone post a reproducible example?"
        },
        {
            "created_at": "2019-03-13T20:01:52.843Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16792065) by sacheendra talluri (sacheendra):*\nThe problem occurs when the **order** of columns in the pandas dataframe doesn't match the order of columns specified in the UDF return type.\r\n\r\nA reproducible code example is present below. In it, notice that the first group-apply succeeds while the second one fails.\r\n\r\nIdeally, the order of columns in the pandas dataframe and the type definition of the UDF shouldn't matter. If it does, it should be explicitly documented here: https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html\r\n\r\n\u00a0\r\n```java\n\r\nimport pandas as pd\r\nimport pyspark.sql.functions as F\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.sql import SparkSession\r\n\r\n\r\ntype_info1 = StructType([\r\n    StructField(\"a\", StringType(), False),\r\n    StructField(\"b\", LongType(), False),\r\n])\r\n\r\ntype_info2 = StructType([\r\n    StructField(\"b\", LongType(), False),\r\n    StructField(\"a\", StringType(), False),\r\n])\r\n\r\n\r\n@F.pandas_udf(returnType=type_info1, functionType=F.PandasUDFType.GROUPED_MAP)\r\ndef fun1(df):\r\n    return pd.DataFrame([{\"a\": \"cool\", \"b\": 1}])\r\n\r\n@F.pandas_udf(returnType=type_info2, functionType=F.PandasUDFType.GROUPED_MAP)\r\ndef fun2(df):\r\n    return pd.DataFrame([{\"a\": \"cool\", \"b\": 1}])\r\n\r\ndef main():\r\n    spark = SparkSession.builder \\\r\n        .master(\"local[4]\") \\\r\n        .appName(\"type error demo\") \\\r\n        .getOrCreate()\r\n\r\n    df1 = spark.createDataFrame([{\"sample\": 1}])\r\n# First groupby\r\n    df1_mod1 = df1.groupBy(\"sample\").apply(fun1)\r\n    print(df1_mod1.collect())\r\n# Second groupby\r\n    df1_mod2 = df1.groupBy(\"sample\").apply(fun2)\r\n    print(df1_mod2.collect())\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```"
        },
        {
            "created_at": "2019-04-15T21:50:36.184Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16818419) by Christopher Groskopf (onyxfish):*\nI experienced this issue running on DataBricks 5.3 (Spark 2.4). My traceback is nearly identical to the OP:\u00a0\r\n```java\n\r\n...\r\nFile \"/databricks/spark/python/pyspark/serializers.py\", line 264, in create_array\r\nreturn pa.Array.from_pandas(s, mask=mask, type=t, safe=False)\r\nFile \"pyarrow/array.pxi\", line 536, in pyarrow.lib.Array.from_pandas\r\nFile \"pyarrow/array.pxi\", line 176, in pyarrow.lib.array\r\nFile \"pyarrow/array.pxi\", line 85, in pyarrow.lib._ndarray_to_array\r\nFile \"pyarrow/error.pxi\", line 81, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: 'utf-32-le' codec can't decode bytes in position 0-3: code point in surrogate code point range(0xd800, 0xe000)\r\n\u00a0\n```\r\nAs best I can tell this is something _specifically_ to do with returning string data when LongType is expected.\u00a0The reproducible example above does **not** fail for me and so far I haven't been able to write a test that\u00a0replicates the issue locally.\r\n\r\nI have managed to\u00a0work around it fairly easily by changing the column to return string instead of long. (Which is actually what it should have been in the first place.) However, this was pretty difficult to diagnose and I suspect there is some general case here that should be getting\u00a0validated higher up the call stack.\r\n\r\nVersions:\r\n \\* PySpark: 2.4.0\r\n \\* pandas 0.23.4\r\n \\* pyarrow: 0.12.1"
        },
        {
            "created_at": "2019-04-22T14:38:05.579Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16823143) by Wes McKinney (wesm):*\nThanks, hopefully `[~bryanc]` or someone more intimately familiar with the details can reproduce the issue"
        },
        {
            "created_at": "2019-04-22T17:54:20.037Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=16823289) by Bryan Cutler (bryanc):*\nI wasn't able to reproduce the exact error from above, when I mix up a long column with string data I get:\r\n```\n\r\npyarrow.lib.ArrowNotImplementedError: No cast implemented from string to int64\n```\r\nI am using Linux Ubuntu 18.04 and tried Spark 2.3 and master, pyarrow 0.8.0,\u00a0 0.10.0, 0.12.1.\r\n\r\nIf this is a matter of mixing up columns, there was a JIRA SPARK-24324 that changed column assignment to use name by default, fixed in 2.4.0. You can work around that by specifying column labels explicitly when creating the DataFrame to match the expected schema, for example:\r\n```java\n\r\n@pandas_udf(\"z string, a long\", PandasUDFType.GROUPED_MAP)\r\ndef foo(_):\r\n    return pd.DataFrame({'a': [1], 'z': ['hi']}, columns=['z', 'a'])\n```\r\nI'll close this for now, but feel free to reopen if this doesn't solve your issue."
        },
        {
            "created_at": "2022-08-27T14:42:05.462Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2590?focusedCommentId=17586034) by @toddfarmer:*\nTransitioning issue from Resolved to Closed to based on resolution field value."
        }
    ]
}