{
    "issue": {
        "title": "[R] Convert R character vector with data exceeding 2GB to Large type",
        "body": "***Note**: This issue was originally created as [ARROW-3308](https://issues.apache.org/jira/browse/ARROW-3308). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n",
        "created_at": "2018-09-24T09:48:37.000Z",
        "updated_at": "2020-07-10T16:24:52.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2020-07-10T16:23:53.000Z"
    },
    "comments": [
        {
            "created_at": "2019-01-03T12:58:43.020Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=16733003) by Romain Francois (romainfrancois):*\nI'm not sure how this plays with the global string cache in R.\u00a0"
        },
        {
            "created_at": "2019-01-03T17:41:38.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=16733277) by Wes McKinney (wesm):*\nThis shouldn't have to do with the global string hash table. Try creating an R character array with 2^20 values with the same string string having more than 2^11 characters, then converting it to Arrow"
        },
        {
            "created_at": "2019-02-06T15:10:41.972Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=16761831) by Wes McKinney (wesm):*\n`[~romainfrancois]` do you want to look at this? Fixing it amounts to using `arrow::internal::ChunkedBinaryBuilder` instead of `arrow::BinaryBuilder`"
        },
        {
            "created_at": "2020-06-16T16:38:04.988Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17136801) by Neal Richardson (npr):*\nWhat criteria do we use to determine that the data exceeds 2GB in Arrow (without trying first, catching the error, and retrying with chunks)? Or should we always use some chunked builder and then concatenate the resulting chunks if possible?"
        },
        {
            "created_at": "2020-06-24T21:03:48.540Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17144390) by Wes McKinney (wesm):*\nAlways using the chunked builder is actually faster I've found if you don't care about squeezing into a single array when it fits"
        },
        {
            "created_at": "2020-06-24T21:08:34.603Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17144395) by Neal Richardson (npr):*\nIIUC that works if you're converting to a Table, but what about in a RecordBatch, where you can't have chunked arrays? Use LargeBinary/LargeString instead of chunking?"
        },
        {
            "created_at": "2020-06-24T21:24:25.784Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17144412) by Wes McKinney (wesm):*\nYeah, either that or error. Does it error now?"
        },
        {
            "created_at": "2020-06-24T21:30:21.974Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17144417) by Neal Richardson (npr):*\nYes, this errors now, hence the ticket."
        },
        {
            "created_at": "2020-07-10T16:23:53.662Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3308?focusedCommentId=17155590) by Wes McKinney (wesm):*\nIssue resolved by pull request 7611\n<https://github.com/apache/arrow/pull/7611>"
        }
    ]
}