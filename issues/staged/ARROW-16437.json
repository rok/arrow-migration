{
    "issue": {
        "title": "[Python] Mocking S3 tests with moto not currently feasible",
        "body": "***Note**: This issue was originally created as [ARROW-16437](https://issues.apache.org/jira/browse/ARROW-16437). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n## Unable to use moto to mock S3 for testing purposes.\r\n\r\n\u00a0\r\n\r\nI've been using AWSWrangler as a loading utility in a custom application and am attempting to remove it as a dependency because PyArrow Dataset is capable of providing all the s3 functionality I need.\r\n\r\n\u00a0\r\n\r\nThe issue stems from the fact that when PyArrow attempts to determine the FileSystem type it appears to be sidestepping moto and is failing with:\r\n\r\n\u00a0\r\n```java\n\r\nroot@996521aaba72:/workspaces/custom_application# python -m pytest tests/custom_utilities/loading/test_load.py::test__pull_cached_data\r\n====================================================================================================================== test session starts =======================================================================================================================\r\nplatform linux -- Python 3.9.10, pytest-7.1.2, pluggy-1.0.0\r\nrootdir: /workspaces/custom_application, configfile: tox.ini\r\nplugins: hypothesis-6.45.0, cov-3.0.0\r\ncollected 1 item \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0tests/custom_utilities/loading/test_load.py F \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0[100%]============================================================================================================================ FAILURES ============================================================================================================================\r\n_____________________________________________________________________________________________________________________ test__pull_cached_data _____________________________________________________________________________________________________________________\u00a0 \u00a0 @pytest.mark.usefixtures(\"s3\")\r\n\u00a0 \u00a0 def test__pull_cached_data():\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Tests pull cached data, both happy and sad.\"\"\"\r\n# Here we're going to make a folder, transfer in some files,\r\n# \u00a0 and pull them!\r\n\u00a0 \u00a0 \u00a0 \u00a0 with tempfile.TemporaryDirectory() as t:\r\n# This works fine\r\n# from awswrangler.s3 import read_parquet\r\n# sillything = read_parquet('s3://test-bucket/test_metadata.parquet')\r\n> \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 a = ds.dataset('s3://test-bucket/test_metadata.parquet')tests/custom_utilities/loading/test_load.py:156:\u00a0\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\u00a0\r\n/usr/local/lib/python3.9/site-packages/pyarrow/dataset.py:667: in dataset\r\n\u00a0 \u00a0 return _filesystem_dataset(source, **kwargs)\r\n/usr/local/lib/python3.9/site-packages/pyarrow/dataset.py:412: in _filesystem_dataset\r\n\u00a0 \u00a0 fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n/usr/local/lib/python3.9/site-packages/pyarrow/dataset.py:373: in _ensure_single_source\r\n\u00a0 \u00a0 filesystem, path = _resolve_filesystem_and_path(path, filesystem)\r\n/usr/local/lib/python3.9/site-packages/pyarrow/fs.py:179: in _resolve_filesystem_and_path\r\n\u00a0 \u00a0 filesystem, path = FileSystem.from_uri(path)\r\npyarrow/_fs.pyx:350: in pyarrow._fs.FileSystem.from_uri\r\n\u00a0 \u00a0 ???\r\npyarrow/error.pxi:143: in pyarrow.lib.pyarrow_internal_check_status\r\n\u00a0 \u00a0 ???\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\u00a0> \u00a0 ???\r\nE \u00a0 OSError: When resolving region for bucket 'test-bucket': AWS Error [code 99]: curlCode: 35, SSL connect errorpyarrow/error.pxi:114: OSError\r\n--------------------------------------------------------------------------------------------------------------------- Captured stderr setup ----------------------------------------------------------------------------------------------------------------------\r\nINFO:botocore.credentials:Found credentials in environment variables.\r\n----------------------------------------------------------------------------------------------------------------------- Captured log setup -----------------------------------------------------------------------------------------------------------------------\r\nINFO \u00a0 \u00a0 botocore.credentials:credentials.py:1114 Found credentials in environment variables.\r\n==================================================================================================================== short test summary info =====================================================================================================================\r\nFAILED tests/custom_utilities/loading/test_load.py::test__pull_cached_data - OSError: When resolving region for bucket 'test-bucket': AWS Error [code 99]: curlCode: 35, SSL connect error\r\n======================================================================================================================= 1 failed in 17.69s =======================================================================================================================\r\nroot@996521aaba72:/workspaces/custom_application#\u00a0\n```\r\n\u00a0\r\n\r\nIf someone is willing to tell me what I should be doing to appropriately format the shell output above, please let me know, but I think the takeaway is `OSError: When resolving region for bucket 'test-bucket': AWS Error [code 99]: curlCode: 35, SSL connect error`.\r\n\r\nPlease let me know if you need additional information!",
        "created_at": "2022-05-02T15:48:06.000Z",
        "updated_at": "2022-05-03T13:42:41.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-05-03T07:48:11.000Z"
    },
    "comments": [
        {
            "created_at": "2022-05-02T15:56:19.072Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17530795) by Antoine Pitrou (apitrou):*\nCan you show the code you're using?"
        },
        {
            "created_at": "2022-05-02T16:01:03.870Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17530799) by Timothy Luna (timothyluna):*\nAbsolutely. This is a simplified conftest which reproduces the basic capability.\r\n```java\n\r\nimport boto3\r\nimport os\r\nimport pandas as pd\r\nimport pytest\r\nimport tests.test_data.test_data_generator as tdg\r\nfrom tests.test_data.metadata_dataset import metadata_dataset\r\nfrom moto import mock_s3# Add more stuff to here to make more smarter.\r\n__LOCAL_TESTING__ = os.getenv(\"TESTING\") == \"1\" or os.getenv(\"TESTING\") == 1\r\n@pytest.fixture(scope=\"package\")\r\ndef _aws_credentials():\r\n\u00a0 \u00a0 \"\"\"Mock AWS Credentials for moto.\"\"\"\r\n\u00a0 \u00a0 os.environ[\"AWS_ACCESS_KEY_ID\"] = \"testing\" \u00a0# nosec\r\n\u00a0 \u00a0 os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"testing\" \u00a0# nosec\r\n\u00a0 \u00a0 os.environ[\"AWS_SECURITY_TOKEN\"] = \"testing\" \u00a0# nosec\r\n\u00a0 \u00a0 os.environ[\"AWS_SESSION_TOKEN\"] = \"testing\" \u00a0# nosec\r\n\u00a0 \u00a0 os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\" \u00a0# nosec\r\n@pytest.fixture(scope=\"package\", autouse=True)\r\ndef s3(_aws_credentials):\r\n\u00a0 \u00a0 \"\"\"Add an s3 fixture for use in testing.\"\"\"\r\n\u00a0 \u00a0 with mock_s3():\r\n\u00a0 \u00a0 \u00a0 \u00a0 if not __LOCAL_TESTING__:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise Exception(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"CANNOT MODIFY PRODUCTION METADATA! If you're running this locally run `export TESTING=1`\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\r\n\u00a0 \u00a0 \u00a0 \u00a0 client = boto3.client(\"s3\", region_name=\"us-east-1\") \r\n# Below here I create a bucket and upload some files.\r\n\r\n@pytest.mark.usefixtures(\"s3\")\r\ndef test__pull_cached_data():\r\n\u00a0 \u00a0 \"\"\"Tests pull cached data, both happy and sad.\"\"\"\r\n# Here we're going to make a folder, transfer in some files,\r\n# \u00a0 and pull them!\r\n\u00a0 \u00a0 with tempfile.TemporaryDirectory() as t:\r\n\u00a0 \u00a0 \u00a0 \u00a0 #from awswrangler.s3 import read_parquet\r\n# stupidthing = read_parquet('s3://test-bucket/test_metadata.parquet')\r\n\u00a0 \u00a0 \u00a0 \u00a0 a = ds.dataset('s3://test-bucket/test_metadata.parquet')\n```\r\n\u00a0\r\n\r\nI think I found my issue with pasting and I'll clean up the above.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-05-02T16:16:09.011Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17530803) by Antoine Pitrou (apitrou):*\nAh, thank you. Well, this is simple: when you give it a \"`s3://...`\" URI, Arrow uses its own native S3 support (not based on `boto`), so `moto` isn't able to intercept anything.\r\n\r\nI'm not sure why you want to mock S3 calls, though. If you just want to test your own code, perhaps you can give it a non-S3 URI?"
        },
        {
            "created_at": "2022-05-02T16:16:21.421Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17530805) by Antoine Pitrou (apitrou):*\ncc `[~jorisvandenbossche]`"
        },
        {
            "created_at": "2022-05-02T17:41:53.029Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17530866) by Timothy Luna (timothyluna):*\nAntoine, thanks.\r\n\r\n\u00a0\r\n\r\nSome of my code is a loading utility which allows pulling files by 'name' from a managed metadata solution; one of the enabled technologies is S3.\r\n\r\n\u00a0\r\n\r\nMy testing validates that the loading utility appropriately pulls data into scope (now using PyArrow Dataset instead of wrangler.)"
        },
        {
            "created_at": "2022-05-03T07:47:34.098Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17531089) by Antoine Pitrou (apitrou):*\nIf you really want to test S3, there are possible solutions for this such as Minio, which allows to run a local S3 server: https://min.io/ (we use it for our own tests)."
        },
        {
            "created_at": "2022-05-03T07:48:11.120Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17531090) by Antoine Pitrou (apitrou):*\nThis is not something that can possibly be fixed, as our S3 filesystem is implemented in C++."
        },
        {
            "created_at": "2022-05-03T13:42:41.737Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16437?focusedCommentId=17531206) by Joris Van den Bossche (jorisvandenbossche):*\nI don't think you can use the \"mock_s3\" method of moto directly with our S3 filesystem integration (as Antoine said, we don't use boto).  A possible way to do this is to use fsspec's s3fs, and then pass this filesystem object (and pyarrow will wrap that in a PyFileSystem), but that adds another layer of indirection which you don't need here (and which can be another source of failures).\r\n\r\nAnother option is to use the \"moto_server\" feature of moto (http://docs.getmoto.org/en/latest/docs/getting_started.html#stand-alone-server-mode), which gives you an endpoint url, that can be used to construct a pyarrow S3FileSystem that interacts with the moto server. \r\n\r\nThis is basically the approach that we use ourselves (but with MinIO instead of moto), and eg also dask switched from mock_s3 to moto_server in their tests (see eg https://github.com/dask/dask/blob/4d6a5f08c45be56302f696ca4ef6038a1cd1e734/dask/bytes/tests/test_s3.py#L84)"
        }
    ]
}