{
    "issue": {
        "title": "[Python] to_pandas() causing datetimes > pd.Timestamp.max to wrap around",
        "body": "***Note**: This issue was originally created as [ARROW-7856](https://issues.apache.org/jira/browse/ARROW-7856). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen writing a dataframe containing `datetime.datetime` in an object columns any datetime that is greater than pd.Timestamp.max or less than pd.Timestamp.min is wrapped around.\r\n\r\n\u00a0\r\n\r\nFor reference these are the timestamp min and max values.\r\n\r\n\u00a0\r\n```java\n\r\nIn [43]: pd.Timestamp.max\r\nOut[43]: Timestamp('2262-04-11 23:47:16.854775807')\r\nIn [44]: pd.Timestamp.min\r\nOut[44]: Timestamp('1677-09-21 00:12:43.145225')\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nTo reproduce the error using pandas\r\n\r\n\u00a0\r\n```java\n\r\nIn [49]: df = pd.DataFrame({\"A\":[datetime.datetime(2262,4,12)]})\r\n\r\nIn [50]: df\r\nOut[50]:\r\n                     A\r\n0  2262-04-12 00:00:00\r\n\r\nIn [51]: df.to_parquet(\"datetimething.parquet\")\r\n\r\nIn [52]: pd.read_parquet(\"datetimething.parquet\")\r\nOut[52]:\r\n                              A\r\n0 1677-09-21 00:25:26.290448384\r\n\r\n```\r\nI have narrowed it down as far as to note that it is happening when converting a `pa.Table` using the `to_pandas()` method.\r\n```java\n\r\nIn [30]: df = pd.DataFrame({\"A\":[datetime.datetime(2262,4,12)]})\r\nIn [31]: tf = pa.Table.from_pandas(df)\r\nIn [32]: tf.columns\r\nOut[32]: [<pyarrow.lib.ChunkedArray object at 0x7f23884deef8>\r\n [\r\n   [\r\n     2262-04-12 00:00:00.000000\r\n   ]\r\n ]\r\n]\r\nIn [33]: tf.to_pandas()\r\nOut[33]:                      A\r\n0 1677-09-21 00:25:26.290448384\r\n```",
        "created_at": "2020-02-14T06:02:30.000Z",
        "updated_at": "2020-02-14T12:43:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-02-14T12:43:37.000Z"
    },
    "comments": [
        {
            "created_at": "2020-02-14T10:45:22.590Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7856?focusedCommentId=17036899) by Joris Van den Bossche (jorisvandenbossche):*\n`[~kevinglasson]` Thanks for the report! \r\n\r\nThis looks similar as ARROW-7758, and is going to fixed by https://github.com/apache/arrow/pull/6358. However, that PR will ensure it properly raises an error instead of silently giving wrong timetamps. Ideally there should also be an option to convert to datetime.datetime objects. There is a `date_as_object=True` option, but that doesn't seem to work for timestamps. "
        },
        {
            "created_at": "2020-02-14T12:38:36.771Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7856?focusedCommentId=17036953) by Joris Van den Bossche (jorisvandenbossche):*\nSo with the linked PR, you would now get:\r\n\r\n```Java\n\r\nIn [1]: pd.read_parquet(\"datetimething.parquet\")                                                                                                                                                                   \r\n...\r\nArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds timestamp: 9223372800000000\r\n```\r\n\r\n(the error message could be nicer by formatting it as a timestamp instead of integer)\r\n\r\nThat at least avoids you get wrong data, but still doesn't give a way to actually read the data into pandas."
        },
        {
            "created_at": "2020-02-14T12:43:15.220Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7856?focusedCommentId=17036956) by Joris Van den Bossche (jorisvandenbossche):*\nFor that last part, there is this issue: ARROW-5359 that proposes to add a `timestamp_as_object` option for that. \r\n\r\nSo I am going to close this issue, as I think it is covered by the other linked issues."
        }
    ]
}