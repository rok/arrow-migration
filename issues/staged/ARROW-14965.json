{
    "issue": {
        "title": "[Python][C++] Contention when reading Parquet files with multi-threading",
        "body": "***Note**: This issue was originally created as [ARROW-14965](https://issues.apache.org/jira/browse/ARROW-14965). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm attempting to read a table from multiple Parquet files where I already know which row_groups I want to read from each file. I also want to apply a filter expression while reading. To do this my code looks roughly like this:\r\n\r\n\u00a0\r\n```java\n\r\ndef read_file(filepath):\r\n\u00a0 \u00a0 format = ds.ParquetFileFormat(...)\r\n\u00a0 \u00a0 fragment = format.make_fragment(filepath, row_groups=[0, 1, 2, ...])\r\n\u00a0 \u00a0 scanner = ds.Scanner.from_fragment(\r\n        fragment, \r\n        use_threads=True,\r\n        use_async=False,\r\n        filter=...\r\n    )\r\n\u00a0 \u00a0 return scanner.to_reader().read_all()\r\n\r\nwith ThreadPoolExecutor() as pool:\r\n\u00a0 \u00a0 pa.concat_tables(pool.map(read_file, file_paths)) \n```\r\nRunning with a ProcessPoolExecutor, each of my 13 read_file calls takes at most 2 seconds. However, with a ThreadPoolExecutor some of the read_file calls take 20+ seconds.\r\n\r\n\u00a0\r\n\r\nI've tried running this with various combinations of use_threads and use_async to try and see what's happening. The code blocks are sourced from py-spy, and identifying contention was done with viztracer.\r\n\r\n\u00a0\r\n\r\n**use_threads: False, use_async: False**\r\n \\* It looks like pyarrow._dataset.Scanner.to_reader doesn't release the GIL: <https://github.com/apache/arrow/blob/be9a22b9b76d9cd83d85d52ffc2844056d90f367/python/pyarrow/_dataset.pyx#L3278-L3283>\r\n \\* pyarrow._dataset.from_fragment seems to be contended. Py-spy suggests this is around getting the physical_schema from the fragment?\r\n\r\n\u00a0\r\n```java\n\r\nfrom_fragment (pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so)\r\n__pyx_getprop_7pyarrow_8_dataset_8Fragment_physical_schema (pyarrow/_dataset.cpython-37m-x86_64-linux-gnu.so)\r\n__pthread_cond_timedwait (libpthread-2.17.so) \n```\r\n\u00a0\r\n\r\n**use_threads: False, use_async: True**\r\n \\* There's no longer any contention for pyarrow._dataset.from_fragment\r\n \\* But there's lots of contention for pyarrow.lib.RecordBatchReader.read_all\r\n\r\n\u00a0\r\n```java\n\r\narrow::RecordBatchReader::ReadAll (pyarrow/libarrow.so.600)\r\narrow::dataset::(anonymous namespace)::ScannerRecordBatchReader::ReadNext (pyarrow/libarrow_dataset.so.600)\r\narrow::Iterator<arrow::dataset::TaggedRecordBatch>::Next<arrow::GeneratorIterator<arrow::dataset::TaggedRecordBatch> > (pyarrow/libarrow_dataset.so.600)\r\narrow::FutureImpl::Wait (pyarrow/libarrow.so.600) \r\nstd::condition_variable::wait (libstdc++.so.6.0.19)\n```\r\n**use_threads: True, use_async: False**\r\n \\* Appears to be some contention on Scanner.to_reader\r\n \\* But most contention remains for RecordBatchReader.read_all\r\n\r\n```java\n\r\narrow::RecordBatchReader::ReadAll (pyarrow/libarrow.so.600)\r\narrow::dataset::(anonymous namespace)::ScannerRecordBatchReader::ReadNext (pyarrow/libarrow_dataset.so.600)\r\narrow::Iterator<arrow::dataset::TaggedRecordBatch>::Next<arrow::FunctionIterator<arrow::dataset::(anonymous namespace)::SyncScanner::ScanBatches(arrow::Iterator<std::shared_ptr<arrow::dataset::ScanTask> >)::{lambda()#1}, arrow::dataset::TaggedRecordBatch> > (pyarrow/libarrow_dataset.so.600)\r\nstd::condition_variable::wait (libstdc++.so.6.0.19)\r\n__pthread_cond_wait (libpthread-2.17.so) \n```\r\n**use_threads: True, use_async: True**\r\n \\* Contention again mostly for RecordBatchReader.read_all, but seems to complete in ~12 seconds rather than 20\r\n\r\n```java\n\r\narrow::RecordBatchReader::ReadAll (pyarrow/libarrow.so.600)\r\narrow::dataset::(anonymous namespace)::ScannerRecordBatchReader::ReadNext (pyarrow/libarrow_dataset.so.600)\r\narrow::Iterator<arrow::dataset::TaggedRecordBatch>::Next<arrow::GeneratorIterator<arrow::dataset::TaggedRecordBatch> > (pyarrow/libarrow_dataset.so.600)\r\narrow::FutureImpl::Wait (pyarrow/libarrow.so.600)\r\nstd::condition_variable::wait (libstdc++.so.6.0.19)\r\n__pthread_cond_wait (libpthread-2.17.so) \n```\r\nIs this expected behaviour? Or should it be possible to achieve the same performance from multi-threading as from multi-processing?\r\n\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2021-12-02T13:17:16.000Z",
        "updated_at": "2021-12-08T21:52:49.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-12-02T19:01:35.874Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17452563) by Weston Pace (westonpace):*\nI'm not sure that contention is the right word here.  ReadAll is indeed a long blocking method but it should be releasing the GIL and should not be holding any locks during this operation.  Do you think that is not happening?  I will try and play with this some more later today.\r\n"
        },
        {
            "created_at": "2021-12-03T02:59:25.796Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17452707) by Weston Pace (westonpace):*\n> It looks like pyarrow._dataset.Scanner.to_reader doesn't release the GIL: https://github.com/apache/arrow/blob/be9a22b9b76d9cd83d85d52ffc2844056d90f367/python/pyarrow/_dataset.pyx#L3278-L3283\r\n\r\nThis is a fast operation.  I see no reason to release the GIL here.\r\n\r\n> XYZ seems to be contended\r\n\r\nArrow has its own internal thread pool.  Typically what happens is thread tasks are added to the thread pool and the calling thread is blocked until all the necessary tasks are finished.  So this contention is expected.  As long as we aren't holding the GIL this should be relatively harmless.\r\n\r\n> Running with a ProcessPoolExecutor, each of my 13 read_file calls takes at most 2 seconds. However, with a ThreadPoolExecutor some of the read_file calls take 20+ seconds.\r\n...\r\n> Contention again mostly for RecordBatchReader.read_all, but seems to complete in ~12 seconds rather than 20\r\n\r\nAre you saying that all 13 read_file calls run in parallel and complete in 2 seconds?  This is an issue if ProcessPoolExecutor can read all 13 files in 2 seconds but ThreadPoolExecutor requires 12 seconds.\r\n\r\nHowever, there could certainly be reasons for it.  For example, ARROW-14974.  Using ThreadPoolExecutor means you are sharing a single CPU thread pool.  Using ProcessPoolExecutor means each process will have its own CPU thread pool.  In theory this shouldn't be a problem, we defer slow I/O tasks to the I/O thread pool and so we should only put compute tasks on the CPU thread pool.  Since the CPU thread pool is the same size as the # of compute units on the system there wouldn't be much advantage to having multiple CPU thread pools (e.g. it doesn't matter if I number crunch on 10 threads or 20 threads if I only have 8 cores).  In practice we could certainly have mistakes.\r\n\r\nI tried to run a number of experiments myself.  I created 13 parquet files, each 12MB.  I tried reading them with a ThreadPoolExecutor, a ProcessPoolExecutor, and a dataset.  I didn't use a filter or limit the row groups (I'll experiment with these later) but just read in the entire dataset.\r\n\r\nWith cold-I/O I had a lot of variability and the three approaches performed more or less the same (ProcessPoolExecutor seemed a bit slower but I didn't run enough experiments to verify).\r\n\r\nWith hot-I/O the ProcessPoolExecutor performed much worse than the other two approaches (which performed similarly).\r\n\r\nSo, basically, I am not reproducing the same behavior yet.  I will try adding a filter.\r\n\r\n"
        },
        {
            "created_at": "2021-12-03T03:07:33.232Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17452712) by Weston Pace (westonpace):*\nAdding a filter (~50% selectivity) didn't seem to have much effect.  The hot-I/O path slowed down slightly for threads & dataset and it sped up for process (presumably because less data has to be copied to the main process) but it wasn't enough to close the gap between the two."
        },
        {
            "created_at": "2021-12-03T09:08:38.689Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17452843) by Nick Gates (gatesn):*\nReally appreciate you looking into this - I will work to extract a minimal reproduction of this with executable code rather than the psuedo code above."
        },
        {
            "created_at": "2021-12-06T13:15:44.271Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17454007) by Nick Gates (gatesn):*\nSo I've done some more digging today and I can only seem to reproduce this against S3.\u00a0\r\n\r\nMy best guess is that the S3 maxConnections parameter which defaults to 25 is too low for the bandwidth I have when running with multi-threading. Multi-processing presumably forks / creates separate S3 clients so I get process count \\* 25 connections to S3.\r\n\r\nWould it be possible to expose that parameter through the S3FileSystem constructor?"
        },
        {
            "created_at": "2021-12-06T18:28:03.920Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17454174) by Weston Pace (westonpace):*\nThat makes a lot of sense.  I didn't realize you were using S3.  There is another throttle before you even get to the maxConnections throttle which is the I/O thread pool size.  Although, since this is parquet, it might be the CPU thread pool size.  Can you try modifying the CPU and I/O thread pool sizes to see if they have an effect on performance?  We should also bump that maxConnections parameter up too.\r\n\r\nThe python calls are:\r\n\r\n[pyarrow.set_cpu_count](https://arrow.apache.org/docs/python/generated/pyarrow.set_cpu_count.html)\r\npyarrow.set_io_thread_count (which appears to be missing from the docs, I'll open a ticket on that)\r\n"
        },
        {
            "created_at": "2021-12-08T09:30:21.448Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17455078) by Nick Gates (gatesn):*\nInitially cpu=8, and io=8.\r\n\r\nCPU count didn't seem to make a difference when I bumped that.\r\n\r\nBut as expected, IO threads made a big difference. From ~30 seconds with 8 threads to ~8 seconds with 32 threads. Although any higher and I didn't see any improvement.\r\n\r\n\u00a0\r\n\r\nSo to summarise:\r\n- When running with multi-processing the maximum duration of Scanner.to_table is ~2 seconds.\n- When running with multi-threading, the Scanner.to_table max is ~25 seconds\n- When running with multi-threading, and set_io_thread_count(32), the Scanner.to_table is ~5 seconds\n  \n  \n  Should the default io thread pool be set to 5 \\* cpu_count, as per Python's ThreadPoolExecutor?\n  And it seems like it would also be helpful to be able to configure S3 SDK max connections (not sure if they're threaded or non-blocking?) to capture that last bit of performance."
        },
        {
            "created_at": "2021-12-08T21:52:49.803Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14965?focusedCommentId=17456009) by Weston Pace (westonpace):*\nThe latest default value is actually min(32, os.cpu_count() + 4) which would be too small in this case.  It's hard to say that there is a good single default for this.  5\\*cpu_count would be too high for an HDD (and probably even an SSD).  There is some prerequisite work to allow the thread pool to be configured per-filesystem which might help us come up with better defaults.  I've created ARROW-15035 as a placeholder to finish that work.  However, I don't know when someone will have time to get to it.  Being able to change the global thread pool size is probably \"good enough\" for some time.\r\n\r\nAlso, it seems like we are not quite reaching the peak multi-process performance.  I agree it would be interesting to test a higher max connections.  I've created ARROW-15036 for this.\r\n\r\nThis issue we can leave open in case there is some other factor preventing us from reaching that 2 seconds."
        }
    ]
}