{
    "issue": {
        "title": "[C++][Dataset] Support reading date/time-partitioned datasets accounting for URL encoding (Spark)",
        "body": "***Note**: This issue was originally created as [ARROW-12644](https://issues.apache.org/jira/browse/ARROW-12644). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm using Spark (3.1.1) to write a dataframe to a partitioned parquet dataset (using delta.io) which is partitioned by a timestamp field.\r\n\r\nThe relevant Spark code:\r\n```java\n\r\n// code placeholder\r\n(\r\n  df.withColumn(\r\n                \"Date\",\r\n                sf.date_trunc(\r\n                    \"DAY\",\r\n                    sf.from_unixtime(\r\n                        (sf.col(\"MyEpochField\")),\r\n                    ),\r\n                ),\r\n            )\r\n    .write.format(\"delta\")\r\n    .mode(\"append\")\r\n    .partitionBy(\"Date\")\r\n    .save(\"...\")\r\n\r\n```\r\nThis gives a structure like following:\r\n```java\n\r\n// code placeholder\r\n/tip\r\n/tip/Date=2021-05-04 00%3A00%3A00\r\n/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00\r\n/tip/Date=2021-05-04 00%3A00%3A00/Time=2021-05-04 07%3A27%3A00/part-00000-8846eb80-a369-43f6-a715-fec9cf1adf95.c000.snappy.parquet\r\n\r\n```\r\nNotice the : character is (url?) encoded because of fs protocol violation.\r\n\r\nWhen i try to open this dataset using delta-rs (<https://github.com/delta-io/delta-rs)>\u00a0which uses Arrow below the hood, then an error is raised trying to parse the Date (folder) value.\r\n```java\n\r\n// code placeholder\r\npyarrow.lib.ArrowInvalid: error parsing '2021-05-03 00%3A00%3A00' as scalar of type timestamp[ns]\r\n```\r\nIt seems this error is raised in\u00a0ScalarParseImpl =>\u00a0ParseValue =>\u00a0StringConverter<TimestampType>::Convert =>\u00a0ParseTimestampISO8601\r\n\r\nThe mentioned parse method does support for format:\r\n```java\n\r\n// code placeholder\r\nstatic inline bool ParseTimestampISO8601(const char* s, size_t length,\r\n                                         TimeUnit::type unit,\r\n                                         TimestampType::c_type* out) {\r\n  using seconds_type = std::chrono::duration<TimestampType::c_type>;  // We allow the following formats for all units:\r\n  // - \"YYYY-MM-DD\"\r\n  // - \"YYYY-MM-DD[ T]hhZ?\"\r\n  // - \"YYYY-MM-DD[ T]hh:mmZ?\"\r\n  // - \"YYYY-MM-DD[ T]hh:mm:ssZ?\"\r\n<...>\n```\r\nBut may not support (url?) decoding the value upfront?\r\n\r\nQuestions we have:\r\n \\* Should Arrow support timestamp fields when used as partitioned field?\r\n \\* Where to decode?\r\n\r\n\u00a0\r\n\r\nSome more information from the writing side.\r\n\r\nThe writing is initiated using\u00a0FileFormatWriter.write that eventually uses a\u00a0DynamicPartitionDataWriter (passing in the partitionColumns through the job description).\r\n\r\nHere the actual \"value\" is rendered and concatennated.\r\n```java\n\r\n// code placeholder\r\n  /** Expression that given partition columns builds a path string like: col1=val/col2=val/... */\r\n  private lazy val partitionPathExpression: Expression = Concat(\r\n    description.partitionColumns.zipWithIndex.flatMap { case (c, i) =>\r\n      val partitionName = ScalaUDF(\r\n        ExternalCatalogUtils.getPartitionPathString _,\r\n        StringType,\r\n        Seq(Literal(c.name), Cast(c, StringType, Option(description.timeZoneId))))\r\n      if (i == 0) Seq(partitionName) else Seq(Literal(Path.SEPARATOR), partitionName)\r\n    })\r\n\r\n```\r\nWhere the encoding is done in:\r\n\r\n<https://github.com/apache/spark/blob/v3.0.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils.scala#L66>\r\n\r\nIf i understand correct, then Arrow should provide the equivalent of\u00a0unescapePathName for fields used as partitioned columns.\r\n\r\n\u00a0",
        "created_at": "2021-05-04T09:31:50.000Z",
        "updated_at": "2021-11-17T12:59:55.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-06-07T16:24:50.000Z"
    },
    "comments": [
        {
            "created_at": "2021-05-04T13:14:09.204Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17338980) by David Li (lidavidm):*\nYes, it looks like we may need/want to URL-decode paths at some point, though we need to figure out how to handle that. (For instance: presumably it usually doesn't make sense to URL-decode local file system paths, but it looks like Spark may URL-encode paths so we need some way to enable that optionally?)\r\n\r\nThis also looks somewhat related to ARROW-11378, in that partitioning by a timestamp is rather unusual (the cardinality would normally be too high to make for a worthwhile partition, no?)."
        },
        {
            "created_at": "2021-05-04T15:14:27.165Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17339072) by Paul Bormans (peebor@gmail.com):*\n`[~lidavidm]` \u00a0It looks a bit wierd at first sight maybe but the benifit of doing it like this is:\r\n \\* In Spark it's a (string-typed) timestamp field, thus allowing >= / <= operators for running a query, without any risk of misinterpreting date/time as a string.\r\n \\* Secondly this example is a truncated timestamp value, so with\u00a0\r\nsf.date_trunc(\"DAY\",\r\nyou do see hours/minutes, etc... but it's all trunc'ed values.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-05-04T17:14:02.649Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17339152) by Weston Pace (westonpace):*\nI'll add that we do also have a date32 data type (32 bit days since the epoch) which might be more intuitive for truncating to the day and will certainly be more compact.\r\n\r\nThat being said, even if we decide (ala ARROW-11378) to be strict on what formats we allow for output-partitioning, we could probably be considerably more lenient on what we allow Arrow to read in.\r\n\r\nDigging around a bit in Jira it appears that Spark is using java.sql.Timestamp to write out timestamps which states \"Formats a timestamp in JDBC timestamp escape format\" which at least explains why it is in this format.\r\n\r\nSeems like there are potentially 3 issues:\r\n \\* Should Arrow allow timestamps to be used as partition when writing datasets and, if so, what format (ARROW-11378)\r\n \\* Add the ability for Arrow to url decode filenames (this issue?)\r\n \\* Arrow should be able to infer columns written using JDBC escape format (may not be an issue after this issues is resolved but, if it is, it is probably a new issue)\r\n\r\n\u00a0\r\n\r\n> presumably it usually doesn't make sense to URL-decode local file system paths, but it looks like Spark may URL-encode paths so we need some way to enable that optionally?\r\n\r\nI don't think it's too unusual and I think URL decoding has a pretty low false positive rate (i.e. it's not likely to decode something that wasn't meant to be decoded since it's syntax is so obtuse).\u00a0 I'd probably vote towards making it configurable but the default."
        },
        {
            "created_at": "2021-05-04T17:21:13.380Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17339166) by David Li (lidavidm):*\nSounds reasonable to me, thanks for digging up the format & chiming in. Let's use this issue for URL-decoding paths in datasets (and even if we don't infer the type here, it should at least be readable by setting the partition schema). And I don't know if we've 'officially' said this, but presumably we want to maintain some level of Spark compatibility."
        },
        {
            "created_at": "2021-05-05T13:57:10.148Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17339675) by Joris Van den Bossche (jorisvandenbossche):*\nAgreed that we should try to be able to read what Spark produces, and that URL-decoding the paths in datasets before parsing the values with a typed converted sounds as a good default. "
        },
        {
            "created_at": "2021-06-07T16:24:50.744Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12644?focusedCommentId=17358709) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 10264\n<https://github.com/apache/arrow/pull/10264>"
        }
    ]
}