{
    "issue": {
        "title": "[Python] Reading from hdfs using pyarrow 10.0.0 throws OSError: [Errno 22] Opening HDFS file",
        "body": "***Note**: This issue was originally created as [ARROW-18276](https://issues.apache.org/jira/browse/ARROW-18276). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHey!\r\nI am trying to read a CSV file using pyarrow together with fsspec from HDFS.\r\nI used to do this with pyarrow 9.0.0 and fsspec 2022.7.1, however, after I upgraded to pyarrow 10.0.0 this stopped working.\r\n\r\nI am not quite sure if this is an incompatibility introduced in the new pyarrow version or if it is a Bug in fsspec. So if I am in the wrong place here, please let me know.\r\n\r\nApart from pyarrow 10.0.0 and fsspec 2022.7.1, I am using pandas version 1.3.3 and python 3.8.11.\r\n\r\nHere is the full stack trace\r\n```python\n\r\npd.read_csv(\"hdfs://10.0.2.15:8020/Projects/testing/testing_Training_Datasets/transactions_view_fraud_batch_fv_1_1/validation/part-00000-42b57ad2-57eb-4a63-bfaa-7375e82863e8-c000.csv\")\r\n---------------------------------------------------------------------------\r\nOSError \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Traceback (most recent call last)\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\r\n\u00a0 \u00a0 584 \u00a0 \u00a0 kwds.update(kwds_defaults)\r\n\u00a0 \u00a0 585\u00a0\r\n--> 586 \u00a0 \u00a0 return _read(filepath_or_buffer, kwds)\r\n\u00a0 \u00a0 587\u00a0\r\n\u00a0 \u00a0 588\u00a0\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)\r\n\u00a0 \u00a0 480\u00a0\r\n\u00a0 \u00a0 481 \u00a0 \u00a0 # Create the parser.\r\n--> 482 \u00a0 \u00a0 parser = TextFileReader(filepath_or_buffer, **kwds)\r\n\u00a0 \u00a0 483\u00a0\r\n\u00a0 \u00a0 484 \u00a0 \u00a0 if chunksize or iterator:\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py in _init_(self, f, engine, **kwds)\r\n\u00a0 \u00a0 809 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\r\n\u00a0 \u00a0 810\u00a0\r\n--> 811 \u00a0 \u00a0 \u00a0 \u00a0 self._engine = self._make_engine(self.engine)\r\n\u00a0 \u00a0 812\u00a0\r\n\u00a0 \u00a0 813 \u00a0 \u00a0 def close(self):\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/readers.py in _make_engine(self, engine)\r\n\u00a0 \u00a01038 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\r\n\u00a0 \u00a01039 \u00a0 \u00a0 \u00a0 \u00a0 # error: Too many arguments for \"ParserBase\"\r\n-> 1040 \u00a0 \u00a0 \u00a0 \u00a0 return mapping[engine](self.f, **self.options) \u00a0# type: ignore[call-arg]\r\n\u00a0 \u00a01041\u00a0\r\n\u00a0 \u00a01042 \u00a0 \u00a0 def _failover_to_python(self):\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py in _init_(self, src, **kwds)\r\n\u00a0 \u00a0 \u00a049\u00a0\r\n\u00a0 \u00a0 \u00a050 \u00a0 \u00a0 \u00a0 \u00a0 # open handles\r\n---> 51 \u00a0 \u00a0 \u00a0 \u00a0 self._open_handles(src, kwds)\r\n\u00a0 \u00a0 \u00a052 \u00a0 \u00a0 \u00a0 \u00a0 assert self.handles is not None\r\n\u00a0 \u00a0 \u00a053\u00a0\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py in _open_handles(self, src, kwds)\r\n\u00a0 \u00a0 220 \u00a0 \u00a0 \u00a0 \u00a0 Let the readers open IOHandles after they are done with their potential raises.\r\n\u00a0 \u00a0 221 \u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\r\n--> 222 \u00a0 \u00a0 \u00a0 \u00a0 self.handles = get_handle(\r\n\u00a0 \u00a0 223 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 src,\r\n\u00a0 \u00a0 224 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"r\",\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\r\n\u00a0 \u00a0 607\u00a0\r\n\u00a0 \u00a0 608 \u00a0 \u00a0 # open URLs\r\n--> 609 \u00a0 \u00a0 ioargs = _get_filepath_or_buffer(\r\n\u00a0 \u00a0 610 \u00a0 \u00a0 \u00a0 \u00a0 path_or_buf,\r\n\u00a0 \u00a0 611 \u00a0 \u00a0 \u00a0 \u00a0 encoding=encoding,\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pandas/io/common.py in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\r\n\u00a0 \u00a0 356\u00a0\r\n\u00a0 \u00a0 357 \u00a0 \u00a0 \u00a0 \u00a0 try:\r\n--> 358 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 file_obj = fsspec.open(\r\n\u00a0 \u00a0 359 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 filepath_or_buffer, mode=fsspec_mode, **(storage_options or {})\r\n\u00a0 \u00a0 360 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ).open()\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/fsspec/core.py in open(self)\r\n\u00a0 \u00a0 133 \u00a0 \u00a0 \u00a0 \u00a0 during the life of the file-like it generates.\r\n\u00a0 \u00a0 134 \u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\r\n--> 135 \u00a0 \u00a0 \u00a0 \u00a0 return self._enter_()\r\n\u00a0 \u00a0 136\u00a0\r\n\u00a0 \u00a0 137 \u00a0 \u00a0 def close(self):\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/fsspec/core.py in _enter_(self)\r\n\u00a0 \u00a0 101 \u00a0 \u00a0 \u00a0 \u00a0 mode = self.mode.replace(\"t\", \"\").replace(\"b\", \"\") + \"b\"\r\n\u00a0 \u00a0 102\u00a0\r\n--> 103 \u00a0 \u00a0 \u00a0 \u00a0 f = self.fs.open(self.path, mode=mode)\r\n\u00a0 \u00a0 104\u00a0\r\n\u00a0 \u00a0 105 \u00a0 \u00a0 \u00a0 \u00a0 self.fobjects = [f]\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/fsspec/spec.py in open(self, path, mode, block_size, cache_options, compression, **kwargs)\r\n\u00a0 \u00a01092 \u00a0 \u00a0 \u00a0 \u00a0 else:\r\n\u00a0 \u00a01093 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ac = kwargs.pop(\"autocommit\", not self._intrans)\r\n-> 1094 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f = self._open(\r\n\u00a0 \u00a01095 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path,\r\n\u00a0 \u00a01096 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mode=mode,\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/fsspec/implementations/arrow.py in wrapper(*args, **kwargs)\r\n\u00a0 \u00a0 \u00a015 \u00a0 \u00a0 def wrapper(*args, **kwargs):\r\n\u00a0 \u00a0 \u00a016 \u00a0 \u00a0 \u00a0 \u00a0 try:\r\n---> 17 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return func(*args, **kwargs)\r\n\u00a0 \u00a0 \u00a018 \u00a0 \u00a0 \u00a0 \u00a0 except OSError as exception:\r\n\u00a0 \u00a0 \u00a019 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not exception.args:\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/fsspec/implementations/arrow.py in _open(self, path, mode, block_size, **kwargs)\r\n\u00a0 \u00a0 157 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # disable compression auto-detection\r\n\u00a0 \u00a0 158 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 _kwargs[\"compression\"] = None\r\n--> 159 \u00a0 \u00a0 \u00a0 \u00a0 stream = method(path, **_kwargs)\r\n\u00a0 \u00a0 160\u00a0\r\n\u00a0 \u00a0 161 \u00a0 \u00a0 \u00a0 \u00a0 return ArrowFile(self, stream, path, mode, block_size, **kwargs)\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pyarrow/_fs.pyx in pyarrow._fs.FileSystem.open_input_stream()\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n/srv/hops/anaconda/envs/theenv/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\nOSError: [Errno 22] Opening HDFS file '10.0.2.15:8020/Projects/test/test_Training_Datasets/td_1/td/part-00000-acb66eca-636e-4917-9673-e6646f0b4486-c000.csv' failed. Detail: [errno 22] Invalid argument\r\n```\r\n\r\nHowever, if I leave out the namenode IP and port, it works as expected:\r\n\r\n```python\n\r\npd.read_csv(\"hdfs:///Projects/testing/testing_Training_Datasets/transactions_view_fraud_batch_fv_1_1/validation/part-00000-42b57ad2-57eb-4a63-bfaa-7375e82863e8-c000.csv\")\n```\r\nAny help is appreciated, thank you!",
        "created_at": "2022-11-07T23:01:17.000Z",
        "updated_at": "2022-11-18T09:43:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-11-17T11:19:39.433Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18276?focusedCommentId=17635303) by Alenka Frim (alenka):*\nHi `[~moritzmeister]` !\r\n\r\nCould you try using `pyarrow` directly to see if you then get the same error when opening the file? \r\nYou can instantiate a `HadoopFileSystem` object [from an URI string](https://arrow.apache.org/docs/python/generated/pyarrow.fs.HadoopFileSystem.html#pyarrow.fs.HadoopFileSystem.from_uri), or using the class constructor directly (https://arrow.apache.org/docs/dev/python/filesystems.html#hadoop-distributed-file-system-hdfs). Something similar to this:\r\n\r\n```Java\n\r\nfrom pyarrow import fs\r\nhdfs, _ = fs.HadoopFileSystem.from_uri('hdfs://10.0.2.15:8020/Projects/testing/testing_Training_Datasets/transactions_view_fraud_batch_fv_1_1/validation/')\r\nhdfs.open_input_file(\"/Projects/testing/testing_Training_Datasets/transactions_view_fraud_batch_fv_1_1/validation/part-00000-42b57ad2-57eb-4a63-bfaa-7375e82863e8-c000.csv\")\r\n```\r\n\r\nIf that works, you can then use `hdfs` with `{}fsspec{`}:\r\n<https://arrow.apache.org/docs/python/filesystems.html#using-arrow-filesystems-with-fsspec>\r\n\r\nand `fsspec` API to open the files:\r\n<https://filesystem-spec.readthedocs.io/en/latest/api.html>\r\n\r\nSomething similar to this:\r\n```python\n\r\nfrom pyarrow import fs\r\nhdfs = fs.HadoopFileSystem.from_uri('hdfs://10.0.2.15:8020/Projects/testing/testing_Training_Datasets/transactions_view_fraud_batch_fv_1_1/validation/')\r\nfrom fsspec.implementations.arrow import ArrowFSWrapper\r\nhdfs_fsspec = ArrowFSWrapper(hdfs)\r\nhdfs_fsspec.open_files(...)\r\n```\r\nThis way you can see if pyarrow 10.0.0 works or errors. And it is more direct so less likely to error :)\r\n\r\nAlso, do you maybe know if the Hadoop installation has changed in this time?"
        }
    ]
}