{
    "issue": {
        "title": "[C++] Take kernel can't handle ChunkedArrays that don't fit in an Array",
        "body": "***Note**: This issue was originally created as [ARROW-9773](https://issues.apache.org/jira/browse/ARROW-9773). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nTake() currently concatenates ChunkedArrays first. However, this breaks down when calling Take() from a ChunkedArray or Table where concatenating the arrays would result in an array that's too large. While inconvenient to implement, it would be useful if this case were handled.\r\n\r\nThis could be done as a higher-level wrapper around Take(), perhaps.\r\n\r\nExample in Python:\r\n```python\n\r\n>>> import pyarrow as pa\r\n>>> pa.__version__\r\n'1.0.0'\r\n>>> rb1 = pa.RecordBatch.from_arrays([[\"a\" * 2**30]], names=[\"a\"])\r\n>>> rb2 = pa.RecordBatch.from_arrays([[\"b\" * 2**30]], names=[\"a\"])\r\n>>> table = pa.Table.from_batches([rb1, rb2], schema=rb1.schema)\r\n>>> table.take([1, 0])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow/table.pxi\", line 1145, in pyarrow.lib.Table.take\r\n  File \"/home/lidavidm/Code/twosigma/arrow/venv/lib/python3.8/site-packages/pyarrow/compute.py\", line 268, in take\r\n    return call_function('take', [data, indices], options)\r\n  File \"pyarrow/_compute.pyx\", line 298, in pyarrow._compute.call_function\r\n  File \"pyarrow/_compute.pyx\", line 192, in pyarrow._compute.Function.call\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n```\r\n\r\nIn this example, it would be useful if Take() or a higher-level wrapper could generate multiple record batches as output.",
        "created_at": "2020-08-17T19:47:10.000Z",
        "updated_at": "2022-09-24T08:00:06.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-12-09T16:26:00.118Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17246649) by Antoine Pitrou (apitrou):*\nNote that this can happen with regular arrays too:\r\n```python\n\r\n>>> import pyarrow as pa\r\n>>> arr = pa.array([\"x\" * (1<<20)])\r\n>>> t = arr.take(pa.array([0]*((1<<12) + 1), type=pa.int8()))\r\n>>> t.validate(full=True)\r\nTraceback (most recent call last):\r\n  [...]\r\nArrowInvalid: Offset invariant failure: non-monotonic offset at slot 2048: -2147483648 < 2146435072\r\n```\r\n"
        },
        {
            "created_at": "2021-01-24T23:17:46.287Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17271015) by Leonard Lausen (lausen):*\nThere is a similar issue with large tables (many rows) of medium size lists (~512 elements per list). When using `pa.list_` type, `take` will fail due to `offset overflow while concatenating arrays`. Using `pa.large_list` works. (But in practice it doesn't help as `.take` performs 3 orders of magnitude (~1s vs ~1ms) slower than indexing operations on pandas..)"
        },
        {
            "created_at": "2022-05-12T02:10:46.671Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17535809) by Chris Fregly (cfregly):*\nSeeing this error through Ray 1.13 when I run the following code:\r\n\r\nimport ray\r\n\r\nray.init(address=\"auto\")\r\n\r\ndf = ray.data.read_parquet(\"\r\n[s3://amazon-reviews-pds/parquet/]\r\n\")\r\n\r\nprint(df.groupby(\"product_category\").count())\r\n\u00a0\r\n\r\nHere's the error:\r\n(_partition_and_combine_block pid=1933) 2022-05-06 20:51:29,275 INFO worker.py:431 \u2013 Task failed with retryable exception: TaskID(7f0166b85ffd7f1fffffffffffffffffffffffff01000000).\r\n(_partition_and_combine_block pid=1933) Traceback (most recent call last):\r\n(_partition_and_combine_block pid=1933) File \"python/ray/_raylet.pyx\", line 625, in ray._raylet.execute_task\r\n(_partition_and_combine_block pid=1933) File \"python/ray/_raylet.pyx\", line 629, in ray._raylet.execute_task\r\n(_partition_and_combine_block pid=1933) File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/grouped_dataset.py\", line 436, in _partition_and_combine_block\r\n(_partition_and_combine_block pid=1933) descending=False)\r\n(_partition_and_combine_block pid=1933) File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/impl/arrow_block.py\", line 308, in sort_and_partition\r\n(_partition_and_combine_block pid=1933) table = self._table.take(indices)\r\n(_partition_and_combine_block pid=1933) File \"pyarrow/table.pxi\", line 1382, in pyarrow.lib.Table.take\r\n(_partition_and_combine_block pid=1933) File \"/home/ray/anaconda3/lib/python3.7/site-packages/pyarrow/compute.py\", line 625, in take\r\n(_partition_and_combine_block pid=1933) return call_function('take', [data, indices], options, memory_pool)\r\n(_partition_and_combine_block pid=1933) File \"pyarrow/_compute.pyx\", line 528, in pyarrow._compute.call_function\r\n(_partition_and_combine_block pid=1933) File \"pyarrow/_compute.pyx\", line 327, in pyarrow._compute.Function.call\r\n(_partition_and_combine_block pid=1933) File \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\n(_partition_and_combine_block pid=1933) File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\n(_partition_and_combine_block pid=1933) pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\nGroupBy Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [01:31<00:00, 2.18it/s]\r\nGroupBy Reduce: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [00:00<00:00, 19776.52it/s]\r\nTraceback (most recent call last):\r\nFile \"/home/ray/parquet-raydata.py\", line 10, in <module>\r\nprint(df.groupby(\"product_category\").count().sort())\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/grouped_dataset.py\", line 147, in count\r\nreturn self.aggregate(Count())\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/grouped_dataset.py\", line 114, in aggregate\r\nmetadata = ray.get(metadata)\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\r\nreturn func(\\*args, \\*\\*kwargs)\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/worker.py\", line 1713, in get\r\nraise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(ArrowInvalid): ray::_aggregate_combined_blocks() (pid=27147, ip=172.31.14.160)\r\nAt least one of the input arguments for this task could not be computed:\r\nray.exceptions.RayTaskError: ray::_partition_and_combine_block() (pid=1930, ip=172.31.14.160)\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/grouped_dataset.py\", line 436, in _partition_and_combine_block\r\ndescending=False)\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/data/impl/arrow_block.py\", line 308, in sort_and_partition\r\ntable = self._table.take(indices)\r\nFile \"pyarrow/table.pxi\", line 1382, in pyarrow.lib.Table.take\r\nFile \"/home/ray/anaconda3/lib/python3.7/site-packages/pyarrow/compute.py\", line 625, in take\r\nreturn call_function('take', [data, indices], options, memory_pool)\r\nFile \"pyarrow/_compute.pyx\", line 528, in pyarrow._compute.call_function\r\nFile \"pyarrow/_compute.pyx\", line 327, in pyarrow._compute.Function.call\r\nFile \"pyarrow/error.pxi\", line 143, in pyarrow.lib.pyarrow_internal_check_status\r\nFile \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays"
        },
        {
            "created_at": "2022-05-31T19:35:21.619Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17544575) by Mayur Srivastava (mayuropensource):*\nHi `[~lidavidm]` , is there any progress on this jira? (This issue is blocking a few use cases we have.)\r\n\r\n\u00a0\r\n\r\nThanks,\r\n\r\nMayur Srivastava"
        },
        {
            "created_at": "2022-05-31T19:40:59.692Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17544576) by David Li (lidavidm):*\nIt needs someone motivated to sit down and work through the implementation. I can review/offer suggestions but probably don't have the time to implement this right now.\r\n\r\nNote that I think the cases described in the comments above are fundamentally different from the original issue: they also require upgrading the output from Array to ChunkedArray (or from String/List to LargeString/LargeList) and so can't be done automatically. "
        },
        {
            "created_at": "2022-06-06T22:25:58.003Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17550684) by Will Jones (willjones127):*\nI'm interested in working on this soon. I'll look through the issue a little deeper and ping you `[~lidavidm]` to get some ideas on the design."
        },
        {
            "created_at": "2022-06-06T22:38:20.853Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17550686) by David Li (lidavidm):*\n`[~willjones127]` great! Looking forward to it."
        },
        {
            "created_at": "2022-06-07T19:50:22.616Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17551259) by Will Jones (willjones127):*\nI've looked through the code and I think there are three related issues. I'll try to describe them here. If you think I am missing some case, let me know. Otherwise, I'll open three sub-tasks and start work on those.\r\n## Problem 1: We concatenate when we shouldn't need to\r\n\r\nThis fails:\r\n```python\n\r\narr = pa.chunked_array([[\"a\" * 2**30]] * 2)\r\narr.take([0,1])\r\n# Traceback (most recent call last):\r\n#   File \"<stdin>\", line 1, in <module>\r\n#   File \"pyarrow/table.pxi\", line 998, in pyarrow.lib.ChunkedArray.take\r\n#   File \"/Users/willjones/Documents/test-env/venv/lib/python3.9/site-packages/pyarrow/compute.py\", line 457, in take\r\n#     return call_function('take', [data, indices], options, memory_pool)\r\n#   File \"pyarrow/_compute.pyx\", line 542, in pyarrow._compute.call_function\r\n#   File \"pyarrow/_compute.pyx\", line 341, in pyarrow._compute.Function.call\r\n#   File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n#   File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\n# pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n```\r\n\u00a0because we concatenate input values [here](https://github.com/apache/arrow/blob/0d5cf1882228624271062e6c19583c8b0c361a20/cpp/src/arrow/compute/kernels/vector_selection.cc#L2024-L2025). If that were corrected, it would then fail on the concatenation [here](https://github.com/apache/arrow/blob/0d5cf1882228624271062e6c19583c8b0c361a20/cpp/src/arrow/compute/kernels/vector_selection.cc#L2046-L2047) if the indices were a chunked array.\r\n\r\nThe first concatenation could be avoided somewhat easily in special cases (sorted / fall in same chunk), which was [partially implement in R](https://github.com/apache/arrow/blob/6f2c9041137001f7a9212f244b51bc004efc29af/r/src/compute.cpp#L123-L151). For the general case, we'd need to address this within the kernel rather than within pre-processing (see Problem 3).\r\n\r\nThe second concatenation shouldn't always be eliminated, but we might want to add a check to validate that there is enough room in offset buffers of arrays to concatenate. TBD if there is an efficient way to test that.\r\n## Problem 2: take_array kernel doesn't handle case of offset overflow\r\n\r\nThis is what Antoine was pointing out:\r\n```python\n\r\nimport pyarrow as pa\r\narr = pa.array([\"x\" * (1<<20)])\r\nt = arr.take(pa.array([0]*((1<<12) + 1), type=pa.int8()))\r\nt.validate(full=True)\r\n# Traceback (most recent call last):\r\n#   [...]\r\n# ArrowInvalid: Offset invariant failure: non-monotonic offset at slot 2048: -2147483648 < 2146435072\r\n```\r\nTo solve this, I think we'd either have to:\r\n1. (optionally?) promote arrays to Large variants of type. Problem is we'd need to do this cast consistently across chunks.\n1. Switch to returning chunked arrays, and create new chunks as needed. (TBD: Could we do that in some cases (String, Binary, List types) and not others?)\n   \n   ## Problem 3: there isn't a take_array kernel that handles ChunkedArrays\n   \n   Finally, for sorting chunked arrays of type string/binary/list (that is, the case for take where the indices are out-of-order), I think we need to implement kernels specialized for chunked arrays. IIUC, everything but string/binary/list types could simply do the concatenation we do now; it's just those three types that need special logic to chunk as necessary to avoid offset overflows.\n   \n   \u00a0\n   \n   \u00a0"
        },
        {
            "created_at": "2022-06-08T10:10:23.673Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17551526) by Antoine Pitrou (apitrou):*\nOk, it's a bit unfortunate that several distinct issues have been amalgamated here :-)\r\n\r\nI'd argue that this issue is primarily about fixing problem 3 (which would also fix problem 1). Besides correctness, concatenating is also a performance problem because we might be allocating a lot of temporary memory."
        },
        {
            "created_at": "2022-06-08T11:31:44.383Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9773?focusedCommentId=17551563) by David Li (lidavidm):*\nAgreed, we should focus on 1/3. Problem 2 is also interesting, but I'm not sure how best to handle it: right now the kernels infrastructure assumes a fixed output type and shape up front, and dynamically switching to ChunkedArray or promoting type would be a surprise.\r\n\r\nI would think we could avoid concatenation for all types, even if it isn't strictly required, to avoid excessive allocation as Antoine mentioned."
        }
    ]
}