{
    "issue": {
        "title": "[Python/C++] High memory usage/potential leak when reading parquet using Dataset API",
        "body": "***Note**: This issue was originally created as [ARROW-18156](https://issues.apache.org/jira/browse/ARROW-18156). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nI have a 2.35 GB DataFrame (1.17 GB on-disk size) which I'm loading using the following snippet:\r\n\r\n\u00a0\r\n```java\n\r\nimport os\r\nimport pyarrow\r\nimport pyarrow.dataset as ds\r\nfrom importlib_metadata import version\r\nfrom psutil import Process\r\nimport pyarrow.parquet as pq\r\n\r\ndef format_bytes(num_bytes: int):\r\n\u00a0 \u00a0 return f\"{num_bytes / 1024 / 1024 / 1024:.2f} GB\"\r\n\u00a0\r\ndef main():\r\n\u00a0 \u00a0 print(version(\"pyarrow\"))\r\n    print(pyarrow.default_memory_pool().backend_name)\r\n\u00a0 \u00a0 process = Process(os.getpid())\r\n  \u00a0 runs = 10\r\n\u00a0 \u00a0 print(f\"Runs: {runs}\")\r\n\u00a0 \u00a0 for i in range(runs):\r\n  \u00a0 \u00a0 \u00a0 dataset = ds.dataset(\"df.pq\")\r\n  \u00a0 \u00a0 \u00a0 table = dataset.to_table()\r\n  \u00a0 \u00a0 \u00a0 df = table.to_pandas()\r\n  \u00a0 \u00a0 \u00a0 print(f\"After run {i}: RSS = {format_bytes(process.memory_info().rss)}, PyArrow Allocated Bytes = {format_bytes(pyarrow.total_allocated_bytes())}\")\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nOn PyArrow v4.0.1 the output is as follows:\r\n```java\n\r\n4.0.1\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 7.59 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 1: RSS = 13.36 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 2: RSS = 14.74 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 3: RSS = 15.78 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 4: RSS = 18.36 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 5: RSS = 19.69 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 6: RSS = 21.21 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 7: RSS = 21.52 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 8: RSS = 21.49 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 9: RSS = 21.72 GB, PyArrow Allocated Bytes = 6.09 GB\r\nAfter run 10: RSS = 20.95 GB, PyArrow Allocated Bytes = 6.09 GB\n```\r\nIf I replace ds.dataset(\"df.pq\").to_table() with pq.ParquetFile(\"df.pq\").read(), the output is:\r\n```java\n\r\n4.0.1\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 2.38 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 1: RSS = 2.49 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 2: RSS = 2.50 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 3: RSS = 2.53 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 4: RSS = 2.53 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 5: RSS = 2.56 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 6: RSS = 2.53 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 7: RSS = 2.51 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 8: RSS = 2.48 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 9: RSS = 2.51 GB, PyArrow Allocated Bytes = 1.34 GB\r\nAfter run 10: RSS = 2.51 GB, PyArrow Allocated Bytes = 1.34 GB\n```\r\nThe usage profile of the older non-dataset API is much lower - it matches the size of the dataframe much closer. It also seems like in the former example, there is a memory leak? I thought that the increase in RSS was just due to PyArrow's usage of jemalloc, but I seem to be using the system allocator here.\r\n\r\n\u00a0",
        "created_at": "2022-10-25T13:52:07.000Z",
        "updated_at": "2022-10-26T16:59:23.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-10-25T15:27:38.489Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17623900) by Joris Van den Bossche (jorisvandenbossche):*\nThanks for the report! Would you be able to test with the latest version of pyarrow? (9.0.0 instead of 4.0.1)\r\n\r\nAlso, for reproducing it, is there anything special about `df.pq`? (how many columns, what kind of data types) Can you generate the file with random numbers using a small code snippet?"
        },
        {
            "created_at": "2022-10-25T18:10:05.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17623966) by Norbert (Norbo11):*\n`[~jorisvandenbossche]` Thanks for the fast response, here is a snippet which generates a representative DataFrame of a similar size with the same dtypes, and exposes the same issue:\r\n\r\n\u00a0\r\n```java\n\r\n\u00a0 \u00a0 dates = pd.date_range(\"2020-10-01\", \"2022-09-30\")\r\n\u00a0 \u00a0 identifiers = [i for i in range(5000)]\r\n\u00a0 \u00a0 times = []\r\n\u00a0 \u00a0 for hour in range(9, 17):\r\n\u00a0 \u00a0 \u00a0 \u00a0 for minute in range(6):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 times.append(f\"{hour:02d}:{minute * 10:02d}:00\")\r\n\u00a0 \u00a0 dates = pd.Series(dates, dtype=\"datetime64[ns]\")\r\n\u00a0 \u00a0 identifiers = pd.Series(identifiers, dtype=\"int64\")\r\n\u00a0 \u00a0 times = pd.Series(times, dtype=\"category\")\r\n\r\n\u00a0 \u00a0 index = pd.MultiIndex.from_product(\r\n\u00a0 \u00a0 \u00a0 \u00a0 [dates, identifiers, times], names=[\"date\", \"identifier\", \"time\"]\r\n\u00a0 \u00a0 )\r\n\u00a0 \u00a0 df = pd.DataFrame({\"value\": np.random.rand(len(index))}, index=index)\r\n\u00a0 \u00a0 print(df)\r\n\u00a0 \u00a0 print(df.dtypes)\r\n\u00a0 \u00a0 print(df.index.dtypes)\r\n\u00a0 \u00a0 print(format_bytes(df.memory_usage(deep=True).sum()))\r\n\u00a0 \u00a0 df.to_parquet(\"df.pq\")\r\n```\r\nOutput:\r\n\r\n\u00a0\r\n```java\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0value\r\ndate \u00a0 \u00a0 \u00a0 identifier time\r\n2020-10-01 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a009:00:00 \u00a00.697832\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:10:00 \u00a00.162727\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:20:00 \u00a00.902748\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:30:00 \u00a00.925639\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:10:00 \u00a00.162727\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:20:00 \u00a00.902748\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:30:00 \u00a00.925639\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 09:40:00 \u00a00.852034\r\n... \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0...\r\n2022-09-30 4999 \u00a0 \u00a0 \u00a0 16:10:00 \u00a00.420191\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16:20:00 \u00a00.223169\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16:30:00 \u00a00.838116\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16:40:00 \u00a00.795141\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16:50:00 \u00a00.882058\r\n[175200000 rows x 1 columns]\r\nvalue \u00a0 \u00a0float64\r\ndtype: object\r\ndate \u00a0 \u00a0 \u00a0 \u00a0 \u00a0datetime64[ns]\r\nidentifier \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 int64\r\ntime \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0category\r\ndtype: object\r\n2.12 GB\r\n```\r\nI can't test on 9.0.0 at the moment - would you be able to run it for me?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-10-25T19:50:09.258Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624010) by Weston Pace (westonpace):*\nI tested on both pyarrow 9.0.0 and 4.0.0 with the test data and wasn't able to reproduce.  Can you also print `table.nbytes`?  Also, can you try the experiment without the `df = table.to_pandas()` line?"
        },
        {
            "created_at": "2022-10-26T08:30:41.028Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624296) by Joris Van den Bossche (jorisvandenbossche):*\nI also cannot reproduce this on pyarrow 4.0:\r\n\r\n```Java\n\r\nIn [8]: main()\r\njemalloc\r\nRuns: 5\r\nAfter run 0: RSS = 6.83 GB, PyArrow Allocated Bytes = 5.94 GB\r\nAfter run 1: RSS = 8.08 GB, PyArrow Allocated Bytes = 5.94 GB\r\nAfter run 2: RSS = 8.09 GB, PyArrow Allocated Bytes = 5.94 GB\r\nAfter run 3: RSS = 8.09 GB, PyArrow Allocated Bytes = 5.94 GB\r\nAfter run 4: RSS = 8.10 GB, PyArrow Allocated Bytes = 5.94 GB\r\n\r\nIn [9]: pa.set_memory_pool(pa.system_memory_pool())\r\n\r\nIn [10]: main()\r\nsystem\r\nRuns: 5\r\nAfter run 0: RSS = 6.81 GB, PyArrow Allocated Bytes = 1.31 GB\r\nAfter run 1: RSS = 8.10 GB, PyArrow Allocated Bytes = 1.31 GB\r\nAfter run 2: RSS = 7.88 GB, PyArrow Allocated Bytes = 1.31 GB\r\nAfter run 3: RSS = 8.09 GB, PyArrow Allocated Bytes = 1.31 GB\r\nAfter run 4: RSS = 8.09 GB, PyArrow Allocated Bytes = 1.31 GB\r\n```\r\n\r\nAnd also checked with the system allocator (as you were using that), and it also doesn't reproduce the issue. But I do note a curious difference in pyarrow allocated bytes, which might also be related to the difference you see for that between the datasets API and the ParquetFile API. \r\n\r\nIt seems that with the system memory pool, we don't track the data that is created by the dataset scanner, but only the additional data created in the `to_pandas` call (if I remove `to_pandas` in the example, the system pool indicates 0.0 GB is allocated). `[~westonpace]` that seems a bug?\r\n\r\n`[~Norbo11]` as Weston mentioned, I would indeed try again without the `to_pandas` call (that is the more expensive part here), to see if that makes a difference"
        },
        {
            "created_at": "2022-10-26T08:38:51.763Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624299) by Joris Van den Bossche (jorisvandenbossche):*\nOpened ARROW-18164 for the scanning memory pool issue"
        },
        {
            "created_at": "2022-10-26T12:44:55.670Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624434) by Norbert (Norbo11):*\nWith the following operations (no to_pandas()) call:\r\n```java\n\r\ntable = ds.dataset(\"new_df.pq\").to_table()\n```\r\nThe output is as follows:\r\n```java\n\r\n4.0.1\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 5.55 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 1: RSS = 10.94 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 2: RSS = 12.55 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 3: RSS = 13.89 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 4: RSS = 14.57 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 5: RSS = 15.72 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 6: RSS = 16.62 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 7: RSS = 16.75 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 8: RSS = 17.41 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 9: RSS = 17.55 GB, PyArrow Allocated Bytes = 4.63 GB\n```\r\nWith the following operations (no to_pandas()) call:\r\n```java\n\r\ntable = pq.ParquetFile(\"new_df.pq\").read()\n```\r\nThe output is as follows:\r\n```java\n\r\n4.0.1\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 4.79 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 1: RSS = 4.80 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 2: RSS = 4.81 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 3: RSS = 4.87 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 4: RSS = 4.91 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 5: RSS = 4.95 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 6: RSS = 5.01 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 7: RSS = 4.97 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 8: RSS = 4.97 GB, PyArrow Allocated Bytes = 4.63 GB\r\nAfter run 9: RSS = 4.97 GB, PyArrow Allocated Bytes = 4.63 GB\n```\r\ntable.nbytes is 4971300000 in both cases.\u00a0\r\n\r\nThe issue is still present - the memory profile of using ds.dataset(\"new_df.pq\").to_table() is much higher than pq.ParquetFile(\"new_df.pq\").read().\r\n\r\nOne very interesting thing is that in the pq.ParquetFile() case, removing the to_pandas() call has actually led to **higher** memory usage at the end of the call. Compare:\r\n\r\npq.ParquetFile(\"new_df.pq\").read()\r\n```java\n\r\nAfter run 0: RSS = 4.91 GB, PyArrow Allocated Bytes = 4.75 GB\n```\r\npq.ParquetFile(\"new_df.pq\").read().to_pandas()\r\n```java\n\r\nAfter run 0: RSS = 2.31 GB, PyArrow Allocated Bytes = 1.37 GB\n```\r\nI'm guessing this is because when you introduce the to_pandas() call on same line, the refcount of the pyarrow.Table object goes down to 0 and it gets garbage collected and only the pd.DataFrame stays in memory - and somehow the pyarrow.Table representation of the data consumes 2x more memory than the pd.DataFrame representation, which also seems odd.\r\n\r\n\u00a0\r\n\r\n`[~jorisvandenbossche]` regarding what you've discovered with byte tracking - as you can see above, for me removing to_pandas (in the dataset API case) does not reduce the reported bytes allocated by the pool down to 0.0 GB, it decreases to 4.63 GB. Your original reply seems to indicate that it's a tracking problem, but the bug report you filed claims that the Dataset scanner always uses the jemalloc allocator. However, if I do pyarrow.jemalloc_memory_pool() I get the following output:\r\n```java\n\r\nArrowNotImplementedError: This Arrow build does not enable jemalloc\n```\r\nSo in my case, is it actually using the system allocator and failing to track bytes properly, or doing something completely different?\r\n\r\nWhat else could affect behavior here? Python version? How the pyarrow package was installed? I'm using Python 3.8.3.\r\n\r\n`[~jorisvandenbossche]` could you also post the output of your pq.ParquetFile().read run? The snippet you posted still shows 8 GB usage, which is lower than what I reported, but still almost 4x the size of the 2.12 GB DataFrame or almost 2x the size of the 4.9 GB Table."
        },
        {
            "created_at": "2022-10-26T16:37:44.849Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624593) by Weston Pace (westonpace):*\n> What else could affect behavior here? Python version? How the pyarrow package was installed? I'm using Python 3.8.3.\r\n\r\nGiven this is the system allocator the OS and glibc version might be more significant than the python version.  This could be a pip vs. conda thing also.  I will try and test out pip.  The dataset version of scanning will generate a lot of temporary allocations (probably too many but that is another story).  It seems the allocator is not releasing the RAM.\r\n\r\nFor reference, here are the results I get.  I'm using python 3.9.7 with Arrow 4.0.0 from conda forge.\r\n\r\n```\n\r\nmode=dataset allocator=jemalloc\r\njemalloc\r\nRuns: 10\r\nAfter run 0: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 1: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 2: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 3: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 4: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 5: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 6: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 7: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 8: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 9: RSS = 9.46 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\n```\r\n\r\n```\n\r\nmode=ParquetFile.read() allocator=jemalloc\r\njemalloc\r\nRuns: 10\r\nAfter run 0: RSS = 6.85 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 1: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 2: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 3: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 4: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 5: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 6: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 7: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 8: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 9: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\n```\r\n\r\n```\n\r\nmode=dataset allocator=jemalloc\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 1: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 2: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 3: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 4: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 5: RSS = 9.47 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 6: RSS = 9.48 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 7: RSS = 9.48 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 8: RSS = 9.48 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 9: RSS = 9.48 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\n```\r\n\r\n```\n\r\nmode=ParquetFile.read() allocator=system\r\nsystem\r\nRuns: 10\r\nAfter run 0: RSS = 6.84 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 1: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 2: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 3: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 4: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 5: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 6: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 7: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 8: RSS = 8.15 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\nAfter run 9: RSS = 8.16 GB, PyArrow Allocated Bytes = 5.94 GB Table Nbytes = 4.63 GB\r\n```\r\n\r\nNote, I am working around the tracking bug `[~jorisvandenbossche]` mentioned by setting the default pool using an environment variable."
        },
        {
            "created_at": "2022-10-26T16:52:02.200Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624612) by Weston Pace (westonpace):*\nAnother experiment might be adding a five second sleep between the to_pandas call and the print.  And then a further experiment might be adding an explicit python garbage collection call in addition to the sleep."
        },
        {
            "created_at": "2022-10-26T16:59:23.414Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18156?focusedCommentId=17624620) by Weston Pace (westonpace):*\nFWIW, I get similar results with Arrow 4 from pip."
        }
    ]
}