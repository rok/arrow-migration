{
    "issue": {
        "title": "[Java] The maximum request memory of BaseVariableWidthVector should limit to Interger.MAX_VALUE",
        "body": "***Note**: This issue was originally created as [ARROW-17338](https://issues.apache.org/jira/browse/ARROW-17338). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWe got a IndexOutOfBoundsException:\r\n```\r\n2022-08-03 09:33:34,076 Error executing query, currentState RUNNING, java.lang.RuntimeException: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3315 in stage 5.0 failed 4 times, most recent failure: Lost task 3315.3 in stage 5.0 (TID 3926) (30.97.116.209 executor 49): java.lang.IndexOutOfBoundsException: index: 2147312542, length: 777713 (expected: range(0, 2147483648))\r\n\tat org.apache.iceberg.shaded.org.apache.arrow.memory.ArrowBuf.checkIndex(ArrowBuf.java:699)\r\n\tat org.apache.iceberg.shaded.org.apache.arrow.memory.ArrowBuf.setBytes(ArrowBuf.java:826)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedParquetDefinitionLevelReader$VarWidthReader.nextVal(VectorizedParquetDefinitionLevelReader.java:418)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedParquetDefinitionLevelReader$BaseReader.nextBatch(VectorizedParquetDefinitionLevelReader.java:235)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedPageIterator$VarWidthTypePageReader.nextVal(VectorizedPageIterator.java:353)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedPageIterator$BagePageReader.nextBatch(VectorizedPageIterator.java:161)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator$VarWidthTypeBatchReader.nextBatchOf(VectorizedColumnIterator.java:191)\r\n\tat org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator$BatchReader.nextBatch(VectorizedColumnIterator.java:74)\r\n\tat org.apache.iceberg.arrow.vectorized.VectorizedArrowReader.read(VectorizedArrowReader.java:158)\r\n\tat org.apache.iceberg.spark.data.vectorized.ColumnarBatchReader.read(ColumnarBatchReader.java:51)\r\n\tat org.apache.iceberg.spark.data.vectorized.ColumnarBatchReader.read(ColumnarBatchReader.java:35)\r\n\tat org.apache.iceberg.parquet.VectorizedParquetReader$FileIterator.next(VectorizedParquetReader.java:134)\r\n\tat org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:98)\r\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)\r\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n```\r\n\r\nThe root cause is the following code of `BaseVariableWidthVector.handleSafe` could fail to relocated because of int overflow and then led to `IndexOutOfBoundsException` when we put the data into the vector.\r\n\r\n```java\r\n  protected final void handleSafe(int index, int dataLength) {\r\n    while (index >= getValueCapacity()) {\r\n      reallocValidityAndOffsetBuffers();\r\n    }\r\n    final int startOffset = lastSet < 0 ? 0 : getStartOffset(lastSet + 1);\r\n    // startOffset + dataLength could overflow\r\n    while (valueBuffer.capacity() < (startOffset + dataLength)) {\r\n      reallocDataBuffer();\r\n    }\r\n  }\r\n```\r\n\r\nThe offset width of `BaseVariableWidthVector` is 4, while the maximum memory allocation is Long.MAX_VALUE. This makes the memory allocation check invalid.\r\n",
        "created_at": "2022-08-08T12:07:22.000Z",
        "updated_at": "2022-08-18T01:59:19.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Java",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-08-17T18:31:25.000Z"
    },
    "comments": [
        {
            "created_at": "2022-08-09T17:58:35.952Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17338?focusedCommentId=17577566) by @toddfarmer:*\n`[~coneyliu]` : Thank you for the bug report and the pull request! Your account has been given the \"contributor\" role, which allows ARROW issues - such as this - to be assigned to you. I've assigned this issue to you to reflect your already-made contributions - please assign the issue to me if you have any concern with that."
        },
        {
            "created_at": "2022-08-17T18:31:25.807Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17338?focusedCommentId=17580936) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 13815\n<https://github.com/apache/arrow/pull/13815>"
        }
    ]
}