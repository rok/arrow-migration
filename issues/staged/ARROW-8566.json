{
    "issue": {
        "title": "[R] error when writing POSIXct to spark",
        "body": "***Note**: This issue was originally created as [ARROW-8566](https://issues.apache.org/jira/browse/ARROW-8566). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n`{{monospaced text```` r\r\nlibrary(DBI)\r\nlibrary(sparklyr)\r\nlibrary(arrow)\r\n#> \r\n#> Attaching package: 'arrow'\r\n#> The following object is masked from 'package:utils':\r\n#> \r\n#> timestamp\r\nsc <- spark_connect(master = \"local\")\r\nsparklyr::spark_version(sc)\r\n#> [1] '2.4.5'\r\nx <- data.frame(y = Sys.time())\r\ndbWriteTable(sc, \"test_posixct\", x)\r\n#> Error: org.apache.spark.SparkException: Job aborted.\r\n#> at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n#> at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n#> at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:503)\r\n#> at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:217)\r\n#> at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:176)\r\n#> at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n#> at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n#> at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n#> at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n#> at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n#> at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n#> at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n#> at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n#> at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n#> at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n#> at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n#> at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n#> at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n#> at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n#> at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n#> at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n#> at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n#> at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)\r\n#> at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)\r\n#> at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)\r\n#> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n#> at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n#> at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n#> at java.lang.reflect.Method.invoke(Method.java:498)\r\n#> at sparklyr.Invoke.invoke(invoke.scala:147)\r\n#> at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)\r\n#> at sparklyr.StreamHandler.read(stream.scala:61)\r\n#> at sparklyr.BackendHandler$$anonfun$channelRead0$1.apply$mcV$sp(handler.scala:58)\r\n#> at scala.util.control.Breaks.breakable(Breaks.scala:38)\r\n#> at sparklyr.BackendHandler.channelRead0(handler.scala:38)\r\n#> at sparklyr.BackendHandler.channelRead0(handler.scala:14)\r\n#> at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)\r\n#> at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)\r\n#> at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:328)\r\n#> at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:302)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352)\r\n#> at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1422)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374)\r\n#> at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360)\r\n#> at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:931)\r\n#> at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\r\n#> at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:700)\r\n#> at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)\r\n#> at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)\r\n#> at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)\r\n#> at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)\r\n#> at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n#> at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n#> at java.lang.Thread.run(Thread.java:748)\r\n#> Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): java.lang.UnsupportedOperationException\r\n#> at org.apache.spark.sql.vectorized.ArrowColumnVector.<init>(ArrowColumnVector.java:173)\r\n#> at sparklyr.ArrowConvertersImpl$$anon$1$$anonfun$1.apply(arrowconverters.scala:210)\r\n#> at sparklyr.ArrowConvertersImpl$$anon$1$$anonfun$1.apply(arrowconverters.scala:209)\r\n#> at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n#> at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n#> at scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n#> at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n#> at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n#> at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\r\n#> at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n#> at scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n#> at sparklyr.ArrowConvertersImpl$$anon$1.nextBatch(arrowconverters.scala:209)\r\n#> at sparklyr.ArrowConvertersImpl$$anon$1.<init>(arrowconverters.scala:172)\r\n#> at sparklyr.ArrowConvertersImpl.fromPayloadIterator(arrowconverters.scala:170)\r\n#> at sparklyr.ArrowConvertersImpl.fromPayloadIterator(arrowconverters.scala:157)\r\n#> at sparklyr.ArrowConverters$$anonfun$3.apply(arrowconverters.scala:293)\r\n#> at sparklyr.ArrowConverters$$anonfun$3.apply(arrowconverters.scala:290)\r\n#> at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n#> at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\r\n#> at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n#> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n#> at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n#> at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n#> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n#> at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n#> at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n#> at org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n#> at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n#> at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n#> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n#> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n#> at java.u\r\nsessionInfo()\r\n#> R version 3.6.3 (2020-02-29)\r\n#> Platform: x86_64-apple-darwin15.6.0 (64-bit)\r\n#> Running under: macOS Mojave 10.14.6\r\n#> \r\n#> Matrix products: default\r\n#> BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib\r\n#> LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\r\n#> \r\n#> locale:\r\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\r\n#> \r\n#> attached base packages:\r\n#> [1] stats graphics grDevices utils datasets methods base \r\n#> \r\n#> other attached packages:\r\n#> [1] arrow_0.17.0 sparklyr_1.2.0 DBI_1.1.0 \r\n#> \r\n#> loaded via a namespace (and not attached):\r\n#> [1] Rcpp_1.0.4 compiler_3.6.3 pillar_1.4.3 dbplyr_1.4.3 \r\n#> [5] highr_0.8 r2d3_0.2.3 base64enc_0.1-3 tools_3.6.3 \r\n#> [9] bit_1.1-15.2 digest_0.6.25 jsonlite_1.6.1 evaluate_0.14 \r\n#> [13] tibble_3.0.1 lifecycle_0.2.0 pkgconfig_2.0.3 rlang_0.4.5 \r\n#> [17] rstudioapi_0.11 parallel_3.6.3 yaml_2.2.1 xfun_0.13 \r\n#> [21] withr_2.2.0 httr_1.4.1 stringr_1.4.0 dplyr_0.8.5 \r\n#> [25] knitr_1.28 askpass_1.1 generics_0.0.2 htmlwidgets_1.5.1\r\n#> [29] vctrs_0.2.4 rprojroot_1.3-2 bit64_0.9-7 tidyselect_1.0.0 \r\n#> [33] glue_1.4.0 forge_0.2.0 R6_2.4.1 rmarkdown_2.1 \r\n#> [37] purrr_0.3.4 magrittr_1.5 backports_1.1.6 htmltools_0.4.0 \r\n#> [41] ellipsis_0.3.0 assertthat_0.2.1 config_0.3 stringi_1.4.6 \r\n#> [45] openssl_1.4.1 crayon_1.3.4\r\n```\r\n\r\n<sup>Created on 2020-04-23 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>}}",
        "created_at": "2020-04-23T12:42:04.000Z",
        "updated_at": "2020-05-21T01:32:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-05-21T01:32:36.000Z"
    },
    "comments": [
        {
            "created_at": "2020-04-23T14:58:20.787Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17090673) by Neal Richardson (npr):*\nIs this consistently reproducible? Do any other data types cause issues? I can't tell from the spark traceback what is failing exactly."
        },
        {
            "created_at": "2020-04-23T16:01:56.441Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17090719) by Curt Bergmann (zzcurt):*\nYes, this fails every time. It is also reproduced on my colleague's machine.  \r\n\r\nThe failure is only with a posixct data type.  This data type did not fail in version 16. It seems to be associated with this in the traceback:\r\n\r\njava.lang.UnsupportedOperationException\r\nat org.apache.spark.sql.vectorized.ArrowColumnVector.<init>(ArrowColumnVector.java:173)"
        },
        {
            "created_at": "2020-04-23T17:13:50.815Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17090772) by Neal Richardson (npr):*\nHmm. Unfortunately, `java.lang.UnsupportedOperationException` doesn't tell me anything about what is unsupported.\r\n\r\nThe only thing about posixt types that changed in the last `arrow` release was a fix for ARROW-3543, specifically https://github.com/apache/arrow/commit/507762fa51d17e61f08d36d3626ab8b8df716198. I wonder, does it work if you explicitly set `tz=\"GMT\"` on a POSIXct and send that?\r\n\r\n"
        },
        {
            "created_at": "2020-04-23T19:25:27.716Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17090861) by Curt Bergmann (zzcurt):*\nAssigning the timezone solved the problem. Even though Sys.time() when printed shows a time zone, apparently the tzone attribute is not set. When I set it then I have success writing to the file. The column type that gets created also comes back as a posixct. Following is a run that shows the failure followed by success. To avoid re-showing the long java trace I just print \"Failed\" for when it fails.\r\n\r\nThank you!\r\n\r\nlibrary(DBI)\r\nlibrary(sparklyr)\r\nlibrary(arrow)\r\n#> \r\n#> Attaching package: 'arrow'\r\n#> The following object is masked from 'package:utils':\r\n#> \r\n#>     timestamp\r\nsc <- spark_connect(master = \"local\")\r\nsparklyr::spark_version(sc)\r\n#> [1] '2.4.4'\r\nSys.timezone()\r\n#> [1] \"America/Chicago\"\r\nx <- data.frame(y = Sys.time())\r\nx$y\r\n#> [1] \"2020-04-23 14:17:18 CDT\"\r\nlubridate::tz(x$y)\r\n#> [1] \"\"\r\ntryCatch(dbWriteTable(sc, \"test_posixct\", x), error = function(e) print(\"Failed\"))\r\n#> [1] \"Failed\"\r\nattr(x$y, \"tzone\") <- Sys.timezone()\r\nx$y\r\n#> [1] \"2020-04-23 14:17:18 CDT\"\r\nlubridate::tz(x$y)\r\n#> [1] \"America/Chicago\"\r\ndbWriteTable(sc, \"test_posixct\", x)\r\nresult_df <- dbReadTable(sc, \"test_posixct\")\r\nlubridate::tz(x$y)\r\n#> [1] \"America/Chicago\""
        },
        {
            "created_at": "2020-04-23T20:25:49.825Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17090916) by Neal Richardson (npr):*\nGreat, thanks for debugging with me. I created https://github.com/sparklyr/sparklyr/issues/2439 because I think the current `arrow` behavior is correct (certainly the 0.16 behavior was not correct, unless you happen to live in UTC) so this might need to be worked around in `sparklyr`. "
        },
        {
            "created_at": "2020-05-21T01:32:36.171Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8566?focusedCommentId=17112704) by Neal Richardson (npr):*\nDone in https://github.com/sparklyr/sparklyr/pull/2519"
        }
    ]
}