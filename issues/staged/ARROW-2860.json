{
    "issue": {
        "title": "[Python][Parquet][C++] Null values in a single partition of Parquet dataset, results in invalid schema on read",
        "body": "***Note**: This issue was originally created as [ARROW-2860](https://issues.apache.org/jira/browse/ARROW-2860). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pandas as pd\r\n\r\nfrom datetime import datetime, timedelta\r\n\r\n\r\ndef generate_data(event_type, event_id, offset=0):\r\n    \"\"\"Generate data.\"\"\"\r\n    now = datetime.utcnow() + timedelta(seconds=offset)\r\n    obj = {\r\n        'event_type': event_type,\r\n        'event_id': event_id,\r\n        'event_date': now.date(),\r\n        'foo': None,\r\n        'bar': u'hello',\r\n    }\r\n    if event_type == 2:\r\n        obj['foo'] = 1\r\n        obj['bar'] = u'world'\r\n    if event_type == 3:\r\n        obj['different'] = u'data'\r\n        obj['bar'] = u'event type 3'\r\n    else:\r\n        obj['different'] = None\r\n    return obj\r\n\r\n\r\ndata = [\r\n    generate_data(1, 1, 1),\r\n    generate_data(1, 1, 3600 * 72),\r\n    generate_data(2, 1, 1),\r\n    generate_data(2, 1, 3600 * 72),\r\n    generate_data(3, 1, 1),\r\n    generate_data(3, 1, 3600 * 72),\r\n]\r\n\r\ndf = pd.DataFrame.from_records(data, index='event_id')\r\ntable = pa.Table.from_pandas(df)\r\n\r\npq.write_to_dataset(table, root_path='/tmp/events', partition_cols=['event_type', 'event_date'])\r\n\r\ndataset = pq.ParquetDataset('/tmp/events')\r\ntable = dataset.read()\r\nprint(table.num_rows)\r\n```\r\n\r\nExpected output:\r\n```python\n\r\n6\r\n```\r\n\r\nActual:\r\n```python\n\r\npython example_failure.py\r\nTraceback (most recent call last):\r\n  File \"example_failure.py\", line 43, in <module>\r\n    dataset = pq.ParquetDataset('/tmp/events')\r\n  File \"/Users/sam/.virtualenvs/test-parquet/lib/python2.7/site-packages/pyarrow/parquet.py\", line 745, in __init__\r\n    self.validate_schemas()\r\n  File \"/Users/sam/.virtualenvs/test-parquet/lib/python2.7/site-packages/pyarrow/parquet.py\", line 775, in validate_schemas\r\n    dataset_schema))\r\nValueError: Schema in partition[event_type=2, event_date=0] /tmp/events/event_type=3/event_date=2018-07-16 00:00:00/be001bf576674d09825539f20e99ebe5.parquet was different.\r\nbar: string\r\ndifferent: string\r\nfoo: double\r\nevent_id: int64\r\nmetadata\r\n--------\r\n{'pandas': '{\"pandas_version\": \"0.23.3\", \"index_columns\": [\"event_id\"], \"columns\": [{\"metadata\": null, \"field_name\": \"bar\", \"name\": \"bar\", \"numpy_type\": \"object\", \"pandas_type\": \"unicode\"}, {\"metadata\": null, \"field_name\": \"different\", \"name\": \"different\", \"numpy_type\": \"object\", \"pandas_type\": \"unicode\"}, {\"metadata\": null, \"field_name\": \"foo\", \"name\": \"foo\", \"numpy_type\": \"float64\", \"pandas_type\": \"float64\"}, {\"metadata\": null, \"field_name\": \"event_id\", \"name\": \"event_id\", \"numpy_type\": \"int64\", \"pandas_type\": \"int64\"}], \"column_indexes\": [{\"metadata\": null, \"field_name\": null, \"name\": null, \"numpy_type\": \"object\", \"pandas_type\": \"bytes\"}]}'}\r\n\r\nvs\r\n\r\nbar: string\r\ndifferent: null\r\nfoo: double\r\nevent_id: int64\r\nmetadata\r\n--------\r\n{'pandas': '{\"pandas_version\": \"0.23.3\", \"index_columns\": [\"event_id\"], \"columns\": [{\"metadata\": null, \"field_name\": \"bar\", \"name\": \"bar\", \"numpy_type\": \"object\", \"pandas_type\": \"unicode\"}, {\"metadata\": null, \"field_name\": \"different\", \"name\": \"different\", \"numpy_type\": \"object\", \"pandas_type\": \"empty\"}, {\"metadata\": null, \"field_name\": \"foo\", \"name\": \"foo\", \"numpy_type\": \"float64\", \"pandas_type\": \"float64\"}, {\"metadata\": null, \"field_name\": \"event_id\", \"name\": \"event_id\", \"numpy_type\": \"int64\", \"pandas_type\": \"int64\"}], \"column_indexes\": [{\"metadata\": null, \"field_name\": null, \"name\": null, \"numpy_type\": \"object\", \"pandas_type\": \"bytes\"}]}'}\r\n```\r\n\r\nApparently what is happening is that pyarrow is interpreting the schema from each of the partitions individually and the partitions for `event_type=3 / event_date=\\*`  both have values for the column `different` whereas the other columns do not. The discrepancy causes the `None` values of the other partitions to be labeled as `pandas_type` `empty` instead of `unicode`.",
        "created_at": "2018-07-16T23:51:08.000Z",
        "updated_at": "2022-07-12T14:04:47.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2018-09-15T14:49:46.691Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16616333) by Wes McKinney (wesm):*\nThere's other issues reported about this. We should handle schema normalization across components of a dataset. I don't have time to work on this for 0.11 but I hope that someone else can take a look soon"
        },
        {
            "created_at": "2018-12-04T03:12:15.788Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16708137) by Tanya Schlusser (tanya):*\nI think this was\u00a0resolved with https://issues.apache.org/jira/browse/ARROW-2891\r\n\r\npull request 2302\r\n<https://github.com/apache/arrow/pull/2302>\r\n\r\nWhen I run `example_failure.py` it does not fail and returns the expected result."
        },
        {
            "created_at": "2018-12-04T03:17:22.175Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16708141) by Wes McKinney (wesm):*\nThanks for checking that. There's a couple of related issues that may be the same thing"
        },
        {
            "created_at": "2018-12-19T16:55:25.869Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16725179) by Karl Koster (kkoster):*\nI am running into the same problem. It looks like\u00a0https://issues.apache.org/jira/browse/ARROW-2891\u00a0addresses this within a single write (i.e. multiple partitions on a single write where one or more have empty column sets). The problem I am running in to is across writes (e.g. incrementally adding partitions) when a write contains a column that has only\u00a0NaN's but other partitions have values. Accessing the earliest or latest partition for the schema would likely solve this. Something like the read does when reading multiple partitions (it takes the earliest schema)."
        },
        {
            "created_at": "2019-01-07T19:08:09.942Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736189) by Krisztian Szucs (kszucs):*\nAllowing null typed columns to any nullable columns fix this particular issue. On long term We could check column type compatibility more robustly, however upcasting may not always be desirable. "
        },
        {
            "created_at": "2019-01-07T19:20:33.486Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736212) by Wes McKinney (wesm):*\nWe need to address schema normalization in Parquet files more generally, otherwise we are going to continue to have other kinds of bug reports. "
        },
        {
            "created_at": "2019-01-07T19:54:57.506Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736268) by Krisztian Szucs (kszucs):*\n`[~wesmckinn]` What do You mean under schema normalization a bit precisely? AFAICS it's about checking whether `file_schema` is castable to (compatible with) `dataset_schema` in [validate_schemas](https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L939) and handle the conversions during read (if required). \r\n\r\nIf You can recall the related issues please link them!"
        },
        {
            "created_at": "2019-01-07T20:12:19.751Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736293) by Wes McKinney (wesm):*\nIf a field is nullable in one file and non-nullable in another (which happens), it is rejected by `ParquetDataset`. We should probably also permit automatic integer type promotions (as well as null -> any promotions)\r\n\r\nAs soon as we can write partitioned datasets to Parquet without going through pandas for the groupby splitting (which is causing a lot of these problems), some issues will go away automatically"
        },
        {
            "created_at": "2019-01-07T20:23:44.217Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736303) by Wes McKinney (wesm):*\nSee https://github.com/apache/spark/blob/4427a96bcea625bc51fc5e0e999f170ad537a2fc/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L555\r\n\r\nSee also schema resolution logic in Impala https://github.com/apache/impala/blob/07fd332089c262fa8813605f12b927c8602ac0d2/be/src/exec/parquet/parquet-metadata-utils.h#L124. This is a little bit different because the objective is to establish a mapping between the actual representation of data in the file and the logical schema that is fixed in the Hive metastore"
        },
        {
            "created_at": "2019-01-07T20:24:47.257Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736304) by Krisztian Szucs (kszucs):*\nSo You think something like [dt.castable()](https://github.com/ibis-project/ibis/blob/master/ibis/expr/datatypes.py#L1222) in ibis, but implemented in type.cc (and propagated to Schema)?"
        },
        {
            "created_at": "2019-01-07T21:44:37.976Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16736401) by Wes McKinney (wesm):*\nI think we are a bit beyond the scope of the immediate issue. There are some other issues around schema normalization (if not, let's create an appropriate one); let's take the discussion there and make sure we understand the feature requirements and use cases. I have a bit too much on my plate to think very much about it right now"
        },
        {
            "created_at": "2019-01-10T04:59:01.229Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=16739009) by Wes McKinney (wesm):*\nLike ARROW-2659, I'm moving this to 0.13 so that we can take the time to address this properly "
        },
        {
            "created_at": "2020-04-01T14:01:30.664Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=17072797) by Joris Van den Bossche (jorisvandenbossche):*\nSo several things here:\r\n\r\n- The original root cause of this issue (`write_to_dataset` outputting parquet files with different schemas) has already been solved a while ago in ARROW-2891\n- Of course, you can still end up with parquet files where one of the files has a null type by other means, and we should be able to read those:\n      - We have no plans to fix this in the current ParquetDataset python implementation, but\n      - It already works fine in the new Dataset API, and this can be used in ParquetDataset after ARROW-8039 is merged.\n- So in the new Dataset API, basic schema normalization/evolution already works: nullable vs non-nullable, null -> any type promotion, addition/removal of a column, reordering the columns. For other types mentioned above, there are some open JIRAs (eg ARROW-8282 for ints, ARROW-8284 for timestamps)"
        },
        {
            "created_at": "2020-04-01T14:02:48.726Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=17072799) by Joris Van den Bossche (jorisvandenbossche):*\nSo once we use the datasets API under the hood in pyarrow.parquet (ARROW-8039), this issue should be solved (we can still add a test for it to close this issue)"
        },
        {
            "created_at": "2022-07-12T14:04:47.958Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2860?focusedCommentId=17565754) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}