{
    "issue": {
        "title": "[C++][Parquet] Parquet FileMetaData key_value_metadata not always mapped to Arrow Schema metadata",
        "body": "***Note**: This issue was originally created as [ARROW-16339](https://issues.apache.org/jira/browse/ARROW-16339). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nContext: I ran into this issue when reading Parquet files created by GDAL (using the Arrow C++ APIs, <https://github.com/OSGeo/gdal/pull/5477>), which writes files that have custom key_value_metadata, but without storing ARROW:schema in those metadata (cc `[~paleolimbot]`\r\n\r\n\u2014\r\n\r\nBoth in reading and writing files, I expected that we would map Arrow `Schema::metadata` with Parquet `{}FileMetaData::key_value_metadata{`}. But apparently this doesn't (always) happen out of the box, and only happens through the \"ARROW:schema\" field (which stores the original Arrow schema, and thus the metadata stored in this schema).\r\n\r\nFor example, when writing a Table with schema metadata, this is not stored directly in the Parquet FileMetaData (code below is using branch from ARROW-16337 to have the `store_schema` keyword):\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ntable = pa.table({'a': [1, 2, 3]}, metadata={\"key\": \"value\"})\r\npq.write_table(table, \"test_metadata_with_arrow_schema.parquet\")\r\npq.write_table(table, \"test_metadata_without_arrow_schema.parquet\", store_schema=False)\r\n\r\n# original schema has metadata\r\n>>> table.schema\r\na: int64\r\n-- schema metadata --\r\nkey: 'value'\r\n\r\n# reading back only has the metadata in case we stored ARROW:schema\r\n>>> pq.read_table(\"test_metadata_with_arrow_schema.parquet\").schema\r\na: int64\r\n-- schema metadata --\r\nkey: 'value'\r\n# and not if ARROW:schema is absent\r\n>>> pq.read_table(\"test_metadata_without_arrow_schema.parquet\").schema\r\na: int64\r\n```\r\nIt seems that if we store the ARROW:schema, we _also_ store the schema metadata separately. But if `store_schema` is False, we also stop writing those metadata (not fully sure if this is the intended behaviour, and that's the reason for the above output):\r\n```python\n\r\n# when storing the ARROW:schema, we ALSO store key:value metadata\r\n>>> pq.read_metadata(\"test_metadata_with_arrow_schema.parquet\").metadata\r\n{b'ARROW:schema': b'/////7AAAAAQAAAAAAAKAA4ABgAFAA...',\r\n b'key': b'value'}\r\n# when not storing the schema, we also don't store the key:value\r\n>>> pq.read_metadata(\"test_metadata_without_arrow_schema.parquet\").metadata is None\r\nTrue\r\n```\r\nOn the reading side, it seems that we generally do read custom key/value metadata into schema metadata. We don't have the pyarrow APIs at the moment to create such a file (given the above), but with a small patch I could create such a file:\r\n```python\n\r\n# a Parquet file with ParquetFileMetaData::metadata that ONLY has a custom key\r\n>>> pq.read_metadata(\"test_metadata_without_arrow_schema2.parquet\").metadata\r\n{b'key': b'value'}\r\n\r\n# this metadata is now correctly mapped to the Arrow schema metadata\r\n>>> pq.read_schema(\"test_metadata_without_arrow_schema2.parquet\")\r\na: int64\r\n-- schema metadata --\r\nkey: 'value'\r\n```\r\nBut if you have a file that has both custom key/value metadata and an \"ARROW:schema\" key, we actually ignore the custom keys, and only look at the \"ARROW:schema\" one. \r\nThis was the case that I ran into with GDAL, where I have a file with both keys, but where the custom \"geo\" key is not also included in the serialized arrow schema in the \"ARROW:schema\" key:\r\n```python\n\r\n# includes both keys in the Parquet file\r\n>>> pq.read_metadata(\"test_gdal.parquet\").metadata\r\n{b'geo': b'{\"version\":\"0.1.0\",\"...',\r\n b'ARROW:schema': b'/////3gBAAAQ...'}\r\n# the \"geo\" key is lost in the Arrow schema\r\n>>> pq.read_table(\"test_gdal.parquet\").schema.metadata is None\r\nTrue\r\n```",
        "created_at": "2022-04-26T15:34:15.000Z",
        "updated_at": "2022-10-06T09:23:56.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-04-26T15:42:17.109Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16339?focusedCommentId=17528253) by Joris Van den Bossche (jorisvandenbossche):*\nNote: I recently created an issue about _field-level_ metadata (ARROW-15548), but so this issue is about schema-level (for Arrow) / file-level (for Parquet) metadata.\r\n\r\nThe above is a long description with examples, but trying to summarize the findings, and questions to answer:\r\n\r\n- Do we generally want to map the schema-level metadata from Arrow with Parquet file-level metadata? (I think the answer is yes?)\n- When _reading_, and the file metadata does not contain a \"ARROW:schema\" key, we actually already do map the Parquet file metadata to resulting Arrow schema metadata (this is OK)\n- When _writing_, the `store_schema` flag seems to also influence whether we store schema metadata key/values in the Parquet file. This might be a bug? (or at least unintended behaviour?)\n- When _reading_, and the file metadata does contain both an \"ARROW:schema\" key and other keys, we ignore the other keys. Should we merge the keys from the metadata in the serialized \"ARROW:schema\" schema with the other keys in the Parquet FileMetaData key_value_metadata? (those could of course be duplicative / conflicting)\n  \n  cc `[~apitrou]` `[~emkornfield]` \n   \u00a0"
        },
        {
            "created_at": "2022-05-10T04:08:06.142Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16339?focusedCommentId=17534121) by Micah Kornfield (emkornfield):*\nQ1: I also think the answer is yes.\r\n\r\nQ2: Yes, that seems correct to me as well.\u00a0 I think the assumption is Arrow didn't produce this but KV metadata is still useful.\r\n\r\nQ3: Yes, it seems like the two things should be separate flags.\r\n\r\nQ4: Not sure on this one, it is a little surprising they aren't already covered with the Arrow Schema?"
        },
        {
            "created_at": "2022-05-10T08:46:35.705Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16339?focusedCommentId=17534227) by Joris Van den Bossche (jorisvandenbossche):*\n`[~emkornfield]` Thanks for the input!\r\n\r\n> Q4: Not sure on this one, it is a little surprising they aren't already covered with the Arrow Schema?\r\n\r\nYes, for data written using the standard Arrow mechanisms, that typically won't happen. But so this is exactly the case that I ran into while reading Parquet files created by GDAL (using the Arrow C++ APIs, https://github.com/OSGeo/gdal/pull/5477). By using the Arrow C++ APIs, they do create a Parquet file with an `ARROW:schema` metadata field, but they also add additional (geospatial) metadata, and add this directly to the Parquet file metadata, instead of adding it to the Arrow schema that is getting written.  \r\nFrom GDAL's point of view, I think it makes sense that they add it directly to the Parquet file metadata (since this metadata is also meant to be read by other, non-arrow readers).\r\n\r\nI think for this case if the serialized \"ARROW:schema\" schema does not contain custom metadata fields, but the Parquet file metadata does, I would say we can clearly preserve the ones in the Parquet file metadata. \r\nIt is only when both contain key/value metadata that there might be conflicts, and that it is less clear what to do.\r\n\r\n\u2014\r\n\r\nThat also raises yet another question, related to writing:\r\n\r\n_Q5: If we are writing data with schema-level metadata (and we decided to map this to Parquet file-level metadata, Q1 above), do we then drop this metadata fields from the original schema as how it is serialized into \"ARROW:schema\"?_ \r\nBecause writing both would exactly give such a case of duplicated metadata keys (but not writing both (only in the actual parquet file metadata fields, dropping from the \"ARROW:schema\" field) would result in files that don't read any metadata with existing versions of Arrow, because we currently ignore other keys if an \"ARROW:schema\" key is present, i.e. Q4 above)"
        },
        {
            "created_at": "2022-05-17T15:07:53.471Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16339?focusedCommentId=17538269) by Antoine Pitrou (apitrou):*\nQ1, Q2: agreed with `[~emkornfield]`\r\n\r\nQ3: no idea about this\r\n\r\nQ4: it would sound reasonable to merge metadata, with Arrow keys taking precedence over Parquet keys (if Parquet defines `\"key\": \"foo\"` and Arrow defines `\"key\": \"bar\"`, keep only `\"key\": \"bar\"`).\r\n"
        }
    ]
}