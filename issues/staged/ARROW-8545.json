{
    "issue": {
        "title": "[Python] Allow fast writing of Decimal column to parquet",
        "body": "***Note**: This issue was originally created as [ARROW-8545](https://issues.apache.org/jira/browse/ARROW-8545). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently, when one wants to use a decimal datatype in Pandas, the only possibility is to use the `decimal.Decimal` standard-libary type. This is then an \"object\" column in the DataFrame.\r\n\r\nArrow can write a column of decimal type to Parquet, which is quite impressive given that [fastparquet does not write decimals\\|#data-types]] at all. However, the writing is **very** slow, in the code snippet below a factor of 4.\r\n\r\n**Improvements**\r\n\r\nOf course the best outcome would be if the conversion of a decimal column can be made faster, but I am not familiar enough with pandas internals to know if that's possible. (This same behavior also applies to `.to_pickle` etc.)\r\n\r\nIt would be nice, if a warning is shown that object-typed columns are being converted which is very slow. That would at least make this behavior more explicit.\r\n\r\nNow, if fast parsing of a decimal.Decimal object column is not possible, it would be nice if a workaround is possible. For example, pass an int and then shift the dot \"x\" places to the left. (It is already possible to pass an int column and specify \"decimal\" dtype in the Arrow schema during `pa.Table.from_pandas()` but then it simply becomes a decimal without decimals.) Also, it might be nice if it can be encoded as a 128-bit byte string in the pandas column and then directly interpreted by Arrow.\r\n\r\n**Usecase**\r\n\r\nI need to save large dataframes (~10GB) of geospatial data with latitude/longitude. I can't use float as comparisons need to be exact, and the BigQuery \"clustering\" feature needs either an integer or a decimal but not a float. In the meantime, I have to do a workaround where I use only ints (the original number multiplied by 1000.)\r\n\r\n**Snippet**\r\n```java\n\r\nimport decimal\r\nfrom time import time\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\nd = dict()\r\n\r\nfor col in \"abcdefghijklmnopqrstuvwxyz\":\r\n    d[col] = np.random.rand(int(1E7)) * 100\r\n\r\ndf = pd.DataFrame(d)\r\n\r\nt0 = time()\r\n\r\ndf.to_parquet(\"/tmp/testabc.pq\", engine=\"pyarrow\")\r\n\r\nt1 = time()\r\n\r\ndf[\"a\"] = df[\"a\"].round(decimals=3).astype(str).map(decimal.Decimal)\r\n\r\nt2 = time()\r\n\r\ndf.to_parquet(\"/tmp/testabc_dec.pq\", engine=\"pyarrow\")\r\n\r\nt3 = time()\r\n\r\nprint(f\"Saving the normal dataframe took {t1-t0:.3f}s, with one decimal column {t3-t2:.3f}s\")\r\n# Saving the normal dataframe took 4.430s, with one decimal column 17.673s\n```\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2020-04-21T18:59:12.000Z",
        "updated_at": "2020-04-23T08:30:24.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-04-21T19:09:52.576Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8545?focusedCommentId=17088988) by Jacek Pliszka (jacek.pliszka):*\nOnce you have decimal - is writing fast enough?\r\n\r\nBecause actually you are talking about 2 different things:\r\n1. cast to arrow decimal - not sure if it is implemented but it is relatively easy\n1. fast writing decimal to parquet - is it fast enough for you"
        },
        {
            "created_at": "2020-04-21T19:44:17.214Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8545?focusedCommentId=17089011) by Jacek Pliszka (jacek.pliszka):*\nOK, I checked and This is my version:\r\n\r\n\u00a0\r\n```java\n\r\npat = pa.Table.from_pandas(df)\r\nt3 = time()\r\nprint(t3-t2)\r\npat = pat.set_column(0, 'a', pat.column(0).cast(pa.decimal128(38, 3)))\r\nt4 = time()\r\nprint(t4 - t3)\r\npq.write_table(pat, '/tmp/testabd.pq')\r\nt5 = time()\r\nprint(t5 - t4)\r\n```\r\nAnd we are getting here\r\n\r\nA) 0.3s for conversion from pandas to arrow Table\r\n\r\nB) cast to decimal fails: pyarrow.lib.ArrowNotImplementedError: No cast implemented from double to decimal(38, 3)\r\n\r\nC) 2.8s for writing table to parquet file - is it fast enough for you\r\n\r\n\u00a0\r\n\r\nB and C are separate topics and should have separate issues. In B decimal128 should be easier if this is enough for you\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-04-21T21:05:00.263Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8545?focusedCommentId=17089069) by Jacek Pliszka (jacek.pliszka):*\nCast from float should allow quick conversion"
        },
        {
            "created_at": "2020-04-23T08:23:50.513Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8545?focusedCommentId=17090385) by Joris Van den Bossche (jorisvandenbossche):*\nAs [~jacek.pliszka] says, there are indeed two different parts to this: 1) converting pandas dataframe with decimal objects to arrow Table, and 2) writing the arrow table to parquet. \r\nFrom a quick timing, the slowdown you see with decimals is almost entirely due to step 1 (so not related to writing parquet itself).\r\n\r\nUsing the same dataframe creation as your code above (only using 10x less data to easily fit on my laptop):\r\n```python\n\r\n...\r\ndf1 = pd.DataFrame(d)  \r\n# second dataframe with the decimal column\r\ndf2 = df.copy() \r\ndf2[\"a\"] = df2[\"a\"].round(decimals=3).astype(str).map(decimal.Decimal) \r\n\r\n# convert each of them to a pyarrow.Table\r\ntable1 = pa.table(df1)    \r\ntable2 = pa.table(df2)  \r\n```\r\n\r\nTiming the conversion to pyarrow.Table:\r\n```Java\n\r\nIn [13]: %timeit pa.table(df1)                                                                                                                                                                                     \r\n32 ms \u00b1 7.51 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\nIn [14]: %timeit pa.table(df2)                                                                                                                                                                                     \r\n1.54 s \u00b1 221 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nand then timing the writing of the pyarrow.Table to parquet:\r\n```Java\n\r\n\r\nIn [16]: import pyarrow.parquet as pq  \r\n\r\nIn [17]: %timeit pq.write_table(table1, \"/tmp/testabc.parquet\")   \r\n710 ms \u00b1 29.2 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [18]: %timeit pq.write_table(table2, \"/tmp/testabc.parquet\")  \r\n750 ms \u00b1 44.9 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\nand timing `to_parquet()` more or less gives the sum of the two steps above:\r\n\r\n```Java\n\r\nIn [20]: %timeit df1.to_parquet(\"/tmp/testabc.pq\", engine=\"pyarrow\")   \r\n793 ms \u00b1 73.3 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n\r\nIn [21]: %timeit df2.to_parquet(\"/tmp/testabc.pq\", engine=\"pyarrow\")  \r\n2.01 s \u00b1 61.8 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\n```\r\n\r\n\r\nSo you can see here that the actual writing to parquet is only slightly slower, and that the large slowdown comes from converting the python Decimal objects to a pyarrow decimal column.\r\n\r\nOf course, when starting from a pandas DataFrame to write to parquet, this conversion of pandas to pyarrow.Table is part of the overall process. But, to improve this, I think the only solution is to _not_ use python `decimal.Decimal` objects in an object-dtype column.  \r\nSome options for this:\r\n\r\n- Do the casting to decimal on the pyarrow side. However, as [~jacek.pliszka] linked, this is not yet implemented for floats (ARROW-8557). I am not directly sure if other conversion are possible right now in pyarrow (like converting as ints and convert those to decimal with a factor).\n- Use a pandas ExtensionDtype to store decimals in a pandas DataFrame differently (not as python objects). I am not aware of an existing project that already does this (except for Fletcher, which experiments with storing arrow types in pandas dataframes in general).\n  \n  It might be that this python Decimal object -> pyarrow decimal array conversion is not fully optimized, however, since it involves dealing with a numpy array of python objects, it will never be as fast as converting a numpy float array to pyarrow.\n  \n  \n"
        },
        {
            "created_at": "2020-04-23T08:30:24.089Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8545?focusedCommentId=17090391) by Jacek Pliszka (jacek.pliszka):*\nInt to decimal is not implement either.\r\n\r\n\u00a0\r\n\r\nThought it is much simpler than float to decimal as no rounding handling is needed (we have no negative scale as the moment)\r\n\r\n\u00a0\r\n\r\nIf no one steps up before May I may have some time then to do it.\r\n\r\n\u00a0\r\n\r\nIt is similar to what we did with decimal to decimal and decimal to int casting\r\n\r\n\u00a0\r\n\r\nhttps://issues.apache.org/jira/browse/ARROW-3329"
        }
    ]
}