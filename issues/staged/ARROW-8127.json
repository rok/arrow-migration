{
    "issue": {
        "title": "[C++] [Parquet] Incorrect column chunk metadata for multipage batch writes",
        "body": "***Note**: This issue was originally created as [ARROW-8127](https://issues.apache.org/jira/browse/ARROW-8127). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen writing to a buffered column writer using PLAIN encoding, if the size of the batch supplied for writing exceeds the page size for the writer, the resulting file has an incorrect data_page_offset set in its column chunk metadata.  This causes an exception to be thrown when reading the file (file appears to be too short to the reader).\r\n\r\nFor example, the attached code, which attempts to write a batch of 262145 Int32's (= 1048576 + 4 bytes) using the default page size of 1048576 bytes (with buffered writer, PLAIN encoding), fails on reading, throwing the error: \"Tried reading 1048678 bytes starting at position 1048633 from file but only got 333\".\r\n\r\nThe error is caused by the second page write tripping the conditional here https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc#L302, in the serialized in-memory writer wrapped by the buffered writer.\r\n\r\nThe fix builds the metadata with offsets from the terminal sink rather than the in memory buffered sink.  A PR is coming.",
        "created_at": "2020-03-16T04:56:04.000Z",
        "updated_at": "2020-03-18T17:12:18.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-03-18T17:12:18.000Z"
    },
    "comments": [
        {
            "created_at": "2020-03-18T17:12:18.102Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8127?focusedCommentId=17061919) by Wes McKinney (wesm):*\nIssue resolved by pull request 6637\n<https://github.com/apache/arrow/pull/6637>"
        }
    ]
}