{
    "issue": {
        "title": "[Python] Failed to convert 'float' to 'double' with using pandas_udf and pyspark",
        "body": "***Note**: This issue was originally created as [ARROW-5016](https://issues.apache.org/jira/browse/ARROW-5016). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi everyone,\r\n\r\nI would like to report a (potential) bug. I followed an official guide on [Usage Guide for Pandas with Apache Arrow](<https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html)>.\r\n\r\n\u00a0However,\u00a0`libarrrow` throws me error for type conversion from float -> double. Here is the example and its output.\r\n\r\n\u00a0pyarrow==0.12.1\r\n\r\n```Java\n\r\nfrom pyspark.sql import SparkSession, SQLContext\r\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType, col\r\n\r\nspark = SparkSession.builder.appName('ReproduceBug') .getOrCreate()\r\n\r\ndf = spark.createDataFrame(\r\n    [(1, \"a\"), (1, \"a\"), (1, \"b\")],\r\n    (\"id\", \"value\"))\r\ndf.show()\r\n# Spark DataFrame\r\n# +---+-----+\r\n# | id|value|\r\n# +---+-----+\r\n# |  1|    a|\r\n# |  1|    a|\r\n# |  1|    b|\r\n# +---+-----+\r\n\r\n# Potential Bug # \r\n@pandas_udf('double', PandasUDFType.SCALAR)\r\ndef compute_frequencies(value_col):\r\n    total      = value_col.count()\r\n    per_groups = value_col.groupby(value_col).transform('count')\r\n    score      = per_groups / total\r\n    return score\r\n\r\ndf.groupBy(\"id\")\\\r\n  .agg(compute_frequencies(col('value')))\\\r\n  .show()\r\n\r\nspark.stop()\r\n```\r\n\u00a0\r\n```Java\n\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\n<ipython-input-3-d4f781f64db1> in <module>\r\n     32 \r\n     33 df.groupBy(\"id\")\\\r\n---> 34   .agg(compute_frequencies(col('value')))\\\r\n     35   .show()\r\n     36 \r\n\r\n/usr/local/spark/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)\r\n    376         \"\"\"\r\n    377         if isinstance(truncate, bool) and truncate:\r\n--> 378             print(self._jdf.showString(n, 20, vertical))\r\n    379         else:\r\n    380             print(self._jdf.showString(n, int(truncate), vertical))\r\n\r\n/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)\r\n   1255         answer = self.gateway_client.send_command(command)\r\n   1256         return_value = get_return_value(\r\n-> 1257             answer, self.gateway_client, self.target_id, self.name)\r\n   1258 \r\n   1259         for temp_arg in temp_args:\r\n\r\n/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n     61     def deco(*a, **kw):\r\n     62         try:\r\n---> 63             return f(*a, **kw)\r\n     64         except py4j.protocol.Py4JJavaError as e:\r\n     65             s = e.java_exception.toString()\r\n\r\n/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    326                 raise Py4JJavaError(\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n--> 328                     format(target_id, \".\", name), value)\r\n    329             else:\r\n    330                 raise Py4JError(\r\n\r\nPy4JJavaError: An error occurred while calling o186.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 44 in stage 23.0 failed 1 times, most recent failure: Lost task 44.0 in stage 23.0 (TID 601, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 284, in dump_stream\r\n    batch = _create_batch(series, self._timezone)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 253, in _create_batch\r\n    arrs = [create_array(s, t) for s, t in series]\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 253, in <listcomp>\r\n    arrs = [create_array(s, t) for s, t in series]\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 251, in create_array\r\n    return pa.Array.from_pandas(s, mask=mask, type=t)\r\n  File \"pyarrow/array.pxi\", line 536, in pyarrow.lib.Array.from_pandas\r\n  File \"pyarrow/array.pxi\", line 176, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 85, in pyarrow.lib._ndarray_to_array\r\n  File \"pyarrow/error.pxi\", line 81, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Could not convert 0    0.666667\r\n1    0.666667\r\n2    0.333333\r\nName: _0, dtype: float64 with type Series: tried to convert to double\r\n```\r\n\r\nPlease let me know if you would like to know more any further information. ",
        "created_at": "2019-03-26T19:33:48.000Z",
        "updated_at": "2021-08-05T13:22:19.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-08-05T13:22:19.923Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5016?focusedCommentId=17393999) by Antoine Pitrou (apitrou):*\n`[~dat]` sorry for the absence of response. Is this something you have been reproducing with a recent version of Arrow?"
        }
    ]
}