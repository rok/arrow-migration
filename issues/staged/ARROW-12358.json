{
    "issue": {
        "title": "[C++][Python][R][Dataset] Control overwriting vs appending when writing to existing dataset",
        "body": "***Note**: This issue was originally created as [ARROW-12358](https://issues.apache.org/jira/browse/ARROW-12358). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently, the dataset writing (eg with `pyarrow.dataset.write_dataset`) uses a fixed filename template (`\"part\\{i\\}.ext\"`). This means that when you are writing to an existing dataset, you de facto overwrite previous data when using this default template.\r\n\r\nThere is some discussion in ARROW-10695 about how the user can avoid this by ensuring the file names are unique (the user can specify the `basename_template` to be something unique). There is also ARROW-7706 about silently doubling data (so _not_ overwriting existing data) with the legacy `parquet.write_to_dataset` implementation. \r\n\r\nIt could be good to have a \"mode\" when writing datasets that controls the different possible behaviours. And erroring when there is pre-existing data in the target directory is maybe the safest default, because both appending vs overwriting silently can be surprising behaviour depending on your expectations.\r\n",
        "created_at": "2021-04-13T12:25:47.000Z",
        "updated_at": "2022-10-13T17:51:27.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-04-13T13:56:40.330Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17320191) by Joris Van den Bossche (jorisvandenbossche):*\nAs mentioned by `[~ldacey]` in [ARROW-10695](https://issues.apache.org/jira/browse/ARROW-10695?focusedCommentId=17320167&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17320167) (and also in the comment on my [SO answer](https://stackoverflow.com/questions/67071323/how-to-control-whether-pyarrow-dataset-write-dataset-will-overwrite-previous-dat/67074697#67074697)), one of the consequences of the current default behaviour is that it will sometimes overwrite and sometimes append data, depending on what files are already present and how many parts your are writing.  \r\nIt would probably be useful to be able to either fully overwrite or either always append. \r\n\r\nTaking inspiration for possible \"modes\" from ARROW-7706:\r\n\r\n- `\"overwrite\"`: overwrite existing data\n  - But should it clear all existing data first, or only overwrite files when file names match (i.e. basically the current behaviour)?\n  - Both behaviours might actually be useful depending on your use case?\n    \n- `\"append\"`: append new data to existing data\n  - But can we do this automatically with the default filename template? Because then if there are already `part-0.parquet` and `part-1-.parquet` files present in a certain partition, should it automatically infer the \"current counter\" to write a `part-2.parquet\"`? (that seems rather complicated, especially if the max counter varies across partitions)\n    \n- `\"error\"`: raise an error if data already exists\n  - It can check if the specified base directory is empty or not (it's probably fine to have the directory itself already exist, as long as it is empty), and error if not empty.\n    \n    In addition, it is mentioned that Spark also has a `\"ignore\"` option (silently ignore the write operation if data already exists), but not sure this is important to add.\n    \n"
        },
        {
            "created_at": "2021-04-13T14:29:59.496Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17320221) by Lance Dacey (ldacey):*\nI think that having an \"overwrite\" option would satisfy my need for the partition_filename_cb\u00a0 https://issues.apache.org/jira/browse/ARROW-12365\u00a0if we can replace _all_ data inside the partition. This would be great for file compaction as well - we could read a dataset with a lot of tiny file fragments and then overwrite it.\r\n\r\nOverwriting a specific file is also useful. For example, my basename_template is usually my f\"\\{task-id}-\\{schedule-timestamp}-\\{file-count}-\\{i}.parquet\". I am able clear a task and overwrite a file which already exists. The only flaw here is that we cannot control the \\{i} variable so I guess it is not guaranteed. I could live without this.\r\n\r\nFor \"append\", is it possible for the counter to be per partition instead (potential race conditions if multiple tasks write to the same partition in parallel perhaps, and it seems to be a more demanding step for large datasets..)? Or could the \\{i} variable optionally be a uuid instead of the fragment count?\r\n\r\n\"error\" makes sense.\u00a0"
        },
        {
            "created_at": "2021-04-22T15:53:20.098Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17329223) by Antoine Pitrou (apitrou):*\ncc `[~westonpace]` \u00a0 `[~bkietz]`"
        },
        {
            "created_at": "2021-04-22T18:09:41.337Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17329335) by Weston Pace (westonpace):*\ntl;dr: Do what `[~jorisvandenbossche]` said and interpret \"overwrite\" as \"overwrite the entire partition\".\r\n\r\n\u00a0\r\n\r\n<https://stackoverflow.com/questions/27033823/how-to-overwrite-the-output-directory-in-spark> is related (talks about this issue and how it is handled in Spark).\u00a0 Even reading through all the answers however I cannot tell if \"overwrite\" replaces the entire partition or the entire dataset.\u00a0 It does appear to do one or the other and not just replacing some of a partition.\u00a0 Only replacing some of a partition does not seem like it would ever be useful.\r\n\r\nOverwriting the entire table could always be easily achieved without pyarrow by simply removing the dataset beforehand so I don't see much value in adding that capability.\u00a0 Although it does bring up the question of repartitioning which would require deleting the old data as it is read, but I think that is a different topic (and related to the update topic I mention below).\u00a0 Deleting a partition isn't very hard for the user either.\u00a0 The tricky part though is knowing which partition to delete.\r\n\r\nWith that in mind I'd suggest the following:\r\n\r\nOverwrite-partition: If the dataset write will write to partition X then delete all data in partition X first.\r\n\r\nAppend: Same as `[~jorisvandenbossche]` \u00a0 mentioned.\u00a0 Similar to how we behave today but add logic to make sure we never overwrite a file that happens to have the same counter (e.g. detect the max counter value before we start writing and continue the old counter)\r\n\r\nError: Same as `[~jorisvandenbossche]` mentioned.\r\n\r\n\u00a0\r\n\r\nThe overwrite-partition mode is useful for the case of \"Load the entire dataset (or an entire partition), modify it, write it back out\".\r\n\r\n\u00a0\r\n\r\nHowever, I think the use case that is still missing is:\r\n \\* Run a filtered scan of the data\r\n \\* Modify this subset of data\r\n \\* Write it back out, intending to overwrite the old rows\r\n\r\nIn other words, something equivalent to the SQL \"UPDATE dogs SET favorite=1 WHERE breed='poodle'\"\r\n\r\n\u00a0\r\n\r\nOverwrite-partition won't work because it would delete any non-poodle data.\u00a0 Append wouldn't work because it would duplicate the poodle data.\r\n\r\n\u00a0\r\n\r\nPerhaps however, that can be a separate operation.\u00a0 It brings up troubling atomicity and consistency concerns.\u00a0 Although if we created such an operation then presumably there would be no need for an overwrite mode.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-05-17T12:00:25.862Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17346110) by Lance Dacey (ldacey):*\nBeing able to update and replace specific rows would be very powerful. For my use case, I am basically overwriting the entire partition in order to update a (sometimes tiny) subset of rows. That means that I need to read the existing data for that partition which was saved previously, and the new data with updated or new rows. Then I need to sort and drop duplicates (I use pandas because there is no simple .drop_duplicates() for a pyarrow table, but adding a step with pandas can add some complication sometimes with data types), then I need to overwrite the partition (I use the partition_filename_cb to guarantee that the final file for the partition is the same)."
        },
        {
            "created_at": "2021-05-17T17:37:05.026Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17346298) by Weston Pace (westonpace):*\nSo looking on this with fresh eyes, the \"overwrite mode\" feature is fairly different from an \"update\" feature.\u00a0 So I don't think update related topics are relevant for this ticket.\u00a0 Update generally (and specifically in `[~ldacey]` 's case) implies reading and writing to the same set of files.\u00a0\u00a0 Overwrite-partition mode wouldn't allow for that.\u00a0 Overwrite-partition mode could be useful in some limited circumstances (e.g. somehow someone regenerates an entire new set of data for one or more partitions) but I think those are rare enough, and would be handled by a general \"update\" feature anyways, that I don't see much benefit in creating a separate feature and the complexity would just confuse users.\r\n\r\n\u00a0\r\n\r\nSo I'll walk back my earlier comment.\u00a0 I'd now argue that dataset write should only allow \"append\" and \"error\" options.\r\n\r\n\u00a0\r\n\r\nDataset update could be created as a separate Jira ticket (I'll go ahead and draft one).\u00a0 Dataset update would mean scanning and rewriting a dataset (or parts thereof)."
        },
        {
            "created_at": "2021-08-12T11:18:15.198Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17398007) by Lance Dacey (ldacey):*\nWhat is the common workflow pattern for folks trying to imitate something similar to a view in a database?\r\n\r\n\u00a0\r\n\r\nIn many of my sources I have a dataset which is append only (using UUIDs in the basename template), normally partitioned by date. If this data is downloaded frequently or is generated from multiple sources (for example, several endpoints or servers), then each partition might have many files. Most likely there are also different versions of each row (one ID will have a row for each time it was updated, for example).\r\n\r\n\u00a0\r\n\r\nI then write to a new dataset which is used for reporting and visualization.\u00a0\r\n1. Get the list of files which were saved to the append-only dataset during the most recent schedule\n1. Create a dataset from the list of paths which were just saved and\u00a0use .get_fragments() and ds._get_partition_keys(fragment.partition_expression) to generate a filter expression (this allows me to query for **all** of the data in each relevant partition which was recently modified - so if only a single row was modified in the 2021-08-05 partition, then I still need to read all of the other data in that partition in order to finalize it)\n1. Create a dataframe, sort the data and drop duplicates on a primary key, convert back to a table (it would be nice to be able to do this purely in a pyarrow table so I could leave out pandas!)\n1. Use pq.write_to_dataset() with partition_filename_cb=lambda\u00a0x:\u00a0str(x[-1])\u00a0+\u00a0\".parquet\" to write to a final dataset. This allows me to overwrite the relevant partitions because the filenames are the same. I can be certain that I only have the latest version of each row.\n   \n   \u00a0\n   \n   This is my approach to come close to what I would achieve with a view in the database. It works fine, but the storage is essentially doubled since I am maintaining two datasets (append-only and final). Our visualization tool connects directly to these parquet files, so there is some benefit in having less files (one per partition instead of potentially hundreds) as well."
        },
        {
            "created_at": "2021-08-12T18:32:01.280Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17398227) by Weston Pace (westonpace):*\nDo you clear your append only dataset after step 4?  In other words, is it just a temporary staging area (which stays rather small) or are you wanting to keep the duplicate rows in your base dataset?\r\n\r\nSo, to check my understanding, I think what you are describing is a materialized view with incremental refresh.  Does this sound right?\r\n\r\nIn other words, the results of a query (in your case the query is sort of a \"group by date\" where you take the latest row instead of aggregating anything) are saved off (saved off meaning you don't have to recompute the query each time) as a view and you want to update the view when new data arrives but should only have to read the new data when computing the update.\r\n\r\nSome thoughts to your current approach...\r\n\r\n- If you can perform the update often enough then the append-only table ought to still be cached in memory in the kernel disk cache so reading the newly added data should be fast.  If you don't need to keep this data then write it in IPC format (perhaps even to a tmpfs mount) and the access should be even faster.  There are some durability concerns here but Arrow doesn't generally make durability guarantees anyways.\n- For steps 2 & 3 there is more and more work being done in the CPU layer to add compute and relational algebra into Arrow itself.  Eventually the hope is to support a sort of low level query IR which is currently being discussed in the ML.  This may ease some of the work here but the learning curve is pretty steep at the moment.  You could scan the append-only dataset to get the minimum date value.  Then you could create a second dataset which is a scan of the old view >= min_date.  Then you could union these two datasets, apply an order by date, and drop duplicates.  This would allow you to do everything in Arrow.  However, it's currently all in-progress.  \"order by\" was just added (ARROW-13540) and there is no drop duplicates yet that I am aware of although there may be a way to do this with group by and the right aggregate kernel.\n  \n"
        },
        {
            "created_at": "2021-08-13T12:40:53.576Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17398635) by Lance Dacey (ldacey):*\n\r\nI do not clear my append dataset, but I need to add tasks to consolidate the small files someday. If I download a source every hour, I will have a minimum of 24 files in a single daily partition and some of them might be small. \r\n\r\nBut yes, I am basically describing a materialized view. I cannot rely on an incremental refresh in many cases because I partition data based on the created_at date and not the updated_at date.\r\n\r\nHere is an example where the data was all updated today, but there were some rows originally created days or even months ago.\r\n\r\n```python\n\r\ntable = pa.table(\r\n    {\r\n        \"date_id\": [20210114, 20210811, 20210812, 20210813],    #based on the created_at timestamp\r\n        \"created_at\": [\"2021-01-14 16:45:18\", \"2021-08-11 15:10:00\", \"2021-08-12 11:19:26\", \"2021-08-13 23:01:47\"],\r\n        \"updated_at\": [\"2021-08-13 00:04:12\", \"2021-08-13 02:16:23\", \"2021-08-13 09:55:44\", \"2021-08-13 22:36:01\"],\r\n        \"category\": [\"cow\", \"sheep\", \"dog\", \"cat\"],\r\n        \"value\": [0, 99, 17, 238],\r\n    }\r\n)\r\n```\r\n\r\nPartitioning this by date_id would save the following files in my \"append\" dataset. Note that this has one row which is from January, so I cannot do an incremental refresh from the minimum date because it would be too much data in a real world scenario. \r\n\r\n```python\n\r\nwritten_paths = [\r\n    \"dev/test/date_id=20210812/test-20210813114024-2.parquet\",\r\n    \"dev/test/date_id=20210813/test-20210813114024-3.parquet\",\r\n    \"dev/test/date_id=20210811/test-20210813114024-1.parquet\",\r\n    \"dev/test/date_id=20210114/test-20210813114024-0.parquet\",\r\n]\r\n```\r\n\r\n\r\nDuring my next task, I create a new dataset from the written_paths above (so a dataset of only the new/changed data). Using .get_fragments() and partition expressions, I ultimately generate a filter expression:\r\n\r\n```python\n\r\nfragments = ds.dataset(written_paths, fs).get_fragments()\r\nfor frag in fragments:\r\n    partitions = ds._get_partition_keys(frag.partition_expression)\r\n#... other stuff\r\nfilter_expression = \r\n<pyarrow.dataset.Expression is_in(date_id, {value_set=int32:[\r\n  20210114,\r\n  20210811,\r\n  20210812,\r\n  20210813\r\n], skip_nulls=true})>\r\n```\r\n\r\nFinally, I use that filter to query my \"append\" dataset which has all historical data. So I read all of the data in each partition \r\n```python\n\r\ndf = ds.dataset(source, fs).to_table(filters=filter_expression).to_pandas()\r\n```\r\n, convert the table to pandas, sort and drop duplicates, convert back to a table, and then save to my \"final\" dataset with partition_filename_cb to overwrite whatever was there. This means that if even a single row was updated within a partition, I will be read all of the data in that partition and recompute the final version of each row. This also requires me to use the \"use_legacy_dataset\" flag to support overwriting the existing partitions.\r\n\r\nI found a custom implementation of drop_duplicates (https://github.com/TomScheffers/pyarrow_ops/blob/main/pyarrow_ops/ops.py) using pyarrow Tables, but I am still just using pandas for now. I keep a close eye on the pyarrow.compute() docs and have been slowly replacing stuff I do with pandas directly in the pyarrow tables, which is great.\r\n\r\nYou mentioning the temporary staging area got me to realize that I could replace my messy staging append dataset (many small files) with something temporary that I delete each schedule, and then read from it and create a consolidated historical append-only dataset similar to what I am doing in the example above (one file per partition instead of potentially hundreds)\r\n\r\n\r\n\r\n\r\n"
        },
        {
            "created_at": "2021-08-24T02:29:37.471Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17403461) by Weston Pace (westonpace):*\nI've added some customization here in https://github.com/apache/arrow/pull/10955 via \"existing_data_behavior\". This will provide the options...\r\n\r\n    kError - Raise an error if there are any files or directories in `base_dir` (the new default)\r\n    kOverwriteOrIgnore - Existing files will be ignored unless the filename is one of those chosen by the dataset writer in which case they will be overwritten (the old default)\r\n    kDeleteMatchingPartitions - This is similar to the dynamic partition overwrite mode in parquet. The first time a directory is written to it will delete any existing data.\r\n\r\nThis was based partially on discussion in ARROW-7706.  I think kDeleteMatchingPartitions might simplify your step 4 but I'm not entirely sure.  Feedback welcome!\r\n"
        },
        {
            "created_at": "2021-08-24T12:10:41.462Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17403772) by Lance Dacey (ldacey):*\nkDeleteMatchingPartitions - So this only deletes the individual partitions and not the entire dataset correct? So if I save a dataset made up of hundreds of partitions but only 4 of them are written to, then only those 4 partitions will have their existing files cleared? If so, then yes that should work for me.\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-08-24T18:02:23.379Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17403979) by Weston Pace (westonpace):*\nYes, that is correct.  It will only delete a partition that will be updated / modified as part of the write_dataset operation."
        },
        {
            "created_at": "2021-11-30T01:45:41.550Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17450796) by Lance Dacey (ldacey):*\nI was not able to install 6.0.1 until the latest version of turbodbc supported it. Finally have it up and running and I see that the `existing_data_behavior` argument has been added.\r\n\r\n\u00a0Is this the proper way to use the \"delete_matching\" feature? When I tried to set that as default, there was a FileNotFound error (because the base_dir did not exist at all).\r\n\r\nEDIT - using the try, except does not really work. I need to save the dataset as \"overwrite_or_ignore\" first, then save the dataset again as \"delete_matching\"\r\n\u00a0\r\n```python\n\r\ntry:\r\n    ds.write_dataset(\r\n        data=table,\r\n        existing_data_behavior=\"error\",\r\n    )\r\nexcept pa.lib.ArrowInvalid:\r\n    ds.write_dataset(\r\n        data=table,\r\n        ...,\r\n        existing_data_behavior=\"delete_matching\",\r\n    )\r\n```\r\n\r\n\r\nI created a dataset using my old method (`use_legacy_dataset` = True with a `partition_filename_cb` to overwrite partitions) and the output matched the new \"delete_matching\" dataset. I believe I can completely retire the use_legacy_dataset code now. Really amazing, thank you.\r\n"
        },
        {
            "created_at": "2021-12-02T22:55:53.930Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17452649) by Lance Dacey (ldacey):*\nAny thoughts on \"delete_matching\" creating the partition if it does not exist already? "
        },
        {
            "created_at": "2021-12-03T03:14:55.863Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17452716) by Weston Pace (westonpace):*\nIf \"delete_matching\" is not creating the base directory or the partition directories in the same way as the other methods then I would consider that a bug.  Let's leave this open to fix that."
        },
        {
            "created_at": "2022-01-13T13:34:37.082Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17475363) by Lance Dacey (ldacey):*\n`[~westonpace]` Just wanted to check if this issue with \"delete_matching\" not creating the partition directory is still on the radar. I am currently using \"overwrite_or_ignore\", and then writing the same table again with \"delete_matching\" which is a bit redundant. "
        },
        {
            "created_at": "2022-01-13T21:53:37.591Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17475774) by Weston Pace (westonpace):*\nThanks for checking in.  I did some testing on this today.  I might not be understanding what you are after.  I just tested with the following:\r\n\r\n```Java\n\r\nimport shutil\r\n\r\nimport pyarrow as pa\r\nimport pyarrow.dataset as ds\r\n\r\n# Make sure the /tmp/newdataset directory does not exist                                                                                                                                                           \r\nshutil.rmtree('/tmp/newdataset', ignore_errors=True)\r\n\r\ntab = pa.Table.from_pydict({ 'part': [0, 0, 1, 1], 'value': [0, 1, 2, 3] })\r\nds.write_dataset(tab,\r\n                 '/tmp/newdataset',\r\n                 partitioning_flavor='hive',\r\n                 partitioning=['part'],\r\n                 existing_data_behavior='delete_matching',\r\n                 format='parquet')\r\n```\r\n\r\nI used the 6.0.1 release and did not run into any issues.  Am I misunderstanding the use case?  Or is it possible you are using a certain filesystem?  Or maybe you are on a particular OS?"
        },
        {
            "created_at": "2022-01-14T12:48:16.740Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476120) by Lance Dacey (ldacey):*\nAh, so it must be related to the filesystem. I am using adlfs / fsspec to save datasets on Azure Blob:\r\n\r\n\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.dataset as ds\r\n\r\nprint(type(fs))\r\ntab = pa.Table.from_pydict({ 'part': [0, 0, 1, 1], 'value': [0, 1, 2, 3] })\r\nds.write_dataset(data=tab,\r\n                 base_dir='/dev/newdataset',\r\n                 partitioning_flavor='hive',\r\n                 partitioning=['part'],\r\n                 existing_data_behavior='delete_matching',\r\n                 format='parquet',\r\n                 filesystem=fs)\r\n```\r\n\r\nOutput:\r\n\r\n\r\n```python\n\r\n<class 'adlfs.spec.AzureBlobFileSystem'>\r\n\r\n[2022-01-14 12:45:44,076] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n[2022-01-14 12:45:44,090] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n[2022-01-14 12:45:44,093] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n[2022-01-14 12:45:44,109] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n[2022-01-14 12:45:44,121] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n[2022-01-14 12:45:44,124] {api.py:76} WARNING - Given content is empty, stopping the process very early, returning empty utf_8 str match\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n/tmp/ipykernel_47/3075266795.py in <module>\r\n      4 print(type(fs))\r\n      5 tab = pa.Table.from_pydict({ 'part': [0, 0, 1, 1], 'value': [0, 1, 2, 3] })\r\n----> 6 ds.write_dataset(data=tab,\r\n      7                  base_dir='/dev/newdataset',\r\n      8                  partitioning_flavor='hive',\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/pyarrow/dataset.py in write_dataset(data, base_dir, basename_template, format, partitioning, partitioning_flavor, schema, filesystem, file_options, use_threads, max_partitions, file_visitor, existing_data_behavior)\r\n    876         scanner = data\r\n    877 \r\n--> 878     _filesystemdataset_write(\r\n    879         scanner, base_dir, basename_template, filesystem, partitioning,\r\n    880         file_options, max_partitions, file_visitor, existing_data_behavior\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset._filesystemdataset_write()\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/pyarrow/_fs.pyx in pyarrow._fs._cb_delete_dir_contents()\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/pyarrow/fs.py in delete_dir_contents(self, path)\r\n    357             raise ValueError(\r\n    358                 \"delete_dir_contents called on path '\", path, \"'\")\r\n--> 359         self._delete_dir_contents(path)\r\n    360 \r\n    361     def delete_root_dir_contents(self):\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/pyarrow/fs.py in _delete_dir_contents(self, path)\r\n    347 \r\n    348     def _delete_dir_contents(self, path):\r\n--> 349         for subpath in self.fs.listdir(path, detail=False):\r\n    350             if self.fs.isdir(subpath):\r\n    351                 self.fs.rm(subpath, recursive=True)\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/fsspec/spec.py in listdir(self, path, detail, **kwargs)\r\n   1221     def listdir(self, path, detail=True, **kwargs):\r\n   1222         \"\"\"Alias of `AbstractFileSystem.ls`.\"\"\"\r\n-> 1223         return self.ls(path, detail=detail, **kwargs)\r\n   1224 \r\n   1225     def cp(self, path1, path2, **kwargs):\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/adlfs/spec.py in ls(self, path, detail, invalidate_cache, delimiter, return_glob, **kwargs)\r\n    753     ):\r\n    754 \r\n--> 755         files = sync(\r\n    756             self.loop,\r\n    757             self._ls,\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/fsspec/asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     69         raise FSTimeoutError from return_result\r\n     70     elif isinstance(return_result, BaseException):\r\n---> 71         raise return_result\r\n     72     else:\r\n     73         return return_result\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/fsspec/asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n/opt/conda/envs/airflow/lib/python3.9/site-packages/adlfs/spec.py in _ls(self, path, invalidate_cache, delimiter, return_glob, **kwargs)\r\n    875                     if not finalblobs:\r\n    876                         if not await self._exists(target_path):\r\n--> 877                             raise FileNotFoundError\r\n    878                         return []\r\n    879                     cache[target_path] = finalblobs\r\n\r\nFileNotFoundError: \r\n```\r\n\r\nDo you think I should raise this as an issue on the adlfs project instead?\r\n\r\n"
        },
        {
            "created_at": "2022-01-14T20:31:05.879Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476405) by Weston Pace (westonpace):*\nAh, I think I see.  We call something like...\r\n\r\n```Java\n\r\nfs.CreateDir(partition_dir);\r\nif (delete_matching) {\r\n  fs.DeleteDirContents(partition_dir);\r\n}\r\n```\r\n\r\nMy guess is that ADLFS doesn't handle empty directories very well (I think we have to create an empty file or something when working with S3) so the fs.CreateDir operation is basically a no-op.  Then, when we try to do DeleteDirContents it cannot find the directory.\r\n\r\nThis is a bit of a tricky one.  I wonder if we can come up with some kind of workaround."
        },
        {
            "created_at": "2022-01-14T22:10:23.129Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476451) by Weston Pace (westonpace):*\nThis (https://github.com/apache/arrow/compare/master...westonpace:feature/ARROW-12358--only-call-delete-contents-if-needed) would be one possible fix but not easily regressible (I suppose I could mock out a filesystem to make sure we don't call delete contents on an empty directory but that seems like a lot of complexity)\r\n\r\nThis (https://github.com/apache/arrow/compare/master...westonpace:feature/ARROW-12358--pass-delete-contents-if-dir-not-there) would be another possible fix and it's easily regressible but does lead to different filesystems acting differently (although we already have some of this)\r\n\r\n`[~jorisvandenbossche]` `[~lidavidm]` opinions?"
        },
        {
            "created_at": "2022-01-14T22:48:54.986Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476468) by David Li (lidavidm):*\nHmm, what about calling DeleteDirContents and swallowing a not found error? Or is the error too difficult to distinguish from other errors?"
        },
        {
            "created_at": "2022-01-14T23:20:09.337Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476470) by Weston Pace (westonpace):*\nThe \"not found\" error is thrown from python and then I'd be catching it in `C++`.  I'm not sure how well that would work.  I don't think we have a specific NotFoundError in `C++` so I'd need to examine the message content which is a little icky."
        },
        {
            "created_at": "2022-01-14T23:25:18.225Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476472) by David Li (lidavidm):*\nI think we use KeyError for such things (or else we could use StatusDetail to propagate a more detailed error code) but yes, that would be a little icky if we only had the message content.\r\n\r\nI would probably prefer the second fix (delete and ignore FileNotFound) or perhaps just specifying that DeleteDirContents ignores not-found errors."
        },
        {
            "created_at": "2022-01-14T23:36:18.048Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476475) by Weston Pace (westonpace):*\n> or perhaps just specifying that DeleteDirContents ignores not-found errors.\r\n\r\nThat would mean adding the behavior to local filesystem and the native s3 filesystem which wouldn't be too much trouble.  I wait and let Joris weigh in."
        },
        {
            "created_at": "2022-01-15T17:35:37.688Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17476668) by Antoine Pitrou (apitrou):*\nWe could perhaps add an _option_ to ignore not-found errors."
        },
        {
            "created_at": "2022-02-02T11:22:34.537Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17485722) by Lance Dacey (ldacey):*\nIs this slated for a fix in 7.0.0? I am writing a dataset using \"overwrite_or_ignore\" and then \"delete_matching\" if my initial save fails (FileNotFoundError) using \"delete_matching\"."
        },
        {
            "created_at": "2022-02-02T14:58:37.285Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17485862) by Weston Pace (westonpace):*\nNo, this was not fixed as part of 7.0.0 (that build is pretty much finalized).  I think we have some consensus here on how to fix it (optionally ignoring not-found errors in DeleteDirContents).  I will assign it to myself and expect I will find some time for it in 8.0.0."
        },
        {
            "created_at": "2022-02-02T14:59:13.016Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17485863) by Weston Pace (westonpace):*\nIf anyone wants to grab it in the meantime I don't expect I will be getting to it immediately."
        },
        {
            "created_at": "2022-03-04T13:33:15.661Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17501328) by Lance Dacey (ldacey):*\nIs this issue sufficient to track this? In the meantime, is there a more efficient way to create the partitions instead using \"overwrite_or_ignore\" and then \"delete_matching\" if the first attempt failed?"
        },
        {
            "created_at": "2022-04-05T13:17:25.021Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17517431) by Lance Dacey (ldacey):*\nIs this on the radar to be fixed for the next release?"
        },
        {
            "created_at": "2022-04-05T22:15:00.746Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17517725) by Weston Pace (westonpace):*\nYes, I will tackle this on Friday."
        },
        {
            "created_at": "2022-04-09T02:54:57.369Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17519862) by Weston Pace (westonpace):*\nI created ARROW-16159 (and a PR) to address the \"allow DeleteDirContents to succeed if the dir is not found\" issue.  Once that is merged in we can test this agian."
        },
        {
            "created_at": "2022-04-22T14:58:46.285Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17526483) by Weston Pace (westonpace):*\n`[~ldacey]` now that ARROW-16159 has merged this is probably ready to test again.  Are you able to test with the nightly builds?  Or do you want to wait for the release?"
        },
        {
            "created_at": "2022-04-22T15:11:07.192Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17526487) by Lance Dacey (ldacey):*\nNice, thanks. I can try to test with a nightly build this weekend."
        },
        {
            "created_at": "2022-10-13T17:51:27.681Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12358?focusedCommentId=17617228) by Apache Arrow JIRA Bot (arrowjira):*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned per [project policy](https://arrow.apache.org/docs/dev/developers/bug_reports.html#issue-assignment). Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}