{
    "issue": {
        "title": "[Python] ParquetWriter use_compliant_nested_type=True does not preserve ExtensionArray when reading back",
        "body": "***Note**: This issue was originally created as [ARROW-16348](https://issues.apache.org/jira/browse/ARROW-16348). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI've been happily making ExtensionArrays, but recently noticed that they aren't preserved by round-trips through Parquet files when `{}use_compliant_nested_type=True{`}.\r\n\r\nConsider this writer.py:\r\n\r\n\u00a0\r\n```java\n\r\nimport json\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nclass AnnotatedType(pa.ExtensionType):\r\n\u00a0 \u00a0 def __init__(self, storage_type, annotation):\r\n\u00a0 \u00a0 \u00a0 \u00a0 self.annotation = annotation\r\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__(storage_type, \"my:app\")\r\n\u00a0 \u00a0 def __arrow_ext_serialize__(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return json.dumps(self.annotation).encode()\r\n\u00a0 \u00a0 @classmethod\r\n\u00a0 \u00a0 def __arrow_ext_deserialize__(cls, storage_type, serialized):\r\n\u00a0 \u00a0 \u00a0 \u00a0 annotation = json.loads(serialized.decode())\r\n\u00a0 \u00a0 \u00a0 \u00a0 return cls(storage_type, annotation)\r\n\u00a0 \u00a0 @property\r\n\u00a0 \u00a0 def num_buffers(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return self.storage_type.num_buffers\r\n\u00a0 \u00a0 @property\r\n\u00a0 \u00a0 def num_fields(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return self.storage_type.num_fields\r\npa.register_extension_type(AnnotatedType(pa.null(), None))\r\narray = pa.Array.from_buffers(\r\n\u00a0 \u00a0 AnnotatedType(pa.list_(pa.float64()), {\"cool\": \"beans\"}),\r\n\u00a0 \u00a0 3,\r\n\u00a0 \u00a0 [None, pa.py_buffer(np.array([0, 3, 3, 5], np.int32))],\r\n\u00a0 \u00a0 children=[pa.array([1.1, 2.2, 3.3, 4.4, 5.5])],\r\n)\r\ntable = pa.table({\"\": array})\r\nprint(table)\r\npq.write_table(table, \"tmp.parquet\", use_compliant_nested_type=True)\r\n```\r\nAnd this reader.py:\r\n\r\n\u00a0\r\n```java\n\r\nimport json\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nclass AnnotatedType(pa.ExtensionType):\r\n\u00a0 \u00a0 def __init__(self, storage_type, annotation):\r\n\u00a0 \u00a0 \u00a0 \u00a0 self.annotation = annotation\r\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__(storage_type, \"my:app\")\r\n\u00a0 \u00a0 def __arrow_ext_serialize__(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return json.dumps(self.annotation).encode()\r\n\u00a0 \u00a0 @classmethod\r\n\u00a0 \u00a0 def __arrow_ext_deserialize__(cls, storage_type, serialized):\r\n\u00a0 \u00a0 \u00a0 \u00a0 annotation = json.loads(serialized.decode())\r\n\u00a0 \u00a0 \u00a0 \u00a0 return cls(storage_type, annotation)\r\n\u00a0 \u00a0 @property\r\n\u00a0 \u00a0 def num_buffers(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return self.storage_type.num_buffers\r\n\u00a0 \u00a0 @property\r\n\u00a0 \u00a0 def num_fields(self):\r\n\u00a0 \u00a0 \u00a0 \u00a0 return self.storage_type.num_fields\r\npa.register_extension_type(AnnotatedType(pa.null(), None))\r\ntable = pq.read_table(\"tmp.parquet\")\r\nprint(table)\r\n```\r\n(The AnnotatedType is the same; I wrote it twice for explicitness.)\r\n\r\nWhen the writer.py has `{}use_compliant_nested_type=False{`}, the output is\r\n```java\n\r\n% python writer.py\u00a0\r\npyarrow.Table\r\n: extension<my:app<AnnotatedType>>\r\n----\r\n: [[[1.1,2.2,3.3],[],[4.4,5.5]]]\r\n% python reader.py\u00a0\r\npyarrow.Table\r\n: extension<my:app<AnnotatedType>>\r\n----\r\n: [[[1.1,2.2,3.3],[],[4.4,5.5]]]\n```\r\nIn other words, the AnnotatedType is preserved. When `{}use_compliant_nested_type=True{`}, however,\r\n```java\n\r\n% rm tmp.parquet\r\nrm: remove regular file 'tmp.parquet'? y\r\n% python writer.py\u00a0\r\npyarrow.Table\r\n: extension<my:app<AnnotatedType>>\r\n----\r\n: [[[1.1,2.2,3.3],[],[4.4,5.5]]]\r\n% python reader.py\u00a0\r\npyarrow.Table\r\n: list<element: double>\r\n\u00a0 child 0, element: double\r\n----\r\n: [[[1.1,2.2,3.3],[],[4.4,5.5]]]\n```\r\nThe issue doesn't seem to be in the writing, but in the reading: regardless of whether `use_compliant_nested_type`\u00a0is `True` or `{}False{`}, I can see the extension metadata in the Parquet \u2192 Arrow converted schema.\r\n```java\n\r\n>>> import pyarrow.parquet as pq\r\n>>> pq.ParquetFile(\"tmp.parquet\").schema.to_arrow_schema()\r\n: list<item: double>\r\n\u00a0 child 0, item: double\r\n\u00a0 -- field metadata --\r\n\u00a0 ARROW:extension:metadata: '{\"cool\": \"beans\"}'\r\n\u00a0 ARROW:extension:name: 'my:app'\n```\r\nversus\r\n```java\n\r\n>>> import pyarrow.parquet as pq\r\n>>> pq.ParquetFile(\"tmp.parquet\").schema.to_arrow_schema()\r\n: list<element: double>\r\n\u00a0 child 0, element: double\r\n\u00a0 -- field metadata --\r\n\u00a0 ARROW:extension:metadata: '{\"cool\": \"beans\"}'\r\n\u00a0 ARROW:extension:name: 'my:app'\n```\r\nNote that the first has \"`{}item: double{`}\" and the second has \"`{}element: double{`}\".\r\n\r\n(I'm also rather surprised that `use_compliant_nested_type=False` is an option. Wouldn't you want the Parquet files to always be written with compliant lists? I noticed this when I was having trouble getting the data into BigQuery.)",
        "created_at": "2022-04-26T19:52:56.000Z",
        "updated_at": "2022-05-03T12:13:37.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}