{
    "issue": {
        "title": "[C++] Parquet arrow::Table reads error when overflowing capacity of BinaryArray",
        "body": "***Note**: This issue was originally created as [ARROW-3762](https://issues.apache.org/jira/browse/ARROW-3762). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n1. When reading a parquet file with binary data > 2 GiB, we get an ArrowIOError due to it not creating chunked arrays. Reading each row group individually and then concatenating the tables works, however.\n   \n   \u00a0\n   {code:java}\n   import pandas as pd\n   import pyarrow as pa\n   import pyarrow.parquet as pq\n   \n   \n   x = pa.array(list('1' \\* 2\\*\\*30))\n   \n   demo = 'demo.parquet'\n   \n   \n   def scenario():\n       t = pa.Table.from_arrays([x], ['x'])\n       writer = pq.ParquetWriter(demo, t.schema)\n       for i in range(2):\n           writer.write_table(t)\n       writer.close()\n   \n       pf = pq.ParquetFile(demo)\n   \n1. pyarrow.lib.ArrowIOError: Arrow error: Invalid: BinaryArray cannot contain more than 2147483646 bytes, have 2147483647\n       t2 = pf.read()\n   \n1. Works, but note, there are 32 row groups, not 2 as suggested by:\n1. https://arrow.apache.org/docs/python/parquet.html#finer-grained-reading-and-writing\n       tables = [pf.read_row_group(i) for i in range(pf.num_row_groups)]\n       t3 = pa.concat_tables(tables)\n   \n   scenario()\n   {code}",
        "created_at": "2018-03-01T20:07:47.000Z",
        "updated_at": "2019-09-09T21:13:32.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-09-09T21:13:30.000Z"
    },
    "comments": [
        {
            "created_at": "2018-03-01T20:09:56.780Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16382568) by Left Screen (LeftScreenCorner):*\nRelated ticket is not code-related, but workflow-related in terms of reading/writing binary data"
        },
        {
            "created_at": "2018-03-01T22:05:39.993Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16382733) by Alex Hagerman (alexhagerman):*\nI think these may be related? https://github.com/apache/arrow/issues/1677"
        },
        {
            "created_at": "2018-03-01T22:23:04.307Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16382757) by Wes McKinney (wesm):*\nI moved this issue to Apache Parquet because that's where the fix will need to be implemented. Thank you!"
        },
        {
            "created_at": "2018-09-03T17:09:46.143Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16602358) by Jakub Oko\u0144ski (farnoy):*\nIs there any update on this? How hard would the fix be? Is it something a newcomer can contribute?"
        },
        {
            "created_at": "2018-09-04T20:58:56.222Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16603593) by Wes McKinney (wesm):*\nIt's hard to say but things are going to be much easier once the codebase merge https://github.com/apache/arrow/pull/2453 is done"
        },
        {
            "created_at": "2018-11-11T21:21:33.203Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16683014) by Jason Kiley (jtkiley):*\nLike someone mentioned on the Github issue, I also see this consistently and without any categorical columns in my (pandas) dataframes. In my case, I tend to have one large text column (the body text of documents, where a row is an individual\u00a0document with various columns for its components and associated\u00a0metadata)."
        },
        {
            "created_at": "2018-11-11T22:00:18.157Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16683023) by Wes McKinney (wesm):*\nThis will get fixed eventually but it's hard to predict when. "
        },
        {
            "created_at": "2018-11-11T22:05:12.611Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16683024) by Wes McKinney (wesm):*\nI just moved this issue back to Apache Arrow since it's more Arrow-related and the codebases are now one"
        },
        {
            "created_at": "2018-11-12T02:22:29.204Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16683132) by Jason Kiley (jtkiley):*\nNo worries; I know there's a ton going on. When I ran into it again yesterday, I looked up the issues and noticed that there were some differences in what folks were reporting (e.g. categoricals), so I thought it might help to point out that I see it without them (and with a little context about when I do and what the data looks like). Sorry if I'm the motivation for your tweet, but my intent was to add information."
        },
        {
            "created_at": "2018-11-12T14:38:16.843Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16683875) by Wes McKinney (wesm):*\nNo \u2013 there have been lots of cases where people have written on bug reports (not necessarily in Apache Arrow) things \"This is really causing a problem for me, when will it be fixed?\" which you did not do, so thank you =) One time the comment was \"This is a deal breaker for me\""
        },
        {
            "created_at": "2018-12-11T17:00:09.399Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16717540) by Wes McKinney (wesm):*\nI started working on this. Since we have some other places where we deal with binary / string builder overflows, I'm going to make a \"chunked binary\" builder class that composes the base builders. This can also be used to resolve ARROW-2970"
        },
        {
            "created_at": "2018-12-12T03:56:53.829Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16718435) by Wes McKinney (wesm):*\nThis change is going to be pretty ugly because most of the APIs in `parquet::arrow::FileReader` assume that the result of a `NextBatch` call is a contiguous, non-chunked array\r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/reader.cc#L218\r\n\r\nTo fix this we have to change pretty much every place where we return an Array to instead return a ChunkedArray. This is a lot of code to refactor. \r\n\r\nThis issue is affecting a lot of people so I'm going to try to grind through it tomorrow and see how far I can get. "
        },
        {
            "created_at": "2018-12-14T20:49:17.853Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16721774) by Wes McKinney (wesm):*\nIssue resolved by pull request 3171\n<https://github.com/apache/arrow/pull/3171>"
        },
        {
            "created_at": "2019-06-12T14:20:27.541Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16862150) by Marcin (Cylke):*\nI\u00a0still see this error when using 0.13.0, also tested with 0.12.0.\r\n\r\n\u00a0\r\n\r\nIs this a regression, or has never been fixed?"
        },
        {
            "created_at": "2019-06-12T14:28:02.465Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16862159) by Wes McKinney (wesm):*\nI reopened the issue at tagged with 0.14.0 so we can look into what's going on"
        },
        {
            "created_at": "2019-06-13T13:54:22.217Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16863096) by Wes McKinney (wesm):*\nAnother incidence of this reported here: https://github.com/apache/arrow/issues/4514"
        },
        {
            "created_at": "2019-06-27T13:51:21.002Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16874162) by Wes McKinney (wesm):*\nIssue resolved by pull request 4695\n<https://github.com/apache/arrow/pull/4695>"
        },
        {
            "created_at": "2019-08-12T13:03:24.379Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16905160) by Igor Yastrebov (Igor Yastrebov):*\n`[~wesmckinn]` `[~bkietz]` I am seeing this issue again using arrow 0.14.1"
        },
        {
            "created_at": "2019-08-13T11:28:49.407Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16906096) by Igor Yastrebov (Igor Yastrebov):*\nInterestingly enough, converting string columns to category in pd.DataFrame results in the same error while trying to use to_parquet instead of while reading it afterwards."
        },
        {
            "created_at": "2019-08-13T13:07:02.055Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16906173) by Wes McKinney (wesm):*\nReopened. There have been a lot of patches affecting the Parquet read and write path since 0.13.0 so we should take a look. [~Igor Yastrebov] do you have a reproduction?"
        },
        {
            "created_at": "2019-08-14T10:14:20.577Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16907129) by Igor Yastrebov (Igor Yastrebov):*\nIt seems the issue comes from pandas boolean indexing. CSV file with data (~400000000 lines, couldn't reduce without missing the error): [Google Drive](https://drive.google.com/file/d/1QMwxl4tgo8W-wOL1ih4nXV2vzRq2NaVW/view?usp=sharing)\r\n\r\nReproduction code:\r\n```java\n\r\n>>> import pandas as pd\r\n>>> tst = pd.read_csv('test.csv', dtype = {'col1': 'float32', 'col2': 'str'})\r\n>>> tst = tst[~tst.col1.isnull()]\r\n>>> tst.to_parquet('test.parquet', engine = 'pyarrow', index = False)\r\n>>> tst = pd.read_parquet('test.parquet')\r\n```\r\nConda environment reproduction:\r\n```java\n\r\nconda install python=3.7 pandas=0.25.0 pyarrow=0.14.1 -c conda-forge\r\n```"
        },
        {
            "created_at": "2019-09-03T17:54:17.578Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16921610) by Ben Kietzman (bkietz):*\nThis is caused by the parquet column reader using a [chunk size of `INT_MAX`](https://github.com/apache/arrow/blob/5d907dd/cpp/src/parquet/column_reader.cc#L1214) but BinaryBuilder's [limit is `INT_MAX - 1`](https://github.com/apache/arrow/blob/master/cpp/src/arrow/array/builder_binary.h#L38). ChunkedBinaryBuilder should not be constructible with a chunk size larger than BinaryBuilder's limit, and parquet's column reader should probably just use `chunk size = BinaryBuilder::memory_limit()` and/or that limit should be the default chunk size for ChunkedBinaryBuilder. A unit test in Parquet with string data of total length `INT_MAX` should be added (and maybe for total length `INT_MAX + 1` and `INT_MAX - 1` for good measure)"
        },
        {
            "created_at": "2019-09-03T18:28:36.104Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16921645) by Wes McKinney (wesm):*\nThanks. I'm reworking some of this code path for ARROW-6417 \u2013 I'll construct some unit tests"
        },
        {
            "created_at": "2019-09-06T21:35:50.614Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16924624) by Ben Kietzman (bkietz):*\n`[~wesmckinn]` is this resolved by https://github.com/apache/arrow/pull/5268 ?"
        },
        {
            "created_at": "2019-09-09T21:13:30.209Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3762?focusedCommentId=16926101) by Wes McKinney (wesm):*\nIssue resolved by pull request 5312\n<https://github.com/apache/arrow/pull/5312>"
        }
    ]
}