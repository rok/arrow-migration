{
    "issue": {
        "title": "[Python][R] Expose (hide?) ScanOptions",
        "body": "***Note**: This issue was originally created as [ARROW-12311](https://issues.apache.org/jira/browse/ARROW-12311). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently R completely hides the `ScanOptions` class.\r\n\r\nIn python the class is exposed but the documentation prefers `dataset.scan` (which hides both the scanner and the scan options).\r\n\r\nHowever, there is some useful information in the `ScanOptions`.\u00a0 Specifically, the projected schema (which is a product of the dataset schema and the projection expression and not easily recreated) and the materialized fields (the list of fields referenced by either the filter or the projection) which might be useful for reporting purposes.\r\n\r\nCurrently R uses the projected schema to convert a list of column names into a partition schema.\u00a0 Python does not rely on either field.\r\n\r\n\u00a0\r\n\r\nOptions:\r\n\r\n\u00a0- Keep the status quo\r\n\r\n\u00a0- Expose the ScanOptions object (which itself is exposed via the Scanner)\r\n\r\n\u00a0- Expose the interesting fields via the Scanner\r\n\r\n\u00a0\r\n\r\nCurrently the C++ design is halfway between the latter two (projected schema is exposed and options).\u00a0 My preference would be the third option.\u00a0 It raises a further question about how to expose the scanner itself in Python?\u00a0 Should the user be using ScannerBuilder?\u00a0 Should they use NewScan?\u00a0 Should they use the scanner directly at all or should it be hidden?",
        "created_at": "2021-04-09T10:15:23.000Z",
        "updated_at": "2022-10-04T08:33:46.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-04-09T11:57:17.387Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17317934) by David Li (lidavidm):*\nPython is a bit inconsistent IMO: there are methods on Dataset that create a scanner for you and call to_table/scan_batches (this is fine). The scanner itself is also exposed (but not the builder), getting a scanner itself is less obvious (have to manually construct a scanner). The better thing would be to have `dataset.scan(**kwargs)` exposed. And then I would also lean towards #3. Particularly the projected schema doesn't really feel like an 'option' anyways."
        },
        {
            "created_at": "2021-04-09T13:52:21.066Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17318001) by Joris Van den Bossche (jorisvandenbossche):*\nAgreed on option 3, if we can more easily expose the Scanner object in Python, that seems the appropriate place to expose interesting fields (like the projected schema). \r\n\r\nIt's a bit a pity that `Dataset.scan()` already creates a Scanner \u00e0nd starts the scan, otherwise that method could have returned a Scanner instead. Basically it's `Dataset._scanner(**kwargs)` that we want to expose from the Dataset, right? (without the user having to call `Scanner.from_dataset(dataset, **kwargs)`"
        },
        {
            "created_at": "2021-04-09T15:06:17.659Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17318059) by Neal Richardson (npr):*\nIf there are useful things on ScanOptions and we can get them from Scanner, that would work from my perspective in R. Users generally don't create a Scanner themselves but Scanner$create(dataset, filter_expr, projection, ...) exists and is used inside functions that do the scanning. If Scanner had useful information about the scan-to-be and were cheap to construct, that could be useful."
        },
        {
            "created_at": "2021-04-22T21:15:24.528Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17329419) by David Li (lidavidm):*\nFrom `[~westonpace]` on [this PR](https://github.com/apache/arrow/pull/10134#discussion_r618679685), there are some other things we can clean up on ScanOptions:\r\n\r\n> Not for this PR but I feel like there are a number of places where we have to do something like this to maintain implicit invariants. It might be nice to have a `ScanOptions` cleanup at some point. Change the name so it isn't `Options`, hide it from the public API, give it more methods (like `SetFilter`) and private state."
        },
        {
            "created_at": "2021-12-17T19:45:56.062Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17461633) by Dewey Dunnington (paleolimbot):*\nIf this is still a go, here's the `Scanner` object: https://github.com/apache/arrow/blob/master/r/R/dataset-scan.R#L63-L66\r\n\r\nAnd we expose `options()->projected_schema` here: https://github.com/apache/arrow/blob/master/r/src/dataset.cpp#L495-L498\r\n\r\nDefinition of the `ScanOptions`: https://github.com/apache/arrow/blob/master/cpp/src/arrow/dataset/scanner.h#L60-L144\r\n"
        },
        {
            "created_at": "2021-12-17T20:20:19.292Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17461653) by Weston Pace (westonpace):*\n> If this is still a go\r\n\r\nIt's still very much a valid cleanup in my mind (though I probably won't be working on it in the very near future).  "
        },
        {
            "created_at": "2021-12-21T13:06:11.927Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17463238) by Dewey Dunnington (paleolimbot):*\nWhat I should have asked is...should I work on a PR to implement access to any more of the options members from the R side for the 7.0.0 release? It's not particularly difficult to do but if the API is going to change then perhaps the 7.0.0 release isn't the right fit?"
        },
        {
            "created_at": "2022-01-05T19:00:22.829Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17469484) by Weston Pace (westonpace):*\nI don't think this is going to fit into 7.0.0 (sorry for the delay).  Also, as I'm working on ARROW-13554 I'm wondering if there might be a simpler approach to take here.  If we combine the `ScanOptions` fields `projected_schema` and `projection` into a single type `ProjectionOptions` which has utility methods for creation (e.g. create from column names, create from expressions, create a default project-everything) then I think we can get rid of ScannerBuilder and just have ScanOptions.  This will also simplify ARROW-15257 since we can easily create scan options for use in scan nodes."
        },
        {
            "created_at": "2022-02-11T22:25:58.537Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17491181) by Weston Pace (westonpace):*\nI spent some time today thinking about scan options.  I'd like to propose a simplification.  Note that this simplification requires changes to the existing behavior of the scanner itself in some substantial ways.\r\n\r\nHere are my proposed options (with a bullet list of changes below)\r\n```\n\r\n/// Scan-specific options, which can be changed between scans of the same dataset.\r\nstruct ARROW_DS_EXPORT ScanOptions {\r\n  /// A row filter\r\n  ///\r\n  /// This is an opportunistic pushdown filter.  Filtering capabilities will\r\n  /// vary between fragments.\r\n  ///\r\n  /// Each fragment will do its best to filter the data based on the information\r\n  /// (partitioning guarantees, statistics) available to it.  If it is able to\r\n  /// apply some filtering then it will indicate what filtering it was able to\r\n  /// apply by attaching a guarantee to the batch.\r\n  ///\r\n  /// For example, if a filter is x < 50 && y > 40 then a batch may be able to\r\n  /// apply a guarantee x < 50.  Post-scan filtering would then only need to\r\n  /// consider y > 40 (for this specific batch).  The next batch may not be able\r\n  /// to attach any guarantee and both clauses would need to be applied to that batch.\r\n  ///\r\n  /// The filter expression may not be compatible with the schema of the underlying\r\n  /// file / statistics.  For example, a filter expression may be x > literal(-50,\r\n  /// int32()) The underlying data type might be uint64.  If possible, the reader may\r\n  /// attempt to apply the filter (in this case the gaurantee x > -50 can be attached to\r\n  /// the batch since we know all values will be >= 0 and thus x > -50 is true.\r\n  ///\r\n  /// If a reader is not capable of handling mismatched types in a filter then it should\r\n  /// ignore the filter.\r\n  ///\r\n  /// A single guarantee-aware filtering operation should generally be applied to all\r\n  /// resulting batches.  The scan node consumer is responsible for doing this.\r\n  ///\r\n  /// Note: If you are using a Scanner (as opposed to creating a scan node) there\r\n  /// are a number of convenience methods such as Scanner::Head, Scanner::ScanBatches,\r\n  /// etc.  These \"convenience\" methods will create an exec plan that includes a\r\n  /// scan node, a filter node, and a projection node.  So if you are using these\r\n  /// methods you do not need to worry about this final filtering step.\r\n  compute::Expression filter = compute::literal(true);\r\n\r\n  /// A mapping describing the desired output schema\r\n  ///\r\n  /// If a fragment contains a field that is not included in this mapping the field\r\n  /// should not be returned.  If possible the column should not be loaded from disk.\r\n  ///\r\n  /// If a fragment does not contain a field specified by the mapping then this is\r\n  /// not an error and the fragment should not return anything.  In particular, the\r\n  /// reader should not attempt to create an all-null placeholder.\r\n  ///\r\n  /// If a reference is a nested reference then readers should only return the\r\n  /// desired subset of the column.  For example, if the physical schema has a\r\n  /// field with the type \"points\": struct<{\"real\": double, \"imaginary\": double}>\r\n  /// and the ref is \"points\" then the entire column should be loaded and the data\r\n  /// type of the column will be struct<...>.  If the ref is \"points.real\" then\r\n  /// only the \"real\" portion of the column should be loaded and the data type of\r\n  /// the column will be \"real\"\r\n  ///\r\n  /// If the reader does not support partial loading of nested columns (e.g. CSV) then\r\n  /// the reader should load the entire column and then select the relevant nested\r\n  /// portion.  Note that a single source nested column many end up populating several\r\n  /// output columns.  Readers should take care not to load the source column from disk\r\n  /// multiple times.\r\n  ///\r\n  /// The output field type is included for opportunistic purposes.  If the underlying\r\n  /// column type does not match then this is not an error and the reader should return\r\n  /// the underlying column as is.  In particular, the reader should not cast.  However,\r\n  /// if the column type is unknown then the reader should attempt to infer the column\r\n  /// based on the desired output type.\r\n  ///\r\n  /// For example, consider the mapping: [ \"items\": {\"values\": int64 } ].  If the fragment\r\n  /// is a parquet file with a column named \"items\" that has a float64 data type then the\r\n  /// reader should emit the column \"values\":float64.\r\n  ///\r\n  /// If the fragment is a CSV file with a column named \"items\" it should attempt to parse\r\n  /// the column as int64, inserting null/na or erroring (depending on the reader options)\r\n  /// if it cannot parse a particular cell.\r\n  ///\r\n  /// The scan node itself is responsible for a final casting of data from whatever\r\n  /// physical type the reader was able to produce to the desired output type.  This means\r\n  /// that a scan node will always emit batches according to the output mapping's schema.\r\n  /// If a cast is impossible then the error_on_incompatible_types option will take\r\n  /// effect.\r\n  ///\r\n  /// If empty then no columns will be loaded from the file.  This can be useful\r\n  /// when you need row-count information only.  File readers that support a\r\n  /// metadata-only scan should utilize such a scan when an empty vector is specified.\r\n  std::vector<std::pair<FieldRef, Field>> output_mapping;\r\n\r\n  /// If true the scanner will error when a fragment cannot satisfy a type\r\n  /// requested by the output mapping\r\n  ///\r\n  /// If false the scanner will emit an all-null column (of the requested type) instead\r\n  bool error_on_incompatible_types = true;\r\n\r\n  /// If true the scanner will error when a batch does not include one of the fields\r\n  /// requested in the output_mapping.\r\n  ///\r\n  /// If false the scanner will emit an all-null column (of the requested type) instead\r\n  bool error_on_missing_fields = false;\r\n\r\n  /// Maximum row count for scanned batches.\r\n  int64_t batch_size = kDefaultBatchSize;\r\n\r\n  /// How many batches to read ahead within a file\r\n  ///\r\n  /// Set to 0 to disable batch readahead\r\n  ///\r\n  /// Note: May not be supported by all formats\r\n  int32_t batch_readahead = kDefaultBatchReadahead;\r\n\r\n  /// How many files to read ahead\r\n  ///\r\n  /// Set to 0 to disable fragment readahead\r\n  int32_t fragment_readahead = kDefaultFragmentReadahead;\r\n\r\n  /// A pool from which materialized and scanned arrays will be allocated.\r\n  MemoryPool* pool = arrow::default_memory_pool();\r\n\r\n  /// If true the scanner will scan in parallel\r\n  ///\r\n  /// Note: If true, this will use threads from both the cpu_executor and the\r\n  /// io_context.executor\r\n  bool use_threads = false;\r\n};\r\n```\r\n\r\n- The filtering behavior is not really different from what we have today but hopefully more explicit.  The only thing we don't handle great today is the case when the data type of the filter expression does not match the data type of the column.  See: ARROW-15146\n- The projection behavior we have today is very confusing to the point that I don't even know if what I'm proposing is that much of a change.  The actual syntax is different.  The old syntax (projection_schema + projection + dataset_schema) was rather confusing so I think it's worth the cost of introducing a new almost-schema vector of field ref/field pairs.  Unlike the filtering I don't think it is acceptable to leave some projection for the exec plan.  I've learned more of relational algebra and it seems that nodes should really have a consistent output type known at plan creation time.  Therefore, I think the scan node should be responsible for the final projection.  However, I think the scan node can take care of the casting to keep the reader logic simple.  So basically, readers will do the best they can, and the scan node will finish the rest.\n- Users often have clunky datasets that are full of inconsistencies.  As such, I introduced `error_on_incompatible_types` and `error_on_missing_fields` to allow the user to get nulls instead of errors.\n- Similar to the above, we should be setup to handle datasets that have different files in different formats.  I've removed fragment_scan_options.  This should be a property of the fragment.  If we want to make it easier for users then I think they can provide a default set of options during dataset discovery.  This would also mean that FileSystemDataset eventually loses it's \"format\" property but that can maybe be handled in a follow-up.\n- I removed the io_context option as that should come from the filesystem."
        },
        {
            "created_at": "2022-02-11T22:26:17.933Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17491182) by Weston Pace (westonpace):*\nCC `[~lidavidm]` `[~bkietz]`"
        },
        {
            "created_at": "2022-02-11T22:48:14.541Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17491199) by David Li (lidavidm):*\n- I think the projection behavior is the same, but this is a \"user friendly\" way of putting it. I suppose before you could have the projection be something completely arbitrary, if you implemented your own kernel returning a Struct type, but I don't think that's useful to support. However it seems we only support selecting/renaming/casting fields, not anything more complex? (Oh wait - \"scan node\" will become something distinct from the \"scanner\"? I'm a little confused here now.)\n- Do we want to error on reordered fields? That could matter if we allow indices for projection.\n- For multiple formats, we should perhaps consider the proposal in ARROW-11981: \"Dataset could be simplified to a concrete class containing a set of compatibly typed/formatted Fragments\". "
        },
        {
            "created_at": "2022-02-14T18:52:11.764Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17492169) by Weston Pace (westonpace):*\n> I suppose before you could have the projection be something completely arbitrary, if you implemented your own kernel returning a Struct type, but I don't think that's useful to support.\r\n\r\nAgreed, this is a little less flexible but pretty soon users will be able to issue full fledged queries through Ibis, etc. so they can still go that route and use an actual project node.\r\n\r\n> Oh wait - \"scan node\" will become something distinct from the \"scanner\"? I'm a little confused here now.\r\n\r\nIt's already confusing.  `ScanNode` takes a `ScanOptions` and creates a `Scanner` yet a `Scanner` takes a `ScanOptions` and creates a `ScanNode` (?!).  Scanner is both a high level \"lightweight query plan producer\" and a low level \"file and dataset reading\" utility.  The low level utility can probably become more and more internal.  Users will either interact with \"scan node\" via Ibis, dplyr, etc. or, if they don't want to use a 3rd party query plan producer, they can use Scanner, which is more of a substitute for those libraries with minimal functionality.\r\n\r\nIf we don't want to lose behavior in the high-level \"convenience methods\" (the ones that belong in the \"lightweight query plan producer\") like \"ToTable\", \"ScanBatches\", \"Head\", \"Count\" then we could have two \"scan options\" type classes.  A lower level ScanOptions that is consumed by the scan node and takes the limited projection and a higher level ScanOptions that is consumed by the high-level methods and creates a project node as part of the plan.  The convenience methods would have to extract the lower level projection out of the higher level dictionary of expressions but we already have logic to do that kind of thing today.\r\n\r\n> Do we want to error on reordered fields? That could matter if we allow indices for projection.\r\n\r\nGood point but I think that would mean the schema has to be identical (or maybe a subset from left to right) since there isn't any good way to have a \"hole\" in the schema.  I think we want to allow users some way to specify the dataset schema (e.g. the one the plan is built against and the output of the scan operator) and some kind of \"schema column resolution behavior\".  For example:\r\n \\* Error if non-matching - If the schema doesn't match exactly then error\r\n \\* Error if not-subset - If the schema doesn't match but allow missing columns at the end\r\n \\* Resolve by name - Requires unique names.  Find the output column position by looking for a column in the master schema with the same name\r\n \\* Resolve by id - Same as resolve by name but use the parquet ID field\r\n\r\n> For multiple formats, we should perhaps consider the proposal in ARROW-11981\r\n\r\nThat's a great idea.  I agree completely and it helps keep things simple.\r\n"
        },
        {
            "created_at": "2022-09-28T16:50:23.737Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12311?focusedCommentId=17610656) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned per [project policy](https://arrow.apache.org/docs/dev/developers/bug_reports.html#issue-assignment). Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}