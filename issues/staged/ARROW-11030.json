{
    "issue": {
        "title": "[Rust] [DataFusion] HashJoinExec slow with many batches",
        "body": "***Note**: This issue was originally created as [ARROW-11030](https://issues.apache.org/jira/browse/ARROW-11030). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nPerformance of joins slows down dramatically with smaller batches.\r\n\r\nThe issue is related to slow performance of MutableDataArray::new() when passed a high number of batches. This happens when passing in all of the batches from the build side of the join and this happens once per build-side join key for each probe-side batch.\r\n\r\nIt seems to get exponentially slower as the number of arrays increases even though the number of rows is the same.\r\n\r\nI modified hash_join.rs to have this debug code:\r\n```java\n\r\nlet start = Instant::now();\r\nlet row_count: usize = arrays.iter().map(|arr| arr.len()).sum();\r\nlet num_arrays = arrays.len();\r\nlet mut mutable = MutableArrayData::new(arrays, true, capacity);\r\nif num_arrays > 0 {\r\n    debug!(\"MutableArrayData::new() with {} arrays containing {} rows took {} ms\", num_arrays, row_count, start.elapsed().as_millis());\r\n} \n```\r\nBatch size 131072:\r\n```java\n\r\nMutableArrayData::new() with 4584 arrays containing 3115341 rows took 1 ms\r\nMutableArrayData::new() with 4584 arrays containing 3115341 rows took 1 ms\r\nMutableArrayData::new() with 4584 arrays containing 3115341 rows took 1 ms \n```\r\nBatch size 16384:\r\n```java\n\r\nMutableArrayData::new() with 36624 arrays containing 3115341 rows took 19 ms\r\nMutableArrayData::new() with 36624 arrays containing 3115341 rows took 16 ms\r\nMutableArrayData::new() with 36624 arrays containing 3115341 rows took 17 ms \n```\r\nBatch size 4096:\r\n```java\n\r\nMutableArrayData::new() with 146496 arrays containing 3115341 rows took 88 ms\r\nMutableArrayData::new() with 146496 arrays containing 3115341 rows took 89 ms\r\nMutableArrayData::new() with 146496 arrays containing 3115341 rows took 88 ms \n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2020-12-24T16:36:24.000Z",
        "updated_at": "2021-02-24T01:51:49.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Rust - DataFusion",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-01-07T14:02:29.000Z"
    },
    "comments": [
        {
            "created_at": "2020-12-29T02:22:12.237Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17255771) by Andy Grove (andygrove):*\n`[~jorgecarleitao]` I have more time tomorrow to work on this but please let me know if you have any ideas that may help me."
        },
        {
            "created_at": "2020-12-29T02:47:36.410Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17255780) by Andy Grove (andygrove):*\nI am going to work on https://issues.apache.org/jira/browse/ARROW-11058 next and this will likely help with this issue."
        },
        {
            "created_at": "2020-12-29T07:02:41.792Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17255842) by Jorge Leit\u00e3o (jorgecarleitao):*\nNo idea why: I cannot find a N\\*N operation in `new`, so AFAI can tell, it should not even be polynomial. :/\r\n\r\nI agree that having an average of 20 rows per batch may be breaking some of the assumption we make about the performance of arrays."
        },
        {
            "created_at": "2020-12-29T18:42:30.880Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256105) by Andy Grove (andygrove):*\nI have a theory on what might be happening here but I am struggling to really understand this.\r\n\r\nIt looks like we create a buffer and for each input array, we extend this buffer. Each time we extend it, the buffer is larger so the cost of extending it again gets higher each time?\r\n\r\nIs there a way we can compute upfront how much to extend it by and do one extend operation?"
        },
        {
            "created_at": "2020-12-29T18:46:45.886Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256109) by Dani\u00ebl Heres (Dandandan):*\nOne comment I put in a PR, which is I think part of the problem:\r\n\r\nI think part of a further speed up could be moving the building of the left / build-side Vec<&ArrayData> arrays so that it is only created once instead of for each right batch in build_batch_from_indices. Currently when making the batch size smaller, the build-side Vec is built more times, but also contains more (smaller) batches itself, which could explain (part of the) big / exponential slowdown on smaller batches."
        },
        {
            "created_at": "2020-12-29T18:50:18.350Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256115) by Dani\u00ebl Heres (Dandandan):*\nIt's not directly related to the mutablearraydata::new(), but still makes the hash join having a part with exponential time."
        },
        {
            "created_at": "2020-12-29T19:25:34.667Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256138) by Dani\u00ebl Heres (Dandandan):*\nBut the same is applicable for mutablearraydata: for the left side of the join it generates n batches, which are iterated n times in mutablearraydata::new()."
        },
        {
            "created_at": "2020-12-29T19:30:34.973Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256141) by Jorge Leit\u00e3o (jorgecarleitao):*\n`[~andygrove]`, the MutableArrayData is basically:\r\n1. bound to a bunch of existing Arrays (of the same type), so that it can \"copy slices of slots\" from any of those arrays\n1. It uses the arrays' DataType to extend the buffers and child_data according to the spec\n   \n   The parameter \"capacity\" (in number of slots) is used to reserve all buffers that we can reserve upfront (e.g. primitive types and offsets). In the case of a concat, we can compute that parameter exactly, since it is the sum of all arrays lens: https://github.com/apache/arrow/blob/f4ccceb2536bb6ed5bf584843c55c6628ad23494/rust/arrow/src/compute/kernels/concat.rs#L57\n   \n   The `new` function allocates closures (one per array without child data) that are optimized to extend buffers accordingly."
        },
        {
            "created_at": "2020-12-29T21:25:03.387Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256172) by Andy Grove (andygrove):*\nThanks `[~jorgecarleitao]` and `[~Dandandan]` \u00a0 for the information. It looks like I may have misunderstood the issue a bit. I am going to unassign this for now and change the title back to being specific to hash join."
        },
        {
            "created_at": "2020-12-30T11:09:29.132Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256456) by Dani\u00ebl Heres (Dandandan):*\nSo in summary, what looks like is the problem here:\r\n \\* for the left side of the join we\u00a0.iter()\u00a0on the left side batches each iteration (a couple of times, in the join implementation itself and in\u00a0MutableDataArray) when we process a probe-side batch.\r\n \\* if we halve the batch size, the number of items in the build-side array grows by two, but also the number of probe-side batches we process in the join, thus the\u00a0n\\*n\u00a0slowdown. (the MutableArrayData structure also itself seems not super efficient, making it already visible at bigger batch sizes)\r\n\r\nI am not sure what way best to solve it. I think we have to have something for the left-side batches so we can reuse a \"prepared\" structure which can be used in O(1) time in\u00a0build_batch_from_indices."
        },
        {
            "created_at": "2020-12-30T16:02:56.235Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256575) by Andy Grove (andygrove):*\nI just did an experiment where I concatenated all the batches when building the build-side, so we always have a single batch on that side. It did not make any noticeable difference to performance.\r\n\r\nThe work is in <https://github.com/andygrove/arrow/tree/coalesce-left>"
        },
        {
            "created_at": "2020-12-30T17:11:13.733Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256609) by Dani\u00ebl Heres (Dandandan):*\n`[~andygrove]`, interesting take, will check it out! Doesn't that branch already have the coalesce batches optimization though, so it shouldn't be it a small difference now anyway as it doesn't generate the tiny batches like before?"
        },
        {
            "created_at": "2020-12-31T11:08:36.156Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256945) by Dani\u00ebl Heres (Dandandan):*\nI think it would be nice to further experiment with concatenating the build side into a single batch.\r\n\r\nThoughts:\r\n \\* copying is very fast per element (compared to hashing, random access indexing, etc) so it seems a small penalty to pay upfront\r\n\r\n \\* indexing can use arrows take, which is relatively fast as it can avoid the overhead of the extra indirection, the less efficient MutableDataArray structure and the current remaining exponential behavior (small batches will always be slower though as we always have some constant overhead to pay per batch). take also has the potential for further optimization I think (e.g. SIMD), I think that is hard/impossible to do for something like MutableArrayData.\r\n \\* Makes code for left / right side more similar\r\n \\* Allows other simplifications / speed ups / avoiding extra copies utilizing the simplified structure and arrows kernels more.\r\n\r\nI added a PR for extending take\u00a0to support u64 here <https://github.com/apache/arrow/pull/9057>"
        },
        {
            "created_at": "2020-12-31T12:38:16.152Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17256972) by Jorge Leit\u00e3o (jorgecarleitao):*\n`[~Dandandan]`, I agree with that approach.\r\n\r\nI originally implemented the `MutableArrayData` under the assumption that we had to \"hash-merge\" multiple arrays into a single array, building a new array out of N arrays. The original use-cases were \"hash-merge\" and \"sort-merge\". If we can merge all batches in a single batch prior to the join, then I think we should do it and use take instead.\r\n"
        },
        {
            "created_at": "2020-12-31T16:57:23.520Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17257050) by Dani\u00ebl Heres (Dandandan):*\nThanks `[~jorgecarleitao]` \u00a0for the context & your opinion\u00a0(y)"
        },
        {
            "created_at": "2020-12-31T19:37:54.777Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17257086) by Andy Grove (andygrove):*\nOne other thing to note here is that we may not always be able to merge into a single batch due to size restrictions on arrays, so technically we should talk about merging into as few batches as possible, rather than a single batch."
        },
        {
            "created_at": "2020-12-31T20:04:30.903Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17257094) by Dani\u00ebl Heres (Dandandan):*\nAre you referring to maximum 2 ^ 64 elements in an array (because of the length) or some other restriction in Arrow? The only thing I can find is that for multi language it is recommended to keep it at 2 ^ 31 max, but I assumed we can use the full 64 bits here?\r\n\r\nIf the limit is 2 ^ 64, maybe at this length at this time we could say at this point that a higher limit is not realistic yet for an in memory hash join? I think technically you could make it a Vec<RecordBatch> and group/append indices by the first index to be still using the \"direct index\" approach as mentioned instead and still utilizing take, etc. But if technically not possible, is it necessary?"
        },
        {
            "created_at": "2021-01-01T12:49:28.911Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17257187) by Dani\u00ebl Heres (Dandandan):*\nAdded a PR here, still with\u00a0MutableArrayData (so some optimization opportunities left) but with batches merged to one batch https://github.com/apache/arrow/pull/9070\r\n Collecting to one batch seems very promising, and removes a large amount of the remaining overhead when using smaller batches."
        },
        {
            "created_at": "2021-01-07T14:02:29.483Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17260517) by Andrew Lamb (alamb):*\nIssue resolved by pull request 9070\n<https://github.com/apache/arrow/pull/9070>"
        },
        {
            "created_at": "2021-02-24T01:51:49.373Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11030?focusedCommentId=17289510) by Andy Grove (andygrove):*\nYes, that is what I am referring to, as well as the limit on data length in\nvariable-width arrays.\n\nI am specifically thinking about the 32-bit variants, which will be\nimportant on some hardware platforms even when just using the Rust\nimplementation.\n\n\n"
        }
    ]
}