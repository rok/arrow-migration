{
    "issue": {
        "title": "[Java] Arrow-to-JDBC",
        "body": "***Note**: This issue was originally created as [ARROW-4144](https://issues.apache.org/jira/browse/ARROW-4144). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nARROW-1780 reads a query from a JDBC data source and converts the ResultSet to an Arrow VectorSchemaRoot.\u00a0 However, there is no built-in adapter for writing\u00a0an Arrow VectorSchemaRoot back to the database.\r\n\r\nARROW-3966 adds JDBC field metadata:\r\n \\* The Catalog Name\r\n \\* The Table Name\r\n \\* The Field Name\r\n \\* The Field Type\r\n\r\nWe can use this information to ask for the field information\u00a0from the database via the [DatabaseMetaData](https://docs.oracle.com/javase/7/docs/api/java/sql/DatabaseMetaData.html)\u00a0object.\u00a0 We can then create INSERT or UPDATE statements based on the [list of primary keys](https://docs.oracle.com/javase/7/docs/api/java/sql/DatabaseMetaData.html#getPrimaryKeys(java.lang.String,%20java.lang.String,%20java.lang.String))\u00a0in the table:\r\n \\* If the value in the VectorSchemaRoot corresponding to the primary key is NULL, insert that record into the database.\r\n \\* If the value in the VectorSchemaRoot corresponding to the primary key is not NULL, update the existing record in the database.\r\n\r\nWe can also perform the same data conversion in reverse based on the field types queried from the database.",
        "created_at": "2019-01-01T23:52:46.000Z",
        "updated_at": "2022-07-12T14:04:23.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Java",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-06-02T02:17:02.021Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17121502) by Chen (Guoping1):*\nis it\u00a0necessary to do it,?Arrow is\u00a0designed for in-memory data\u00a0analysis, the data won't be\u00a0modified.\u00a0\r\n\r\nrecently i pay much\u00a0 attention on Arrow, i want to make a contribution to the project.\r\n\r\nif this issue is necessary, i am happy to\u00a0resolve it."
        },
        {
            "created_at": "2020-06-02T02:59:00.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17121516) by Michael Pigott (rpimike1022):*\nHi, and thanks for taking a look at this!  I saw Arrow as a tool that\ncould be used to transfer data between systems, and not just for\nin-memory analysis.  However, I don't need this today, and if nobody\nelse has asked for it, then there might be more valuable JIRA tickets\nthan mine for you to dedicate your time to?\n\nThanks again!\nMike\n\n"
        },
        {
            "created_at": "2020-06-02T06:55:52.492Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17123402) by Micah Kornfield (emkornfield@gmail.com):*\n`[~uwe]` have you come across a use-case for writing to JDBC sources?"
        },
        {
            "created_at": "2020-06-02T08:13:48.310Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17123484) by Uwe Korn (uwe):*\nYes, the usecase would be to write large `pandas.DataFrames` to a database layer that only has performant JDBC drivers. Personally, all my JDBC sources are read-only and thus I didn't write a WriteToJDBC function but other people will also use these technologies with more access rights. I have used the \"pyarrow->Arrow Java -> JDBC\" successfully with Apache Drill and Denodo. I also heard that some people use it together with Amazon Athena and here a performant INSERT might be interesting\u00a0<https://docs.aws.amazon.com/athena/latest/ug/insert-into.html>\u00a0as the JDBC driver seems to be the most performant currently."
        },
        {
            "created_at": "2020-06-05T08:34:05.184Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17126539) by Gleb (Sakhnov):*\nOur team is also in need for this feature.\r\n\r\nCurrently we are implementing cross-language data processing inside k8s, the DB component is responsible for pushing/pulling DB data. Arrow is chosen as a data format due to efficiency and gRPC support from the box. The only thing that we lack when working with JDBC sources - is the ability to convert data back to JDBC from Arrow."
        },
        {
            "created_at": "2022-07-12T14:04:23.298Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4144?focusedCommentId=17565574) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}