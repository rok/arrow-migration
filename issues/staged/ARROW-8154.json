{
    "issue": {
        "title": "[Python] HDFS Filesystem does not set environment variables in  pyarrow 0.16.0 release",
        "body": "***Note**: This issue was originally created as [ARROW-8154](https://issues.apache.org/jira/browse/ARROW-8154). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn pyarrow 0.15.x, HDFS filesystem works as follows:\r\n\r\nIf you set HADOOP_HOME env var, it looks for libhdfs.so in\u00a0$HADOOP_HOME/lib/native.\r\n\r\nIn pyarrow 0.16.x, if you set HADOOP_HOME, it looks for libhdfs.so in $HADOOP_HOME, which is incorrect behaviour on all systems I am using.\r\n\r\nAlso, CLASSPATH no longer gets set automatically, which is very convenient. The issue here is that I need to set hadoop home correctly to be able to use other libraries, but have to reset it to use apache arrow. e.g.\r\n\r\nos.environ[\"HADOOP_HOME\"] = \"/usr/lib/hadoop\"\r\n\r\n..do stuff here..\r\n\r\n...then connect to arrow...\r\n\r\nos.environ[\"HADOOP_HOME\"] = \"/usr/lib/hadoop/lib/native\"\r\n\r\nhdfs = pyarrow.hdfs.connect(host, port)\r\n\r\n...then reset my hadoop home...\r\n\r\nos.environ[\"HADOOP_HOME\"] = \"/usr/lib/hadoop\"\r\n\r\netc.\r\n\r\n\u00a0\r\n\r\nExample:\r\n\r\n>>> os.environ[\"HADOOP_HOME\"] = \"/usr/lib/hadoop\"\r\n\r\n>>> hdfs = pyarrow.hdfs.connect(host, port)\r\n\r\nTraceback (most recent call last):\r\n\r\n\u00a0 File \"<stdin>\", line 1, in <module>\r\n\r\n\u00a0 File \"/home/user/.conda/envs/retroscoring/lib/python3.6/site-packages/pyarrow/hdfs.py\", line 215, in connect\r\n\r\n\u00a0 \u00a0 extra_conf=extra_conf)\r\n\r\n\u00a0 File \"/home/user/.conda/envs/retroscoring/lib/python3.6/site-packages/pyarrow/hdfs.py\", line 40, in __init__\r\n\r\n\u00a0 \u00a0 self._connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n\r\n\u00a0 File \"pyarrow/io-hdfs.pxi\", line 89, in pyarrow.lib.HadoopFileSystem._connect\r\n\r\n\u00a0 File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\n\r\nOSError: Unable to load libhdfs: /usr/lib/hadoop/libhdfs.so: cannot open shared object file: No such file or directory\r\n\r\n\u00a0",
        "created_at": "2020-03-18T20:43:09.000Z",
        "updated_at": "2020-03-18T23:27:44.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-03-18T23:13:34.000Z"
    },
    "comments": [
        {
            "created_at": "2020-03-18T21:40:51.256Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8154?focusedCommentId=17062082) by Wes McKinney (wesm):*\nI think this is a dup of ARROW-7841, a regression that has been fixed since 0.16.0 was released"
        },
        {
            "created_at": "2020-03-18T23:13:35.029Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8154?focusedCommentId=17062121) by Kouhei Sutou (kou):*\nI think so too.\r\n\r\nSorry for the regression.\r\nPlease wait for 0.17.0."
        }
    ]
}