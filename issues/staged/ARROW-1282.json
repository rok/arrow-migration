{
    "issue": {
        "title": "Large memory reallocation by Arrow causes hang in jemalloc",
        "body": "***Note**: This issue was originally created as [ARROW-1282](https://issues.apache.org/jira/browse/ARROW-1282). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen reallocating a large amount of memory, Arrow is either triggering a bug in jemalloc or has a bug itself in the memory manager (many different applications reporting same issue but not clear from jemalloc issue description if they're sure it's in jemalloc or caused by other issues like using multiple memory allocation libraries in the same process, multithreaded access, etc).\n\nLink to stack trace is here: https://gist.github.com/jeffknupp/73879feacf9c560afd4f1a20213dc6ef\n\nLink to issue in jemalloc GitHub is here: https://github.com/jemalloc/jemalloc/issues/802\n\nOriginally observed in redis, discussed with jemalloc maintainer here: https://github.com/antirez/redis/issues/3799\n\n**This is entirely reproducible on Ubuntu 16.04 xenial, which uses version 3.6.0 according to `apt` metadata.**",
        "created_at": "2017-07-27T00:11:40.000Z",
        "updated_at": "2017-11-07T22:37:13.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-11-07T22:37:13.000Z"
    },
    "comments": [
        {
            "created_at": "2017-07-27T04:09:17.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16102671) by Wes McKinney (wesm):*\nIs this reproducible in jemalloc 4.4.0? We are pinned at 4.4.0 in conda-forge:\n\nhttps://github.com/conda-forge/arrow-cpp-feedstock/blob/master/recipe/meta.yaml#L38\n\n"
        },
        {
            "created_at": "2017-07-27T04:11:52.463Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16102675) by Wes McKinney (wesm):*\n`[~xhochy]` how are we handling jemalloc in manylinux1 builds?"
        },
        {
            "created_at": "2017-07-27T04:19:00.206Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16102678) by Jeff Knupp (jeff.knupp@enigma.com):*\nIt\u2019s reproducible in 4.5.0 I\u2019ve yet to test against 5.0 which was only released about a month ago, but I\u2019m assuming it is also present in 0.4.4. I\u2019ll try to install conda on one of our instances and test; the only issue is I\u2019m using the dev version of pandas (which has my fix for the crashes that were happening on large columns) and I don\u2019t have a lot of experience using condas, but I\u2019ll do my best.\n\n"
        },
        {
            "created_at": "2017-07-27T12:41:00.163Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103156) by Uwe Korn (uwe):*\nIn the manylinux1 builds, we pin to `jemalloc==4.4.0`."
        },
        {
            "created_at": "2017-07-27T13:13:40.237Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103190) by Wes McKinney (wesm):*\nStatically linking to 4.4.0, right?"
        },
        {
            "created_at": "2017-07-27T14:38:03.093Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103281) by Wes McKinney (wesm):*\n`[~jeffknupp]` you're definitely hitting this in jemalloc 4.5.0, i.e. including this fix https://github.com/jemalloc/jemalloc/commit/adae7cfc4a2ac66c96b0dcc83b3837ac668fc44e ? \n\nIf there's a way we can reproduce this we can do some debugging on Ubuntu 16.04. There are some other issues preventing us from upgrading to 5.0.x at the moment (https://github.com/jemalloc/jemalloc/issues/937) "
        },
        {
            "created_at": "2017-07-27T15:24:11.665Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103337) by Jeff Knupp (jeffknupp):*\nI'm hitting this with both 4.5 **and** 5.0.1. There was talk of this being related to the application's use of jemalloc along with another allocator and/or in a multithreaded environment causing corruption of the jemalloc datastructures, so it's possible it's not a bug that jemalloc has/will fix. I have to do more debugging, but I'll also try to set up a Dockerfile to reproduce this (the biggest issue is that it requires reading and converting a number of large CSVs for a few minutes before it happens, so I need to provide some large fake data files and make sure they still trigger the issue)."
        },
        {
            "created_at": "2017-07-27T15:55:09.826Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103382) by Jeff Knupp (jeffknupp):*\nCorrection, not 5.0.1 as it's not compatible (as you mentioned), but definitely with 4.5."
        },
        {
            "created_at": "2017-07-27T16:08:39.166Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103395) by Wes McKinney (wesm):*\nI see. From the comment at https://github.com/antirez/redis/issues/3799#issuecomment-297832432 this is definitely concerning since there are plenty of other libraries in the stack that will not be using jemalloc. Particularly with multithreaded Parquet reads, there might be two threads which are using different allocators (e.g. the Python interpreter is using the system allocator, while Parquet/Arrow are using jemalloc)\n\n`[~xhochy]` the conservative approach on this might be to disable jemalloc in our conda and PyPI packages until we understand what is going on. What do you think?"
        },
        {
            "created_at": "2017-07-27T19:28:00.448Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103766) by Jeff Knupp (jeff.knupp@enigma.com):*\nProbably safest to roll back. I think I'm pretty close to determining the\ncause but probably better not to have a broken version out.\n\nOn Thu, Jul 27, 2017 at 12:09 PM, Wes McKinney (JIRA) <jira@apache.org>\n\n"
        },
        {
            "created_at": "2017-07-27T19:52:32.560Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103796) by Wes McKinney (wesm):*\nJust opened https://github.com/conda-forge/arrow-cpp-feedstock/pull/35. Will need to build new wheels for PyPI. Luckily `[~xhochy]` made it really easy to turn the jemalloc allocator on and off"
        },
        {
            "created_at": "2017-07-27T19:58:03.562Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103806) by Jeff Knupp (jeffknupp):*\nFWIW, it definitely appears to be an issue caused by other allocators calling `sbrk()`, which it seems `jemalloc` doesn't play nicely with. I'm wondering if it would be fixed by recompiling my Python interpreter to use jemalloc as well. Regardless, I'm still investigating and will try to have a way for everyone to reproduce in the next day or two."
        },
        {
            "created_at": "2017-07-27T20:09:13.022Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16103824) by Wes McKinney (wesm):*\nI appreciate it. I'm a bit spooked by this issue, in the jemalloc wiki it suggests that having applications with both jemalloc and system allocator is OK\n\nhttps://github.com/jemalloc/jemalloc/wiki/Getting-Started\n\nI wonder if this problem goes away / gets better if we compile jemalloc with a prefix and use the prefixed functions in our memory allocator. Redis does this https://github.com/antirez/redis/blob/unstable/deps/Makefile#L80\n\nAs I understand it, if you compile jemalloc unprefixed, then the intended use is to override all malloc/free calls in the application (e.g. using LD_PRELOAD) so that libc malloc/free never get used"
        },
        {
            "created_at": "2017-07-28T01:03:36.024Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16104256) by Jeff Knupp (jeffknupp):*\n`[~wesmckinn]` `[~xhochy]` Actually, we may want to hold off on removing jemalloc for a moment. In testing `Arrow::from_pandas` on a large dataframe (4GB CSV file), the allocation strategy used without `jemalloc` is _very_ conservative and only resizes to fit the next element in a call to `BufferBuilder::append`. This essentially slows the program down so much as to be unusable. We should probably come up with a better way of signaling from `ConvertObjectStrings`/`AppendObjectStrings` a rough estimate of the total size we'll need (which we should know since we already have the complete dataframe in memory, right?).\n\nSorry, I'm trying everything I can to get this all to work together, but it seems that my usage (very large Tables) is non-typical, so I'm hitting every edge-case there is."
        },
        {
            "created_at": "2017-07-28T01:11:03.911Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16104263) by Wes McKinney (wesm):*\nOh, that's a bug that I introduced recently, and would have been caught if we were benchmarking more rigorously. We should be doubling the buffer size when we run out of capacity in BufferBuilder instead of only expanding to fit the next-appended bit. I think this flew under the radar when I changed `BinaryBuilder` from using a `UInt8Builder` under the hood (which does array doubling to grow the arrays). \n\nI opened https://issues.apache.org/jira/browse/ARROW-1290. I am betting that this will reduce the number of calls to `buffer->Resize` in BufferBuilder and maybe make this problem go away for you\n\nSeems we may want to do Arrow 0.6.0 very soon including this fix as soon we can get the Plasma code ready to ship. "
        },
        {
            "created_at": "2017-07-28T04:48:04.072Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16104450) by Wes McKinney (wesm):*\nAs another thing to try, what happens when you compile jemalloc with a prefix and statically link? So we would add something like `je_arrow_` to all of the jemalloc functions in https://github.com/apache/arrow/blob/master/cpp/src/arrow/memory_pool.cc. Because when libjemalloc is loaded it may interfere with the libc malloc and free functions, this might make them play more nicely together. \n\nSo you would set ARROW_JEMALLOC_USE_SHARED=off in https://github.com/apache/arrow/blob/master/cpp/CMakeLists.txt#L105 and then add the prefix in the thirdparty build like in Redis https://github.com/antirez/redis/blob/unstable/deps/Makefile#L80"
        },
        {
            "created_at": "2017-07-28T14:43:42.376Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16105051) by Jeff Knupp (jeffknupp):*\nYeah, that's what I'm working on now. Just about finished making the c++ changes. Will report results soon.\n\nAlso, my process ran successfully for 12 hours without jemalloc (used to hang after 10 minutes), so at least the other fixes I had been waiting on are working."
        },
        {
            "created_at": "2017-07-28T20:48:09.009Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16105639) by Jeff Knupp (jeffknupp):*\n`[~wesmckinn]`, I tried using the jemalloc with a prefix and statically linked, but it the result was the same. I made sure by running in gdb that the `je_arrow_` prefixed symbols were present and that it was statically linked, so I think this is a dead-end."
        },
        {
            "created_at": "2017-07-31T16:42:39.865Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16107562) by Jeff Knupp (jeffknupp):*\n`[~wesmckinn]`, despite it not fixing the issue, it seems that using a prefix it still clearly the correct way to use jemalloc along with another allocator. Do you want me to submit a PR for the changes I made?"
        },
        {
            "created_at": "2017-07-31T17:03:30.751Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16107596) by Wes McKinney (wesm):*\nYes, please. You can create a new JIRA for this change. Maybe `je_arrow_` to avoid conflicts. We may have to do some work in our build toolchain because we have various places where we use the system jemalloc. `[~xhochy]` is more of an expert in this so I defer to his guidance"
        },
        {
            "created_at": "2017-08-02T17:28:31.332Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111371) by Chris Bartak (chrisb1):*\nI've run into a problem that I assume has to be this issue, unfortunately I can't quite get it down to a reproducible example, but I'll share the context in case it's helpful.\n\nec2 2017.03 Amazon Linux AMI  (Red Hat 4.8.3-9)\npython3.4\npyarrow==0.5  (from pip + deps)\n\nReading a very small parquet file, 3kb - two text columns.  Interactively seems to always work.  Serving a webapp with `httpd`/`mod_wsgi` and Flask that reads the same file - almost always (but not always!) it completely hangs.  No spike in CPU/memory"
        },
        {
            "created_at": "2017-08-02T19:06:23.879Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111541) by Wes McKinney (wesm):*\nOK. I'm going to get to work putting up patched 0.5.0 builds on PyPI and conda-forge since these issues persisting is not acceptable. We should still figure out what is happening in jemalloc to cause this but it may take a little while"
        },
        {
            "created_at": "2017-08-02T19:29:10.472Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111614) by Wes McKinney (wesm):*\nMoving this issue to 0.7.0 as it doesn't seem likely the underlying cause will be resolved in time for 0.6.0. I created ARROW-1312 to switch off the allocator by default to triage the situation. "
        },
        {
            "created_at": "2017-08-02T22:01:02.219Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111793) by Wes McKinney (wesm):*\n`[~chrisb1]` I have just posted a patched build on PyPI (0.5.0.post1, because PyPI does not allow new builds with the same version number). If you install with\n\n`pip install pyarrow==0.5.*`\n\nthen this issue should go away. Please let me know if not"
        },
        {
            "created_at": "2017-08-02T22:03:30.162Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111798) by Wes McKinney (wesm):*\nI made a mistake in the build settings, will post a new set of binaries within a half hour or so. "
        },
        {
            "created_at": "2017-08-02T23:17:35.453Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111903) by Chris Bartak (chrisb1):*\nI'm still seeing my issue with `pyarrow==0.5.0.post2`, must be something else.  Certainly possible that it's unrelated to pyarrow, though I thought I had it pretty well isolated.  I'll open  a new issue if I can get it reproducible.  Thanks for the quick upload!"
        },
        {
            "created_at": "2017-08-02T23:51:23.507Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16111935) by Wes McKinney (wesm):*\nGreat, thank you. If you can provide a gdb backtrace from the hung process that would be helpful"
        },
        {
            "created_at": "2017-08-03T13:20:34.170Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16112693) by Chris Bartak (chrisb1):*\nHere's a backtrace - I guess it's actually hanging on re-acquiring the GIL - any theories?  \n\n```Java\nThread 2 (Thread 0x7fe5e97ab700 (LWP 12562)):\n#0  0x00007fe5f6dcda82 in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0\n#1  0x00007fe5eb0f1953 in ?? () from /usr/lib64/libpython3.4m.so.1.0\n#2  0x00007fe5eb0f1f7b in PyEval_RestoreThread () from /usr/lib64/libpython3.4m.so.1.0\n#3  0x00007fe5eb11729b in PyGILState_Ensure () from /usr/lib64/libpython3.4m.so.1.0\n#4  0x00007fe5d0fa143d in __pyx_f_7pyarrow_3lib_check_status(arrow::Status const&) () from /opt/python/run/venv/lib64/python3.4/site-packages/pyarrow/lib.cpython-34m.so\n#5  0x00007fe5d101c040 in __pyx_pw_7pyarrow_3lib_16MemoryMappedFile_5_open(_object*, _object*, _object*) ()\n   from /opt/python/run/venv/lib64/python3.4/site-packages/pyarrow/lib.cpython-34m.so\n#6  0x00007fe5d0fc4198 in __pyx_pw_7pyarrow_3lib_73memory_map(_object*, _object*, _object*) ()\n   from /opt/python/run/venv/lib64/python3.4/site-packages/pyarrow/lib.cpython-34m.so\n#7  0x00007fe5d0fb673d in __pyx_f_7pyarrow_3lib_get_reader(_object*, std::shared_ptr<arrow::io::RandomAccessFile>*) ()\n   from /opt/python/run/venv/lib64/python3.4/site-packages/pyarrow/lib.cpython-34m.so\n#8  0x00007fe5d02303ce in __pyx_pw_7pyarrow_8_parquet_13ParquetReader_3open(_object*, _object*, _object*) ()\n   from /opt/python/run/venv/lib64/python3.4/site-packages/pyarrow/_parquet.cpython-34m.so\n#9  0x00007fe5eb0fa765 in PyEval_EvalFrameEx () from /usr/lib64/libpython3.4m.so.1.0\n#10 0x00007fe5eb0fbdee in PyEval_EvalCodeEx () from /usr/lib64/libpython3.4m.so.1.0\n#11 0x00007fe5eb06e1b3 in ?? () from /usr/lib64/libpython3.4m.so.1.0\n```"
        },
        {
            "created_at": "2017-08-03T13:34:14.617Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16112706) by Wes McKinney (wesm):*\nAwesome, yeah! The GIL is not being released here https://github.com/apache/arrow/blob/master/python/pyarrow/io.pxi#L450 which is apparently causing a deadlock when run in a background thread. I wasn't aware this could happen\n\nOpened https://issues.apache.org/jira/browse/ARROW-1327 to track this fix (I would also like to see if coding errors like this can be made preventable)"
        },
        {
            "created_at": "2017-08-04T16:50:09.963Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16114616) by Uwe Korn (uwe):*\nNotes while digging on this issue:\n\n- I can reproduce this in the environment in https://github.com/xhochy/arrow-dockerfiles/tree/master/ARROW-1282 using the NYC Taxi Tripdata CSV from January 2016 using only VendorID and all columns that would be `object` on using plain `pd.read_csv`\n- Using the script `hanger.py`\u00a0in there, it always hangs on the third iteration.\n- Building `jemalloc==4.5.0`\u00a0using `--with-malloc-conf=dss:disabled` avoids the issue."
        },
        {
            "created_at": "2017-08-04T18:03:00.154Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16114734) by Jeff Knupp (jeff.knupp@enigma.com):*\nFantastic! I wasn't aware of the `dss:disabled` flag but it makes sense\nthat that would fix it as that's where the issue was. Is there anything\nstopping us from moving to 4.5 and statically linking with that compile\noption?\n\n\n"
        },
        {
            "created_at": "2017-08-04T18:10:11.036Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16114743) by Wes McKinney (wesm):*\nI think the idea is that this disables `sbrk`, but this function was only ever being called when a `mmap`-based memory allocation failed. I would prefer to put a stable 0.6.0 release out and then experiment with these changes for a couple of weeks before making it the default. We can go ahead and change the thirdparty toolchain in Arrow to use jemalloc 4.5.0 and build with this config flag, since the default is not to use it. `[~jeffknupp]` since you have a patch going already do you want to include this change in it? "
        },
        {
            "created_at": "2017-08-04T18:13:53.997Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16114752) by Jeff Knupp (jeffknupp):*\nSure, will do."
        },
        {
            "created_at": "2017-08-27T11:15:04.002Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16143080) by Uwe Korn (uwe):*\nCurrent state: It does not seem like large reallocations are a problem but rather certain sequences. I have now a reproducible environment with `jemalloc==4.5.0` where I can reproduce the hangs. The stacktrace is similar to that of `[~jeffknupp]` for the jemalloc part but different for the Arrow trace. So this points to a jemalloc issue and not in the Arrow codebase.\n\nStacktrace: https://gist.github.com/xhochy/b3d484807ab68814092d648adb6e1ff5"
        },
        {
            "created_at": "2017-08-27T17:47:19.358Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16143176) by Uwe Korn (uwe):*\nThere is also now a docker environment to reproduce the bugs consistently: https://github.com/xhochy/arrow-dockerfiles/tree/master/ARROW-1282 I will start playing there with the jemalloc source to better understand the problem."
        },
        {
            "created_at": "2017-08-27T19:46:43.805Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16143201) by Uwe Korn (uwe):*\nThe `mmap` calls are failing internally in jemalloc as `mmap` returns memory but not at the required memory address. These already happens several times before the hanger, so this seems unrelated."
        },
        {
            "created_at": "2017-08-28T13:55:17.786Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16143803) by Jeff Knupp (jeffknupp):*\nWouldn't this be caused by `dss:disabled` as it affects `mmap` allocations?"
        },
        {
            "created_at": "2017-08-28T13:58:16.025Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16143805) by Uwe Korn (uwe):*\n`[~jeffknupp]` no, I'm running with `dss:enabled` at the moment. I would like to have this fixed for the default jemalloc configuration.\n\nBy default, jemalloc first tries to allocate a new page using `mmap`, if this fails to produce the additional memory at the specified location, `jemalloc` will try to allocate the memory using `sbrk`."
        },
        {
            "created_at": "2017-08-29T09:13:07.145Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16144963) by Uwe Korn (uwe):*\nMade a PR that fixes this for the jemalloc 4.x series: https://github.com/jemalloc/jemalloc/pull/1005"
        },
        {
            "created_at": "2017-09-07T02:55:40.613Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16156365) by Wes McKinney (wesm):*\n`[~xhochy]` what do you want to do about this issue? We would have to do an ExternalProject build at a pinned commit and statically link. Perhaps we can delay this until 0.8.0"
        },
        {
            "created_at": "2017-09-07T09:20:53.559Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16156732) by Uwe Korn (uwe):*\nDelayed until 0.8.0, I hope to see the PR merged soon upstream and then we could do a bit more testing before we enable it again."
        },
        {
            "created_at": "2017-11-07T22:37:13.382Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1282?focusedCommentId=16243046) by Uwe Korn (uwe):*\nMerged upstream and the thirdparty builds all pull the new revision. Will deal with the vendoring in a separate story."
        }
    ]
}