{
    "issue": {
        "title": "[R] Proactively remove \"problems\" attributes",
        "body": "***Note**: This issue was originally created as [ARROW-10624](https://issues.apache.org/jira/browse/ARROW-10624). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI cannot able to downgrade arrow from version 2.0.0 to 1.0.1 since `arrow::install_arrow()` always installs the latest version.\r\n\r\nSteps to reproduce:\r\n\r\n\u00a0\r\n```java\n\r\ndevtools::install_version(\"arrow\", version = \"1.0.1\")\r\narrow::install_arrow()\n```\r\n\u00a0\r\n\r\nIf I skip the `arrow::install_arrow()`, I am not able to use the gzip compression (`WARNING: Arrow Gzip is not available, try using arrow::install_arrow()`)\r\n\r\n\u00a0",
        "created_at": "2020-11-16T20:56:27.000Z",
        "updated_at": "2021-01-04T23:50:40.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-01-04T23:50:28.000Z"
    },
    "comments": [
        {
            "created_at": "2020-11-16T21:16:35.184Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233085) by Neal Richardson (npr):*\nTry setting the environment variable `NOT_CRAN=true` prior to `install_version`. That should give you a build with all of the compression libraries, so there's no need to `arrow::install_arrow()`."
        },
        {
            "created_at": "2020-11-16T21:38:19.132Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233101) by Neal Richardson (npr):*\nSee http://arrow.apache.org/docs/r/articles/install.html for details"
        },
        {
            "created_at": "2020-11-16T21:40:46.047Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233104) by Neal Richardson (npr):*\nFTR that warning message is not coming from the `arrow` package: https://github.com/schnorr/starvz/blob/87e1fbc8b152e720096fc3d9b9387528c9b617f2/R/write_functions.R#L24"
        },
        {
            "created_at": "2020-11-16T21:41:46.760Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233106) by Neal Richardson (npr):*\nOut of curiosity, why do you want to downgrade?"
        },
        {
            "created_at": "2020-11-16T21:47:01.773Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233111) by Vinicius Pinto (gpv):*\nThanks `[~npr]` \r\nI just add `Sys.setenv(NOT_CRAN = \"true\")` before the `install_version` and that works."
        },
        {
            "created_at": "2020-11-16T21:56:57.073Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233115) by Vinicius Pinto (gpv):*\nI am debugging an error (`IOError: Couldn't deserialize thrift: TProtocolException: Exceeded size limit)` that appears when I use the latest versions of arrow (2.0) and readr(1.4). A tiny starvz data frame with only 4 rows becomes a parquet file with several MB (it seems a memory leak since the final size varies with the size of other starvz data frames) and this does not happen when using older versions."
        },
        {
            "created_at": "2020-11-16T22:00:46.262Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233120) by Neal Richardson (npr):*\nHmm. Can you share a tiny data frame that triggers this? Sounds like something that should be fixed."
        },
        {
            "created_at": "2020-11-17T15:08:40.255Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17233651) by Vinicius Pinto (gpv):*\nI prepared a \"small case\" (8M) that triggers this error. With smaller data frames, it works, but the parquet files are larger than expected. \r\nDownload link with the data and the commands to reproduce the issue: https://filesender.rnp.br/?s=download&token=cee89b89-3c75-49e6-b69f-db47cc2ab737\r\nWith this dataset, the total size of the parquet files increases from 8M with arrow 1.0.1 to 620M with arrow 2.0.0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-01-04T23:50:28.531Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10624?focusedCommentId=17258588) by Neal Richardson (npr):*\nIssue resolved by pull request 9092\n<https://github.com/apache/arrow/pull/9092>"
        }
    ]
}