{
    "issue": {
        "title": "[Python] Unable to connect to HDFS from a worker/data node on a Kerberized cluster using pyarrow' hdfs API",
        "body": "***Note**: This issue was originally created as [ARROW-5922](https://issues.apache.org/jira/browse/ARROW-5922). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHere's what I'm trying:\r\n\r\n`````\r\n\r\n`import pyarrow as pa `\r\n\r\n`conf = \\{\"hadoop.security.authentication\": \"kerberos\"} `\r\n\r\n`fs = pa.hdfs.connect(kerb_ticket=\"/tmp/krb5cc_44444\", extra_conf=conf)`\r\n\r\n`````\r\n\r\nHowever, when I submit this job to the cluster using\u00a0`Dask-YARN`, I get the following error:\r\n\r\n```\r\n\r\n`File \"test/run.py\", line 3 fs = pa.hdfs.connect(kerb_ticket=\"/tmp/krb5cc_44444\", extra_conf=conf) File \"/opt/hadoop/data/10/hadoop/yarn/local/usercache/hdfsf6/appcache/application_1560931326013_183242/container_e47_1560931326013_183242_01_000003/environment/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 211, in connect File \"/opt/hadoop/data/10/hadoop/yarn/local/usercache/hdfsf6/appcache/application_1560931326013_183242/container_e47_1560931326013_183242_01_000003/environment/lib/python3.7/site-packages/pyarrow/hdfs.py\", line 38, in __init__ File \"pyarrow/io-hdfs.pxi\", line 105, in pyarrow.lib.HadoopFileSystem._connect File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status pyarrow.lib.ArrowIOError: HDFS connection failed`\r\n\r\n`````\r\n\r\nI also tried setting\u00a0`host (to a name node)`\u00a0and\u00a0`port (=8020)`, however I run into the same error. Since the error is not descriptive, I'm not sure which setting needs to be altered. Any clues anyone?",
        "created_at": "2019-07-12T12:26:50.000Z",
        "updated_at": "2019-09-01T21:19:23.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-08-06T17:56:52.000Z"
    },
    "comments": [
        {
            "created_at": "2019-08-30T16:52:43.694Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5922?focusedCommentId=16919722) by Ben Schreck (bschreck):*\nI am getting the same error as you. I noticed that the error for me is in java- under the hood pyarrow tries to load the HDFS java class and can't find it. I can't figure out how to fix it though..."
        },
        {
            "created_at": "2019-08-30T22:33:03.837Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5922?focusedCommentId=16919936) by Saurabh Bajaj (sbajaj):*\nTry setting the environment variable ARROW_LIBHDFS_DIR to the explicit location of libhdfs.so at the worker nodes. That's what worked for me.\u00a0"
        },
        {
            "created_at": "2019-09-01T21:19:23.114Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5922?focusedCommentId=16920516) by Ben Schreck (bschreck):*\nI fixed it by making HADOOP_HOME=/usr in the worker environment. Pyarrow for some reason sets the hadoop executable to $HADOOP_HOME/bin/hadoop on python/pyarrow/hdfs.py:L137. $HADOOP_HOME on my system was already /usr/bin/hadoop, so this resulted in /usr/bin/hadoop/bin/hadoop, which resulted in a wrong $CLASSPATH."
        }
    ]
}