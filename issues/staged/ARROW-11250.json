{
    "issue": {
        "title": "[Python] Inconsistent behavior calling ds.dataset()",
        "body": "***Note**: This issue was originally created as [ARROW-11250](https://issues.apache.org/jira/browse/ARROW-11250). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn a Jupyter notebook, I have noticed that sometimes I am not able to read a dataset which certainly exists on Azure Blob.\r\n\r\n\u00a0\r\n```java\n\r\nfs =\u00a0fsspec.filesystem(protocol=\"abfs\",\u00a0account_name,\u00a0account_key)\r\n```\r\n\u00a0\r\nOne example of this is reading a dataset in one cell:\r\n\r\n\u00a0\r\n```java\n\r\nds.dataset(\"dev/test-split\", partitioning=\"hive\", filesystem=fs)\n```\r\n\u00a0\r\n\r\nThen in another cell I try to read the same dataset:\r\n\r\n\u00a0\r\n```java\n\r\nds.dataset(\"dev/test-split\", partitioning=\"hive\", filesystem=fs)\r\n\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-514-bf63585a0c1b> in <module>\r\n----> 1 ds.dataset(\"dev/test-split\", partitioning=\"hive\", filesystem=fs)\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    669     # TODO(kszucs): support InMemoryDataset for a table input\r\n    670     if _is_path_like(source):\r\n--> 671         return _filesystem_dataset(source, **kwargs)\r\n    672     elif isinstance(source, (tuple, list)):\r\n    673         if all(_is_path_like(elem) for elem in source):\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    426         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\r\n    427     else:\r\n--> 428         fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n    429 \r\n    430     options = FileSystemFactoryOptions(\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in _ensure_single_source(path, filesystem)\r\n    402         paths_or_selector = [path]\r\n    403     else:\r\n--> 404         raise FileNotFoundError(path)\r\n    405 \r\n    406     return filesystem, paths_or_selector\r\n\r\nFileNotFoundError: dev/test-split\r\n```\r\n\u00a0\r\n\r\nIf I reset the kernel, it works again. It also works if I change the path slightly, like adding a \"/\" at the end (so basically it just not work if I read the same dataset twice):\r\n\r\n\u00a0\r\n```java\n\r\nds.dataset(\"dev/test-split/\", partitioning=\"hive\", filesystem=fs)\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nThe other strange behavior I have noticed that that if I read a dataset inside of my Jupyter notebook,\r\n\r\n\u00a0\r\n```java\n\r\n%%time\r\ndataset = ds.dataset(\"dev/test-split\", partitioning=ds.partitioning(pa.schema([(\"date\", pa.date32())]), flavor=\"hive\"), \r\nfilesystem=fs,\r\nexclude_invalid_files=False)\r\n\r\nCPU times: user 1.98 s, sys: 0 ns, total: 1.98 s Wall time: 2.58 s\n```\r\n\u00a0\r\n\r\nNow, on the exact same server when I try to run the same code against the same dataset in Airflow it takes over 3 minutes (comparing the timestamps in my logs between right before I read the dataset, and immediately after the dataset is available to filter):\r\n```java\n\r\n[2021-01-14 03:52:04,011] INFO - Reading dev/test-split\r\n[2021-01-14 03:55:17,360] INFO - Processing dataset in batches\r\n```\r\nThis is probably not a pyarrow issue, but what are some potential causes that I can look into? I have one example where it is 9 seconds to read the dataset in Jupyter, but then 11 **minutes** in Airflow. I don't know what to really investigate - as I mentioned, the Jupyter notebook and Airflow are on the same server and both are deployed using Docker. Airflow is using the CeleryExecutor.\r\n\r\n\u00a0",
        "created_at": "2021-01-14T10:42:39.000Z",
        "updated_at": "2021-04-16T10:16:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-04-16T10:16:00.000Z"
    },
    "comments": [
        {
            "created_at": "2021-01-14T17:32:05.291Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265056) by Antoine Pitrou (apitrou):*\ncc `[~jorisvandenbossche]`"
        },
        {
            "created_at": "2021-01-15T08:49:35.585Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265827) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` this could be an issue with the {adlfs}} package (eg with s3fs we also just had an issue where repeated calls to find() did not give the same result).\r\n\r\nCould you try something like this:\r\n\r\n```python\n\r\nfs = fsspec.filesystem(protocol=\"abfs\", account_name, account_key)\r\n\r\n# call find twice\r\nselected_files1 = fs.find(\"dev/test-split\", maxdepth=None, withdirs=True, detail=True)\r\nselected_files2 = fs.find(\"dev/test-split\", maxdepth=None, withdirs=True, detail=True)\r\n```\r\n\r\nand check if it returns twice the same list?\r\n\r\nOr, based on the error you get, something like:\r\n\r\n```python\n\r\nfs = fsspec.filesystem(protocol=\"abfs\", account_name, account_key)\r\n\r\n# a find from the first dataset read\r\nselected_files = fs.find(\"dev/test-split\", maxdepth=None, withdirs=True, detail=True)\r\n\r\n# checking the base path when starting to read the dataset a second time \r\nfs.info(\"dev/test-split\")\r\n```\r\n\r\nand check what this returns?"
        },
        {
            "created_at": "2021-01-15T10:04:15.098Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265869) by Lance Dacey (ldacey):*\n```java\n\r\nselected_files1 = fs.find(\"dev/test-split\", maxdepth=None, withdirs=True, detail=True)\r\nselected_files2 = fs.find(\"dev/test-split\", maxdepth=None, withdirs=True, detail=True)\r\nselected_files1 == selected_files2\r\n\r\nTrue\n```\r\nI am able to run the above cell over and over again.\r\n\r\n\u00a0\r\n\r\nNow when I use fs.info() without a final slash:\r\n```java\n\r\nfs.info(\"dev/test-split\")\r\n{'name': 'dev/test-split/', 'size': 0, 'type': 'directory'}\n```\r\nIf I add a slash to the folder name, the slash is removed in the fs.info() return - will this impact anything?\u00a0\r\n```java\n\r\nfs.info(\"dev/test-split/\")\r\n{'name': 'dev/test-split', 'size': 0, 'type': 'directory'}\r\n```\r\n\u00a0\r\n```java\n\r\nselected_files3 = fs.info(\"dev/test-split\")\r\nselected_files4 = fs.info(\"dev/test-split/\")\r\nselected_files3 == selected_files4\r\n\r\nFalse\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nEdit - running fs.info() on the same path fails if I do it more than once without changing the name by adding a slash, or resetting my kernel. Even if I delete the fs variable and create a new filesystem, it does not work."
        },
        {
            "created_at": "2021-01-15T10:27:11.329Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265891) by Joris Van den Bossche (jorisvandenbossche):*\nSo for `fs.info()`, when you pass a path without a final slash, it returns the path with slash, and if pass it it with slash, it returns without slash? That sounds weird, but, I can't directly think of a way that it could influence the dataset reading (we mostly use the user specified path, not the \"name\" key in the dict). \r\n\r\nTo test it further directly with our fsspec filesystem wrapper, can you try some variations/combinations:\r\n\r\n```python\n\r\nfrom pyarrow.fs import PyFileSystem, FSSpecHandler, FileSelector\r\n\r\nfs = fsspec.filesystem(protocol=\"abfs\", account_name, account_key)\r\nfs_pa = PyFileSystem(FSSpecHandler(fs))\r\n\r\nfs_pa.get_file_info(\"dev/test-split\")\r\nfs_pa.get_file_info(FileSelector(FileSelector(\"dev/test-split\", recursive=True))\r\nfs_pa.get_file_info(\"dev/test-split\")\r\n```\r\n\r\n"
        },
        {
            "created_at": "2021-01-15T10:30:08.795Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265894) by Joris Van den Bossche (jorisvandenbossche):*\n> Edit - running fs.info() on the same path fails if I do it more than once without changing the name by adding a slash, or resetting my kernel. Even if I delete the fs variable and create a new filesystem, it does not work.\r\n\r\nAh, yes, so that sounds exactly what is happening (and what last my code example above basically does, the get_file_info calls fs.info(), and so my example mimicking dataset reading twice, does that twice). \r\nNow, that sounds as a bug in adlfs then. Can you open an issue report there?"
        },
        {
            "created_at": "2021-01-15T10:48:55.501Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265909) by Lance Dacey (ldacey):*\nSure, I can raise an issue there.\r\n\r\n\u00a0\r\n```java\n\r\nfs_pa.get_file_info(\"dev/test-split\")\r\n<FileInfo for 'dev/test-split': type=FileType.NotFound>\r\n```\r\n\u00a0\r\n\r\nI had to tweak the code you provided a bit to get it to run for the FileSelector:\r\n```java\n\r\nfs_pa.get_file_info(FileSelector(\"dev/test-split\", recursive=True))\r\n\r\n[<FileInfo for 'dev/test-split/row_date=2020-12-31': type=FileType.Directory>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/428445ed3a854cbfb3025389477811a3-0.parquet': type=FileType.File, size=2261024>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/6f9f4f5c5d0e494fbf7420540765afbc-0.parquet': type=FileType.File, size=713840>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/7a29a1eb3f464c9c955e23e05c2e2c28-0.parquet': type=FileType.File, size=627492>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/7bfb6fdd2b404ad88022357c486f17de-0.parquet': type=FileType.File, size=290697>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/c448b5f025f241d9b6f77ed5eead239c-0.parquet': type=FileType.File, size=463202>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/cde0bb54687642e3aace011d7a106947-0.parquet': type=FileType.File, size=713840>,\r\n <FileInfo for 'dev/test-split/row_date=2020-12-31/d5a0ebc05c974158a818f9b216e4093a-0.parquet': type=FileType.File, size=928676>,\r\n...\r\n]\n```\r\n\u00a0\r\n\r\nFYI - if I add an ending slash to the path I get type=Directory instead of NotFound:\r\n```java\n\r\nfs_pa.get_file_info(\"dev/test-split/\")\r\n<FileInfo for 'dev/test-split/': type=FileType.Directory>\r\n```"
        },
        {
            "created_at": "2021-01-15T11:02:45.892Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265922) by Lance Dacey (ldacey):*\nDo you have any idea at all what could also be causing my Airflow scheduler to take SO long to read the same dataset that I am able to read in under 10 seconds on Jupyter? Could it be an overlay network or something? I have ensured that my tasks calling ds.dataset() are running on the same node that my Jupyterhub is running on. All software between the environments seems to be identical as well (same requirements.txt).\r\n\r\n\u00a0\r\n\r\n11 minutes on the latest airflow run and 9 seconds if I run it in a notebook.. is there a way to narrow down my troubleshooting scope for this?\r\n```java\n\r\ndataset = ds.dataset(\r\n source=input_path,\r\n format=\"parquet\",\r\n partitioning=partitioning,\r\n filesystem=fs,\r\n )\n```\r\n\u00a0"
        },
        {
            "created_at": "2021-01-15T11:16:09.390Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17265932) by Joris Van den Bossche (jorisvandenbossche):*\nSorry, no idea, I have no experience with airflow, so can't give insight on that.  \r\nIf you run those `info`/`find(.., maxdepth=None, detail=True` calls from the above examples in airflow instead of the jupyter notebook, does that already take a longer time as well?"
        },
        {
            "created_at": "2021-01-15T16:01:16.859Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17266151) by Lance Dacey (ldacey):*\nGood idea - I was a able to list all of the files and print the info quickly, one interesting thing is that the ds.dataset() failed right after though and the error message is a little different.\u00a0\r\n\r\n\u00a0\r\n\r\nMy input path was \"dev/case-history/\" with the final slash. This shows that it took 8 seconds to get the len(fs.find()) which is about the same amount of time it takes to read ds.dataset() in Jupyter. This error message is different than usual though and it mentions something about a dircache:\r\n\r\n\u00a0\r\n```java\n\r\n[2021-01-15 15:51:47,158] INFO - Reading /dev/case-history/\r\n[2021-01-15 15:51:55,607] INFO - 9682\r\n[2021-01-15 15:51:55,892] INFO - {'name': '/dev/case-history', 'size': 0, 'type': 'directory'}\r\n[2021-01-15 15:51:55,893] {taskinstance.py:1150} ERROR - '/dev/case-history/'\r\nTraceback (most recent call last):\r\n...\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 671, in dataset\r\n    return _filesystem_dataset(source, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 428, in _filesystem_dataset\r\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 395, in _ensure_single_source\r\n    file_info = filesystem.get_file_info([path])[0]\r\n  File \"pyarrow/_fs.pyx\", line 434, in pyarrow._fs.FileSystem.get_file_info\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/_fs.pyx\", line 1012, in pyarrow._fs._cb_get_file_info_vector\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/fs.py\", line 195, in get_file_info\r\n    info = self.fs.info(path)\r\n  File \"/opt/conda/lib/python3.7/site-packages/adlfs/spec.py\", line 522, in info\r\n    fetch_from_azure = (path and self._ls_from_cache(path) is None) or refresh\r\n  File \"/opt/conda/lib/python3.7/site-packages/fsspec/spec.py\", line 336, in _ls_from_cache\r\n    return self.dircache[path]\r\n  File \"/opt/conda/lib/python3.7/site-packages/fsspec/dircache.py\", line 62, in __getitem__\r\n    return self._cache[item]  # maybe raises KeyError\r\nKeyError: '/dev/case-history/'\r\n```\r\n\u00a0\r\n\r\nI edited my DAG and changed the input path to be \"dev/case-history\" with no final slash and the error was different (note that fs.info() always either removes or adds the final slash to the name of the path):\r\n```java\n\r\n[2021-01-15 15:36:25,603] INFO - {'name': '/dev/case-history/', 'size': 0, 'type': 'directory'}\r\n[2021-01-15 15:36:25,604] ERROR - /dev/case-history\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 671, in dataset\r\n    return _filesystem_dataset(source, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 428, in _filesystem_dataset\r\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n  File \"/opt/conda/lib/python3.7/site-packages/pyarrow/dataset.py\", line 404, in _ensure_single_source\r\n    raise FileNotFoundError(path)\r\nFileNotFoundError: /dev/case-history\r\n```\r\n\u00a0\r\n\r\nWithout any fs.info() or fs.find() it took 11 minutes to read the same dataset... from 17:45 to 17:56\r\n```java\n\r\n[2021-01-14 17:45:10,470] INFO - Reading /dev/case-history/\r\n[2021-01-14 17:56:58,307] INFO - Processing dataset in batches\r\n```\r\n\u00a0"
        },
        {
            "created_at": "2021-04-16T10:16:00.749Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11250?focusedCommentId=17322758) by Lance Dacey (ldacey):*\nThis was fixed with a new version of the adlfs library"
        }
    ]
}