{
    "issue": {
        "title": "[Python] Allow serializing arbitrary Python objects to parquet",
        "body": "***Note**: This issue was originally created as [ARROW-15826](https://issues.apache.org/jira/browse/ARROW-15826). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm trying to serialize a pandas DataFrame containing custom objects to parquet. Here is some example code:\r\n```java\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nclass Foo: \r\n    pass\r\n\r\ndf = pd.DataFrame({\"a\": [Foo(), Foo(), Foo()], \"b\": [1, 2, 3]})\r\ntable = pyarrow.Table.from_pandas(df)\r\n```\r\nGives me:\r\n```java\n\r\nTraceback (most recent call last):\r\n\u00a0 File \"<stdin>\", line 1, in <module>\r\n\u00a0 File \"pyarrow/table.pxi\", line 1782, in pyarrow.lib.Table.from_pandas\r\n\u00a0 File \"/home/migwell/miniconda3/lib/python3.9/site-packages/pyarrow/pandas_compat.py\", line 594, in dataframe_to_arrays\r\n\u00a0 \u00a0 arrays = [convert_column(c, f)\r\n\u00a0 File \"/home/migwell/miniconda3/lib/python3.9/site-packages/pyarrow/pandas_compat.py\", line 594, in <listcomp>\r\n\u00a0 \u00a0 arrays = [convert_column(c, f)\r\n\u00a0 File \"/home/migwell/miniconda3/lib/python3.9/site-packages/pyarrow/pandas_compat.py\", line 581, in convert_column\r\n\u00a0 \u00a0 raise e\r\n\u00a0 File \"/home/migwell/miniconda3/lib/python3.9/site-packages/pyarrow/pandas_compat.py\", line 575, in convert_column\r\n\u00a0 \u00a0 result = pa.array(col, type=type_, from_pandas=True, safe=safe)\r\n\u00a0 File \"pyarrow/array.pxi\", line 312, in pyarrow.lib.array\r\n\u00a0 File \"pyarrow/array.pxi\", line 83, in pyarrow.lib._ndarray_to_array\r\n\u00a0 File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: ('Could not convert <__main__.Foo object at 0x7fc23e38bfd0> with type Foo: did not recognize Python value type when inferring an Arrow data type', 'Conversion failed for column a with type object')\n```\r\nNow, I realise that there's this disclaimer about arbitrary object serialization: <https://arrow.apache.org/docs/python/ipc.html#arbitrary-object-serialization.> However it isn't clear how this applies to parquet. In my case, I want to have a well-formed parquet file that has binary blobs in one column that _can_ be deserialized to my class, but can otherwise be read by general parquet tools without failing. Using pickle doesn't solve this use case since other languages like R may not be able to read the pickle file.\r\n\r\nAlternatively, if there is a well-defined protocol for telling pyarrow how to translate a given type to and from arrow types, I would be happy to use that instead.\r\n\r\n\u00a0",
        "created_at": "2022-03-02T11:09:50.000Z",
        "updated_at": "2022-04-05T14:36:52.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-03-02T17:27:17.147Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500298) by Will Jones (willjones127):*\nHave you considered using a binary column? That should save to parquet just fine.\r\n\r\n```python\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pickle\r\n\r\nclass Foo: \r\n    pass\r\n\r\ndf = pd.DataFrame({\"a\": [pickle.dumps(Foo()) for _ in range(3)], \"b\": [1, 2, 3]})\r\ntable = pa.Table.from_pandas(df)\r\ntable\r\n# pyarrow.Table\r\n# a: binary\r\n# b: int64\r\n# ----\r\n# a: [[80049517000000000000008C085F5F6D61696E5F5F948C03466F6F9493942981942E,80049517000000000000008C085F5F6D61696E5F5F948C03466F6F9493942981942E,80049517000000000000008C085F5F6D61696E5F5F948C03466F6F9493942981942E]]\r\n# b: [[1,2,3]]\r\n```\r\n\r\n"
        },
        {
            "created_at": "2022-03-02T23:29:48.865Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500422) by Michael Milton (multimeric):*\nThis approach would be fine, I just wonder if I can register some kind of automatic pickle serializer that will serialize/deserialize this for me."
        },
        {
            "created_at": "2022-03-02T23:46:53.616Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500424) by Will Jones (willjones127):*\nYeah there's no hook like that in Table.to_pandas() or Table.from_pandas() to do that kind of custom conversion. Should be pretty straightforward to write a wrapping function that does that though."
        },
        {
            "created_at": "2022-03-03T00:24:49.188Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500437) by Michael Milton (multimeric):*\nSo the `pa.SerializationContext()` won't work with Table.from_pandas? If not, can I make this into a feature request for that feature?"
        },
        {
            "created_at": "2022-03-03T01:29:32.520Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500445) by Will Jones (willjones127):*\n> So the `pa.SerializationContext()` won't work with Table.from_pandas?\r\nThat feature is deprecated since pickle now works just as well.\r\n> If not, can I make this into a feature request for that feature?\r\nCould you describe the feature you have in mind? A mock code example showing how it would work would be helpful. Are you suggesting that Table.from_pandas should automatically pickle columns of dtype object?"
        },
        {
            "created_at": "2022-03-03T01:40:28.353Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500452) by Will Jones (willjones127):*\nAs an aside, this discussion on GitHub might provide some helpful context: https://github.com/apache/arrow/issues/11239"
        },
        {
            "created_at": "2022-03-03T02:27:15.732Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500464) by Michael Milton (multimeric):*\nMy motivation goes something like this: I have a DataFrame that contains a mix of primitive values and complex types. I want to serialize this in a way that can be read by any other language, and therefore pickle does\u00a0_not_\u00a0suffice. Since it's a DataFrame, and most of the data is primitive, it seems that the Arrow formats, including parquet would be a good fit.\r\n\r\nSo now the dilemma is: how can we serialize the complex types that are not native to Arrow? As you suggested, we could by default pickle all object Series, but in my mind this still ties us to Python too much. If I open it in R it's going to be impossible to access this data. So then my instinct is that we should have a customizable serializer for custom types that lets us choose the Arrow representation of a type. If I have a simple data class, I would want this to be represented as the best native type. Does parquet support nested Maps? If so, I would want to be able to tell the serializer that it should first convert my dataclass to dict, and then convert that to a parquet Map. If nested maps don't work then JSON or BSON is fine. However if I now have another class that only makes sense as a binary blob, like an image or something, then it would make more sense to serialize it as such.\r\n\r\nSince you want some mock code, it would look a bit like this:\r\n```java\n\r\nimport dataclasses\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n@dataclasses.dataclass\r\nclass MyClass:\r\n    a: str\r\n    b: int\r\n    c: bool\r\n\r\npa.register_serializer(MyClass, lambda instance: instance._todict())\r\n\r\ndf = pd.DataFrame({\r\n    \"a\": [MyClass(\"a\", 1, True), MyClass(\"b\", 2, True), MyClass(\"c\", 3, False)], \r\n    \"b\": [1, 2, 3]\r\n})\r\ntable = pa.Table.from_pandas(df)\n```"
        },
        {
            "created_at": "2022-03-03T03:03:46.358Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500469) by Will Jones (willjones127):*\n> I want to serialize this in a way that can be read by any other language, and therefore pickle does\u00a0_not_\u00a0suffice.\r\nSo maybe I misunderstood earlier what you meant by serialization. My initial impression was that you cared about saving Python object instances, and didn't care about portability. For example, some users have wanted to save a `requests.Response` object in a column as part of web-scraped data; that's a case where you have to choose between pickling and keeping all data, or convert to a format that other languages could read. But it sounds like you are designing the data structure?\r\n\r\nArrow and Parquet support nested types, including struct, list, and map columns. So if what you really care about is saving nested data, that's already possible. With the dataclass example you gave, all you have to do is convert the classes to dict; PyArrow can automatically convert those into nested Arrow types. And any parquet reader will be able to understand it.\r\n```python\n\r\nfrom dataclasses import dataclass, asdict\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n@dataclass\r\nclass MyClass:\r\n    a: str\r\n    b: int\r\n    c: bool\r\n\r\ndf = pd.DataFrame({\r\n    \"a\": [MyClass(\"a\", 1, True), MyClass(\"b\", 2, True), MyClass(\"c\", 3, False)], \r\n    \"b\": [1, 2, 3]\r\n})\r\n\r\n# PyArrow already knows how to convert lists and dicts into nested types\r\ndf['a'] = [asdict(x) for x in df['a']]\r\n\r\npa.Table.from_pandas(df)\r\n# pyarrow.Table\r\n# a: struct<a: string, b: int64, c: bool>\r\n#   child 0, a: string\r\n#   child 1, b: int64\r\n#   child 2, c: bool\r\n# b: int64\r\n# ----\r\n# a: [  -- is_valid: all not null  -- child 0 type: string\r\n#     [\r\n#       \"a\",\r\n#       \"b\",\r\n#       \"c\"\r\n#     ]  -- child 1 type: int64\r\n#     [\r\n#       1,\r\n#       2,\r\n#       3\r\n#     ]  -- child 2 type: bool\r\n#     [\r\n#       true,\r\n#       true,\r\n#       false\r\n#     ]]\r\n# b: [[1,2,3]]\r\n \n```"
        },
        {
            "created_at": "2022-03-03T04:43:13.748Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15826?focusedCommentId=17500492) by Michael Milton (multimeric):*\nWell I want my approach to be totally compatible with classes I don't control, which is why I want to be able to register serializers rather than doing it all manually. For example, I could check if any given object has a `__dict__` and if so, convert to dict, otherwise pickle it. The `requests.Response` is something that I would like to be able to support, for example."
        }
    ]
}