{
    "issue": {
        "title": "pd.read_parquet using filters consumes too much memory",
        "body": "***Note**: This issue was originally created as [ARROW-16391](https://issues.apache.org/jira/browse/ARROW-16391). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHello!\r\n\r\n\u00a0\r\n\r\nI have found that pyarrow versions **>= 4.0.1** use more than **2x** memory (RSS) when trying to read a parquet using file-level filters. Using the following dataset:\r\n```java\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\na = np.random.randint(1,50,(4_000_000,4))\r\n\r\ndf = pd.DataFrame(a, columns=['A','B','C','D']).to_parquet(\"test.pq\", index=False) \n```\r\nand the reader script ({**}read_with_filters.py{**})\r\n```java\n\r\nimport pyarrow as pa\r\nimport pandas as pd\r\n\r\n\r\nprint(f\"pyarrow version: {pa.__version__}\")\r\nprint(f\"pandas version: {pd.__version__}\")\r\n\r\n\r\ntmp = pd.read_parquet(\"test.pq\", engine='pyarrow', use_legacy_dataset=False, filters=[(\"B\",\"=\",10)])\r\n\r\n\r\nprint(tmp.shape) \n```\r\nI get:\r\n\r\n\u00a0\r\n\r\n**Python 3.8.13 (conda), pyarrow 1.0.1 (pip), pandas 1.4.2 (pip)**\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 1.0.1\r\npandas version: 1.4.2\r\n(81833, 4)\r\nRSS (Kb): 84876 | user (sec): 0.87 | system (sec): 0.32 | real (sec) : 1.32\n```\r\n**Python 3.8.13 (conda), pyarrow 4.0.1 (pip), pandas 1.4.2 (pip)**\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 4.0.1\r\npandas version: 1.4.2\r\n(81833, 4)\r\nRSS (Kb): 172816 | user (sec): 0.77 | system (sec): 0.24 | real (sec) : 0.72 \n```\r\n**Python 3.8.13 (conda), pyarrow 7.0.0 (pip), pandas 1.4.2 (pip)**\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 7.0.0\r\npandas version: 1.4.2\r\n(81833, 4)\r\nRSS (Kb): 240112 | user (sec): 0.71 | system (sec): 0.22 | real (sec) : 0.82 \n```\r\n\u00a0\r\n\r\nIt is more evident when using a larger dataset. However, my personal computer hangs when trying to read larger datasets using **4.0.1** and {**}7.0.0{**}. (That should be a separate issue)\r\n\r\n\u00a0\r\n\r\nIt is worth mentioning that you see a relative the same memory usage when I removed the **filters** keyword.\r\n\r\n**Python 3.8.13 (conda), pyarrow 1.0.1 (pip), pandas 1.4.2 (pip)**\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 1.0.1\r\npandas version: 1.4.2\r\n(4000000, 4)\r\nRSS (Kb): 331424 | user (sec): 0.89 | system (sec): 0.39 | real (sec) : 1.07 \n```\r\n{**}Python 3.8.13 (conda), pyarrow 4.0.1 (pip), pandas 1.4.2 (pip){**}{**}{{**}}\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 4.0.1\r\npandas version: 1.4.2\r\n(4000000, 4)\r\nRSS (Kb): 405916 | user (sec): 0.81 | system (sec): 0.42 | real (sec) : 0.81 \n```\r\n{**}Python 3.8.13 (conda), pyarrow 7.0.0 (pip), pandas 1.4.2 (pip){**}{**}{{**}}\r\n```java\n\r\ngtime -f \"RSS (Kb): %M | user (sec): %U | system (sec): %S | real (sec) : %e\" python read_with_filters.py\r\npyarrow version: 7.0.0\r\npandas version: 1.4.2\r\n(4000000, 4)\r\nRSS (Kb): 364152 | user (sec): 0.78 | system (sec): 0.45 | real (sec) : 1.27 \n```\r\nThank you,\r\n\r\nLuis",
        "created_at": "2022-04-28T03:30:44.000Z",
        "updated_at": "2022-05-03T11:53:22.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-04-28T03:43:53.507Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16391?focusedCommentId=17529175) by Weston Pace (westonpace):*\nWe've worked on this as part of 8.0.0 (see ARROW-15410).  Are you willing to try the 8.0.0 release candidate to see if you get any different behavior?  It looks like you are installing pyarrow via pip so I think the latest RC is available here: https://apache.jfrog.io/ui/native/arrow/python-rc/8.0.0-rc1\r\n\r\n"
        },
        {
            "created_at": "2022-04-28T03:55:32.102Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16391?focusedCommentId=17529178) by Weston Pace (westonpace):*\nAh, looking at your use case a bit more carefully I don't know that our improvements will help much for smaller datasets.  I'll try and run some experiments on this tomorrow.  For larger datasets however, I would expect to see some improvements (e.g. it should not run out of memory)."
        },
        {
            "created_at": "2022-04-28T12:27:54.594Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16391?focusedCommentId=17529407) by Luis E Pastrana (pastranaluis):*\nWeston this was only a toy example that I could run in my personal PC. I can test out this version in both my personal PC and my work server and let you know. Thanks! Also, is there a target date to release v8.0 ?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-04-28T18:17:20.978Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16391?focusedCommentId=17529576) by Weston Pace (westonpace):*\nRelease candidates for 8.0.0 are currently being voted on.  Exact timing will depend on how many RCs we need to build but it should be within the next month or so."
        }
    ]
}