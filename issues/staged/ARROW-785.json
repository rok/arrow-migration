{
    "issue": {
        "title": "possible issue on writing parquet via pyarrow, subsequently read in Hive",
        "body": "***Note**: This issue was originally created as [ARROW-785](https://issues.apache.org/jira/browse/ARROW-785). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\ndetails here: http://stackoverflow.com/questions/43268872/parquet-creation-conversion-from-pandas-dataframe-to-pyarrow-table-not-working-f\n\nThis round trips in pandas->parquet->pandas just fine on released pandas (0.19.2) and pyarrow (0.2).\n\nOP stats that it is not readable in Hive however.",
        "created_at": "2017-04-07T12:28:13.000Z",
        "updated_at": "2022-08-27T14:42:05.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-07-28T01:18:19.000Z"
    },
    "comments": [
        {
            "created_at": "2017-04-07T15:44:34.775Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15960984) by Wes McKinney (wesm):*\nI will take a look at the file in Impala (or Hive if I can figure out how to do that) to see if I can repro"
        },
        {
            "created_at": "2017-04-07T17:14:09.890Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15961132) by Ashima Sood (ash_26):*\nI'm using Zeppelin to create a table on top of the parquet file using below command:\n%sql\nCREATE EXTERNAL TABLE IF NOT EXISTS schema_abc.parquet_table_name(\n  YEAR INT\n, WORD STRING\n)\nSTORED AS PARQUET\nLOCATION 's3://bucket_name/folder/parquet_files/'\n\n\\*\\*\\*Please note: parquet_files folder has the testFile.parquet file in it.\n\nDescribing the table:\n\n%spark.sql\ndescribe table schema_abc.parquet_table_name\n\nGives:\ncol_name   data_type  comment\nYEAR\tint\tnull\nWORD\tstring\tnull\n\n\nbut when I run a select query to read the table.. It gives me below error:\n\nDescribing the table:\n\n%spark.sql\nselect \\* from schema_abc.parquet_table_name\n\njava.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary\n\tat org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:48)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:233)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\nSince I was getting below error I wanted to see if parquet file really is showing the data. hence used Apache Drill to view the data which outputs like below:\n\n\nuser@server:parth_to/parquet-drill/apache-drill-1.10.0$ bin/drill-embedded\nApr 07, 2017 1:04:51 PM org.glassfish.jersey.server.ApplicationHandler initialize\nINFO: Initiating Jersey application, version Jersey: 2.8 2014-04-29 01:25:26...\napache drill 1.10.0\n\"drill baby drill\"\n0: jdbc:drill:zk=local> select \\* from dfs.`/path_to/parquet-drill/apache-drill-1.10.0/sample-data/testFile.parquet`;\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n---------------------+\n\n|YEAR|WORD|\n|-|-|\n\n---------------------+\n\n|2017|null|\n|-|-|\n|2018|[B@5bd466f2|\n\n---------------------+\n2 rows selected (1.433 seconds)\n\n\nInput Txt file:\nYEAR|WORD\n2017|\n2018|Word 2\n\n(i've put null to test if that works)\n"
        },
        {
            "created_at": "2017-04-07T18:24:11.658Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15961226) by Ashima Sood (ash_26):*\nI see that after conversion of pandas dataframe to table, the dataTypes are changed like below:\nDataFrame datatype:\nWORD : object\n\nTable datatype:\nWORD: binary\n\nIs there a way to convert to string instead?"
        },
        {
            "created_at": "2017-04-09T00:40:11.599Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15961999) by Ashima Sood (ash_26):*\nSince we have an option to explicitly provide a schema, I updated the code as below :\ntable=pa.Table.from_pandas(dataFrame,schema=dfschema)\n\nwhere dfschema = pa.Schema.from_fields([ (pa.Field.from_py('YEAR', pa.int64())),( pa.Field.from_py('WORD', pa.string())) ])\n\nRegarless, getting below output:\n\ndfschema::\nYEAR: int64\nWORD: string\n\ntable schema::\nYEAR: int64\nWORD: binary\n\nEventually, since the WORD datatype remains binary, I still am not able to get the string value during query/read.\n\nI also see that string dtype uses object references/pointers. Is there a way to get the value instead of the reference?\nAlso, when we use CAST(SUBSTR) during the select query, the value is show. Is there a right way to create hive table in that case?\n\nThank you!"
        },
        {
            "created_at": "2017-04-13T02:52:05.541Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967036) by Wes McKinney (wesm):*\nI tried to reproduce this issue with Impala on Arrow / Parquet master branches.  I put the file in a temporary directory then ran\n\n```Java\nCREATE EXTERNAL TABLE __ibis_tmp.`__ibis_tmp_57ccb655a5b1425fbc99ea30054c6c60`\nLIKE PARQUET '/tmp/test-parquet-binary/0.parq'\nSTORED AS PARQUET\nLOCATION '/tmp/test-parquet-binary'\n```\n\nThe resulting table, with schema inferred from the Parquet file, is:\n\n```Java\ndescribe __ibis_tmp.`__ibis_tmp_57ccb655a5b1425fbc99ea30054c6c60`\nOut[30]:\n[('year', 'bigint', 'Inferred from Parquet file.'),\n ('word', 'string', 'Inferred from Parquet file.')]\n```\n\nand\n\n```Java\n\tyear\tword\n0\t2017\tWord 1\n1\t2018\tWord 2\n```\n\nstring in Impala is a plain BYTE_ARRAY aka Binary. The Arrow table was\n\n```Java\npyarrow.Table\nYEAR: int64\nWORD: binary\n```\n\nHowever, parquet-tool cat from parquet-mr 1.9.0 gives:\n\n```Java\n$ java -jar target/parquet-tools-1.9.0.jar cat test.parq \nYEAR = 2017\nWORD = V29yZCAx\n\nYEAR = 2018\nWORD = V29yZCAy\n```\n\nThis suggests there's something wrong with the file metadata is Impala is able to read the file OK. I'm looking more closely into it"
        },
        {
            "created_at": "2017-04-13T03:14:31.187Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967051) by Wes McKinney (wesm):*\nSpoke too soon:\n\n```Java\n$ java -jar target/parquet-tools-1.9.0.jar dump ../../arrow/python/test.parq \nrow group 0 \n--------------------------------------------------------------------------------\nYEAR:  INT64 SNAPPY DO:4 FPO:36 SZ:84/80/0.95 VC:2 ENC:RLE,PLAIN_DICTI [more]...\nWORD:  BINARY SNAPPY DO:148 FPO:184 SZ:84/80/0.95 VC:2 ENC:RLE,PLAIN_D [more]...\n\n    YEAR TV=2 RL=0 DL=1 DS: 2 DE:PLAIN_DICTIONARY\n    ----------------------------------------------------------------------------\n    page 0:                  DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY ST:[ [more]... VC:2\n\n    WORD TV=2 RL=0 DL=1 DS: 2 DE:PLAIN_DICTIONARY\n    ----------------------------------------------------------------------------\n    page 0:                  DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY ST:[ [more]... VC:2\n\nINT64 YEAR \n--------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 2 *** \nvalue 1: R:0 D:1 V:2017\nvalue 2: R:0 D:1 V:2018\n\nBINARY WORD \n--------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 2 *** \nvalue 1: R:0 D:1 V:Word 1\nvalue 2: R:0 D:1 V:Word 2\n```\n\nIn Spark 2.2.x I have:\n\n```Java\nsqlContext.read.parquet('/home/wesm/code/arrow/python/test.parq').toPandas()\n\nYEAR\tWORD\n0\t2017\t[87, 111, 114, 100, 32, 49]\n1\t2018\t[87, 111, 114, 100, 32, 50]\n```\n\nThere's some Spark setting to treat binary as strings. If you look up the ASCII codes for the integers in the Spark output, that looks right. I'm not sure what incantation Hive needs to work properly though"
        },
        {
            "created_at": "2017-04-13T03:15:44.890Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967053) by Wes McKinney (wesm):*\nMaking this not a 0.3 blocker, `[~cpcloud]` if you have any ideas how to fix this, let me know"
        },
        {
            "created_at": "2017-04-13T03:29:34.805Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967059) by Wes McKinney (wesm):*\nIf I convert the strings to UTF8, then the problem goes away:\n\n```Java\ndf['WORD'] = df['WORD'].str.decode('utf8')\n```\n\nthen in parquet-mr and Spark\n\n```Java\njava -jar target/parquet-tools-1.9.0.jar test2.parq \nYEAR = 2017\nWORD = Word 1\n\nYEAR = 2018\nWORD = Word 2\n```"
        },
        {
            "created_at": "2017-04-13T14:50:22.338Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967675) by Phillip Cloud (cpcloud):*\nI'm able to run this using `beeline` declaring the `word` column as either `binary` or `string` type in Hive:\n\n```Java\nubuntu@impala:~$ beeline --silent=true --showHeader=false -u jdbc:hive2://localhost:10000/default -n ubuntu   \n0: jdbc:hive2://localhost:10000/default> create external table t (year bigint, word string) stored as parquet location '/user/hive/warehouse/arrow';\n0: jdbc:hive2://localhost:10000/default> select * from t;\n+---------+---------+--+\n| 2017    | Word 1  |\n| 2018    | Word 2  |\n+---------+---------+--+\n0: jdbc:hive2://localhost:10000/default> create external table t2 (year bigint, word binary) stored as parquet location '/user/hive/warehouse/arrow';\n0: jdbc:hive2://localhost:10000/default> select * from t2;\n+----------+----------+--+\n| 2017     | Word 1   |\n| 2018     | Word 2   |\n+----------+----------+--+\n```"
        },
        {
            "created_at": "2017-04-13T14:51:16.006Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967676) by Phillip Cloud (cpcloud):*\nI created the file in Python 2.7 with latest `arrow`/`pyarrow` and latest `parquet-cpp`."
        },
        {
            "created_at": "2017-04-13T15:07:44.174Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15967700) by Phillip Cloud (cpcloud):*\nI'm not sure if there's still a possible issue here. When using drill, if I cast the `WORD` column to `varchar` then the data look fine. When left as `binary` the values are unintelligible:\n\n```Java\n0: jdbc:drill:zk=local> select `YEAR`, cast(`WORD` as varchar) as `WORD` from dfs.`/home/phillip/code/cpp/arrow/python/arrow_parquet.parquet`;\n+-------+---------+\n| YEAR  |  WORD   |\n+-------+---------+\n| 2017  | Word 1  |\n| 2018  | Word 2  |\n+-------+---------+\n```"
        },
        {
            "created_at": "2017-04-24T20:09:07.151Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=15981803) by Wes McKinney (wesm):*\nI can't repro this, took off 0.3 fix version until we get a reliable reproduction"
        },
        {
            "created_at": "2017-07-28T01:18:19.337Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=16104276) by Wes McKinney (wesm):*\nClosing as Cannot Reproduce. Please reopen the issue if you have a reproduction for us to look at"
        },
        {
            "created_at": "2019-11-18T17:31:48.093Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=16976721) by albertoramon (albertoramon):*\nI saw this:(SparkSQL 2.4.4 PyArrow 0.15)\r\n\r\nThe problem is Create table with INT columns (BIGINT works properly)\r\n\r\nSOL: Change INT to BIGINT works fine (I tried to use Double but didn't work) in create table\r\n\r\n\u00a0\r\n\r\nIn my case: these Parquet Files are from SSB benchmark\r\n```java\n\r\nSELECT MAX(LO_CUSTKEY), MAX(LO_PARTKEY), MAX (LO_SUPPKEY)\r\nFROM SSB.LINEORDER;\r\nReturns: 29999 200000 2000\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nIn my Column_Types I Had,: (thus I need review my Python Code :)):\r\n```java\n\r\n'lo_custkey':'int64',\r\n 'lo_partkey':'int64',\r\n 'lo_suppkey':'int64',\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-08-27T14:42:05.856Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-785?focusedCommentId=17586037) by @toddfarmer:*\nTransitioning issue from Resolved to Closed to based on resolution field value."
        }
    ]
}