{
    "issue": {
        "title": "[Python] exitCode: <139> when csv file converting parquet using pandas/pyarrow libraries",
        "body": "***Note**: This issue was originally created as [ARROW-16822](https://issues.apache.org/jira/browse/ARROW-16822). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nOur main requirement is to read source file (structured/semi structured /unstructured) which are residing in AWS s3 through AWS redshift database, where our customer have direct access to analyze the data very quickly/seamlessly for reporting purpose without defining the schema info for the file.\r\n\r\nWe have created an data lake (aws s3) workspace where our customers dumps csv/parquet huge size files (like 10/15 GB). We have developed a framework which is consuming pandas/pyarrow (parquet) libraries to read source files in chunking manner and identifying schema meaning (datatype/length) and push it to AWS Glue where AWS redshift database can talk seamlessly to s3 files can read very quickly.\r\n\r\n\u00a0\r\n\r\nFollowing is the snippet of parquet conversion where i'm getting this error. Please take a look\r\n\r\n\u00a0\r\n\r\nread_csv_args = \\{'filepath_or_buffer': src_object, 'chunksize': self.chunkSizeLimit, 'encoding': 'UTF-8','on_bad_lines': 'error','sep': fileDelimiter, 'low_memory': False, 'skip_blank_lines': True, 'memory_map': True} # 'verbose': True , In order to enable memory consumption logging\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\n\r\nif srcPath.endswith('.gz'):\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 read_csv_args['compression'] = 'gzip'\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if fileTextQualifier:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 read_csv_args['quotechar'] = fileTextQualifier\r\n\r\nwith pd.read_csv(\\*\\*read_csv_args) as reader:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for chunk_number, chunk in enumerate(reader, 1):\r\n\r\n1. To support shape-shifting for the incoming datafiles, need to make sure match file with number of columns if not delete\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if glueMasterSchema is not None:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sessionSchema=copy.deepcopy(glueMasterSchema) #copying using deepcopy() method\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 chunk.columns = chunk.columns.str.lower() # modifying the column header of all columns to lowercase\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fileSchema = list(chunk.columns)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for key in list(sessionSchema):\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if key not in fileSchema:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 del sessionSchema[key]\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fields = []\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for col,dtypes in sessionSchema.items():\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fields.append(pa.field(col, dtypes))\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 glue_schema = pa.schema(fields)\n   \n1. To identify the boolean datatype and convert back to STRING which was done during the BF schema\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for cols in chunk.columns:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 try:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if chunk[cols].dtype =='bool':\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 chunk[cols] = chunk[cols].astype('str')\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if chunk[cols].dtype =='object':\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 chunk[cols] = chunk[cols].fillna('').astype('str').tolist()\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 except (ParserError,ValueError,TypeError):\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pass\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.debug(\"chunk count\", chunk_number, \"chunk length\", len(chunk), 'glue_schema', glue_schema, 'Wrote file', targetKey)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 #log.debug(\"during pandas chunk data \", chunk,\"df schemas:\", chunk.dtypes)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 table = pa.Table.from_pandas(chunk, \u00a0schema=glue_schema , preserve_index=False)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.info('Glue schema:',glue_schema,'for a [file:',targetKey\\|file:///',targetKey])\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.info('pandas memory utilization during chunk process: ', chunk.memory_usage().sum(), 'Bytes.','\\n\\n\\n')\n1. Guess the schema of the CSV file from the first chunk\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 #if pq_writer is None:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if chunk_number == 1:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 #parquet_schema = table.schema\n1. Open a Parquet file for writing\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pq_writer = pq.ParquetWriter(targetKey, schema=glue_schema, compression='snappy') # In PyArrow we use, Snappy generally results in better performance\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.debug(\"table schema :\", pprint.pformat(table.schema).replace('\\n', ',').replace('\\r', ','),' for:', inputFileName)\n   \n1. writing the log information into s3://etl_activity\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 etlActivityLog.append({'tableObjectName': targetDirectory[:-1], 'sourceFileName': inputFileName, 'targetFileName': parquetFileName, 'message': 'File Converted Successfully', 'number of rows processed': str(table.num_rows), 'fileStatus': 'SUCCESS'})\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 logInfo = self.read_logInfo(etlActivityLog)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.s3Handle.putObject(s3Client, 'etl_process_all.json', logInfo, bucketName, self.etlJobActivityLogFolder )\n   \n1. Write CSV chunk to the parquet file\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pq_writer.write_table(table)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 i += 1\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.info( 'chunk count:', i, 'for a given [file:',targetKey,'whitelist:',targetDirectory[:-1\\|file:///',targetKey,'whitelist:',targetDirectory[:-1]])\n1. Close a Parquet file writer\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if pq_writer is not None and pq_writer.is_open:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pq_writer.close()\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pq_writer = None\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 s3key = outputDirectory + targetDirectory + parquetFileName\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.s3Handle.waitForFile(s3Client, bucketName, s3key)\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.info('Metadata info:', table.column_names, 'number of columns:', table.num_columns, 'number of rows:', table.num_rows, 'Glue Object Name:', targetDirectory[:-1])\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.debug('Wrote file', targetKey, 'with chunk count:', chunk_number)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.debug('Stream copy', targetKey, 'to parquet took:', datetime.now() - start_time)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.info('Final parquert convert:',sys.exc_info())\n   \n   \u00a0 \u00a0 \u00a0 \u00a0 except (EOFError, IOError) as x:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.error(\"error in source file for EOFError, IOError\" % \u00a0x)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise SystemExit('convert2Parquet EOFError:'+sys.exc_info())\n   \u00a0 \u00a0 \u00a0 \u00a0 except (ValueError, ParserError) as x:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.error(\"error in source for ValueError, ParserError\" % \u00a0x)\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise SystemExit('convert2Parquet valueError:'+sys.exc_info())\n   \n   \u00a0\n   \n   finally:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if pq_writer is not None and pq_writer.is_open:\n   \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pq_writer.close()\n   \n   \u00a0",
        "created_at": "2022-06-13T06:43:26.000Z",
        "updated_at": "2022-06-29T17:00:31.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-06-13T06:46:37.521Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16822?focusedCommentId=17553405) by Mahesha Subrahamanya (micomahesh1982):*\nfollowing versions have used to reproduce the issue so please suggest incase to upgrade to latest version of libraries have addressed this issue.\u00a0\r\n\r\n`{}pandas{`}`{}==1.3.3; python_full_{`}`{}version{`}` >= \"3.7.1\" \\`\r\n`{}pyarrow==5.0.0; python_full_{`}`{}version{`}`{} >= \"3.6.2\" and python_{`}`{}version{`}`{} < \"3.10\" and python_{`}`{}version{`}` >= \"3.6\"`"
        },
        {
            "created_at": "2022-06-16T19:27:19.871Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16822?focusedCommentId=17555267) by Weston Pace (westonpace):*\nCan you raise a different error other than SystemExit or provide a traceback?  This is a rather large snippet of code to parse through to figure out which line might be failing.  Also, the exit code you are mentioning (139) does not seem like something that pyarrow would configure.  Pyarrow doesn't really interact with exit codes."
        },
        {
            "created_at": "2022-06-16T19:54:19.772Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16822?focusedCommentId=17555274) by Mahesha Subrahamanya (micomahesh1982):*\n![convertCSV2Parquet.png](https://issues.apache.org/jira/secure/attachment/13045177/convertCSV2Parquet.png)"
        },
        {
            "created_at": "2022-06-16T19:58:03.937Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16822?focusedCommentId=17555276) by Mahesha Subrahamanya (micomahesh1982):*\nwe did try with traceback, exception handler however nothing worked here.\r\n\r\nwhen pandas hand over the chunk data to pyarrow which is responsible to converting into parquet file. we suspect during the process of converting it's failing however it's not throwing the right error code/error message hence need your help. kindly let me know if anything can be helpful is really appreciated.\u00a0 since we are running into this issue we couldn't deliver this project as it's dependency at the python libraries like pandas/pyarrow.\u00a0"
        }
    ]
}