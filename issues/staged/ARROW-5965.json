{
    "issue": {
        "title": "[Python] Regression: segfault when reading hive table with v0.14",
        "body": "***Note**: This issue was originally created as [ARROW-5965](https://issues.apache.org/jira/browse/ARROW-5965). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm working with pyarrow on a cloudera cluster (CDH 6.1.1), with pyarrow installed in a conda env.\r\n\r\nThe data I'm reading is a hive(-registered) table written as parquet, and with v0.13, reading this table (that is partitioned) does not cause any issues.\r\n\r\nThe code that worked before and now crashes with v0.14 is simply:\r\n\r\n```\r\nimport pyarrow.parquet as pq\r\npq.ParquetDataset('hdfs:///data/raw/source/table').read()\r\n\r\n```\r\n\r\nSince it completely crashes my notebook (resp. my REPL ends with \"Killed\"), I cannot report much more, but this is a pretty severe usability restriction. So far the solution is to enforce `pyarrow<0.14`",
        "created_at": "2019-07-17T08:50:15.000Z",
        "updated_at": "2019-08-19T19:41:37.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-08-19T19:41:37.000Z"
    },
    "comments": [
        {
            "created_at": "2019-07-17T14:42:45.547Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887119) by Neal Richardson (npr):*\nThanks for the report. A few questions:\r\n1. Is this reproducible if you try again with the same file? (I wonder if \"Killed\" means OOM and not segfault)\n1. Could you provide a (preferably as small as possible) Parquet file that triggers this behavior? I think we'll need that in order to identify and fix any issues."
        },
        {
            "created_at": "2019-07-17T14:57:05.588Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887129) by H. Vetinari (h-vetinari):*\nHey Neal,\r\n\r\nI tried a couple of times before filing the report, and all (~5) invocations on 0.14 crashed, and all invocations on 0.13 worked. The machine itself has lots of memory, so I don't think it's that. Not sure I'll be able to pare this down to a minimal reproducing parquet file. I'll try."
        },
        {
            "created_at": "2019-07-17T15:05:47.336Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887142) by Wes McKinney (wesm):*\nA gdb backtrace would help us a lot. Do you know how to get one?"
        },
        {
            "created_at": "2019-07-17T19:08:19.750Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887352) by H. Vetinari (h-vetinari):*\n`[~wesmckinn]`\r\nWould like to provide it, but would only be able to install through conda (which has a hole in the firewall).\r\nUnfortunately,\r\n`# conda install pyarrow=0.14 gdb`\r\n`Collecting package metadata (current_repodata.json): done`\r\n`Solving environment: failed`\r\n`Collecting package metadata (repodata.json): done`\r\n`Solving environment: failed`\r\n\r\n`UnsatisfiableError: The following specifications were found to be incompatible with each other:`\r\n\r\n  `- pip -> python[version='>=3.7,<3.8.0a0']`\r\n\r\nwhich, I believe, is due to the fact that gdb has [not yet](https://github.com/conda-forge/gdb-feedstock/pull/12) been built for python 3.7. (although, just as I was preparing this message, I triggered a rerender there and this has caused some further action and the first passing 3.7 build; not yet merged because 2.7 is failing).\r\n\r\nIn the meantime I tried downgrading my whole environment to 3.6, where the program also crashes or hangs on v0.14. However, I haven't yet been able to get a gdb output. Might need some more reading of the GDB manual..."
        },
        {
            "created_at": "2019-07-17T19:11:26.571Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887356) by Wes McKinney (wesm):*\nNote I linked this with ARROW-2652 since many users aren't familiar with producing gdb backtraces generated in Python programs"
        },
        {
            "created_at": "2019-07-18T06:32:08.856Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16887676) by H. Vetinari (h-vetinari):*\n`[~wesmckinn]`\r\n Thanks for the tips. Unfortunately, I can't follow that example because the code does not generate a core-dump but only prints \"Killed\". I found some ways to run it in gdb that **should** work (best as I can tell), like `gdb -ex r --args python fail.py` or interactively:\r\n`gdb python`\r\n`(gdb) run fail.py`\r\n\r\nbut I always get:\r\n`[...]`\r\n`warning: Could not trace the inferior process`\r\n`Error:`\r\n`warning: ptrace: Operation not permitted`\r\n`During startup program exited with code 127.`\r\n\r\nNot sure if that's a mistake on my side or something in the setup/interplay of conda-gdb."
        },
        {
            "created_at": "2019-08-19T19:41:37.786Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5965?focusedCommentId=16910694) by Wes McKinney (wesm):*\nI'm guessing this is a dup of the memory issue from ARROW-6060. If you obtain a repro or additional information to suggest it's not a memory problem please reopen"
        }
    ]
}