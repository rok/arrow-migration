{
    "issue": {
        "title": "[Python] pyarrow dataset API fails to read s3 directory",
        "body": "***Note**: This issue was originally created as [ARROW-16438](https://issues.apache.org/jira/browse/ARROW-16438). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen an s3 file system as `file_system` is passed to [pyarrow.dataset.dataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.dataset.html#pyarrow.dataset.dataset) API and the `source` is a directory name with bucket, there is an error:\r\n\r\n```python\n\r\nIn [5]: from fsspec.core import get_fs_token_paths\r\n\r\nIn [6]: fs, _, path = get_fs_token_paths(\"s3://prem-rapids-test/folder/\", mode=\"rb\")\r\n\r\nIn [7]: fs\r\nOut[7]: <s3fs.core.S3FileSystem at 0x7f3d02cc1460>\r\n\r\nIn [8]: path\r\nOut[8]: ['prem-rapids-test/folder']\r\n\r\nIn [10]: pa.dataset.dataset(path, filesystem=fs, format=\"parquet\")\r\n---------------------------------------------------------------------------\r\nFileNotFoundError \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Traceback (most recent call last)\r\nInput In [10], in <cell line: 1>()\r\n----> 1 pa.dataset.dataset(path, filesystem=fs, format=\"parquet\")\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/dataset.py:670, in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n\u00a0 \u00a0 668 elif isinstance(source, (tuple, list)):\r\n\u00a0 \u00a0 669 \u00a0 \u00a0 if all(_is_path_like(elem) for elem in source):\r\n--> 670 \u00a0 \u00a0 \u00a0 \u00a0 return _filesystem_dataset(source, **kwargs)\r\n\u00a0 \u00a0 671 \u00a0 \u00a0 elif all(isinstance(elem, Dataset) for elem in source):\r\n\u00a0 \u00a0 672 \u00a0 \u00a0 \u00a0 \u00a0 return _union_dataset(source, **kwargs)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/dataset.py:422, in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n\u00a0 \u00a0 414 options = FileSystemFactoryOptions(\r\n\u00a0 \u00a0 415 \u00a0 \u00a0 partitioning=partitioning,\r\n\u00a0 \u00a0 416 \u00a0 \u00a0 partition_base_dir=partition_base_dir,\r\n\u00a0 \u00a0 417 \u00a0 \u00a0 exclude_invalid_files=exclude_invalid_files,\r\n\u00a0 \u00a0 418 \u00a0 \u00a0 selector_ignore_prefixes=selector_ignore_prefixes\r\n\u00a0 \u00a0 419 )\r\n\u00a0 \u00a0 420 factory = FileSystemDatasetFactory(fs, paths_or_selector, format, options)\r\n--> 422 return factory.finish(schema)\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/_dataset.pyx:1680, in pyarrow._dataset.DatasetFactory.finish()\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/error.pxi:143, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/_fs.pyx:1179, in pyarrow._fs._cb_open_input_file()\r\n\r\nFile /nvme/0/pgali/envs/cudfdev/lib/python3.8/site-packages/pyarrow/fs.py:394, in FSSpecHandler.open_input_file(self, path)\r\n\u00a0 \u00a0 391 from pyarrow import PythonFile\r\n\u00a0 \u00a0 393 if not self.fs.isfile(path):\r\n--> 394 \u00a0 \u00a0 raise FileNotFoundError(path)\r\n\u00a0 \u00a0 396 return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\r\n\r\nFileNotFoundError: prem-rapids-test/folder\r\n```\u00a0\r\n\r\nBut it works only if the folder is passed as a full string:\r\n```python\n\r\nIn [3]: import pyarrow.dataset\r\n\r\nIn [4]: pa.dataset.dataset(\"s3://prem-rapids-test/folder/\", format=\"parquet\")\r\nOut[4]: <pyarrow._dataset.FileSystemDataset at 0x7f3ce502d870>\r\n\r\n```\r\n\r\n\u00a0",
        "created_at": "2022-05-02T17:58:31.000Z",
        "updated_at": "2022-05-03T15:17:33.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-05-02T17:59:22.630Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17530875) by Prem Sagar Gali (galipremsagar):*\nFrom the stack trace this issue seems to be similar to: https://issues.apache.org/jira/browse/ARROW-10923, but 10923 doesn't have a reproducer."
        },
        {
            "created_at": "2022-05-03T07:14:54.064Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531075) by Antoine Pitrou (apitrou):*\nArrow has its own S3 filesystem that gets used when you pass the URI as a string, but if you pass a FSSPec filesystem instance, then a compatibility layer is being used, and it might have bugs (and/or FSSpec changed its semantics slightly).\r\n\r\nSee https://arrow.apache.org/docs/python/filesystems.html"
        },
        {
            "created_at": "2022-05-03T07:49:18.346Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531091) by Antoine Pitrou (apitrou):*\ncc `[~jorisvandenbossche]` for the FSSpec compatibility issue."
        },
        {
            "created_at": "2022-05-03T09:02:23.849Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531112) by Joris Van den Bossche (jorisvandenbossche):*\nIt might be that this is not related to passing a native vs fsspec filesystem, but just that _if_ you pass a list of strings, we assume that it is a list of files, and not a directory. "
        },
        {
            "created_at": "2022-05-03T09:20:27.773Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531116) by Joris Van den Bossche (jorisvandenbossche):*\nSo reproducing this locally with a pyarrow local filesystem:\r\n\r\n```Java\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as ds\r\n\r\ntable = pa.table({'a': [1, 2, 3]})\r\npq.write_to_dataset(table, \"test_parquet_dataset/\")\r\n\r\nIn [9]: ds.dataset([\"test_parquet_dataset/\"], format=\"parquet\", filesystem=LocalFileSystem())\r\n---------------------------------------------------------------------------\r\nIsADirectoryError                         Traceback (most recent call last)\r\n<ipython-input-9-8e554a28b381> in <module>\r\n----> 1 ds.dataset([\"test_parquet_dataset/\"], format=\"parquet\", filesystem=LocalFileSystem())\r\n\r\n~/scipy/repos/arrow/python/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    695     elif isinstance(source, (tuple, list)):\r\n    696         if all(_is_path_like(elem) for elem in source):\r\n--> 697             return _filesystem_dataset(source, **kwargs)\r\n    698         elif all(isinstance(elem, Dataset) for elem in source):\r\n    699             return _union_dataset(source, **kwargs)\r\n\r\n~/scipy/repos/arrow/python/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    435 \r\n    436     if isinstance(source, (list, tuple)):\r\n--> 437         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\r\n    438     else:\r\n    439         fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n\r\n~/scipy/repos/arrow/python/pyarrow/dataset.py in _ensure_multiple_sources(paths, filesystem)\r\n    356                 raise FileNotFoundError(info.path)\r\n    357             elif file_type == FileType.Directory:\r\n--> 358                 raise IsADirectoryError(\r\n    359                     'Path {} points to a directory, but only file paths are '\r\n    360                     'supported. To construct a nested or union dataset pass '\r\n\r\nIsADirectoryError: Path test_parquet_dataset/ points to a directory, but only file paths are supported. To construct a nested or union dataset pass a list of dataset objects instead.\r\n```\r\n\r\nSo it also errors, although it gives a more clear error message about a directory not being supported (this error message comes from an additional check that we only do if the filesystem is local, I suppose because those checks can potentially be costly for remote filesystems)."
        },
        {
            "created_at": "2022-05-03T09:21:26.833Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531118) by Joris Van den Bossche (jorisvandenbossche):*\n`[~galipremsagar]` just to be sure, but can you test `pa.dataset.dataset(path[0], filesystem=fs, format=\"parquet\")` with the fsspec filesystem? (so passing `path[0]` instead of `path`)"
        },
        {
            "created_at": "2022-05-03T15:16:58.199Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16438?focusedCommentId=17531248) by Prem Sagar Gali (galipremsagar):*\n`[~jorisvandenbossche]` Yup, that worked for me:\r\n\r\n\r\n\r\n```python\n\r\n\r\nIn [7]: pa.dataset.dataset(path[0], filesystem=fs, format=\"parquet\")\r\nOut[7]: <pyarrow._dataset.FileSystemDataset at 0x7fb2b32300f0>\r\n\r\n```"
        }
    ]
}