{
    "issue": {
        "title": "[C++] Allowing ChunkedArrays to contain a mix of DictionaryArray and dense Array (of the dictionary type)",
        "body": "***Note**: This issue was originally created as [ARROW-4083](https://issues.apache.org/jira/browse/ARROW-4083). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn some applications we may receive a stream of some dictionary encoded data followed by some non-dictionary encoded data. For example this happens in Parquet files when the dictionary reaches a certain configurable size threshold.\r\n\r\nWe should think about how we can model this in our in-memory data structures, and how it can flow through to relevant computational components (i.e. certain data flow observers \u2013 like an Aggregation \u2013 might need to be able to process either a dense or dictionary encoded version of a particular array in the same stream)",
        "created_at": "2018-12-19T18:20:21.000Z",
        "updated_at": "2019-08-21T23:11:08.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2019-08-21T23:11:08.000Z"
    },
    "comments": [
        {
            "created_at": "2018-12-19T23:55:54.939Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16725448) by Antoine Pitrou (apitrou):*\nDoesn't it add constraints to every downstream consumer of chunked arrays? Intuitively, that sounds like a rather bad idea. Perhaps we need some kind of \"logical chunked array\" where individual chunks are allowed to have different but compatible types (how compatible remains to be defined, e.g. do we allow int8 then int32?)."
        },
        {
            "created_at": "2018-12-20T01:03:46.368Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16725486) by Wes McKinney (wesm):*\nWell, we need to confront the fact that dictionary encoding is used in some applications as a compression technique, especially in databases. Parquet files are a canonical example. \r\n\r\nIt is not practical in general to convert everything to dense representation both because of memory use and performance issues. I think we are going to get ourselves into a trouble if we implement stream-oriented data processing code that requires that a field in an entire stream either be all dictionary encoded or all dense. \r\n\r\nWe can always wait until it becomes a problem, but I would like to think it through carefully right now to see what we may have to do to accommodate this."
        },
        {
            "created_at": "2018-12-20T01:05:32.097Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16725487) by Wes McKinney (wesm):*\nNote: at the protocol level dictionary encoding is not the same as other logical types. The reason for the logical DictionaryType / DictionaryArray containers in C++ is that it simplifies (IMHO) dynamic dispatch"
        },
        {
            "created_at": "2019-01-05T03:17:16.099Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16734757) by Uwe Korn (uwe):*\nI think this will be something that really confuses users and leads to problems. I would rather stick to\u00a0DictionaryType being a real type. For the described use case, I would rather expect the user to emit a set of RecordBatches in their API. These batches will not have to be all of the same schema but the schema should be evolable to each other. By this path, we keep the ChunkedArray as simple as it currently is but use the RecordBatch as the base to pass data around that doesn't have exactly the same schemas.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2019-01-05T20:09:32.453Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16734990) by Wes McKinney (wesm):*\n`ChunkedArray` is probably the wrong abstraction for this. Eventually we are going to be forced to address this issue in cases where dictionary encoding is used for data compression. But I think it can be handled on a case by case basis. For example, when performing a hash aggregation, it would not make sense to materialize all dictionary encoded data to dense and then hash it again during aggregation. So it would be up to the hash aggregation implementation to treat both `string` and `dictionary<string>` as being \"the same\" from an analysis point of view. \r\n\r\nFWIW, my presumption of Arrow \"users\" is that they are system developers, not end users, so we can expect a certain level of sophistication that would not be expected from, say, a pandas user"
        },
        {
            "created_at": "2019-05-31T01:57:54.553Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16852569) by Wes McKinney (wesm):*\nI think this could be addressed at the dataframe level, removing from any milestone for now"
        },
        {
            "created_at": "2019-08-21T23:11:08.804Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4083?focusedCommentId=16912774) by Wes McKinney (wesm):*\nI will take care of this elsewhere when it is actually needed"
        }
    ]
}