{
    "issue": {
        "title": "Improve performance of repeated filtered parquet reads",
        "body": "***Note**: This issue was originally created as [ARROW-11826](https://issues.apache.org/jira/browse/ARROW-11826). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n`pq.read_table(..., filters=...)` is slower than it probably should be... In particular, there should be a faster solution for repeatedly reading from the same file.\r\n\r\nFor a 1,8 GB parquet file with about 5000 row groups (written with row-group metadata statistics), reading with a filter that selects a sub-section of a single row group takes 1.65 s.\r\n\r\nRepeated, similarly small reads from the same file each take the same amount of time with `pq.read_table(..., filters=...)`.\r\n\r\nI say \"slower than it probably should be\" because with the rudimentary example code below, the initial filtered read takes about 975 ms. Any subsequent read takes only 14ms.\r\n\r\nI have no idea what makes the C++ code slow, but in Python I observed two dominant time-wasters for repeated queries:\r\n1. `pq.ParquetFile('test.parquet')` takes 170ms (this might be a Windows issue), so opening the file only once helps\n1. iterating though metadata is surprisingly slow: 80 ms for a single pass, e.g. `[pq_file.metadata.row_group(i).column(0).statistics.min for i in range(pq_file.metadata.num_row_groups)]`. E.g. six passes add up to about 500ms. (One pass each for minimum and maximum on three columns.) Caching the statistics really helps here.\n   \n   {code:python}\n   from functools import cached_property, lru_cache\n   \n   import pyarrow as pa\n   import pyarrow.compute as pc\n   import pyarrow.parquet as pq\n   \n   \n   def equal(x, y, \\*\\*kwargs):\n       \"\"\"Enable equal comparison for dictionary array vs. scalar.\"\"\"\n       type_x = type(x.type)\n       type_y = type(y.type)\n   \n1. fastpath\n       if type_x is pa.DataType and type_y is pa.DataType:\n           return pc._equal(x, y, \\*\\*kwargs)\n   \n       if type_x is pa.DictionaryType and type_y is pa.DataType:\n           array_arg = x\n           scalar_arg = y\n       elif type_x is pa.DataType and type_x is pa.DictionaryType:\n           array_arg = y\n           scalar_arg = x\n       else:\n1. fallback to default implemetation (which will error out...)\n           return pc._equal(x, y, \\*\\*kwargs)\n   \n1. have dictionary vs. scalar comparison\n       def chunk_generator():\n           for chunk in array_arg.iterchunks():\n1. for large dictionaries use pyarrow to search index\n               if len(chunk.dictionary) > 30:\n                   index = pc.index_in(scalar_arg, options=pc.SetLookupOptions(\n                       value_set=chunk.dictionary))\n                   if index.is_valid:\n                       yield pc.equal(chunk.indices, index)\n                       continue\n1. for small dictionaries index search in python is faster\n               try:\n                   index = chunk.dictionary.to_pylist().index(scalar_arg.as_py())\n                   yield pc.equal(chunk.indices, pa.scalar(index, chunk.indices.type))\n                   continue\n               except ValueError:\n                   pass\n1. value not found in dictionary\n               yield pa.nulls(len(chunk), pa.bool_()).fill_null(False)\n       return pa.chunked_array(chunk_generator())\n   \n   \n   pc._equal = pc.equal\n   pc.equal = equal\n   \n   \n   @lru_cache(maxsize=None)\n   def column_names_from_parquet_file(parquet_file):\n       \"\"\"\"Returns a dict associating column names with column number\n       for a given parquet file. The result is cached.\"\"\"\n       return {\n           parquet_file.metadata.row_group(0).column(i).path_in_schema: i\n           for i in range(parquet_file.metadata.row_group(0).num_columns)\n       }\n   \n   \n   @lru_cache(maxsize=None)\n   def metadata_from_parquet_file(parquet_file, field_name):\n       \"\"\"Returns a tuple(min_, max_) where min_ and max_ are lists with the \n       row group metadata statistics min and max respectively.\n       The result is cached.\"\"\"\n       column_id = column_names_from_parquet_file(parquet_file)[field_name]\n       pq_metadata = parquet_file.metadata\n       min_ = [\n           pq_metadata.row_group(i).column(column_id).statistics.min\n           for i in range(pq_metadata.num_row_groups)\n       ]\n       max_ = [\n           pq_metadata.row_group(i).column(column_id).statistics.max\n           for i in range(pq_metadata.num_row_groups)\n       ]\n       return min_, max_\n   \n   \n   class Node:\n       \"\"\"Base class for a node in a computation graph.\"\"\"\n   \n       def __init__(self, left=None, right=None):\n           self.left = left\n           self.right = right\n   \n       def __eq__(self, other):\n           return Node_Equal(self, other)\n   \n       def __and__(self, other):\n           return Node_And(self, other)\n   \n       def __or__(self, other):\n           return Node_Or(self, other)\n   \n       def filter_table(self, table):\n           \"\"\"Applies the computation graph as a filter on a table.\"\"\"\n           mask = self.evaluate_on_table(table)\n           return table.filter(mask)\n   \n   \n   class Node_Equal(Node):\n       def _identify_field_and_literal(self):\n           if isinstance(self.left, Field):\n               self.__dict__['field'] = field = self.left\n               self.__dict__['literal'] = literal = self.right\n           else:\n               self.field = self.right\n               self.literal = self.left\n           assert isinstance(literal, (str, float, int))\n           return field, literal\n   \n       @cached_property\n       def field(self):\n           field = self.__dict__.get(\n               'field', None) or self._identify_field_and_literal()[0]\n           return field\n   \n       @cached_property\n       def literal(self):\n           literal = self.__dict__.get(\n               'literal', None) or self._identify_field_and_literal()[1]\n           return literal\n   \n       def evaluate_on_metadata(self, parquet_file, row_groups=None):\n           field = self.field\n           literal = self.literal\n           min_, max_ = metadata_from_parquet_file(parquet_file, field.name)\n           row_groups = row_groups or range(len(min_))\n           return [\n               i\n               for i in row_groups\n               if min_[i] <= literal <= max_[i]\n           ]\n   \n       def evaluate_on_table(self, table):\n           field = self.field\n           literal = self.literal\n           column = table[field.name]\n   \n           if type(column.type) is pa.DictionaryType:\n               pa_scalar = pa.scalar(literal, column.type.value_type)\n           else:\n               pa_scalar = pa.scalar(literal, column.type)\n           return pc.equal(column, pa_scalar)\n   \n   \n   class Node_And(Node):\n       def evaluate_on_metadata(self, parquet_file, row_groups=None):\n           filtered_row_groups = self.left.evaluate_on_metadata(\n               parquet_file, row_groups)\n           return self.right.evaluate_on_metadata(parquet_file, filtered_row_groups)\n   \n       def evaluate_on_table(self, table):\n           mask1 = self.left.evaluate_on_table(table)\n           mask2 = self.right.evaluate_on_table(table)\n           return pc.and_(mask1, mask2)\n   \n   \n   class Node_Or(Node):\n       def evaluate_on_metadata(self, parquet_file, row_groups=None):\n           row_groups1 = self.left.evaluate_on_metadata(parquet_file, row_groups)\n           row_groups2 = self.right.evaluate_on_metadata(parquet_file, row_groups)\n           return sorted(set(row_groups1) | set(row_groups2))\n   \n       def evaluate_on_table(self, table):\n           mask1 = self.left.evaluate_on_table(table)\n           mask2 = self.right.evaluate_on_table(table)\n           return pc.or_(mask1, mask2)\n   \n   \n   class Field:\n       def __init__(self, name):\n           self.name = name\n   \n       def __eq__(self, other):\n           return Node_Equal(self, other)\n   \n   \n   def read_table_filtered(parquet_file, filter=None):\n       \"\"\"Fast filtered read from a parquet file.\"\"\"\n       if isinstance(parquet_file, str):\n           parquet_file = pq.ParquetFile(parquet_file)\n       row_groups = filter.evaluate_on_metadata(parquet_file)\n       table = parquet_file.read_row_groups(row_groups)\n       return filter.filter_table(table)\n   \n   \n   if __name__ == '__main__':\n       pq_file = pq.ParquetFile('test.parquet')\n       filter = (Field('code') == 12345) & (\n           Field('foo') == 'foo value') & (Field('bar') == 'bar value')\n       table = read_table_filtered(pq_file, filter)\n   {code}",
        "created_at": "2021-02-28T17:29:49.000Z",
        "updated_at": "2021-02-28T17:31:33.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}