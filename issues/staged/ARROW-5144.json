{
    "issue": {
        "title": "[Python] ParquetDataset and ParquetPiece not serializable",
        "body": "***Note**: This issue was originally created as [ARROW-5144](https://issues.apache.org/jira/browse/ARROW-5144). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nSince 0.13.0, parquet instances are no longer serialisable, which means that dask.distributed cannot pass them between processes in order to load parquet in parallel.\r\n\r\nExample:\r\n```\r\n>>> import cloudpickle\r\n>>> import pyarrow.parquet as pq\r\n>>> pf = pq.ParquetDataset('nation.impala.parquet')\r\n>>> cloudpickle.dumps(pf)\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dumps(obj, protocol)\r\n    893     try:\r\n    894         cp = CloudPickler(file, protocol=protocol)\r\n--> 895         cp.dump(obj)\r\n    896         return file.getvalue()\r\n    897     finally:\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/cloudpickle/cloudpickle.py in dump(self, obj)\r\n    266         self.inject_addons()\r\n    267         try:\r\n--> 268             return Pickler.dump(self, obj)\r\n    269         except RuntimeError as e:\r\n    270             if 'recursion' in e.args[0]:\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in dump(self, obj)\r\n    407         if self.proto >= 4:\r\n    408             self.framer.start_framing()\r\n--> 409         self.save(obj)\r\n    410         self.write(STOP)\r\n    411         self.framer.end_framing()\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    519\r\n    520         # Save the reduce() output and finally memoize the object\r\n--> 521         self.save_reduce(obj=obj, \\*rv)\r\n    522\r\n    523     def persistent_id(self, obj):\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    632\r\n    633         if state is not None:\r\n--> 634             save(state)\r\n    635             write(BUILD)\r\n    636\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819\r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822\r\n    823     dispatch[dict] = save_dict\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n~/anaconda/envs/py36/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    494             reduce = getattr(obj, \"__reduce_ex__\", None)\r\n    495             if reduce is not None:\r\n--> 496                 rv = reduce(self.proto)\r\n    497             else:\r\n    498                 reduce = getattr(obj, \"__reduce__\", None)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/pyarrow/_parquet.cpython-36m-darwin.so in pyarrow._parquet.ParquetSchema.__reduce_cython__()\r\n\r\nTypeError: no default __reduce__ due to non-trivial __cinit__\r\n```\r\nThe indicated schema instance is also referenced by the ParquetDatasetPiece s.\r\n\r\nref: https://github.com/dask/distributed/issues/2597",
        "created_at": "2019-04-08T22:03:22.000Z",
        "updated_at": "2019-05-02T22:11:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-04-19T11:22:52.000Z"
    },
    "comments": [
        {
            "created_at": "2019-04-10T15:13:59.735Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16814535) by Matthew Rocklin (mrocklin):*\nHi everyone, \r\n\r\nThis is pretty critical for Dask usage.  Anyone trying to use PyArrow 0.13 in a Dask workflow will break pretty hard here.  This isn't something that we can work around easily on our side.  It would be great to know if this is likely to be resolved quickly, or if we should warn users strongly away from 0.13.\r\n\r\nIn general, I recommend serialization tests for any project looking to interact with distributed computing libraries in Python.  Often this consists of tests like the following for any type that you think a parallel computing framework might want to interact with.\r\n\r\n```Java\n\r\ndef test_serialization():\r\n    obj = MyObj()\r\n    obj2 = pickle.loads(pickle.dumps(obj))\r\n\r\n    assert obj == obj2\r\n```"
        },
        {
            "created_at": "2019-04-10T23:25:36.879Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16814937) by Dave Hirschfeld (dhirschfeld):*\nAs a user of `pyarrow` and `dask.distributed` together this issue will prevent me from upgrading to `0.13` which I'm otherwise very keen to do."
        },
        {
            "created_at": "2019-04-11T05:57:49.197Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16815095) by Wes McKinney (wesm):*\nI'm on vacation until April 22. Are you able to submit a PR?"
        },
        {
            "created_at": "2019-04-15T19:32:06.703Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818320) by Antoine Pitrou (apitrou):*\nIs it just with `cloudpickle` or also with plain `pickle`?"
        },
        {
            "created_at": "2019-04-15T19:45:12.293Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818331) by Martin Durant (mdurant):*\nCloudpickle actually does a better job than ordinary pickle, which also has trouble with the opener function closure attached to the dataset instance and the pieces."
        },
        {
            "created_at": "2019-04-16T08:22:06.939Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818763) by Antoine Pitrou (apitrou):*\n`[~mdurant]` At which one point did this work? I've tried all PyArrow versions from 0.7 to 0.13 and serializing a ParquetDataset always fails with the same error you're reporting."
        },
        {
            "created_at": "2019-04-16T09:15:41.373Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818813) by Krisztian Szucs (kszucs):*\n`[~pitrou]` I'm starting to implement the reducers to make cloudpickle working. "
        },
        {
            "created_at": "2019-04-16T09:46:34.103Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818845) by Krisztian Szucs (kszucs):*\nI also don't understand how could have it been working without proper `___reduce___` methods (at least pickling fails with 0.12 too)."
        },
        {
            "created_at": "2019-04-16T09:56:47.993Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818854) by Antoine Pitrou (apitrou):*\nThat's what I said above: picking fails from 0.7 to 0.13 at least."
        },
        {
            "created_at": "2019-04-16T13:00:42.701Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16818983) by Krisztian Szucs (kszucs):*\nSo this seems like a new feature rather than a bug. I suppose something must have changed on the caller/dask side."
        },
        {
            "created_at": "2019-04-16T13:14:53.330Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16819005) by Martin Durant (mdurant):*\nWell this is confusing! We don't explicitly have pyarrow-parquet-in-distributed tests (apparently we should), but people have certainly been reading parquet for some time. Could they all have been using fastparquet? That seems unlikely, especially in HDFS contexts."
        },
        {
            "created_at": "2019-04-16T13:32:19.083Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16819024) by Matthew Rocklin (mrocklin):*\nMost objects in Python are serializable by default.  My guess is that this object recently gained a non-serializable attribute, perhaps like an open file?"
        },
        {
            "created_at": "2019-04-16T13:38:06.647Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16819027) by Antoine Pitrou (apitrou):*\n`[~mrocklin]` Can you post a snippet that used to work and that doesn't work anymore?"
        },
        {
            "created_at": "2019-04-17T20:04:40.249Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16820463) by Sarah Bird (birdsarah):*\nThis might not be what you were looking for `[~pitrou]` but this is what breaks consistently for me from 0.12.1 to 0.13.0: \r\n\r\n```python\n\r\nimport dask.dataframe as dd\r\nfrom dask.distributed import ClientClient()\r\ndf = dd.read_parquet('my_data.parquet', engine='pyarrow')\r\ndf.head()\r\n```\r\n\u00a0\r\n(dask 1.2.0, distributed 1.27.0)\r\n\r\nLet me know if I can better help."
        },
        {
            "created_at": "2019-04-17T20:11:28.995Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16820466) by Krisztian Szucs (kszucs):*\n`[~birdsarah]` with any parquet file?"
        },
        {
            "created_at": "2019-04-17T20:29:33.250Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16820480) by Sarah Bird (birdsarah):*\n`[~kszucs]` this is with variations on my dataset. I have attached one piece which is sufficient to reproduce the error. It is web crawl data. The dtypes are:\r\n```Java\n\r\nargument_0                      object\r\nargument_1                      object\r\nargument_2                      object\r\nargument_3                      object\r\nargument_4                      object\r\nargument_5                      object\r\nargument_6                      object\r\nargument_7                      object\r\narguments                       object\r\narguments_len                    int64\r\ncall_stack                      object\r\ncrawl_id                         int32\r\ndocument_url                    object\r\nfunc_name                       object\r\nin_iframe                         bool\r\noperation                       object\r\nscript_col                       int64\r\nscript_line                      int64\r\nscript_loc_eval                 object\r\nscript_url                      object\r\nsymbol                          object\r\ntime_stamp         datetime64[ns, UTC]\r\ntop_level_url                   object\r\nvalue_1000                      object\r\nvalue_len                        int64\r\nvisit_id                         int64\r\ndtype: object\r\n```\r\nMy traceback is:\r\n```java\n\r\ndistributed.protocol.pickle - INFO - Failed to serialize (<function safe_head at 0x7f3d57f2c7b8>, (<function _read_pyarrow_parquet_piece at 0x7f3d57ef9268>, <dask.bytes.local.LocalFileSystem object at 0x7f3db58\r\nea4e0>, ParquetDatasetPiece('javascript_10percent_value_1000_only.parquet/part.0.parquet', row_group=None, partition_keys=[]), ['argument_0', 'argument_1', 'argument_2', 'argument_3', 'argument_4', 'argument_5'\r\n, 'argument_6', 'argument_7', 'arguments', 'arguments_len', 'call_stack', 'crawl_id', 'document_url', 'func_name', 'in_iframe', 'operation', 'script_col', 'script_line', 'script_loc_eval', 'script_url', 'symbol\r\n', 'time_stamp', 'top_level_url', 'value_1000', 'value_len', 'visit_id'], [], False, None, []), 5). Exception: no default __reduce__ due to non-trivial __cinit__\r\ndistributed.protocol.core - CRITICAL - Failed to Serialize\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\nTraceback (most recent call last):\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/core.py\", line 54, in dumps\r\n\u00a0\u00a0\u00a0 for key, value in data.items()\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/core.py\", line 55, in <dictcomp>\r\n\u00a0\u00a0\u00a0 if type(value) is Serialize}\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 164, in serialize\r\n\u00a0\u00a0\u00a0 raise TypeError(msg, str(x)[:10000])\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\nTypeError: ('Could not serialize object of type tuple.', \"(<function safe_head at 0x7f3d57f2c7b8>, (<function _read_pyarrow_parquet_piece at 0x7f3d57ef9268>, <dask.bytes.local.LocalFileSystem object at 0x7f3db5\r\n8ea4e0>, ParquetDatasetPiece('javascript_10percent_value_1000_only.parquet/part.0.parquet', row_group=None, partition_keys=[]), ['argument_0', 'argument_1', 'argument_2', 'argument_3', 'argument_4', 'argument_5\r\n', 'argument_6', 'argument_7', 'arguments', 'arguments_len', 'call_stack', 'crawl_id', 'document_url', 'func_name', 'in_iframe', 'operation', 'script_col', 'script_line', 'script_loc_eval', 'script_url', 'symbo\r\nl', 'time_stamp', 'top_level_url', 'value_1000', 'value_len', 'visit_id'], [], False, None, []), 5)\")\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\ndistributed.comm.utils - INFO - Unserializable Message: [{'op': 'update-graph', 'tasks': {\"('head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": <Serialize: ('read-parquet-head-1-5-read-parquet-daacce\r\ne11e9cff29ad1ee5622ffd6c69', 0)>, \"('read-parquet-head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": <Serialize: (<function safe_head at 0x7f3d57f2c7b8>, (<function _read_pyarrow_parquet_piece at 0x7\r\nf3d57ef9268>, <dask.bytes.local.LocalFileSystem object at 0x7f3db58ea4e0>, ParquetDatasetPiece('javascript_10percent_value_1000_only.parquet/part.0.parquet', row_group=None, partition_keys=[]), ['argument_0', '\r\nargument_1', 'argument_2', 'argument_3', 'argument_4', 'argument_5', 'argument_6', 'argument_7', 'arguments', 'arguments_len', 'call_stack', 'crawl_id', 'document_url', 'func_name', 'in_iframe', 'operation', 's\r\ncript_col', 'script_line', 'script_loc_eval', 'script_url', 'symbol', 'time_stamp', 'top_level_url', 'value_1000', 'value_len', 'visit_id'], [], False, None, []), 5)>}, 'dependencies': {\"('head-1-5-read-parquet\r\n-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": [\"('read-parquet-head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\"], \"('read-parquet-head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": []}, 'keys'\r\n: [\"('head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\"], 'restrictions': {}, 'loose_restrictions': None, 'priority': {\"('read-parquet-head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": 0,\r\n\u00a0\"('head-1-5-read-parquet-daaccee11e9cff29ad1ee5622ffd6c69', 0)\": 1}, 'user_priority': 0, 'resources': None, 'submitting_task': None, 'retries': None, 'fifo_timeout': '60s', 'actors': None}]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\r\ndistributed.comm.utils - ERROR - ('Could not serialize object of type tuple.', \"(<function safe_head at 0x7f3d57f2c7b8>, (<function _read_pyarrow_parquet_piece at 0x7f3d57ef9268>, <dask.bytes.local.LocalFileSys\r\ntem object at 0x7f3db58ea4e0>, ParquetDatasetPiece('javascript_10percent_value_1000_only.parquet/part.0.parquet', row_group=None, partition_keys=[]), ['argument_0', 'argument_1', 'argument_2', 'argument_3', 'ar\r\ngument_4', 'argument_5', 'argument_6', 'argument_7', 'arguments', 'arguments_len', 'call_stack', 'crawl_id', 'document_url', 'func_name', 'in_iframe', 'operation', 'script_col', 'script_line', 'script_loc_eval'\r\n, 'script_url', 'symbol', 'time_stamp', 'top_level_url', 'value_1000', 'value_len', 'visit_id'], [], False, None, []), 5)\")\r\nTraceback (most recent call last):\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/batched.py\", line 94, in _background_send\r\n\u00a0\u00a0\u00a0 on_error='raise')\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\r\n\u00a0\u00a0\u00a0 value = future.result()\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/tornado/gen.py\", line 736, in run\r\n\u00a0\u00a0\u00a0 yielded = self.gen.throw(*exc_info)\u00a0 # type: ignore\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 224, in write\r\n\u00a0\u00a0\u00a0 'recipient': self._peer_addr})\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\r\n\u00a0\u00a0\u00a0 value = future.result()\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/tornado/gen.py\", line 736, in run\r\n\u00a0\u00a0\u00a0 yielded = self.gen.throw(*exc_info)\u00a0 # type: ignore\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/comm/utils.py\", line 50, in to_frames\r\n\u00a0\u00a0\u00a0 res = yield offload(_to_frames)\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/tornado/gen.py\", line 729, in run\r\n\u00a0\u00a0\u00a0 value = future.result()\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/concurrent/futures/_base.py\", line 425, in result\r\n\u00a0\u00a0\u00a0 return self.__get_result()\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\r\n\u00a0\u00a0\u00a0 raise self._exception\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\r\n\u00a0\u00a0\u00a0 result = self.fn(*self.args, **self.kwargs)\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/comm/utils.py\", line 43, in _to_frames\r\n\u00a0\u00a0\u00a0 context=context))\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/core.py\", line 54, in dumps\r\n\u00a0\u00a0\u00a0 for key, value in data.items()\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/core.py\", line 55, in <dictcomp>\r\n\u00a0\u00a0\u00a0 if type(value) is Serialize}\r\n\u00a0 File \"/home/bird/miniconda3/envs/pyarrowtest/lib/python3.7/site-packages/distributed/protocol/serialize.py\", line 164, in serialize\r\n\u00a0\u00a0\u00a0 raise TypeError(msg, str(x)[:10000])\r\nTypeError: ('Could not serialize object of type tuple.', \"(<function safe_head at 0x7f3d57f2c7b8>, (<function _read_pyarrow_parquet_piece at 0x7f3d57ef9268>, <dask.bytes.local.LocalFileSystem object at 0x7f3db5\r\n8ea4e0>, ParquetDatasetPiece('javascript_10percent_value_1000_only.parquet/part.0.parquet', row_group=None, partition_keys=[]), ['argument_0', 'argument_1', 'argument_2', 'argument_3', 'argument_4', 'argument_5\r\n', 'argument_6', 'argument_7', 'arguments', 'arguments_len', 'call_stack', 'crawl_id', 'document_url', 'func_name', 'in_iframe', 'operation', 'script_col', 'script_line', 'script_loc_eval', 'script_url', 'symbo\r\nl', 'time_stamp', 'top_level_url', 'value_1000', 'value_len', 'visit_id'], [], False, None, []), 5)\")\n```\r\n[part.0.parquet](part.0.parquet)"
        },
        {
            "created_at": "2019-04-17T20:36:20.959Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16820483) by Sarah Bird (birdsarah):*\nI should add: I can also get the cloudpickle error that is reported by the original report with the same file, but it is invariant between pyarrow 0.12.1 and 0.13.0. The above distributed error is what is changing for me."
        },
        {
            "created_at": "2019-04-19T11:22:52.574Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16821852) by Krisztian Szucs (kszucs):*\nIssue resolved by pull request 4156\n<https://github.com/apache/arrow/pull/4156>"
        },
        {
            "created_at": "2019-04-22T12:04:44.764Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16823055) by Wes McKinney (wesm):*\n`[~mrocklin]` We might be able to do a 0.13.1 release including this and some other fixes in a week or two. If that would be helpful let us know. \r\n\r\nFWIW, ParquetDatasetPiece is regarded as an implementation detail and wasn't intended necessarily to be serializable. So will keep this in mind for the future"
        },
        {
            "created_at": "2019-04-22T14:58:42.298Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5144?focusedCommentId=16823161) by Matthew Rocklin (mrocklin):*\nThat would be helpful, yes.  We're currently raising an error in master telling people to downgrade.  We get bug reports about this issue most days it seems."
        }
    ]
}