{
    "issue": {
        "title": "[R] read_parquet returns Invalid UTF8 payload",
        "body": "***Note**: This issue was originally created as [ARROW-12162](https://issues.apache.org/jira/browse/ARROW-12162). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n## EDIT:\r\n\r\nI've found a solution for my specific use case. If I add the argument `encoding=\"latin1\"` to the `DBI::dbConnect` function, then everything works! This issue might still be valid for other cases where Parquet tries to save invalid data though. It would be nice to get an error on write, rather than on read!\r\n## Background\r\n\r\nI am using the R arrow library.\r\n\r\nI am reading from an SQL Server database with the `latin1` encoding using `dbplyr` and saving the output as a parquet file:\u00a0\r\n```java\n\r\n# Assume `con` is a previously established connection to the database created with DBI::dbConnect\r\ntbl(con, in_schema(\"dbo\", \"latin1_table\")) %>%\r\n\r\n\u00a0 collect() %>%\r\n\r\n\u00a0 write_parquet(\"output.parquet\")\r\n```\r\n\u00a0\r\n\r\nHowever, when I try to read the file back, I get the error \"Invalid UTF8 payload\":\u00a0\r\n```java\n\r\n> read_parquet(\"output.parquet\")\r\n\r\nError: Invalid: Invalid UTF8 payload\r\n```\r\n\u00a0\r\n\r\nWhat I would really like is a way to tell arrow \"This data is latin1 encoded. Please convert it to UTF-8 before you save it as a Parquet file\".\r\n\r\nOr alternatively \"This Parquet file contains latin1 encoded data\".\r\n## Minimal Reproducible Example\r\n\r\nI have isolated this issue to a minimal reproducible example.\r\n\r\nIf the database table contains the latin1 single quote character, then it will trigger the error.\r\n\r\nI have attached a `.rds` file which contains an example tibble.\r\n\r\nTo reproduce, run the following:\u00a0\r\n```java\n\r\nreadRDS(file.path(data_dir, \"bad_char.rds\")) %>% write_parquet(file.path(data_dir, \"bad_char.parquet\"))\r\n\r\nread_parquet(file.path(data_dir, \"bad_char.parquet\"))\r\n```\r\n## Possibly related issues\r\n\r\nhttps://issues.apache.org/jira/browse/ARROW-12007",
        "created_at": "2021-03-31T03:08:03.000Z",
        "updated_at": "2021-08-17T14:06:21.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-08-17T14:06:20.000Z"
    },
    "comments": [
        {
            "created_at": "2021-05-04T09:37:52.127Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12162?focusedCommentId=17338873) by Pal (PalamitoGal):*\nHi,\r\n\r\nI encounter the same issue. I have a dataset with some character strings non-UTF8. The dataset works fine under Arrow 2.0.0 but not with 3 or 4. When I read those columns I get \"Erreur : Invalid: Invalid UTF8 payload\".\r\n\r\nHow can I fix this issue without rebuilding the dataset ?\r\n\r\nThanks"
        },
        {
            "created_at": "2021-05-05T16:20:38.970Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12162?focusedCommentId=17339750) by Nicola Crane (thisisnic):*\nThanks for the bug report and the reproducible example, and great that you found a solution there in adding in the encoding to the dbConnect function.\r\n\r\nIt looks like what has happened here is that without alternative encoding specified, the default in R is to presume that the data is UTF-8 encoded, and so it was written out fine as there are not checks there but failed the validation checks when read back in.\r\n\r\nIf encoding is set to something other than UTF-8, the R Arrow code will convert it to UTF-8 when writing, which is why your solution works - the data has encoding set to \"latin1\", and so the Arrow R code converts it to UTF-8 correctly before writing the Parquet file. An alternative, if you were working with data that was currently in R, would be set the encoding manually, e.g. \r\n```java\n\r\nEncoding(col) <- \"latin1\"\n```\r\n\u00a0"
        }
    ]
}