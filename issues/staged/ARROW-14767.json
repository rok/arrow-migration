{
    "issue": {
        "title": "[C++][Parquet] Preserve the bithwidth of the integer dictionary indices on rountrip to Parquet?",
        "body": "***Note**: This issue was originally created as [ARROW-14767](https://issues.apache.org/jira/browse/ARROW-14767). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen converting from a pandas dataframe to a table, categorical variables are by default given an index type int8 (presumably because there are fewer than 128 categories) in the schema. When this is written to a parquet file, the schema changes such that the index type is int32 instead. This causes an inconsistency between the schemas of tables derived from pandas and those read from disk.\r\n\r\nA minimal recreation of the issue is as follows:\r\n```java\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [\"a\", \"a\", \"b\", \"c\", \"b\"]})\r\ndtypes = {\r\n\u00a0 \u00a0 \"A\": np.dtype(\"int8\"),\r\n\u00a0 \u00a0 \"B\": pd.CategoricalDtype(categories=[\"a\", \"b\", \"c\"], ordered=None),\r\n}\r\ndf = df.astype(dtypes)\r\n\r\ntbl = pa.Table.from_pandas(\r\n\u00a0 \u00a0 df, \r\n) \u00a0\r\nwhere = \"tmp.parquet\"\r\nfilesystem = pa.fs.LocalFileSystem()\r\n\r\npq.write_table(\r\n\u00a0 \u00a0 tbl,\r\n\u00a0 \u00a0 filesystem.open_output_stream(\r\n\u00a0 \u00a0 \u00a0 \u00a0 where,\r\n\u00a0 \u00a0 \u00a0 \u00a0 compression=None,\r\n\u00a0 \u00a0 ),\r\n\u00a0 \u00a0 version=\"2.0\",\r\n)\r\n\r\nschema = tbl.schema\r\n\r\nread_schema = pq.ParquetFile(\r\n\u00a0 \u00a0 filesystem.open_input_file(where),\r\n).schema_arrow\n```\r\nBy printing schema and read_schema, you can the inconsistency.\r\n\r\nI have workarounds in place for this, but am raising the issue anyway so that you can resolve it properly.",
        "created_at": "2021-11-18T13:46:07.000Z",
        "updated_at": "2021-11-23T12:57:16.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-11-23T12:48:33.386Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14767?focusedCommentId=17447989) by Joris Van den Bossche (jorisvandenbossche):*\nThanks for the report!\r\n\r\nI can reproduce your issue. Looking at the written Parquet file's schema, you can see that the column \"B\" is indicated to be of type \"String\":\r\n\r\n```Java\n\r\nIn [29]: parquet_metadata = pq.ParquetFile(\r\n    ...:     filesystem.open_input_file(where),\r\n    ...: )\r\n\r\nIn [30]: parquet_metadata.schema\r\nOut[30]: \r\n<pyarrow._parquet.ParquetSchema object at 0x7f60be97fe00>\r\nrequired group field_id=-1 schema {\r\n  optional int32 field_id=-1 A (Int(bitWidth=8, isSigned=true));\r\n  optional binary field_id=-1 B (String);\r\n}\r\n```\r\n\r\nParquet still uses dictionary encoding for compression reasons, but it is not a separate type in their type system.\r\n\r\nArrow has the ability to read such dictionary encoded string columns directly into an Arrow dictionary type. But at that point it doesn't preserve the int8 type for the indices, since that is not part of the parquet spec, and rather uses Arrow's default of int32 indices.\r\n\r\nNow, we do write the Arrow schema into the Parquet file metadata, so we can use that information to ensure a more faithful roundtrip of Arrow tables to Parquet and back.\r\n\r\nThat doesn't happen here, but in principle could be done (although that will probably require an additional cast after the fact)"
        }
    ]
}