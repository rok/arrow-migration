{
    "issue": {
        "title": "[Python] pyarrow.parquet.read_* should use pre_buffer=True",
        "body": "***Note**: This issue was originally created as [ARROW-12428](https://issues.apache.org/jira/browse/ARROW-12428). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIf the user is synchronously reading a single file, we should try to read it as fast as possible. The one sticking point might be whether it's beneficial to enable this no matter the filesystem or whether we should try to only enable it on high-latency filesystems.",
        "created_at": "2021-04-16T18:08:00.000Z",
        "updated_at": "2021-05-05T07:32:54.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-05-05T07:32:54.000Z"
    },
    "comments": [
        {
            "created_at": "2021-04-16T19:35:22.357Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17324041) by David Li (lidavidm):*\nHere's a quick comparison between Pandas/S3FS and PyArrow with a pre_buffer option implemented:\r\n\r\n```\n\r\nPython: 3.9.2\r\nPandas: 1.2.3\r\nPyArrow: 5.0.0 master (9c1e5bd19347635ea9f373bcf93f2cea0231d50a)\r\n\r\nPandas/S3FS: 107.31099020410329 seconds\r\nPandas/S3FS (no readahead): 676.9701101030223 seconds\r\nPyArrow: 213.81073790509254 seconds\r\nPyArrow (pre-buffer): 29.330630503827706 seconds\r\nPandas/S3FS (pre-buffer): 54.61801828909665 seconds\r\nPandas/S3FS (pre-buffer, no readahead): 46.7531590978615 seconds \n```\r\n\r\n```python\n\r\nimport time\r\nimport pandas as pd\r\nimport pyarrow.parquet as pq\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\")\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS:\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n})\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\")\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow:\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n}, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer, no readahead):\", duration, \"seconds\")\r\n```"
        },
        {
            "created_at": "2021-04-16T19:39:07.408Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17324043) by David Li (lidavidm):*\nAnd for local files, to confirm that pre_buffer isn't a negative:\r\n```\n\r\nPandas: 14.584974920144305 seconds\r\nPyArrow: 6.650648137088865 seconds\r\nPyArrow (pre-buffer): 6.587288308190182 seconds\r\n```\r\nThis is on a system with NVME storage, so results may vary for spinning-rust or SATA SSDs.\r\n\r\n(Updated results to read once without measuring before taking the measurement, in case disk cache is a factor)"
        },
        {
            "created_at": "2021-04-16T20:21:55.374Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17324063) by David Li (lidavidm):*\nFinally, if we perform column selection, fsspec's readahead is actually extremely detrimental:\r\n```\n\r\nPandas/S3FS (no pre-buffer): 88.26093492098153 seconds\r\nPandas/S3FS (pre-buffer): 107.76374901900999 seconds\r\nPyArrow (no pre-buffer): 55.75352717819624 seconds\r\nPyArrow (pre-buffer): 9.941459016874433 seconds \n```\r\n\r\n```python\n\r\ncolumns = ['vendor_id', 'pickup_latitude', 'pickup_longitude', 'extra']\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (no pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (pre-buffer):\", duration, \"seconds\")\r\n```"
        },
        {
            "created_at": "2021-04-19T08:07:23.971Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17324821) by Joris Van den Bossche (jorisvandenbossche):*\n`[~lidavidm]` small comment on the benchmark code: for the pyarrow cases, you need to add a `.to_pandas()` call for it to be equivalent with the pandas pd.read_parquet version (although I would expect this not be that significant compared to reading from S3). \r\n(the `read_pandas` is a bit confusing name, but it still reads into a pyarrow.Table, it only uses the pandas metadata by default to eg ensure to read the pandas index column as well)"
        },
        {
            "created_at": "2021-04-19T11:57:46.229Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17324968) by David Li (lidavidm):*\nD'oh, and you already explained this in the SO question :) I'll re-run the benchmarks to make sure they're fair."
        },
        {
            "created_at": "2021-04-19T12:39:02.677Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17325004) by David Li (lidavidm):*\n```\n\r\nWhole file:\r\nPandas/S3FS (no pre-buffer, no readahead): 692.2505334559828 seconds\r\nPandas/S3FS (no pre-buffer, readahead): 99.55904859001748 seconds\r\nPandas/S3FS (pre-buffer, no readahead): 39.282157234149054 seconds\r\nPandas/S3FS (pre-buffer, readahead): 41.564441804075614 seconds\r\nPyArrow (no pre-buffer): 242.97687190794386 seconds\r\nPyArrow (pre-buffer): 39.5321765630506 seconds\r\n===\r\nColumn selection:\r\nPandas/S3FS (no pre-buffer, no readahead): 153.64498204295523 seconds\r\nPandas/S3FS (no pre-buffer, readahead): 82.44589220592752 seconds\r\nPandas/S3FS (pre-buffer, no readahead): 114.55768134980462 seconds\r\nPandas/S3FS (pre-buffer, readahead): 133.1232347697951 seconds\r\nPyArrow (no pre-buffer): 54.11452938010916 seconds\r\nPyArrow (pre-buffer): 12.865494727157056 seconds\r\n```\r\n```python\n\r\nimport time\r\nimport pandas as pd\r\nimport pyarrow.fs\r\nimport pyarrow.parquet as pq\r\n\r\ncolumns = ['vendor_id', 'pickup_latitude', 'pickup_longitude', 'extra']\r\n\r\nprint(\"Whole file:\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n}, pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no pre-buffer, no readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no pre-buffer, readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n}, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer, no readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer, readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=False).to_pandas()\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (no pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", pre_buffer=True).to_pandas()\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (pre-buffer):\", duration, \"seconds\")\r\n\r\nprint(\"===\")\r\nprint(\"Column selection:\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n}, columns=columns, pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no pre-buffer, no readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=False)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (no pre-buffer, readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", storage_options={\r\n    'default_block_size': 1,  # 0 is ignored\r\n    'default_fill_cache': False,\r\n}, columns=columns, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer, no readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pd.read_parquet(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=True)\r\nduration = time.monotonic() - start\r\nprint(\"Pandas/S3FS (pre-buffer, readahead):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=False).to_pandas()\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (no pre-buffer):\", duration, \"seconds\")\r\n\r\nstart = time.monotonic()\r\ndf = pq.read_pandas(\"s3://ursa-labs-taxi-data/2012/01/data.parquet\", columns=columns, pre_buffer=True).to_pandas()\r\nduration = time.monotonic() - start\r\nprint(\"PyArrow (pre-buffer):\", duration, \"seconds\")\r\n```"
        },
        {
            "created_at": "2021-05-05T07:32:54.936Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12428?focusedCommentId=17339483) by Joris Van den Bossche (jorisvandenbossche):*\nIssue resolved by pull request 10074\n<https://github.com/apache/arrow/pull/10074>"
        }
    ]
}