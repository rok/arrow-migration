{
    "issue": {
        "title": "[Python] `pyarrow._plasma.PlasmaClient.delete` behavior undocumented",
        "body": "***Note**: This issue was originally created as [ARROW-12872](https://issues.apache.org/jira/browse/ARROW-12872). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi all,\r\n\r\nI've been using plasma to speed up some multiprocessing I'm doing and have had issues where my plasma server runs out of memory even though my logs show that I should have plenty of space accounting for `size_created - size_deleted` . The documentation for plasma is [a bit scarce\\|#using-arrow-and-pandas-with-plasma] and does not mention how to use the `.delete` method but I'd expect that running a `.delete([oid])` would free up that space immediately which does not seem to be the case. The tests for this are [actually commented out](https://github.com/apache/arrow/blob/b4f2c1c72f745acf12e8a6d2d031750745ab2de2/python/pyarrow/tests/test_plasma.py#L592), and doing some testing of my own I've found that delete actually working is implicitly linked to some reference counter on the buffer. This kinda makes sense from the zero-copy perspective but makes it really difficult to keep a lid on the amount of data in the plasma store as its not immediately clear which refs are hanging around where. Should I be explicitly copying buffer data after pulling from plasma to make these deletes happen (if so how do i do that)?\r\n\r\nFor what its worth I've captured this behavior inline below, if someone could just tell me if this is expected and if there's an easy work around I'd really appreciate that. I'm sure the docs could use a bit of love too.\r\n```java\n\r\nimport pyarrow as pa\r\nimport pyarrow.plasma as pl\r\n\r\ndef table_to_plasma(\r\n    table: pa.Table,\r\n    cli: pl.PlasmaClient,\r\n) -> pl.ObjectID:\r\n    batches = table.to_batches()\r\n    size = sum(b.nbytes for b in batches)\r\n# actual buffer space is a tiny bit more than the size of the tables so add\r\n# some wiggle room\r\n    size = int(max(size * 1.01, size + 512))\r\n    oid = pl.ObjectID.from_random()\r\n    buf = cli.create(oid, size)\r\n    writer = pa.ipc.new_stream(\r\n        pa.FixedSizeBufferWriter(buf), batches[0].schema\r\n    )\r\n    for b in batches:\r\n        writer.write_batch(b)\r\n    writer.close()\r\n    cli.seal(oid)\r\n    return oid\r\n\r\ndef table_from_plasma(\r\n    oid: pl.ObjectID,\r\n    cli:pl.PlasmaClient,\r\n) -> pa.Table:\r\n    buf = cli.get_buffers([oid])\r\n    assert len(buf) == 1\r\n    buf = buf[0]\r\n    stream = pa.ipc.open_stream(buf)\r\n    return stream.read_all()\r\n\r\ndef test():\r\n    t = pa.table([pa.array([1])], schema=pa.schema([pa.field('a', pa.int64())]))\r\n    with pl.start_plasma_store(int(1e8)) as (pl_name, pl_proc):\r\n        cli = pl.connect(pl_name)\r\n        oid = table_to_plasma(t, cli)\r\n        t2 = table_from_plasma(oid, cli)\r\n        assert len(t2) == len(t)\r\n        cli.delete([oid])\r\n        assert not cli.contains(oid)  # this unexpectedly fails\r\n        \r\n        del t2\r\n        import gc\r\n        gc.collect()\r\n        assert not cli.contains(oid)  # this succeeds\r\n\r\n```",
        "created_at": "2021-05-25T13:23:35.000Z",
        "updated_at": "2021-06-01T08:46:22.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++ - Plasma",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}