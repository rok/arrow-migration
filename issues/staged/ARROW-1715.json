{
    "issue": {
        "title": "[Python] Implement pickling for Column, ChunkedArray, RecordBatch, Table",
        "body": "***Note**: This issue was originally created as [ARROW-1715](https://issues.apache.org/jira/browse/ARROW-1715). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nAt the moment the types `pyarrow.Column/ChunkedArray/RecordBatch/Table` cannot be pickled. Although it may not be the fastest way to transport them from one process to another, it is a very comfortable one. We should implement a `__reduce__()` for all of them.",
        "created_at": "2017-10-23T18:54:22.000Z",
        "updated_at": "2018-07-19T20:23:57.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2018-07-19T20:23:54.000Z"
    },
    "comments": [
        {
            "created_at": "2018-01-28T12:34:34.809Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16342559) by Uwe Korn (uwe):*\n`[~wesmckinn]` Do you have a better idea here on how to do simple serialization instead of wrapping the Array and Column into small RecordBatches and then using the IPC code?"
        },
        {
            "created_at": "2018-01-29T21:00:19.623Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16344015) by Wes McKinney (wesm):*\nNot really, this will introduce a little bit of overhead but I'm not sure what are the best alternatives \u2013 we could support \"Pythonizing\" the internal `ArrayData` object and serializing that"
        },
        {
            "created_at": "2018-03-22T15:35:44.578Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16409731) by Antoine Pitrou (apitrou):*\nIt seems pickling arrays can simply reuse `Array.buffers()` and `Array.from_buffers()` (once ARROW-2281 is resolved). That will involve copying data but that's unavoidable with pickle.\r\n\r\nIs there any advantage in using the IPC code instead?"
        },
        {
            "created_at": "2018-03-22T16:53:35.760Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16409880) by Uwe Korn (uwe):*\nNo, the advantage of the IPC code was that it is already there. Using `buffers()` and `from_buffers()`\u00a0is definitely the better approach as this will allow pickling with less copies of the data."
        },
        {
            "created_at": "2018-03-22T23:38:50.589Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16410565) by Wes McKinney (wesm):*\nIt'd be worth comparing a naive pickle version with a pickle version using `pyarrow.serialize(...).to_components()`. The component-based serialization minimizes copying. I'm due to write a blog post about it, as the difference with large objects can be quite significant"
        },
        {
            "created_at": "2018-03-23T09:00:11.484Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16411037) by Antoine Pitrou (apitrou):*\n`pa.serialize()` currently doesn't seem to support Arrow arrays or other arrow types."
        },
        {
            "created_at": "2018-03-23T15:30:22.257Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16411571) by Wes McKinney (wesm):*\nRight, what I'm saying is that we could unpack the Array into its buffer + metadata representation (to use `from_buffers` on the way back from pickle). I could take a shot at this, maybe next week. I'm not sure which strategy will be more efficient without some closer analysis. "
        },
        {
            "created_at": "2018-03-23T15:33:30.075Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16411574) by Antoine Pitrou (apitrou):*\nI think it will be down to differences in metadata serialization overhead, because in both cases you end up copying the buffer contents into the pickle stream (and actually there's a second copy before that, as pickle will only allow serializing a bytes object, not a zero-copy buffer)."
        },
        {
            "created_at": "2018-06-29T14:51:15.650Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16527758) by Antoine Pitrou (apitrou):*\nRelated: ARROW-2660 (zero-copy pickling)."
        },
        {
            "created_at": "2018-07-08T03:36:45.690Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16535982) by Dave Hirschfeld (dhirschfeld):*\nThis has come up in the context of dask.distributed also:\r\nhttps://github.com/dask/distributed/issues/2103"
        },
        {
            "created_at": "2018-07-19T20:23:54.658Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1715?focusedCommentId=16549807) by Uwe Korn (uwe):*\nIssue resolved by pull request 2292\n<https://github.com/apache/arrow/pull/2292>"
        }
    ]
}