{
    "issue": {
        "title": "[Python] Scanning batch size is limited to 65536 (2**16).",
        "body": "***Note**: This issue was originally created as [ARROW-16015](https://issues.apache.org/jira/browse/ARROW-16015). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n[Scanning batches](https://arrow.apache.org/docs/python/dataset.html#iterative-out-of-core-or-streaming-reads)\u00a0is documented to default to a batch size of 1,000,000. But the behavior is that batch size defaults to - and is limited to - 65536.\r\n```python\n\r\nIn []: dataset.count_rows()\r\nOut[]: 538038292\r\n\r\nIn []: next(dataset.to_batches()).num_rows\r\nOut[]: 65536\r\n\r\nIn []: next(dataset.to_batches(batch_size=10**6)).num_rows\r\nOut[]: 65536\r\n\r\nIn []: next(dataset.to_batches(batch_size=10**4)).num_rows\r\nOut[]: 10000\r\n```\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2022-03-24T03:29:35.000Z",
        "updated_at": "2022-03-24T18:39:54.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-03-24T04:37:23.640Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16015?focusedCommentId=17511588) by Weston Pace (westonpace):*\nThe batch size can split up larger row groups to fit small batch sizes but it won't, at the moment, merge together small row groups to fit large batch sizes.  Performance-wise this tends to be expensive (you'd need to allocate space big enough for both and then copy the data) but I can see how it might be useful is some scenarios."
        },
        {
            "created_at": "2022-03-24T11:06:55.449Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16015?focusedCommentId=17511785) by Joris Van den Bossche (jorisvandenbossche):*\nWe can maybe clarify in the documentation of `batch_size` that the actual upper limit might depend on your files (such as row group size in Parquet). \r\n\r\nI suppose this is also the same for the record batch size in IPC / Feather files? (or stripe size in ORC files)"
        },
        {
            "created_at": "2022-03-24T18:39:54.550Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16015?focusedCommentId=17512021) by Weston Pace (westonpace):*\nYes.  Clarification would be good.  We could even change the parameter name to `max_batch_size`.  That is what we did in the table source node.  Also, a `min_batch_size` is still useful I think, though we would just want to document that it has performance implications.\r\n\r\nYou are correct that this would apply to IPC & ORC.  This even applies to CSV because the batch size is independent of the block size (which is specified in the CSV read options)."
        }
    ]
}