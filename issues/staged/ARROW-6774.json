{
    "issue": {
        "title": "[Rust] Reading parquet file is slow",
        "body": "***Note**: This issue was originally created as [ARROW-6774](https://issues.apache.org/jira/browse/ARROW-6774). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nUsing the example at\u00a0<https://github.com/apache/arrow/tree/master/rust/parquet>\u00a0is slow.\r\n\r\nThe snippet\u00a0\r\n```none\n\r\nlet\u00a0reader\u00a0=\u00a0SerializedFileReader::new(file).unwrap();\r\nlet\u00a0mut\u00a0iter\u00a0=\u00a0reader.get_row_iter(None).unwrap();\r\nlet\u00a0start\u00a0=\u00a0Instant::now();\r\nwhile\u00a0let\u00a0Some(record)\u00a0=\u00a0iter.next()\u00a0{}\r\nlet\u00a0duration\u00a0=\u00a0start.elapsed();\r\nprintln!(\"{:?}\",\u00a0duration);\r\n```\r\nRuns for 17sec for a ~160MB parquet file.\r\n\r\nIf there is a more effective way to load a parquet file, it would be nice to add it to the readme.\r\n\r\nP.S.: My goal is to construct an ndarray from it, I'd be happy for any tips.",
        "created_at": "2019-10-02T22:39:11.000Z",
        "updated_at": "2021-04-26T11:24:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Rust",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-04-26T11:24:36.000Z"
    },
    "comments": [
        {
            "created_at": "2019-10-03T16:24:47.724Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=16943703) by Wes McKinney (wesm):*\nRow-by-row iteration is going to be slow compared with vectorized / column-by-column reads. This unfinished PR was related to this (I think?) but there are Arrow-based readers available that don't require it\r\n\r\nhttps://github.com/apache/arrow/pull/3461"
        },
        {
            "created_at": "2019-10-03T17:02:23.855Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=16943755) by Adam Lippai (alippai):*\nI've seen some nice work in\u00a0<https://github.com/apache/arrow/blob/master/rust/parquet/src/column/reader.rs>\u00a0and\u00a0<https://github.com/apache/arrow/blob/master/rust/parquet/src/arrow/array_reader.rs>\u00a0but I couldn't figure it out how to use it. `[~liurenjie1024]` \u00a0can you help me perhaps?\u00a0"
        },
        {
            "created_at": "2019-10-05T10:38:16.039Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=16945027) by Renjie Liu (liurenjie1024):*\nThis is part of a reader for reading parquet files into arrow arrays. It's almost complete, and we have still one PR (<https://github.com/apache/arrow/pull/5523>) waiting for review, which contains documentations and examples."
        },
        {
            "created_at": "2019-10-07T10:37:11.985Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=16945776) by Adam Lippai (alippai):*\nWhile it doesn't support reading Utf8 type, dropping that column then reading the same file takes less than 3 seconds! Thank you for the contribution. (size 10k row \\* 3k Float64 columns)"
        },
        {
            "created_at": "2020-02-01T07:47:36.619Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=17028002) by Neville Dipale (nevi_me):*\nHi `[~alippai]`, UTF8 types are now supported. Is the performance still a concern, or can we close this?"
        },
        {
            "created_at": "2020-09-30T22:15:00.262Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=17205061) by Sietse Brouwer (sietsebb):*\nI'm not sure what test data `[~alippai]` \u00a0 used, so I used a test data set with 500k rows and two columns:\r\n \\* a column x containing random floating point numbers,\r\n \\* and a column y where each cell contains a Unicode string of 100 space-separated mostly-cyrillic words.\r\n\r\nSee the attached\u00a0[data.py](data.py). When I saved that 500k-row table as parquet with gzip compression, the resulting file was 174 MB.\r\n\r\nI tried running Adam's test snippet (the code I used is attached as\u00a0[main.rs](main.rs)) while compiling with different versions of parquet:\r\n \\* parquet=0.15.1\r\n \\* parquet=1.0.1\r\n \\* parquet=2.0.0-SNAPSHOT (specifically git:3fae71b10c42 of\u00a02020-09-30).\r\n\r\n**In all three cases running the snippet took almost exactly 150 seconds,** give or take one second.\r\n\r\nDoes that help you decide whether to close the question, [~nevi_me]? Or perhaps your comment, Adam, from 2019-10-07 used some other version to get that speed improvement? Should I change the test to use the ParquetFileArrowReader example in <https://github.com/apache/arrow/blob/3fae71b10c42/rust/parquet/src/arrow/mod.rs#L25-L50,>\u00a0and then this issue can close if that one is faster?"
        },
        {
            "created_at": "2020-09-30T22:23:42.336Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=17205068) by Adam Lippai (alippai):*\n`[~sietsebb]` \u00a0I used the Arrow reader method, that time it didn't support all the types I needed, but later it was added. It's definitely faster, however I don't remember the benchmark."
        },
        {
            "created_at": "2020-10-03T23:46:00.174Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=17206895) by Sietse Brouwer (sietsebb):*\n`[~alippai]`, I can't get parquet::arrow::ParquetFileArrowReader to be faster than parquet::file::reader::SerializedFileReader under commit `3fae71b10c42`. Timings below, code below that, conclusions at the bottom. Interesting times in\u00a0**bold.**\r\n\r\n\u00a0\r\n\n|n_rows|include utf8-column|reader|iteration unit\r<br>_(loop does not iterate over rows within batches)_|time taken|\r|\n|-|-|-|-|-|-|\n|50_000|yes|ParquetFileArrowReader|1 batch of 50k rows|14.9s|\r|\n|50_000|yes|ParquetFileArrowReader|10 batches of 5k rows|14.8s|\r|\n|50_000|yes|ParquetFileArrowReader|50k batches of 1 row|24.0s|\r|\n|50_000|yes|SerializedFileReader|get_row_iter|**14.5s**|\r|\n|\u00a0|\u00a0|\u00a0|\u00a0|\u00a0|\r|\n|50_000|no|ParquetFileArrowReader|1 batch of 50k rows|**143ms**|\r|\n|50_000|no|ParquetFileArrowReader|10 batches of 5k rows|154ms|\r|\n|50_000|no|ParquetFileArrowReader|50k batches of 1 row|6.5s|\r|\n|50_000|no|SerializedFileReader|\u00a0get_row_iter|**211ms**|\r<br>\r<br>\u00a0\r<br>\r<br>Here is the code I used to load the dataset with ParquetFileArrowReader (see also this version of\u00a0[main.rs](main.rs)):\r<br>\r<br>\u00a0\r<br>```java<br>\r<br>fn read_with_arrow(file: File) -> () {\r<br>    let file_reader = SerializedFileReader::new(file).unwrap();\r<br>    let mut arrow_reader = ParquetFileArrowReader::new(Rc::new(file_reader));\r<br>    println!(\"Arrow schema is: {}\", arrow_reader.get_schema().unwrap());\r<br>    let mut record_batch_reader = arrow_reader\r<br>        .get_record_reader(/* batch size */ 50000)\r<br>        .unwrap();\r<br>\r<br>    let start = Instant::now();\r<br>    while let Some(_record) = record_batch_reader.next_batch().unwrap() {\r<br>        // no-op\r<br>    };\r<br>    let duration = start.elapsed();\r<br>\r<br>    println!(\"{:?}\", duration);\r<br>}\r<br>\r<br>```\r<br>\u00a0\r<br>\r<br>Main observations:\r<br> \\* we can't tell whether the slow loading when we include the UTF8 column is because UTF8 is slow to process, or because the column is very big (100 random Russian words per cell).\r<br> \\* When the big UTF-8 column is included, iterating over every row with SerializedFileReader is as fast as iterating over a few batches with\u00a0ParquetFileArrowReader. Even when you skip the rows within the batches!\r<br> \\* Should I try this again with\u00a0\u00a0(size 10k row \\* 3k Float64 columns) plus one small UTF-8 column?\r<br> \\* I'm not even sure what result I'm trying to reproduce-or-falsify here... whether adding a small UTF-8 column causes disproportional slowdown? Or whether switching between\u00a0SerializedFileReader and\u00a0ParquetFileArrowReader causes slowdown? Right now, I feel like everything and nothing is in scope of the issue. I wouldn't mind if somebody made it narrower and clearer.|\n"
        },
        {
            "created_at": "2021-04-26T11:24:34.530Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6774?focusedCommentId=17332039) by Andrew Lamb (alamb):*\nMigrated to github: https://github.com/apache/arrow-rs/issues/55"
        }
    ]
}