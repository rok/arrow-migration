{
    "issue": {
        "title": "[R] \"open_dataset(f) %>% filter(id %in% myvec) %>% collect\" causes  CPP11::unwind_execption, crashed R",
        "body": "***Note**: This issue was originally created as [ARROW-18314](https://issues.apache.org/jira/browse/ARROW-18314). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis is running on a windows environment, arrow 10.0.0 (see arrow_info() below). data size is large maybe\u00a0\r\n\r\nI issued two calls\r\n\r\n```\r\n\r\nft <- path_to_dataset1\r\nfa <- path_to_dataset2\r\n\r\n#1)\r\n\r\ntic()\r\nd2 <- ft %>% open_dataset %>% filter( pis %in% mypis ) %>% collect\r\ntoc()\r\n927.11 sec elapsed\r\n\r\n#returned a dataset with 44 obs, 38 columns, took abnormal time, 16min\r\n\r\n#1)\r\n\r\ntic()\r\nd3 <- fa %>% open_dataset %>% filter( pis %in% mypis ) %>% collect\r\nterminate called after throwing an instance of 'cpp11::unwind_exception'\r\n\r\n```\r\n\r\nThen I got an error that craspad_hendler.exe stopped working. And R becomes frozen, after a while R crashed too.\r\n\r\n![image-2022-11-11-14-59-30-132.png](https://issues.apache.org/jira/secure/attachment/13052122/image-2022-11-11-14-59-30-132.png)\r\n\r\n\u00a0\r\n\r\narrow_info()\r\nArrow package version: 10.0.0\r\n\r\nCapabilities:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\ndataset \u00a0 \u00a0TRUE\r\nsubstrait FALSE\r\nparquet \u00a0 \u00a0TRUE\r\njson \u00a0 \u00a0 \u00a0 TRUE\r\ns3 \u00a0 \u00a0 \u00a0 \u00a0 TRUE\r\ngcs \u00a0 \u00a0 \u00a0 \u00a0TRUE\r\nutf8proc \u00a0 TRUE\r\nre2 \u00a0 \u00a0 \u00a0 \u00a0TRUE\r\nsnappy \u00a0 \u00a0 TRUE\r\ngzip \u00a0 \u00a0 \u00a0 TRUE\r\nbrotli \u00a0 \u00a0 TRUE\r\nzstd \u00a0 \u00a0 \u00a0 TRUE\r\nlz4 \u00a0 \u00a0 \u00a0 \u00a0TRUE\r\nlz4_frame \u00a0TRUE\r\nlzo \u00a0 \u00a0 \u00a0 FALSE\r\nbz2 \u00a0 \u00a0 \u00a0 \u00a0TRUE\r\njemalloc \u00a0FALSE\r\nmimalloc \u00a0 TRUE\r\n\r\nArrow options():\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\narrow.use_threads FALSE\r\n\r\nMemory:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\nAllocator mimalloc\r\nCurrent \u00a0 \u00a00 bytes\r\nMax \u00a0 \u00a0 \u00a0 \u00a00 bytes\r\n\r\nRuntime:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0\r\nSIMD Level \u00a0 \u00a0 \u00a0 \u00a0 \u00a0avx2\r\nDetected SIMD Level avx2\r\n\r\nBuild:\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\nC++ Library Version \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a010.0.0\r\nC++ Compiler \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0GNU\r\nC++ Compiler Version \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 10.3.0\r\nGit ID \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 aa7118b6e5f49b354fa8a93d9cf363c9ebe9a3f0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2022-11-11T18:01:33.000Z",
        "updated_at": "2022-11-18T17:19:13.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-11-17T08:17:16.008Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18314?focusedCommentId=17635195) by Nicola Crane (thisisnic):*\nHi `[~lucasmation]`, thanks for reporting this.  I notice that the code above uses `collect()` - this pulls the data into memory, and so you could be correct that the data size is the issue here.  What is the size of each of those datasets, and how much memory do you have on this machine?\r\n\r\nWhat are the values of `ft %>% open_dataset() %>% nrow()` and  `ft %>% open_dataset %>% filter( pis %in% mypis ) %>% nrow()`, so we can so how much of this data is then being read?"
        },
        {
            "created_at": "2022-11-17T12:57:58.571Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18314?focusedCommentId=17635345) by Lucas Mation (lucasmation):*\n`[~thisisnic]` , the filtered dataset is tiny: 44 obs and 38cols. The original dataset is huge: 801million obs (801435094). The server is large 512Gb or RAM. There are other users sharing the server, but I haven't seen it error due to maxing out the RAM."
        },
        {
            "created_at": "2022-11-18T17:19:13.913Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18314?focusedCommentId=17635963) by Nicola Crane (thisisnic):*\nHmm, not sure what to suggest here, though I wonder if this has similar causes as ARROW-18313"
        }
    ]
}