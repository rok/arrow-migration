{
    "issue": {
        "title": "[C++][Datasets] Improve memory usage of datasets API when scanning parquet",
        "body": "***Note**: This issue was originally created as [ARROW-15410](https://issues.apache.org/jira/browse/ARROW-15410). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis is a more targeted fix to improve memory usage when scanning parquet files.  It is related to broader issues like ARROW-14648 but those will likely take longer to fix.  The goal here is to make it possible to scan large parquet datasets with many files where each file has reasonably sized row groups (e.g. 1 million rows).  Currently we run out of memory scanning a configuration as simple as:\r\n\r\n21 parquet files\r\nEach parquet file has 10 million rows split into row groups of size 1 million",
        "created_at": "2022-01-22T01:40:52.000Z",
        "updated_at": "2022-04-27T19:51:41.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-04-22T22:54:45.000Z"
    },
    "comments": [
        {
            "created_at": "2022-04-22T22:54:45.507Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15410?focusedCommentId=17526713) by Weston Pace (westonpace):*\nIssue resolved by pull request 12228\n<https://github.com/apache/arrow/pull/12228>"
        }
    ]
}