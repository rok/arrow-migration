{
    "issue": {
        "title": "[C++] Create a device abstraction",
        "body": "***Note**: This issue was originally created as [ARROW-2447](https://issues.apache.org/jira/browse/ARROW-2447). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nRight now, a plain Buffer doesn't carry information about where it actually lies. That information also cannot be passed around, so you get APIs like `PlasmaClient` which take or return device number integers, and have implementations which hardcode operations on CUDA buffers. Also, unsuspecting receivers of a `Buffer` pointer may try to act on the underlying memory without knowing whether it's CPU-reachable or not.\r\n\r\nHere is a sketch for a proposed Device abstraction:\r\n```Java\n\r\nclass Device {\r\n    enum DeviceKind { KIND_CPU, KIND_CUDA };\r\n\r\n    virtual DeviceKind kind() const;\r\n    //MemoryPool* default_memory_pool() const;\r\n    //std::shared_ptr<Buffer> Allocate(...);\r\n};\r\n\r\nclass CpuDevice : public Device {};\r\n\r\nclass CudaDevice : public Device {\r\n    int device_num() const;\r\n};\r\n\r\nclass Buffer {\r\n    virtual DeviceKind device_kind() const;\r\n    virtual std::shared_ptr<Device> device() const;\r\n    virtual bool on_cpu() const {\r\n        return true;\r\n    }\r\n\r\n    const uint8_t* cpu_data() const {\r\n        return on_cpu() ? data() : nullptr;\r\n    }\r\n    uint8_t* cpu_mutable_data() {\r\n        return on_cpu() ? mutable_data() : nullptr;\r\n    }\r\n\r\n    virtual CopyToCpu(std::shared_ptr<Buffer> dest) const;\r\n    virtual CopyFromCpu(std::shared_ptr<Buffer> src);\r\n};\r\n\r\nclass CudaBuffer : public Buffer {\r\n    virtual bool on_cpu() const {\r\n        return false;\r\n    }\r\n};\r\n\r\nCopyBuffer(std::shared_ptr<Buffer> dest, const std::shared_ptr<Buffer> src);\r\n```",
        "created_at": "2018-04-11T11:14:02.000Z",
        "updated_at": "2020-02-18T18:16:13.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: GPU",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2020-02-12T23:28:09.000Z"
    },
    "comments": [
        {
            "created_at": "2019-02-01T12:55:38.796Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16758270) by Pearu Peterson (pearu):*\n`[~wesmckinn]`,\u00a0 `[~pitrou]`, and others interested in this:\r\n\r\nI'd like to revive this issue and discuss the memory buffer model on heterogeneous systems where accessing device memory is possible only via copying of some memory content. Other than this restriction, one would wish to treat the device memory buffers in the same way as host memory buffers. That is, operations and algorithms developed for host memory buffer should be applicable to device memory buffer without any extra work required.\r\n\r\nSo, let may lay down some of the key features and issues of this problem as follows.\r\n\r\nIn CUDA, one can allocate memory that is accessible both from host and device without explicit copying. This includes:\r\n(i) managed memory (allocated using `cudaMallocManaged`) where the device driver will handle the copying step on demand triggered by page faults;\r\n(ii) host memory (allocated using `cudaMallocHost`) where host memory is accessible from a device via DMA;\r\n(iii) host memory (allocated using `malloc` or `new`) that is page-locked using `cudaHostRegister`, and again, the host memory is accessible from a device via DMA.\r\n\r\nDEFINITION: The notion \"accessible from a device\" (CPU or GPU) means that with a given memory pointer to data one can read and write data by pointer dereferencing within the device process. This is possible when host and device memories uses the same virtual memory area (VMA) or there exists a mapping between host and device VMAs where the access requires pointer value transformation (but no data copying) using `cudaHostGetDevicePointer`, for instance.\r\n\r\nOn the other hand, one can allocate memory that will not be accessible neither from device nor from host:\r\n(iv) host memory (allocated using `malloc` or `new`) is generally not accessible from device;\r\n(v) device memory (allocate using `cudaMalloc`) is generally not accessible from host.\r\n\r\nFinally, there is also a need to provide a way for one device to access the memory of another device. While CUDA provides various (copy or no-copy) methods for establishing the access between different GPU devices, there exists other accelerators (FPGA, etc) that memory buffers would need to be made accessible by other devices. Although, when direct connection between the two devices is missing, the host RAM can be used as a copy buffer.\r\n\r\nSo, my first suggestion is to revise the title of this issue, \"Create a device abstraction\" as well as the proposed Device abstraction semantics because these consider only the cases (iv) and (v) while the Device abstraction would suggest suboptimal usage for the cases (i), (ii) and (iii). For instance, managed memory could be hold both by `Buffer` as well as `CudaBuffer` while in the latter case unnecessary copies will be made by algorithms that need to access the `CudaBuffer` memory from a host process.\r\n\r\nThe memory buffer abstraction should capture the following data flow cases:\r\n(A) if buffer data is accessible within a device process (using pointer value or its transformed value), then the process will interpret the buffer data pointer as device pointer.\r\n(B) if buffer data is not accessible within a device process, then the process or the buffer object needs to implement a copy method. The copy method would involve a data copy as well as memory management of a temporary buffer.\r\n\r\nThe proposed Device abstraction already involves components for both (A) and (B) but I could see that it can be generalized for arbitrary devices and provide optimal abstraction for data access and movement. For instance:\r\n1. replace `cpu_data()` and `on_cpu` with `accessible_data(const Device& other)` and `is_accessible(const Device& other)`, respectively;\r\n2. replace `CopyToCpu` and `CopyFromCpu` with `CopyTo` and `CopyFrom`, respectively;\r\n3. instead of `Buffer`, `CudaBuffer`, `FPGABuffer`, etc just have `Buffer`;\r\n4. internally, use `uintptr_t` to hold buffer pointer value instead of `uint8_t\\*` to prevent accidental dereferencing when the pointer is not accessible from the given device . `accessible_data` can still return `uint8_t\\*`;\r\n\r\nAtm, I don't have a good replacement for a `Device` as its substance is not to specify a particular \"device\" but a \"memory area\". While in many cases these coincide, there are cases where a memory area can represent the memory of multiple devices (see cases (i)-(iii) above). Perhaps replace `Device` with `VirtualMemoryArea`? Or `MemoryPool`? Other ideas?"
        },
        {
            "created_at": "2019-02-01T13:53:19.288Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16758320) by Wes McKinney (wesm):*\nI marked for 0.13. I won't be able to comment in detail until next week but I agree we should sort a solution that meets the requirements for this"
        },
        {
            "created_at": "2019-02-06T12:06:10.029Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16761692) by Antoine Pitrou (apitrou):*\nThanks `[~pearu]`. Regarding your propositions:\r\n\r\n1) sounds fine to me, though we might keep `cpu_data` and `on_cpu` as shortcuts for `accessible_data(CPUDevice())` and `is_accessible(CPUDevice())`, respectively. Expecting CPU-accessible memory will still be the dominant case in code using this API (since, after all, this is code running on the CPU).\r\n\r\n(a concern here is that querying the CPU address of a buffer should ideally not go through a virtual function call)\r\n\r\n2) sounds fine to me (but might also keep shortcuts, see above)\r\n\r\n3) sounds fine as well... but need a way to query device-specific buffer properties (such as `cuda_buffer->context()`) in another way, then.\r\n\r\n4) I have no particular opinion about this.\r\n\r\nIn case (iii), the memory still resides on a particular device, it just happens to be readable from another device as well. So e.g. the memory sits on a GPU, but is CPU-accessible (at higher cost than normal) which suggests that `accessible_data(CPUDevice())` would return the appropriate CPU memory pointer.\r\n"
        },
        {
            "created_at": "2019-03-09T18:34:42.873Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16788766) by Pearu Peterson (pearu):*\nRe `[~pitrou]` comment:\u00a0In case (iii), the memory still resides on a particular device, it just happens to be readable from another device as well. So e.g. the memory sits on a GPU, but is CPU-accessible..\r\n\r\nDid you meant the case (i) as in the case (iii) the memory should reside on RAM (since it allocated using `malloc`)?"
        },
        {
            "created_at": "2019-03-09T18:51:10.763Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16788770) by Pearu Peterson (pearu):*\nFYI, CUDA introduces [MemoryType](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g13de56a8fe75569530ecc3a3106e9b6d)\u00a0concept that together with [cudaPointerGetAttributes](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__UNIFIED.html#group__CUDART__UNIFIED_1gd89830e17d399c064a2f3c3fa8bb4390)\u00a0allows to determine if the given pointer value can be accessed from device or host or from both, and when on device then what is the device number.\u00a0"
        },
        {
            "created_at": "2019-03-09T19:11:45.172Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16788777) by Pearu Peterson (pearu):*\nIf a CUDA device supports compute capability 7.0 or higher (ATS over NVLink), [all of the system memory would be accessible by a device](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-system-allocator). So, if ATS over NVLink is available, tackling the current issue simplifies considerably."
        },
        {
            "created_at": "2019-03-09T19:28:54.616Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16788780) by Pearu Peterson (pearu):*\nRe `[~pitrou]` comment:\u00a0\u00a0need a way to query device-specific buffer properties (such as `cuda_buffer->context()`) ..\r\n\r\nCurrently, Arrow CUDA support uses primary context management which means that to get the CUDA context, one only needs to know the device number (use [cuDevicePrimaryCtxRetain](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PRIMARY__CTX.html#group__CUDA__PRIMARY__CTX_1g9051f2d5c31501997a6cb0530290a300)). The device number can be retrieved from the memory pointer (use [cudaPointerGetAttributes](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__UNIFIED.html#group__CUDART__UNIFIED_1gd89830e17d399c064a2f3c3fa8bb4390)). So, it would be sufficient to know that the pointer is a CUDA device pointer to establish its accessibility properties as well as context if needed."
        },
        {
            "created_at": "2019-03-10T11:06:53.105Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16788883) by Pearu Peterson (pearu):*\nAlso the MemoryPool plays an important role in this issue: MemoryPool allocate the memory and hence is the first source of information that determines the pointer's accessibility properties. In the Device class proposal, MemoryPool is suggested to become a part of the Device class, but see below.\r\n\r\nIn the CUDA case, the pointer's accessibility property is defined by the method that is used for allocating the memory. The device number is more a parameter that is not sufficient for determining the pointer's accessibility. From that I would conclude that one should attach to the Buffer the allocation method information (that includes device number), not the Device instance which does not provide all of the required information.\r\n\r\nSo, perhaps we should be attaching MemoryPool to the Buffer (instead of introducing Device class) so that the accessibility of memory pointer is determined by the MemoryPool instance which contains also the process information. Recall, CUDA context is essentially a device process.\r\n\r\nCurrently, CPU based Arrow uses DefaultMemoryPool. For CUDA support, several new memory pools would be defined:\r\nCudaManagedMemoryPool\r\nCudaMemoryPool\r\nCudaHostMemoryPool\r\nCudaRegistrededMemoryPool\r\n\r\nor in more general, one should be able to define their own MemoryPool instance that manages the memory of any device or uses some custom memory manager such as [RMM](https://github.com/rapidsai/rmm)\u00a0for allocating memory for Arrow buffers.\r\n\r\n`Buffer.is_accessible` would take a MemoryPool instance as argument and pairing this with the MemoryPool instance attached to the Buffer will determine the Buffer pointer accessibility properties. For that, MemoryPool classes would implement a method, say, `is_compatible(<other MemoryPool instance>)` that returns `true` if the pointer can be accessed from the process that the other MemoryPool represents. In addition, MemoryPool instances would implement the CopyTo and CopyFrom methods.\r\n\r\n`[~wesmckinn]`, `[~pitrou]`, and others, what do you think?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2019-03-11T12:39:07.054Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16789498) by Pearu Peterson (pearu):*\nIt turns out that MemoryPool should be always attached to its Buffers, otherwise it may be destructed before all its memory is released. See https://issues.apache.org/jira/browse/ARROW-4825\u00a0."
        },
        {
            "created_at": "2019-03-11T14:45:31.161Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16789651) by Wes McKinney (wesm):*\nIt _is_ attached, but not by a shared_ptr\r\n\r\nhttps://github.com/apache/arrow/blob/8b65bf845a2bc9ad071451c6924a9c33e29a4e9f/cpp/src/arrow/buffer.cc#L99\r\n\r\nOne possibility is to use shared_ptr. There are probably other possibilities to extend the life of allocated memory; probably some microbenchmarks should be implemented to evaluate whatever strategy is employed"
        },
        {
            "created_at": "2019-03-11T16:55:39.989Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16789753) by Pearu Peterson (pearu):*\nThanks for the pointer! I'll give shared_ptr-approach a try and prepare a PR for ARROW-4825.\r\n\r\nBtw, I have implemented CudaMemoryPool(<cuda.Context>) and the approach seems to work fine. For instance, `pa.allocate_buffer(512, memory_pool=<CudaMemoryPool instance>)` creates an arrow Buffer with device pointers. It only required overriding ZeroPadding that used `memset` with a method that uses `cuMemsetD8`."
        },
        {
            "created_at": "2019-03-26T14:09:24.603Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16801755) by Antoine Pitrou (apitrou):*\n> For instance, `pa.allocate_buffer(512, memory_pool=<CudaMemoryPool instance>)` creates an arrow Buffer with device pointers.\r\n\r\nI'm not sure that's the right approach, API-wise. What if allocating a given buffer type requires more than just getting a pointer from the underlying memory pool? So, rather than pass a CUDA memory pool to `pa.allocate`, it sounds more future proof to have a dedicated `pa.cuda.allocate_buffer` (or something).\r\n\r\n> So, perhaps we should be attaching MemoryPool to the Buffer (instead of introducing Device class) so that the accessibility of memory pointer is determined by the MemoryPool instance which contains also the process information.\r\n\r\nI don't know. Getting device information in itself may be useful. Inspecting a MemoryPool doesn't sound sufficient. Of course, perhaps the device could be attached to the MemoryPool rather than to the Buffer itself?\r\n\r\nYou're right about accessibility information: it seems more fine-grained than I initially assumed. It would still be worthwhile to find a solution to make uses of Buffer are less error-prone, though. Right now the only thing that protects the user is that CUDA buffers are hardly ever used, so you don't risk encountering them ;-)\r\n"
        },
        {
            "created_at": "2019-09-17T15:30:13.156Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16931565) by Antoine Pitrou (apitrou):*\nIdeally we would have this in 1.0.0, but it may be starting to be a bit short."
        },
        {
            "created_at": "2019-09-17T18:02:12.155Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=16931710) by Wes McKinney (wesm):*\nAgreed. I don't think we should stress ourselves about creating a stable API for an initial cut, but having the basics in place would be beneficial, I think"
        },
        {
            "created_at": "2020-02-12T23:28:09.285Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2447?focusedCommentId=17035789) by Wes McKinney (wesm):*\nIssue resolved by pull request 6295\n<https://github.com/apache/arrow/pull/6295>"
        }
    ]
}