{
    "issue": {
        "title": "Expanding pyarrow buffer size more than 2GB for pandas_udf functions",
        "body": "***Note**: This issue was originally created as [ARROW-10957](https://issues.apache.org/jira/browse/ARROW-10957). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThere is 2GB limit for data that can be passed to any pandas_udf function and the aim of this issue is to expand this limit. It's very small buffer size if we use pyspark and our goal is fitting machine learning models.\r\n\r\nSteps to reproduce - just use following spark-submit for executing following after python function.\r\n\r\n```java\n\r\n%sh\r\ncd /home/zeppelin/code && \\\r\nexport PYSPARK_DRIVER_PYTHON=/home/zeppelin/envs/env3/bin/python && \\\r\nexport PYSPARK_PYTHON=./env3/bin/python && \\\r\nexport ARROW_PRE_0_15_IPC_FORMAT=1 && \\\r\nspark-submit \\\r\n--master yarn \\\r\n--deploy-mode client \\\r\n--num-executors 5 \\\r\n--executor-cores 5 \\\r\n--driver-memory 8G \\\r\n--executor-memory 8G \\\r\n--conf spark.executor.memoryOverhead=4G \\\r\n--conf spark.driver.memoryOverhead=4G \\\r\n--archives /home/zeppelin/env3.tar.gz#env3 \\\r\n--jars \"/opt/deltalake/delta-core_2.11-0.5.0.jar\" \\\r\n--py-files jobs.zip,\"/opt/deltalake/delta-core_2.11-0.5.0.jar\" main.py \\\r\n--job temp\r\n```\r\n\u00a0\r\n```java\n\r\nimport pyspark\r\nfrom pyspark.sql import functions as F, types as T\r\nimport pandas as pd\r\n\r\ndef analyze(spark):\r\n\r\n    pdf1 = pd.DataFrame(\r\n        [[1234567, 0.0, \"abcdefghij\", \"2000-01-01T00:00:00.000Z\"]],\r\n        columns=['df1_c1', 'df1_c2', 'df1_c3', 'df1_c4']\r\n    )\r\n    df1 = spark.createDataFrame(pd.concat([pdf1 for i in range(429)]).reset_index()).drop('index')\r\n\r\n    pdf2 = pd.DataFrame(\r\n        [[1234567, 0.0, \"abcdefghijklmno\", \"2000-01-01\", \"abcdefghijklmno\", \"abcdefghijklmno\"]],\r\n        columns=['df2_c1', 'df2_c2', 'df2_c3', 'df2_c4', 'df2_c5', 'df2_c6']\r\n    )\r\n    df2 = spark.createDataFrame(pd.concat([pdf2 for i in range(48993)]).reset_index()).drop('index')\r\n    df3 = df1.join(df2, df1['df1_c1'] == df2['df2_c1'], how='inner')\r\n\r\n    def myudf(df):\r\n        import os\r\n        os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\r\n        return df\r\n\r\n    df4 = df3 \\\r\n        .withColumn('df1_c1', F.col('df1_c1').cast(T.IntegerType())) \\\r\n        .withColumn('df1_c2', F.col('df1_c2').cast(T.DoubleType())) \\\r\n        .withColumn('df1_c3', F.col('df1_c3').cast(T.StringType())) \\\r\n        .withColumn('df1_c4', F.col('df1_c4').cast(T.StringType())) \\\r\n        .withColumn('df2_c1', F.col('df2_c1').cast(T.IntegerType())) \\\r\n        .withColumn('df2_c2', F.col('df2_c2').cast(T.DoubleType())) \\\r\n        .withColumn('df2_c3', F.col('df2_c3').cast(T.StringType())) \\\r\n        .withColumn('df2_c4', F.col('df2_c4').cast(T.StringType())) \\\r\n        .withColumn('df2_c5', F.col('df2_c5').cast(T.StringType())) \\\r\n        .withColumn('df2_c6', F.col('df2_c6').cast(T.StringType()))\r\n    print(df4.printSchema())\r\n\r\n    udf = F.pandas_udf(df4.schema, F.PandasUDFType.GROUPED_MAP)(myudf)\r\n\r\n    df5 = df4.groupBy('df1_c1').apply(udf)\r\n    print('df5.count()', df5.count())\r\n```\r\n\r\nIf you need more details please let me know.",
        "created_at": "2020-12-18T11:14:35.000Z",
        "updated_at": "2021-03-24T04:13:34.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Java",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-03-24T04:13:34.000Z"
    },
    "comments": [
        {
            "created_at": "2020-12-18T11:15:01.170Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17251704) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya]\u00a0here we go."
        },
        {
            "created_at": "2020-12-18T13:50:20.834Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17251771) by Dmitry Kravchuk (dishka_krauch):*\nI've also tried this code with env spark 3.0.1/scala 2.12 and everyting went okay.\r\n\r\nIt this issue related just for spark with version lover than 3.0.0?"
        },
        {
            "created_at": "2020-12-21T02:26:17.655Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17252575) by Liya Fan (fan_li_ya):*\n[~dishka_krauch] I guess the reason is that the metadata size for Spark 3.0.1 is smaller than 2GB?\r\n\r\nAnd this is the root cause of the problem. Currently, Arrow supports data buffers larger than 2GB, but not metadata.\r\n\r\nCould you please help investigate why is the metadata size so large?"
        },
        {
            "created_at": "2020-12-21T10:09:31.678Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17252742) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya]\u00a0I've tested pandas dataframe with size 1.72 gb with spark 2.4.4 and it fails.\r\n\r\nAlso I've tested padnas datafraem withsize 1.72 gb and 2.5 gb with spark 3.0.1 and it works.\r\n\r\nWhat does actually you mean as metadata in process of serialization data between scala and python using pandas_udf? Is it spark dataframe schema?"
        },
        {
            "created_at": "2020-12-21T12:36:05.726Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17252829) by Liya Fan (fan_li_ya):*\nYes. I have copied the stack trace of the original exception:\r\n\r\n```\n\r\njava.lang.IllegalArgumentException\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\r\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\r\n\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\r\n\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\r\n\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\r\n```\r\n\r\nIt can be seen that the exception was thrown when reading the schema. So I am wondering if the schema size was too large?\r\n\r\nBTW, are you running the program in a BigEndian platform? Byte order may also casue the negative message length. "
        },
        {
            "created_at": "2020-12-22T09:07:35.150Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17253350) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya]\u00a0sorry, but I can't catch the main meaning of your question.\r\n\r\nI have a spark dataset with size 1.72 gb. When I pass this data through pandas_udf I get this exception.\r\n\r\nHow can I give you information about schema size?\r\n\r\nI'm using Hadoop from Hortonworks Cloudera repository."
        },
        {
            "created_at": "2020-12-22T11:58:32.433Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17253444) by Liya Fan (fan_li_ya):*\n[~dishka_krauch] Sorry I don't know how to help. It seems difficult to share a data set with 1.72 gb size.\r\n\r\nCurrently, Arrow supports buffer sizes larger than 2GB. So it is OK to transfer large vectors/arrays/record batches. However, for metadata (e.g. schema), the 2gb limit still exists. This is because in most applications, the metadata size is relatively small, so 2gb should be more than enough. \r\n\r\nSo my suggestion is to debug your code to see why the metadata size is so large. Is it plausible, or there are some bugs?\r\n\r\n"
        },
        {
            "created_at": "2020-12-22T13:34:05.377Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17253503) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya]\u00a0I can't understand how it is possible that metadata size (information about columns, types) can be more than 2 gb in my case. I can see than data size is 1.72 gb after convertion to pandas.\r\n\r\nI gave you code that generates dataset above.\r\n```java\n\r\nimport pyspark\r\nfrom pyspark.sql import functions as F, types as T\r\nimport pandas as pd\r\n\r\ndef analyze(spark):\r\n\r\n    pdf1 = pd.DataFrame(\r\n        [[1234567, 0.0, \"abcdefghij\", \"2000-01-01T00:00:00.000Z\"]],\r\n        columns=['df1_c1', 'df1_c2', 'df1_c3', 'df1_c4']\r\n    )\r\n    df1 = spark.createDataFrame(pd.concat([pdf1 for i in range(429)]).reset_index()).drop('index')\r\n\r\n    pdf2 = pd.DataFrame(\r\n        [[1234567, 0.0, \"abcdefghijklmno\", \"2000-01-01\", \"abcdefghijklmno\", \"abcdefghijklmno\"]],\r\n        columns=['df2_c1', 'df2_c2', 'df2_c3', 'df2_c4', 'df2_c5', 'df2_c6']\r\n    )\r\n    df2 = spark.createDataFrame(pd.concat([pdf2 for i in range(48993)]).reset_index()).drop('index')\r\n    df3 = df1.join(df2, df1['df1_c1'] == df2['df2_c1'], how='inner')\r\n\r\n    def myudf(df):\r\n        import os\r\n        os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\r\n        return df\r\n\r\n    df4 = df3 \\\r\n        .withColumn('df1_c1', F.col('df1_c1').cast(T.IntegerType())) \\\r\n        .withColumn('df1_c2', F.col('df1_c2').cast(T.DoubleType())) \\\r\n        .withColumn('df1_c3', F.col('df1_c3').cast(T.StringType())) \\\r\n        .withColumn('df1_c4', F.col('df1_c4').cast(T.StringType())) \\\r\n        .withColumn('df2_c1', F.col('df2_c1').cast(T.IntegerType())) \\\r\n        .withColumn('df2_c2', F.col('df2_c2').cast(T.DoubleType())) \\\r\n        .withColumn('df2_c3', F.col('df2_c3').cast(T.StringType())) \\\r\n        .withColumn('df2_c4', F.col('df2_c4').cast(T.StringType())) \\\r\n        .withColumn('df2_c5', F.col('df2_c5').cast(T.StringType())) \\\r\n        .withColumn('df2_c6', F.col('df2_c6').cast(T.StringType()))\r\n    print(df4.printSchema())\r\n\r\n    udf = F.pandas_udf(df4.schema, F.PandasUDFType.GROUPED_MAP)(myudf)\r\n\r\n    df5 = df4.groupBy('df1_c1').apply(udf)\r\n    print('df5.count()', df5.count())\r\n```\r\n\r\nI am sure that size of df4.schema output is less than 2 gb, but size of df4.toPandas() is approximately 2 gb (1.72 gb).\r\nMay be there is some misunterstanding between us."
        },
        {
            "created_at": "2020-12-23T12:01:12.329Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17254054) by Liya Fan (fan_li_ya):*\n[~dishka_krauch] Thanks for the information. \r\nI can take a look with the program you provided. However, I do not have a python or Spark environment, so it may take me quite a while to reproduce the problem. \r\n\r\nI think the most efficient approach is that you can debug the program to see why the metadata is so large?"
        },
        {
            "created_at": "2020-12-24T08:07:54.145Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17254460) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya] Okay, it will be easy. Just help me to find out how to get info about metadata size. Can you share some code examples?"
        },
        {
            "created_at": "2020-12-28T05:40:23.200Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17255385) by Liya Fan (fan_li_ya):*\n[~dishka_krauch] Sorry for my late reply.\r\nI am not farmiliar with python code base.\r\nFor Java code, you can look at TestRoundTrip#testMetadata, which contains a complete example of serializing/deserializing data schema. "
        },
        {
            "created_at": "2020-12-29T08:38:26.491Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17255871) by Dmitry Kravchuk (dishka_krauch):*\n[~fan_li_ya] Where should I search Java code example of TestRoundTrip#testMetadata? Can you share link?"
        },
        {
            "created_at": "2020-12-29T12:15:17.066Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17255968) by Liya Fan (fan_li_ya):*\nSure. Please find the test case here: https://github.com/apache/arrow/blob/master/java/vector/src/test/java/org/apache/arrow/vector/ipc/TestRoundTrip.java#L254"
        },
        {
            "created_at": "2021-02-11T21:37:11.458Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17283388) by Dmitry Kravchuk (dishka_krauch):*\n`[~liyafan]` sorry for late reply...\nI have no any environment or any possible way to use Java code from your example.\nCan you help me with metadata size calculation?\nMay be it can be done with known schema if my data?\nI mean it should possible to calculate metadata size using schema columns and theirs types, am I right?"
        },
        {
            "created_at": "2021-02-16T05:14:05.408Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17285036) by Micah Kornfield (emkornfield):*\nQuick question: I was under the impression that \"\r\nexport ARROW_PRE_0_15_IPC_FORMAT=1 \r\nwas insufficient to make this work properly.\u00a0 Per [instructions\\|conf/spark-env.sh] it needs to be added to you spark-conf?"
        },
        {
            "created_at": "2021-02-16T19:35:22.567Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17285445) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` i didn't help anyway.\r\n As I understood from here <https://stackoverflow.com/questions/58269565/how-to-solve-pyspark-org-apache-arrow-vector-util-oversizedallocationexception>\u00a0this issue can be resolver only with spark3 and pyarrow after 0.15.0 version (take a look at the lat comment).\r\n\r\nPyspark version below 0.15.0 and spark2 have buffer size limit 2^(32-1) bytes what equeals to 2gb."
        },
        {
            "created_at": "2021-02-17T03:47:37.316Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17285623) by Micah Kornfield (emkornfield):*\n[~dishka_krauch]\u00a0 This is correct, the work needs to be done on the spark side. I might have lost the thread but I thought you were seeing this even with very small data sizes.\u00a0 Apologies for the confusion."
        },
        {
            "created_at": "2021-02-17T06:35:38.607Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17285647) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` it's okay. Currently right now I am testing pyarrow 2.0 and spark 3.0.1 to be sure that's everything is okay."
        },
        {
            "created_at": "2021-03-01T08:05:54.324Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17292705) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` [~fan_li_ya]\r\n\r\nHello.\r\n\r\nI've tested code at another cluster with spark 3.0.2 env and still have error.\r\n\r\nCan you help me out?\r\n\r\n\u00a0\r\n\r\nEnvironment:\r\n ![spark3 env.png](https://issues.apache.org/jira/secure/attachment/13021363/spark3+env.png)\r\n\r\nPython env:\r\n ![python env.png](https://issues.apache.org/jira/secure/attachment/13021361/python+env.png)\r\n\r\nError:\r\n ![spark3 error.png](https://issues.apache.org/jira/secure/attachment/13021362/spark3+error.png)\r\n\r\nTesting code:\r\n```java\n\r\nfrom pyspark.sql import functions as F, types as T\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.context import SparkContext\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\nspark = SparkSession.builder.appName('spark3_example').getOrCreate()\r\nspark.sparkContext.setLogLevel('ERROR')\r\nspark.conf.set('spark.sql.shuffle.partitions', '200')\r\nspark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')\r\n\r\npdf1 = pd.DataFrame(\r\n[[1234567, 0.0, 'abcdefghij', '2000-01-01T00:00:00.000Z']],\r\ncolumns=['df1_c1', 'df1_c2', 'df1_c3', 'df1_c4']\r\n)\r\ndf1 = spark.createDataFrame(pd.concat([pdf1 for i in range(500)]).reset_index()).drop('index')\r\n\r\npdf2 = pd.DataFrame(\r\n[[1234567, 0.0, 'abcdefghijklmno', '2000-01-01', 'abcdefghijklmno', 'abcdefghijklmno']],\r\ncolumns=['df2_c1', 'df2_c2', 'df2_c3', 'df2_c4', 'df2_c5', 'df2_c6']\r\n)\r\ndf2 = spark.createDataFrame(pd.concat([pdf2 for i in range(50000)]).reset_index()).drop('index')\r\ndf3 = df1.join(df2, df1['df1_c1'] == df2['df2_c1'], how='inner')\r\n\r\ndef myudf(df):\r\n    return df\r\n\r\ndf4 = df3 \\\r\n.withColumn('df1_c1', F.col('df1_c1').cast(T.IntegerType())) \\\r\n.withColumn('df1_c2', F.col('df1_c2').cast(T.DoubleType())) \\\r\n.withColumn('df1_c3', F.col('df1_c3').cast(T.StringType())) \\\r\n.withColumn('df1_c4', F.col('df1_c4').cast(T.StringType())) \\\r\n.withColumn('df2_c1', F.col('df2_c1').cast(T.IntegerType())) \\\r\n.withColumn('df2_c2', F.col('df2_c2').cast(T.DoubleType())) \\\r\n.withColumn('df2_c3', F.col('df2_c3').cast(T.StringType())) \\\r\n.withColumn('df2_c4', F.col('df2_c4').cast(T.StringType())) \\\r\n.withColumn('df2_c5', F.col('df2_c5').cast(T.StringType())) \\\r\n.withColumn('df2_c6', F.col('df2_c6').cast(T.StringType()))\r\n\r\ndf5 = df4.groupBy('df1_c1').applyInPandas(myudf, df4.schema)\r\ndf5.show()\r\n```\r\nspark-submit:\r\n\r\nexport SPARK_HOME=/opt/spark31/spark-3.0.2-bin-hadoop3.2 && \\\r\n export HADOOP_CONF_DIR=/etc/hadoop && \\\r\n export YARN_CONF_DIR=/etc/hadoop && \\\r\n export PYSPARK_DRIVER_PYTHON=/home/zeppelin/envs/env/bin/python && \\\r\n export PYSPARK_PYTHON=/usr/bin/python3 && \\\r\n export HADOOP_USER_NAME=hdfs && \\\r\n spark-submit \\\r\n --master yarn \\\r\n --deploy-mode client \\\r\n --num-executors 7 \\\r\n --executor-cores 2 \\\r\n --driver-memory 11G \\\r\n --executor-memory 11G \\\r\n --archives /home/zeppelin/env.tar.gz#env \\\r\n --py-files /home/zeppelin/code/spark3_testing.py /home/zeppelin/code/spark3_testing.py"
        },
        {
            "created_at": "2021-03-01T08:28:02.400Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17292720) by Micah Kornfield (emkornfield):*\n[~dishka_krauch]\u00a0 \u00a0it looks\u00a0spark like 3.0.1 still uses arrow 0.15.1 on the java side.\u00a0 I think you need to use 3.1.0 or later to have any chance of success (which was upgraded to arrow 2.0.0).\u00a0\u00a0\r\n\r\n[https://github.com/apache/arrow/commit/9742007c463e253e2b916e65f668146953456a00#diff-2e086b32ec292aae20695dd4341c647c9a9d7d3d77816bf849f7fbf68e9fa6cfR209\u00a0](https://github.com/apache/arrow/commit/9742007c463e253e2b916e65f668146953456a00#diff-2e086b32ec292aae20695dd4341c647c9a9d7d3d77816bf849f7fbf68e9fa6cfR209)\u00a0(along with the some later bug fixed) is the fix for getting negative values from the java side.\u00a0 \u00a0This wasn't available until 0.16.0."
        },
        {
            "created_at": "2021-03-01T08:54:59.360Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17292737) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` \u00a0can help spark community with sharing this issue at github as pull request for spark 3.1.0?"
        },
        {
            "created_at": "2021-03-01T09:35:56.871Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17292773) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` \u00a0btw I'm using spark 3.0.2 and looks like arrow 2.0.0+ is recently added to support here https://issues.apache.org/jira/browse/SPARK-33189\r\n\r\nI've also tried spark-submit with pyarrow versions 2.0.0 and 1.0.1 but still got error with negative legth."
        },
        {
            "created_at": "2021-03-01T17:14:21.529Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17293035) by Micah Kornfield (emkornfield):*\nI think there might be some confusion.\u00a0 It looks like 3.0.2 might support pyarrow >= 2.0 but as far as I can tell\u00a0 the java version is still 0.15.1.\u00a0 I'm looking here <https://github.com/apache/spark/blob/branch-3.0/pom.xml#L209>\u00a0(but maybe that is the wrong location)."
        },
        {
            "created_at": "2021-03-02T06:13:43.104Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17293387) by Dmitry Kravchuk (dishka_krauch):*\n`[~emkornfield]` \u00a0okay, I've created this issue for spark https://issues.apache.org/jira/browse/SPARK-34588"
        },
        {
            "created_at": "2021-03-24T04:13:25.845Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10957?focusedCommentId=17307551) by Micah Kornfield (emkornfield):*\nper spark issue.\u00a0 This is fixed in spark 3.1"
        }
    ]
}