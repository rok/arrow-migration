{
    "issue": {
        "title": "[C++] Create a configurable implementation of RecordBatchReader that reads from Apache Parquet files",
        "body": "***Note**: This issue was originally created as [ARROW-1012](https://issues.apache.org/jira/browse/ARROW-1012). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis will be enabled by -ARROW-1008.-\r\n\r\nA preliminary implementation\u00a0of an\u00a0`arrow::RecordBatchReader` was added in PARQUET-1166 but does not support configuring the batch size.\u00a0\u00a0",
        "created_at": "2017-05-11T21:50:12.000Z",
        "updated_at": "2019-07-02T09:53:12.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2019-06-24T01:33:58.000Z"
    },
    "comments": [
        {
            "created_at": "2019-05-07T14:56:12.451Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1012?focusedCommentId=16834835) by Hatem Helal (hatem):*\nIt looks like\u00a0PARQUET-1166 added a basic implementation of an `arrow::RecordBatchReader`\u00a0that\u00a0can be used to read from Parquet.\u00a0 ARROW-3771 is a follow-up issue that I was looking into.\r\n\r\n`[~wesmckinn]` was there\u00a0some additional functionality that you had in mind for this issue?"
        },
        {
            "created_at": "2019-05-07T15:30:42.259Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1012?focusedCommentId=16834881) by Wes McKinney (wesm):*\n`[~hatem]` yes \u2013 the objective is to be able to configure a maximum record batch length (e.g. 64K, as an example) that obtain a stream of record batches no larger than that size. The `GetRecordBatchReader` returns an object that performs entire-row-group reads without configuration. This should be clarified in the JIRA description"
        },
        {
            "created_at": "2019-06-21T00:02:12.729Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1012?focusedCommentId=16869040) by Wes McKinney (wesm):*\nMoving this to the next release. I'd like to have at least a use case to drive this (the Datasets API should be that thing)"
        },
        {
            "created_at": "2019-06-21T13:03:03.541Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1012?focusedCommentId=16869452) by Hatem Helal (hatem):*\nI added my use cases to the PR here:\r\n\r\n<https://github.com/apache/arrow/pull/4304#issuecomment-504417220>\r\n\r\n\u00a0\r\n\r\nThe primary use-cases for this API that I have come across are:\r\n1. Iterative reading\u00a0`N`\u00a0rows a a time from a Parquet file. This might be necessary if the deserialized row group would be too large to fit in memory\n1. Cheap-preview: with this change we can efficiently read the first\u00a0`N`\u00a0rows of a Parquet file."
        },
        {
            "created_at": "2019-06-24T01:33:58.297Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1012?focusedCommentId=16870736) by Wes McKinney (wesm):*\nIssue resolved by pull request 4304\n<https://github.com/apache/arrow/pull/4304>"
        }
    ]
}