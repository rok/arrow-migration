{
    "issue": {
        "title": "[Python] Inconsistent handling of integer-valued partitions in dataset filters API",
        "body": "***Note**: This issue was originally created as [ARROW-13578](https://issues.apache.org/jira/browse/ARROW-13578). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen creating a partitioned data set via the pandas.to_parquet() method, partition columns are ostensibly cast to strings in the partition metadata.\u00a0 When reading specific partitions via the filters parameter in pandas.read_parquet(), string values must be used for filter operands _except when_ the partition column has an integer value.\u00a0\u00a0\r\n\r\nConsider the following example:\r\n```python\n\r\nimport datetime\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({\r\n    \"key1\": ['0', '1', '2'], \r\n    \"key2\": [0, 1, 2],\r\n    \"key3\": ['a', 'b', 'c'],\r\n    \"key4\": [1.1, 2.2, 3.3],\r\n    \"key5\": [True, False, True],\r\n    \"key6\": [datetime.date(2021, 6, 2), datetime.date(2021, 6, 3), datetime.date(2021, 6, 4)],\r\n    \"data\": [\"foo\", \"bar\", \"baz\"]\r\n})\r\n\r\ndf['key6'] = pd.to_datetime(df['key6'])\r\n\r\ndf.to_parquet('./test.parquet', partition_cols=['key1', 'key2', 'key3', 'key4', 'key5', 'key6'])\r\n```\r\nReading into a ParquetDataset and inspecting the partition levels suggests that partition keys have been cast to string, regardless of the original type:\r\n```python\n\r\nimport pyarrow.parquet as pq\r\nds = pq.ParquetDataset('./test.parquet')\r\nfor level in ds.partitions.levels:\r\n    print(f\"{level.name}: {level.keys}\")\r\n```\r\n\r\nOutput:\r\n```\n\r\nkey1: ['0', '1', '2']\r\nkey2: ['0', '1', '2']\r\nkey3: ['a', 'b', 'c']\r\nkey4: ['1.1', '2.2', '3.3']\r\nkey5: ['True', 'False']\r\nkey6: ['2021-06-02 00:00:00', '2021-06-03 00:00:00', '2021-06-04 00:00:00']\n```\r\n\r\nFiltering the dataset using any of the non-integer partition keys along with string-valued operands works as expected:\r\n\r\n```python\n\r\ndf2=pd.read_parquet('./test.parquet', filters=[('key4','=','1.1'), ('key5', '=', 'True')])\r\ndf2.head()\r\n```\r\n\r\nOutput:\r\n```\n\r\n\tdata\tkey1\tkey2\tkey3\tkey4\tkey5\tkey6\r\n0\tfoo\t0\t0\ta\t1.1\tTrue\t2021-06-02 00:00:00\r\n```\r\n\r\nHowever, filtering the dataset using either of the integer-valued partition keys with a string-valued operand raises an exception, **even when the original column's data type is string**:\r\n\r\n```python\n\r\ndf2=pd.read_parquet('./test.parquet', filters=[('key1','=','1')])\r\ndf2.head()\r\n```\r\n\r\n```\n\r\nArrowNotImplementedError: Function equal has no kernel matching input types (array[int32], scalar[string])\r\n```\r\n\r\nIt would seem to be less surprising / more consistent if filter operands either (a) are always cast to string, or (b) always retain their original type.\r\n\r\nNote, this issue may be related to ARROW-12114.",
        "created_at": "2021-08-06T16:41:37.000Z",
        "updated_at": "2021-08-10T10:34:50.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-08-10T10:28:15.474Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13578?focusedCommentId=17396598) by Joris Van den Bossche (jorisvandenbossche):*\n`[~mnizol]` thanks a lot for the clear and detailed report. \r\n\r\nWhat's causing the confusion here is that we have both a legacy Python implementation of ParquetDataset and a new generic Datasets API, and that we are still in the middle of moving to the new implementation: `ParquetDataset` still uses the legacy implementation by default (but you can use `use_legacy_dataset=False` to opt in to the new), while `pq.read_table` (which is what `pd.read_parquet` uses under the hood) is already defaulting to the new implementation (but you can fall back to the old with `use_legacy_dataset=True`).\r\n\r\nIn the legacy ParquetDataset implementation, all partition keys are indeed parsed as strings as you show with the output of `ParquetDataset.partitions.levels`. So when passing `use_legacy_dataset=True` to the read function, using a string actually works:\r\n\r\n```Java\n\r\nIn [19]: pd.read_parquet('./test.parquet', filters=[('key1','=','1')], use_legacy_dataset=True)\r\nOut[19]: \r\n  data key1 key2 key3 key4   key5                 key6\r\n0  bar    1    1    b  2.2  False  2021-06-03 00:00:00\r\n```\r\n\r\nBTW, also using an integer works here (`('key1', '=', 1)`), because the legacy implementation will try to interpret the value with the type of the partition levels.\r\n\r\nIn the new Datasets API, the parsing of the directory paths currently supports int32 and strings (when inferring, you can use other types when explicitly passing the schema for the partition keys). \r\nSo when creating a ParquetDataset with use_legacy_dataset=False, we see:\r\n\r\n```Java\n\r\nIn [21]: ds = pq.ParquetDataset('test_partitions', use_legacy_dataset=False)\r\n\r\nIn [22]: ds._dataset.partitioning\r\nOut[22]: <pyarrow._dataset.HivePartitioning at 0x7f2fe010c830>\r\n\r\nIn [23]: ds._dataset.partitioning.schema\r\nOut[23]: \r\nkey1: dictionary<values=int32, indices=int32, ordered=0>\r\nkey2: dictionary<values=int32, indices=int32, ordered=0>\r\nkey3: dictionary<values=string, indices=int32, ordered=0>\r\nkey4: dictionary<values=string, indices=int32, ordered=0>\r\nkey5: dictionary<values=string, indices=int32, ordered=0>\r\nkey6: dictionary<values=string, indices=int32, ordered=0>\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [], \"column_indexes\": [{\"name\": null, \"field_n' + 340\r\n\r\nIn [24]: ds._dataset.partitioning.dictionaries\r\nOut[24]: \r\n[<pyarrow.lib.Int32Array object at 0x7f2f63ad8760>\r\n [\r\n   0,\r\n   1,\r\n   2\r\n ],\r\n <pyarrow.lib.Int32Array object at 0x7f2f62dae220>\r\n [\r\n   0,\r\n   1,\r\n   2\r\n ],\r\n <pyarrow.lib.StringArray object at 0x7f2f62dae8e0>\r\n [\r\n   \"a\",\r\n   \"b\",\r\n   \"c\"\r\n ],\r\n <pyarrow.lib.StringArray object at 0x7f2f62dae0a0>\r\n [\r\n   \"1.1\",\r\n   \"2.2\",\r\n   \"3.3\"\r\n ],\r\n <pyarrow.lib.StringArray object at 0x7f2f62dae1c0>\r\n [\r\n   \"True\",\r\n   \"False\"\r\n ],\r\n <pyarrow.lib.StringArray object at 0x7f2f62dae340>\r\n [\r\n   \"2021-06-02 00:00:00\",\r\n   \"2021-06-03 00:00:00\",\r\n   \"2021-06-04 00:00:00\"\r\n ]]\r\n```\r\n\r\nSo the first two partition keys are inferred as int, the others as string. And that's also the reason that for this case, you actually need to specify the filter using an integer (we decided to not do such automatic casting here in the new implementation).\r\nSidenote: I am using `ds._dataset.partitioning` above, but this will become `ds.partitioning` after ARROW-13525. \r\n\r\nSo with an integer value in the filter this works (adding `use_legacy_dataset=False` explicitly, but so this is the default in `pq.read_table` / `pd.read_parquet`):\r\n\r\n```Java\n\r\nIn [20]: pd.read_parquet('./test_partitions/', filters=[('key1','=', 1)], use_legacy_dataset=False)\r\nOut[20]: \r\n  data key1 key2 key3 key4   key5                 key6\r\n0  bar    1    1    b  2.2  False  2021-06-03 00:00:00\r\n```\r\n\r\nUsing the new datasets API directly, this looks like:\r\n\r\n```Java\n\r\nIn [25]: import pyarrow.dataset as ds\r\n\r\nIn [26]: dataset = ds.dataset(\"test_partitions/\", format=\"parquet\", partitioning=\"hive\")\r\n\r\nIn [28]: dataset.to_table(filter=ds.field(\"key1\") == 1).to_pandas()\r\nOut[28]: \r\n  data  key1  key2 key3 key4   key5                 key6\r\n0  bar     1     1    b  2.2  False  2021-06-03 00:00:00\r\n```"
        }
    ]
}