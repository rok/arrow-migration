{
    "issue": {
        "title": "[Python] test_partitioned_dataset fails when building with PARQUET but without DATASET",
        "body": "***Note**: This issue was originally created as [ARROW-16526](https://issues.apache.org/jira/browse/ARROW-16526). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nOur current [minimal_build examples](https://github.com/apache/arrow/tree/master/python/examples/minimal_build) for python build with -{**}DARROW_PARQUET=ON{**} but without {**}DATASET{**}. This produces the following failure:\r\n```java\n\r\n _________________________________________________________ test_partitioned_dataset[True] _________________________________________________________tempdir = PosixPath('/tmp/pytest-of-root/pytest-0/test_partitioned_dataset_True_0'), use_legacy_dataset = True\u00a0 \u00a0 @pytest.mark.pandas\r\n\u00a0 \u00a0 @parametrize_legacy_dataset\r\n\u00a0 \u00a0 def test_partitioned_dataset(tempdir, use_legacy_dataset):\r\n# ARROW-3208: Segmentation fault when reading a Parquet partitioned dataset\r\n# to a Parquet file\r\n\u00a0 \u00a0 \u00a0 \u00a0 path = tempdir / \"ARROW-3208\"\r\n\u00a0 \u00a0 \u00a0 \u00a0 df = pd.DataFrame({\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'one': [-1, 10, 2.5, 100, 1000, 1, 29.2],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'two': [-1, 10, 2, 100, 1000, 1, 11],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'three': [0, 0, 0, 0, 0, 0, 0]\r\n\u00a0 \u00a0 \u00a0 \u00a0 })\r\n\u00a0 \u00a0 \u00a0 \u00a0 table = pa.Table.from_pandas(df)\r\n> \u00a0 \u00a0 \u00a0 pq.write_to_dataset(table, root_path=str(path),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 partition_cols=['one', 'two'])pyarrow/tests/parquet/test_dataset.py:1544:\u00a0\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\u00a0\r\npyarrow/parquet/__init__.py:3110: in write_to_dataset\r\n\u00a0 \u00a0 import pyarrow.dataset as ds\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\u00a0\u00a0 \u00a0 \"\"\"Dataset is currently unstable. APIs subject to change without notice.\"\"\"\r\n\u00a0 \u00a0\u00a0\r\n\u00a0 \u00a0 import pyarrow as pa\r\n\u00a0 \u00a0 from pyarrow.util import _is_iterable, _stringify_path, _is_path_like\r\n\u00a0 \u00a0\u00a0\r\n> \u00a0 from pyarrow._dataset import ( \u00a0# noqa\r\n\u00a0 \u00a0 \u00a0 \u00a0 CsvFileFormat,\r\n\u00a0 \u00a0 \u00a0 \u00a0 CsvFragmentScanOptions,\r\n\u00a0 \u00a0 \u00a0 \u00a0 Dataset,\r\n\u00a0 \u00a0 \u00a0 \u00a0 DatasetFactory,\r\n\u00a0 \u00a0 \u00a0 \u00a0 DirectoryPartitioning,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FilenamePartitioning,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileFormat,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileFragment,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileSystemDataset,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileSystemDatasetFactory,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileSystemFactoryOptions,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FileWriteOptions,\r\n\u00a0 \u00a0 \u00a0 \u00a0 Fragment,\r\n\u00a0 \u00a0 \u00a0 \u00a0 FragmentScanOptions,\r\n\u00a0 \u00a0 \u00a0 \u00a0 HivePartitioning,\r\n\u00a0 \u00a0 \u00a0 \u00a0 IpcFileFormat,\r\n\u00a0 \u00a0 \u00a0 \u00a0 IpcFileWriteOptions,\r\n\u00a0 \u00a0 \u00a0 \u00a0 InMemoryDataset,\r\n\u00a0 \u00a0 \u00a0 \u00a0 Partitioning,\r\n\u00a0 \u00a0 \u00a0 \u00a0 PartitioningFactory,\r\n\u00a0 \u00a0 \u00a0 \u00a0 Scanner,\r\n\u00a0 \u00a0 \u00a0 \u00a0 TaggedRecordBatch,\r\n\u00a0 \u00a0 \u00a0 \u00a0 UnionDataset,\r\n\u00a0 \u00a0 \u00a0 \u00a0 UnionDatasetFactory,\r\n\u00a0 \u00a0 \u00a0 \u00a0 _get_partition_keys,\r\n\u00a0 \u00a0 \u00a0 \u00a0 _filesystemdataset_write,\r\n\u00a0 \u00a0 )\r\nE \u00a0 ModuleNotFoundError: No module named 'pyarrow._dataset'\r\n```\r\nThis can be reproduced via running the minimal_build examples:\r\n```java\n\r\n$ cd arrow/python/examples/minimal_build\r\n$ docker build -t arrow_ubuntu_minimal -f Dockerfile.ubuntu . \n```\r\nor via building arrow and pyarrow with PARQUET but without DATASET.",
        "created_at": "2022-05-11T10:10:12.000Z",
        "updated_at": "2022-05-16T12:25:46.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-05-12T11:25:35.000Z"
    },
    "comments": [
        {
            "created_at": "2022-05-11T10:18:59.924Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534805) by Ra\u00fal Cumplido (raulcd):*\n`[~alenkaf]` `[~jorisvandenbossche]` should we fix this case? Is building with PARQUET but without DATASET something supported? Do you think we should change the minimal build examples to build with DATASET?"
        },
        {
            "created_at": "2022-05-11T11:10:51.962Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534820) by Alenka Frim (alenka):*\n_I would think_ when building Arrow with Python ON, DATASET is built by default:\r\n\r\n<https://github.com/apache/arrow/blob/a8479e9c252482438b6fc2bc0383ac5cf6a09d59/cpp/CMakeLists.txt#L379-L386>\r\n\r\nSo if I understand correctly a minimal built example for Python should have DATASET on?"
        },
        {
            "created_at": "2022-05-11T12:08:37.295Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534848) by David Li (lidavidm):*\nThis just needs the appropriate pytest mark so that pytest will skip this test if we don't have dataset right?\r\n\r\nDataset/Parquet should be mostly independent, you can have datasets without Parquet and you can use the (base) Parquet functionality without datasets."
        },
        {
            "created_at": "2022-05-11T12:09:20.059Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534849) by David Li (lidavidm):*\nI guess the challenge is that we only want to apply the mark/skip the test when we're not using the legacy parquet dataset."
        },
        {
            "created_at": "2022-05-11T12:19:25.281Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534851) by David Li (lidavidm):*\nAh, there's already a PR: https://github.com/apache/arrow/pull/13116"
        },
        {
            "created_at": "2022-05-11T12:22:16.684Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17534854) by Ra\u00fal Cumplido (raulcd):*\nThanks `[~lidavidm]` !"
        },
        {
            "created_at": "2022-05-12T08:58:56.649Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17535984) by Joris Van den Bossche (jorisvandenbossche):*\nIndeed, we currently should mark those tests appropriately, because we allow building pyarrow with parquet but without dataset or the other way around. But because we no longer have a regular build of pyarrow without datasets, we often forget to add this mark and have to fix it afterwards (I think in the past we had an ursabot build without dataset)."
        },
        {
            "created_at": "2022-05-12T08:59:43.449Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17535985) by Joris Van den Bossche (jorisvandenbossche):*\nIt is somewhat strange that we allow building pyarrow with datasets disabled, though, since on the C++ side enabling python support automatically also enables dataset."
        },
        {
            "created_at": "2022-05-12T11:25:35.011Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16526?focusedCommentId=17536062) by Joris Van den Bossche (jorisvandenbossche):*\nIssue resolved by pull request 13116\n<https://github.com/apache/arrow/pull/13116>"
        }
    ]
}