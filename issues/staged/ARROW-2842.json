{
    "issue": {
        "title": "[Python] Cannot read parquet files with row group size of 1 From HDFS",
        "body": "***Note**: This issue was originally created as [ARROW-2842](https://issues.apache.org/jira/browse/ARROW-2842). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis might be a bug in parquet-cpp, I need to spend a bit more time tracking this down but basically given a file with a single row on hdfs, reading it with pyarrow yields this error\r\n\r\n```\r\n\r\nTcpSocket.cpp: 79: HdfsEndOfStream: Read 8 bytes failed from: End of the stream\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ Unknown\r\n @ arrow::io::HdfsReadableFile::ReadAt(long, long, long\\*, void\\*)\r\n @ parquet::ArrowInputFile::ReadAt(long, long, unsigned char\\*)\r\n @ parquet::SerializedFile::ParseMetaData()\r\n @ parquet::ParquetFileReader::Contents::Open(std::unique_ptr<parquet::RandomAccessSource, std::default_delete<parquet::RandomAccessSource> >, parquet::ReaderProperties const&, std::shared_ptr<parquet::FileMetaData> const&)\r\n @ parquet::ParquetFileReader::Open(std::unique_ptr<parquet::RandomAccessSource, std::default_delete<parquet::RandomAccessSource> >, parquet::ReaderProperties const&, std::shared_ptr<parquet::FileMetaData> const&)\r\n @ parquet::arrow::OpenFile(std::shared_ptr<arrow::io::RandomAccessFile> const&, arrow::MemoryPool\\*, parquet::ReaderProperties const&, std::shared_ptr<parquet::FileMetaData> const&, std::unique_ptr<parquet::arrow::FileReader, std::default_delete<parquet::arrow::FileReader> >\\*)\r\n @ __pyx_pw_7pyarrow_8_parquet_13ParquetReader_3open(_object\\*, _object\\*, _object\\*)\r\n\r\n```\r\n\r\nThe following code causes it:\r\n\r\n```\r\n\r\nimport pyarrow\r\n\r\nimport pyarrow.parquet as pq\r\n\r\n\u00a0\r\n\r\nfs = pyarrow.hdfs.connect('my-namenode-url', driver='libhdfs3') # fill in namenode information\r\n\r\nfile_object = fs.open('single-row.parquet') # update for hdfs path of file\r\n\r\npq.read_metadata(file_object) # this works\r\n\r\nparquet_file = pq.ParquetFile(file_object)\r\n\r\nparquet_file.read_row_group(0) # throws error\r\n\r\n```\r\n\r\n\u00a0\r\n\r\nI am working on\u00a0writing a unit test\u00a0for this. Note that I am using libhdfs3.",
        "created_at": "2018-07-12T21:53:29.000Z",
        "updated_at": "2018-07-27T04:28:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-07-25T19:24:08.000Z"
    },
    "comments": [
        {
            "created_at": "2018-07-25T19:24:08.256Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2842?focusedCommentId=16556160) by Robbie Gruener (rgruener):*\nI have not been able to reproduce well. It likely was due to an hdfs connection issue and not an issue with pyarrow"
        }
    ]
}