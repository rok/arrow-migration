{
    "issue": {
        "title": "[R] Write compressed data streams (particularly over S3)",
        "body": "***Note**: This issue was originally created as [ARROW-16144](https://issues.apache.org/jira/browse/ARROW-16144). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe python bindings have `CompressedOutputStream`, but\u00a0 I don't see how we can do this on the R side (e.g. with `write_csv_arrow()`).\u00a0 It would be wonderful if we could both read and write compressed streams, particularly for CSV and particularly for remote filesystems, where this can provide considerable performance improvements.\u00a0 \r\n\r\n(For comparison, readr will write a compressed stream automatically based on the extension for the given filename, e.g. `readr::write_csv(data, \"file.csv.gz\")` or `write_csv(\"data.file.xz\")`\u00a0 )",
        "created_at": "2022-04-07T17:03:25.000Z",
        "updated_at": "2022-05-19T05:42:24.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-05-18T21:25:03.000Z"
    },
    "comments": [
        {
            "created_at": "2022-04-11T15:04:42.770Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16144?focusedCommentId=17520635) by Dewey Dunnington (paleolimbot):*\nI'm fairly sure this is implemented...is this the kind of behaviour you were looking for?\r\n\r\n```R\n\r\nlibrary(arrow, warn.conflicts = FALSE)\r\n\r\ntf <- tempfile(fileext = \".gz\")\r\nwrite_csv_arrow(mtcars, tf)\r\nreadr::read_csv(gzfile(tf))\r\n#> Rows: 32 Columns: 11\r\n#> \u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n#> Delimiter: \",\"\r\n#> dbl (11): mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\r\n#> \r\n#> \u2139 Use `spec()` to retrieve the full column specification for this data.\r\n#> \u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message.\r\n#> # A tibble: 32 \u00d7 11\r\n#>      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\r\n#>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\r\n#>  1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\r\n#>  2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\r\n#>  3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\r\n#>  4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\r\n#>  5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\r\n#>  6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\r\n#>  7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\r\n#>  8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\r\n#>  9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\r\n#> 10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\r\n#> # \u2026 with 22 more rows\r\n```\r\n"
        },
        {
            "created_at": "2022-04-11T21:25:29.198Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16144?focusedCommentId=17520814) by Carl Boettiger (cboettig):*\nHi, sorry, but I believe this is not compressing the file at all.\u00a0\u00a0 (Note that there's no need to use `gzfile` in your call to readr, nor does it cause a failure if the file is not compressed.\u00a0 `connection` types can be confusing in R.\u00a0 \r\n\r\nWitness this example: with readr, changing the extension to include .gz results in a smaller file with a different hash, but in arrow I get the identical hash and file size, so I think no compression is happening.\u00a0 \r\n\r\n\r\n\r\n```java\n\r\nlibrary(arrow, warn.conflicts = FALSE)tf <- tempfile(fileext = \".gz\")\r\ntf_plain <- tempfile(fileext = \".csv\")## In readr, compressed & non-compressed have different hashes as expected\r\nreadr::write_csv(mtcars, tf)\r\ngz <- openssl::sha256(file(tf, \"rb\"))\r\nreadr::write_csv(mtcars, tf_plain)\r\nplain <- openssl::sha256(file(tf_plain, \"rb\"))\r\ntestthat::expect_lt(fs::file_size(tf), fs::file_size(tf_plain))\r\ntestthat::expect_false(identical(gz, plain))\r\nwrite_csv_arrow(mtcars, tf)\r\ngz <- openssl::sha256(file(tf, \"rb\"))\r\nwrite_csv_arrow(mtcars, tf_plain)\r\nplain <- openssl::sha256(file(tf_plain, \"rb\"))# these fail\r\ntestthat::expect_lt(fs::file_size(tf), fs::file_size(tf_plain))\r\n#> Error: fs::file_size(tf) is not strictly less than fs::file_size(tf_plain). Difference: 0\r\ntestthat::expect_false(identical(gz, plain))\u00a0\r\n#> Error: identical(gz, plain) is not FALSE\r\n#>\u00a0\r\n#> `actual`: \u00a0 TRUE\u00a0\r\n#> `expected`: FALSE\r\n```\r\n\r\nI should also have mentioned, I'm hoping with arrow that I can use this with the S3 object store explicitly, e.g.\u00a0\r\n\r\n\r\n```java\n\r\ns3 <- s3_bucket(\"test\", endpoint_override= \"minio.thelio.carlboettiger.info\")\r\npath <- s3$path(\"cars.csv.gz\")\r\nopenssl::sha256(url(\"https://minio.thelio.carlboettiger.info/test/cars.csv.gz\")) \n```\r\n\u00a0\r\n\r\nBut that doesn't work either.\u00a0 Also I would expect to have finer control over output stream to write generic compressed objects with the filesystem interface, e.g. using something like s3$OpenCompressedOutputStream, just like we can use s3$OpenOutputStream.\u00a0 \r\n\r\nThanks so much for your help.\u00a0 Arrow is just such an amazing library, and really a game-changer for data science workflows in R in particular.\u00a0 Very much appreciate all you do and eagerly watch for each release!\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-04-12T13:04:23.499Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16144?focusedCommentId=17521132) by Dewey Dunnington (paleolimbot):*\nThank you for catching my error here! I know that we did some compression detection but it turns out that's only on read: https://github.com/apache/arrow/blob/master/r/R/io.R#L240-L298\r\n\r\nYou can use `OpenOutputStream` and `CompressedOutputStream` for any filesystem (including S3), although we would need to implement the compression detection based on filename for this to \"just work\" with the .gz suffix:\r\n\r\n```R\n\r\nlibrary(arrow, warn.conflicts = FALSE)\r\n\r\ndir <- tempfile()\r\ndir.create(dir)\r\nsubdir <- file.path(dir, \"bucket\")\r\ndir.create(subdir)\r\n\r\n\r\nminio_server <- processx::process$new(\"minio\", args = c(\"server\", dir), supervise = TRUE)\r\nSys.sleep(1)\r\nstopifnot(minio_server$is_alive())\r\n\r\ns3_uri <- \"s3://minioadmin:minioadmin@?scheme=http&endpoint_override=localhost%3A9000\"\r\nbucket <- s3_bucket(s3_uri)\r\n\r\ndata <- data.frame(x = 1:1e4)\r\n\r\nout_compressed <- CompressedOutputStream$create(bucket$OpenOutputStream(\"bucket/data.csv.gz\"))\r\nwrite_csv_arrow(data, out_compressed)\r\nout_compressed$close()\r\n\r\n\r\nout <- bucket$OpenOutputStream(\"bucket/data.csv\")\r\nwrite_csv_arrow(data, out)\r\nout$close()\r\n\r\nfile.size(file.path(subdir, \"data.csv.gz\"))\r\n#> [1] 22627\r\nfile.size(file.path(subdir, \"data.csv\"))\r\n#> [1] 48898\r\n\r\nminio_server$interrupt()\r\n#> [1] TRUE\r\nSys.sleep(1)\r\nstopifnot(!minio_server$is_alive())\r\n```\r\n"
        },
        {
            "created_at": "2022-05-18T21:25:03.074Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16144?focusedCommentId=17539090) by Neal Richardson (npr):*\nIssue resolved by pull request 13183\n<https://github.com/apache/arrow/pull/13183>"
        }
    ]
}