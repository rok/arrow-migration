{
    "issue": {
        "title": "[R] SF columns in datasets with filters",
        "body": "***Note**: This issue was originally created as [ARROW-12542](https://issues.apache.org/jira/browse/ARROW-12542). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFirst reported at https://issues.apache.org/jira/browse/ARROW-10386?focusedCommentId=17331668&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17331668\r\n\r\nOK, I actually have recreated a similar issue. In the following code, I create an sf object and write it as a dataset to parquet files. I then call open_dataset() on the files.\r\n\r\nIf I collect() the dataset I get back an sf object, no problem.\r\n\r\nBut if I first filter() the dataset then collect() I get an error.\r\n\r\n```r\n\r\nlibrary(sf)\r\nlibrary(arrow)\r\nlibrary(dplyr)\r\n\r\nn <- 10000\r\n\r\nfake <- tibble(\r\n    ID=seq(n),\r\n    Date=sample(seq(as.Date('2019-01-01'), as.Date('2021-04-01'), by=1), size=n, replace=TRUE),\r\n    x=runif(n=n, min=-170, max=170),\r\n    y=runif(n=n, min=-60, max=70),\r\n    text1=sample(x=state.name, size=n, replace=TRUE),\r\n    text2=sample(x=state.name, size=n, replace=TRUE),\r\n    text3=sample(x=state.division, size=n, replace=TRUE),\r\n    text4=sample(x=state.region, size=n, replace=TRUE),\r\n    text5=sample(x=state.abb, size=n, replace=TRUE),\r\n    num1=sample(x=state.center$x, size=n, replace=TRUE),\r\n    num2=sample(x=state.center$y, size=n, replace=TRUE),\r\n    num3=sample(x=state.area, size=n, replace=TRUE),\r\n    Rand1=rnorm(n=n),\r\n    Rand2=rnorm(n=n, mean=100, sd=3),\r\n    Rand3=rbinom(n=n, size=10, prob=0.4)\r\n)\r\n\r\n# make it into an sf object\r\nspat <- fake %>% \r\n    st_as_sf(coords=c('x', 'y'), remove=FALSE, crs = 4326)\r\n\r\nclass(spat)\r\nclass(spat$geometry)\r\n\r\n# create new columns for partitioning and write to disk\r\nspat %>% \r\n    mutate(Year=lubridate::year(Date), Month=lubridate::month(Date)) %>% \r\n    group_by(Year, Month) %>% \r\n    write_dataset('data/splits/', format='parquet')\r\n\r\nspat_in <- open_dataset('data/splits/')\r\n\r\nclass(spat_in)\r\n\r\n# it's an sf as expected\r\nspat_in %>% collect() %>% class()\r\nspat_in %>% collect() %>% pull(geometry) %>% class()\r\n\r\n# it even plots\r\nleaflet::leaflet() %>% \r\n    leaflet::addTiles() %>% \r\n    leafgl::addGlPoints(data=spat_in %>% collect())\r\n\r\n# but if we filter first\r\nspat_in %>% \r\n    filter(Year == 2020 & Month == 2) %>% \r\n    collect()\r\n\r\n# we get this error\r\nError in st_geometry.sf(x) : \r\n  attr(obj, \"sf_column\") does not point to a geometry column.\r\nDid you rename it, without setting st_geometry(obj) <- \"newname\"?\r\nIn addition: Warning message:\r\nInvalid metadata$r \r\n```",
        "created_at": "2021-04-26T13:20:40.000Z",
        "updated_at": "2021-10-19T13:19:43.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-10-19T13:19:43.000Z"
    },
    "comments": [
        {
            "created_at": "2021-04-26T20:01:51.524Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12542?focusedCommentId=17332705) by Jonathan Keane (jonkeane):*\nOk I'm starting to see what's going on here: When collecting the whole dataset everything is fine. But when we filter, we have the filtered data but try to apply the whole list of metdata to that column, but there's a length mismatch, so the metadata application stops (that's what that warning is that you're seeing) and thus the SF column isn't getting reconstructed correctly. \r\n\r\nThis is the branch where that is being called, but we might want/need to filter the metadata higher https://github.com/apache/arrow/blob/master/r/R/metadata.R#L62-L65 I'm looking into this to see if we can fix it."
        },
        {
            "created_at": "2021-04-26T22:14:37.161Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12542?focusedCommentId=17332782) by Jonathan Keane (jonkeane):*\nOh, and interestingly, when writing the dataset it looks like when partitioning we're actually writing all of the metadata out to each partition (instead of partitioning it so that each partition only has the metadata for the rows in its partition)\r\n\r\n```r\n\r\n> df <- tibble::tibble(\r\n+   part = 1:10,\r\n+   a = rep(1:2, 5),\r\n+   x = rep(list(structure(1, foo = \"bar\"), structure(2, baz = \"qux\")), 5)\r\n+ )\r\n> \r\n> # write the dataset, and open it\r\n> tmp <- tempfile()\r\n> write_dataset(df, tmp, partitioning = \"part\", format = \"parquet\")\r\n> ds <- open_dataset(tmp)\r\n> \r\n> read_parquet(file.path(tmp, \"part=1/part-0.parquet\"))\r\n# A tibble: 1 x 2\r\n      a              x\r\n  <int> <list<double>>\r\n1     1            [1]\r\nWarning message:\r\nInvalid metadata$r \r\n> \r\n> tab <- read_parquet(file.path(tmp, \"part=1/part-0.parquet\"), as_data_frame = FALSE)\r\n> \r\n> tab$metadata$r\r\n 'arrow_r_metadata' chr \"A\\n3\\n262148\\n197888\\n5\\nUTF-8\\n531\\n2\\n531\\n0\\n1026\\n1\\n262153\\n5\\nnames\\n16\\n0\\n254\\n531\\n3\\n254\\n254\\n531\\n2\"| __truncated__\r\nList of 2\r\n $ attributes: Named list()\r\n $ columns   :List of 3\r\n  ..$ part: NULL\r\n  ..$ a   : NULL\r\n  ..$ x   :List of 2\r\n  .. ..$ attributes: NULL\r\n  .. ..$ columns   :List of 10\r\n  .. .. ..$ :List of 2\r\n  .. .. .. ..$ attributes:List of 1\r\n  .. .. .. .. ..$ foo: chr \"bar\"\r\n  .. .. .. ..$ columns   : NULL\r\n  .. .. ..$ :List of 2\r\n  [...]\r\n```\r\n\r\nSolving that alone (probably) won't be enough to resolve the `filter()` issue as well, but will be part of the solution"
        },
        {
            "created_at": "2021-04-30T22:42:32.433Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12542?focusedCommentId=17337670) by Neal Richardson (npr):*\nIf we're storing per-row metadata in the schema, that's not going to behave well. Perhaps we need to rethink how we store this in a way that round-trips better."
        }
    ]
}