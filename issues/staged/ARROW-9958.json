{
    "issue": {
        "title": "Error writing record batches to IPC streaming format",
        "body": "***Note**: This issue was originally created as [ARROW-9958](https://issues.apache.org/jira/browse/ARROW-9958). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWriting record batches to the Arrow IPC streaming format with on-the-fly compression generally raises errors of one type or the other when reading it back.\r\n\r\nPFA the code producing each of the below errors. I can't reproduce it for smaller batch sizes, so it probably has to do with size of each record batch. It does not seem specific to pyarrow since I see a similar issue with the C-Glib API. \r\n\r\n\r\n#Error case 1\r\n\r\n```\r\n~/py376/lib/python3.7/site-packages/pyarrow/ipc.pxi in pyarrow.lib._CRecordBatchReader.read_next_batch()\r\n\r\n~/py376/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nOSError: Truncated compressed stream\r\n```\r\n\r\n#Error case 2\r\n\r\n```\r\n~/py376/lib/python3.7/site-packages/pyarrow/ipc.pxi in pyarrow.lib._RecordBatchStreamReader._open()\r\n\r\n~/py376/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/py376/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Tried reading schema message, was null or length 0\r\n```\r\n\r\n",
        "created_at": "2020-09-10T07:13:02.000Z",
        "updated_at": "2020-09-14T05:27:12.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: GLib",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-09-10T21:10:18.000Z"
    },
    "comments": [
        {
            "created_at": "2020-09-10T21:10:18.387Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9958?focusedCommentId=17193856) by Kouhei Sutou (kou):*\nYou need to ensure closing writers and output streams.\r\n\r\nFor `example1.py`:\r\n\r\n```\n\r\nsink = pa.output_stream(FILE, COMPRESSION_TYPE)\r\nwriter = pa.RecordBatchStreamWriter(sink, batch.schema)\r\nfor _ in range(5):\r\n    writer.write_batch(batch)\r\nwriter.close()\r\nsink.close()\r\n```\r\n\r\nFor `example2.py`:\r\n\r\n```\n\r\nsink = pa.output_stream(FILE, COMPRESSION_TYPE)\r\nwriter = pa.RecordBatchStreamWriter(sink, batch.schema)\r\nfor _ in range(5):\r\n    writer.write_batch(batch)\r\nwriter.close()\r\nsink.close()\r\n```"
        },
        {
            "created_at": "2020-09-14T05:27:12.533Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9958?focusedCommentId=17195188) by Ishan (ananis25):*\nThank you. While using the C-Glib API too, I did close the writer but missed the sink."
        }
    ]
}