{
    "issue": {
        "title": "[C++][Flight] See if reading/writing to gRPC get/put streams asynchronously helps performance",
        "body": "***Note**: This issue was originally created as [ARROW-10351](https://issues.apache.org/jira/browse/ARROW-10351). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWe don't use any asynchronous concepts in the way that Flight is implemented now, i.e. IPC deconstruction/reconstruction (which may include compression!) is not performed concurrent with moving FlightData objects through the gRPC machinery, which may yield suboptimal performance. \r\n\r\nIt might be better to apply an actor-type approach where a dedicated thread retrieves and prepares the next raw IPC message (within a Future) while the current IPC message is being processed \u2013 that way reading/writing to/from the gRPC stream is not blocked on the IPC code doing its thing. ",
        "created_at": "2020-10-20T02:57:48.000Z",
        "updated_at": "2021-08-02T18:36:52.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: FlightRPC",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-02-22T06:28:02.491Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17288198) by Yibo Cai (yibocai):*\nHi `[~wesm]`, `[~lidavidm]`, \r\n I'm planning to investigate if gRPC async server may improve Flight performance. Would like to know if community is doing similar things to avoid duplicated effort. Thanks.\r\n I see another Jira looks related https://issues.apache.org/jira/browse/ARROW-1009."
        },
        {
            "created_at": "2021-02-22T12:47:20.494Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17288361) by David Li (lidavidm):*\nWe are thinking about pipelining compression and RPC streaming but have not started this yet. That also doesn't necessarily need to use the gRPC async APIs. We are overall improving support for concurrency in Arrow, e.g. ARROW-10183, ARROW-10117, ARROW-8626 (and its related issues)."
        },
        {
            "created_at": "2021-03-02T15:55:33.551Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17293795) by David Li (lidavidm):*\nFWIW `[~yibocai]`, I actually tried pipelining the Flight client and server: <https://github.com/lidavidm/arrow/tree/arrow-10351-async-compression>\r\n\r\nThis way we would do I/O-bound (gRPC read/write) and CPU-bound (Arrow record batch encoding/decoding) work on separate threads with readahead on the I/O side.\r\n\r\nIn our tests it did not have any benefit. I didn't test the actual async gRPC APIs, however, unless there is a major difference between how those are implemented on the gRPC side, I'd be doubtful that they'd help by themselves unless they unlock some opportunity to parallelize/pipeline work. But if you are investigating we'd be curious to see the results! It could definitely improve how ergonomic the APIs are and/or open a path to asyncio support in the Python bindings. It might also improve latency instead of throughput (our tests have focused on throughput)."
        },
        {
            "created_at": "2021-03-03T08:21:17.647Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17294366) by Yibo Cai (yibocai):*\nThanks `[~lidavidm]`, your information is very helpful.\r\nI haven't started actual work about async gRPC evaluation so it may take some time. Will update when I have some solid findings."
        },
        {
            "created_at": "2021-04-01T10:37:55.868Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17313074) by yibocai#1 (yibocai#1):*\nDid a very rudiment test to interleave data compressing with data sending. Looks it can improve performance.\r\n A thread is spawned to prepare next record batch when main thread is sending current record batch.\r\n Basically, I separated [these two lines](https://github.com/apache/arrow/blob/master/cpp/src/arrow/ipc/writer.cc#L999-L1000) into two different functions running in their own threads.\r\n\r\nFor streams = 1, I see stable improvement of speed (1200 -> 1700) and latency (90 ->70).\r\nNo improvement when streams >= 4. I'm testing on one skylake server with 32 cores and 2 numa nodes. See POC code for details.\r\nPOC code at <https://github.com/cyb70289/arrow/commit/24a55f71325d280a3be42cbbcd41c392471c8af6>"
        },
        {
            "created_at": "2021-04-01T12:25:22.207Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17313140) by David Li (lidavidm):*\n`[~yibocai]` \u00a0interesting, thanks for the results! It seems we should be able to build this in to Flight directly so that the user doesn't have to worry about this (also, we could potentially make things like the ring buffer size configurable).\r\n\r\nWhat is the effect when no compression is used?"
        },
        {
            "created_at": "2021-04-02T02:12:52.638Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17313519) by yibocai#1 (yibocai#1):*\n_>> What is the effect when no compression is used?_\r\nNo effect if compression is not used. The data preparation time is trivial compared with packet delivering.\r\n\r\nPer my profiling, 70% cpu time is used in compression, 30% is packet sending. So an improvement of ~30% looks reasonable when interleaving them."
        },
        {
            "created_at": "2021-04-16T14:18:12.598Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17323855) by David Li (lidavidm):*\nHmm, I was unable to replicate the results here. I checked out your branch and current master branch. I'm running on an Intel Comet Lake laptop with 8 physical cores.\r\n\r\nCurrent master:\r\n```\n\r\n> env OMP_NUM_THREADS=4 ./release/arrow-flight-benchmark -test_put -num_perf_runs=4 -num_streams=4 -num_threads=1 \r\nUsing spawned TCP server\r\nServer running with pid 5988\r\nServer host: localhost\r\nServer port: 31337\r\nTesting method: DoPut\r\nServer host: localhost\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 131040\r\nBatches written: 39072\r\nBytes written: 5120000000\r\nNanos: 2655271083\r\nSpeed: 1838.91 MB/s\r\nThroughput: 14714.9 batches/s\r\nLatency mean: 65 us\r\nLatency quantile=0.5: 65 us\r\nLatency quantile=0.95: 75 us\r\nLatency quantile=0.99: 82 us\r\nLatency max: 941 us\r\n \n```\r\nThis branch:\r\n```\n\r\n> env OMP_NUM_THREADS=4 ./release/arrow-flight-benchmark -test_put -num_perf_runs=4 -num_streams=1 -num_threads=1\r\nUsing spawned TCP server\r\nServer running with pid 5921\r\nServer host: localhost\r\nServer port: 31337\r\nTesting method: DoPut\r\nServer host: localhost\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 131040\r\nBatches written: 9768\r\nBytes written: 1280000000\r\nNanos: 686687591\r\nSpeed: 1777.67 MB/s\r\nThroughput: 14224.8 batches/s\r\nLatency mean: 67 us\r\nLatency quantile=0.5: 67 us\r\nLatency quantile=0.95: 76 us\r\nLatency quantile=0.99: 92 us\r\nLatency max: 958 us\r\n```"
        },
        {
            "created_at": "2021-04-17T02:15:32.110Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17324149) by yibocai#1 (yibocai#1):*\nWill redo the test on an 8 core desktop.\nMaybe too many threads (grpc client, server, compression) compete limited cores."
        },
        {
            "created_at": "2021-04-21T05:47:28.865Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17326276) by yibocai#1 (yibocai#1):*\nI retested on an old i7 machine with 8 cores. Running the same commands as yours, I can see stable improvements. Speed: 800 -> 1000, Latency: 170 -> 120.\r\n\r\nMy POC test code is too bad to use correctly. I hardcoded to use compression in [client.cc](https://github.com/cyb70289/arrow/blob/flight-poc/cpp/src/arrow/flight/client.cc#L73). Master branch won't use compression by default. I meant to comment out [INTERLEAVE_PREPARE_AND_SEND](https://github.com/cyb70289/arrow/blob/flight-poc/cpp/src/arrow/flight/flight_benchmark.cc#L196 ) macro to benchmark again master branch.\r\n\r\nWill provide a patch to add command line options to exercise easily both code paths."
        },
        {
            "created_at": "2021-04-26T12:59:53.433Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17332255) by David Li (lidavidm):*\nI do see an improvement when both comparisons are on this branch, but with the #define toggled. That is, using a background thread helps performance when compression is enabled. However, I don't see a benefit compared to not using compression, which is what I was interested in.\r\n\r\nI'll try to test the 3 cases here (no compression, compression, compression with background thread) on a pair of EC2 instances when I get a chance - testing over localhost probably isn't the fairest comparison."
        },
        {
            "created_at": "2021-04-26T14:20:41.416Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17332450) by yibocai#1 (yibocai#1):*\nPer my test, the data preparation time (without compression) seems trivial compared with data sending time. So it may be hard to see the benefits if compression is not enabled.\r\nI will also do some tests across hosts over network."
        },
        {
            "created_at": "2021-04-26T14:47:36.223Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17332470) by David Li (lidavidm):*\nSure, serializing the batches is otherwise too cheap to justify the thread. However, if compression + background thread can't outperform no compression at all, then there's little point to compression in the first place.\r\n\r\nI tested two EC2 t3.xlarge instances (4 cores, 16 GB RAM). They should have ~5 Gbps of bandwidth between them. All benchmarks were run as `./release/arrow-flight-benchmark -test_put -num_perf_runs=4 -num_streams=4 -num_threads=1 -server_host=(host)`.\r\n\r\nWith compression, with background thread:\r\n```\n\r\nUsing standalone TCP server\r\nServer host: ip-172-31-68-128.ec2.internal\r\nServer port: 31337\r\nTesting method: DoPut\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 131040\r\nBatches written: 39072\r\nBytes written: 5120000000\r\nNanos: 9203507933\r\nSpeed: 530.538 MB/s\r\nThroughput: 4245.34 batches/s\r\nLatency mean: 230 us\r\nLatency quantile=0.5: 182 us\r\nLatency quantile=0.95: 392 us\r\nLatency quantile=0.99: 1411 us\r\nLatency max: 11809 us\r\n```\r\nWith compression, without background thread:\r\n```\n\r\nUsing standalone TCP server\r\nServer host: ip-172-31-68-128.ec2.internal\r\nServer port: 31337\r\nTesting method: DoPut\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 131040\r\nBatches written: 39072\r\nBytes written: 5120000000\r\nNanos: 9256189526\r\nSpeed: 527.519 MB/s\r\nThroughput: 4221.18 batches/s\r\nLatency mean: 232 us\r\nLatency quantile=0.5: 195 us\r\nLatency quantile=0.95: 328 us\r\nLatency quantile=0.99: 874 us\r\nLatency max: 20200 us\n```\r\nWithout compression, without background thread:\r\n```\n\r\nUsing standalone TCP server\r\nServer host: ip-172-31-68-128.ec2.internal\r\nServer port: 31337\r\nTesting method: DoPut\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 131040\r\nBatches written: 39072\r\nBytes written: 5120000000\r\nNanos: 8678223134\r\nSpeed: 562.651 MB/s\r\nThroughput: 4502.3 batches/s\r\nLatency mean: 216 us\r\nLatency quantile=0.5: 55 us\r\nLatency quantile=0.95: 1556 us\r\nLatency quantile=0.99: 2806 us\r\nLatency max: 21395 us\n```\r\nIn short, for Flight, it seems compression is simply not worth it, regardless of whether there's a background thread or not. This tradeoff may change when there's less bandwidth available. It does seem p99 latency is better.\r\n\r\nAnd there are other factors. For instance, benchmark uses random data which may not compress well; a different dataset may perform better. ZSTD is relatively fast, but here we aren't tuning it for compression/decompression speed."
        },
        {
            "created_at": "2021-04-26T15:14:25.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17332487) by yibocai#1 (yibocai#1):*\nRegarding random test data, I do see the compressed data is actually larger than original uncompressed data.\r\n\r\nShall we replace random test data with some real world data? Otherwise, the compression test in current benchmark will always be a pure loss."
        },
        {
            "created_at": "2021-04-27T19:26:11.082Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17333505) by David Li (lidavidm):*\n`[~yibocai]` it would be nice to have the option to use real test data, yes. Perhaps the benchmark (both client/server) could accept a path to an IPC file as an option."
        },
        {
            "created_at": "2021-05-03T13:22:14.481Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17338366) by David Li (lidavidm):*\n`[~yibocai]` I rebased the benchmark (<https://github.com/lidavidm/arrow/tree/flight-poc>) and ran with real data (the NYC taxi dataset, for the month of 2009/01: <https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2009-01.csv>).\r\n\r\nThe command in all cases was `env OMP_NUM_THREADS=4 ./release/arrow-flight-benchmark -test_put -num_perf_runs=4 -num_streams=4 -num_threads=1 -data_file data.feather`.\r\n\r\nMaster (no compression):\r\n```\n\r\nTesting method: DoPut\r\nUsing spawned TCP server\r\nServer running with pid 20909\r\nServer host: localhost\r\nServer port: 31337\r\nServer host: localhost\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 8007363952\r\nSpeed: 2167.44 MB/s\r\nThroughput: 431.603 batches/s\r\nLatency mean: 1309 us\r\nLatency quantile=0.5: 1156 us\r\nLatency quantile=0.95: 2135 us\r\nLatency quantile=0.99: 2783 us\r\nLatency max: 3876 us\r\n```\r\nflight-poc, with compression, without asynchronous compression:\r\n```\n\r\nTesting method: DoPut\r\nUsing spawned TCP server\r\nServer running with pid 13773\r\nServer host: localhost\r\nServer port: 31337\r\nServer host: localhost\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 23333072829\r\nSpeed: 743.815 MB/s\r\nThroughput: 148.116 batches/s\r\nLatency mean: 5666 us\r\nLatency quantile=0.5: 5544 us\r\nLatency quantile=0.95: 6460 us\r\nLatency quantile=0.99: 6831 us\r\nLatency max: 8569 us\r\n```\r\nflight-poc, with compression, with async compression:\r\n```\n\r\nTesting method: DoPut\r\nUsing spawned TCP server\r\nServer running with pid 13689\r\nServer host: localhost\r\nServer port: 31337\r\nServer host: localhost\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 22178585229\r\nSpeed: 782.533 MB/s\r\nThroughput: 155.826 batches/s\r\nLatency mean: 5320 us\r\nLatency quantile=0.5: 5183 us\r\nLatency quantile=0.95: 6227 us\r\nLatency quantile=0.99: 6840 us\r\nLatency max: 9255 us\r\n```\r\nSo it seems with real data, things get even worse. Async compression is better than sync compression, but neither is in the ballpark of simply not compressing. Of course this is all over localhost which is likely not fair to compression so maybe I should try over EC2 next (~600MiB/s max bandwidth)."
        },
        {
            "created_at": "2021-05-03T19:45:37.395Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17338585) by David Li (lidavidm):*\nAnd between two EC2 t3.xlarge instances:\r\n\r\nWithout compression:\r\n```\n\r\nTesting method: DoPut\r\nUsing standalone TCP server\r\nServer host: ip-172-31-73-63.ec2.internal\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 36150890880\r\nSpeed: 480.085 MB/s\r\nThroughput: 95.5993 batches/s\r\nLatency mean: 8485 us\r\nLatency quantile=0.5: 8692 us\r\nLatency quantile=0.95: 9233 us\r\nLatency quantile=0.99: 10627 us\r\nLatency max: 13944 us \n```\r\nflight-poc, with sync compression:\r\n```\n\r\nTesting method: DoPut\r\nUsing standalone TCP server\r\nServer host: ip-172-31-73-63.ec2.internal\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 38743831916\r\nSpeed: 447.955 MB/s\r\nThroughput: 89.2013 batches/s\r\nLatency mean: 9305 us\r\nLatency quantile=0.5: 9312 us\r\nLatency quantile=0.95: 9736 us\r\nLatency quantile=0.99: 10097 us\r\nLatency max: 11723 us \n```\r\nflight-poc, with async compression:\r\n```\n\r\nTesting method: DoPut\r\nUsing standalone TCP server\r\nServer host: ip-172-31-73-63.ec2.internal\r\nServer port: 31337\r\nNumber of perf runs: 4\r\nNumber of concurrent gets/puts: 1\r\nBatch size: 5265782\r\nBatches written: 3456\r\nBytes written: 18198543232\r\nNanos: 36706487822\r\nSpeed: 472.818 MB/s\r\nThroughput: 94.1523 batches/s\r\nLatency mean: 8739 us\r\nLatency quantile=0.5: 8726 us\r\nLatency quantile=0.95: 9258 us\r\nLatency quantile=0.99: 9793 us\r\nLatency max: 12832 us \n```\r\nIt still doesn't seem very beneficial. Maybe if we have a very compressible dataset, and/or tune the compressor used?"
        },
        {
            "created_at": "2021-05-04T07:22:27.284Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17338810) by yibocai#1 (yibocai#1):*\nLooks compression is not very useful for normal cases. Things may be worse if the receiver has to decompress the payload.\r\nI guess compression is only helpful if network bandwidth is limited.\r\n\r\nWhat about encryption? Is it handled transparently by gRPC?"
        },
        {
            "created_at": "2021-05-04T11:58:45.286Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10351?focusedCommentId=17338940) by David Li (lidavidm):*\nEncryption is supported in the form of TLS. In the past, benchmarks showed it had quite a big hit, but I haven't tested recently. That would also be a useful thing to check."
        }
    ]
}