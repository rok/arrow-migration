{
    "issue": {
        "title": "[Rust][Parquet] Use iterators to increase performance of creating Arrow arrays",
        "body": "***Note**: This issue was originally created as [ARROW-11897](https://issues.apache.org/jira/browse/ARROW-11897). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe\u00a0overall\u00a0goal\u00a0is\u00a0to\u00a0create\u00a0an\u00a0efficient\u00a0pipeline\u00a0from\u00a0Parquet\u00a0page\u00a0data\u00a0into\u00a0Arrow\u00a0arrays,\u00a0with\u00a0as\u00a0little\u00a0intermediate\u00a0conversion\u00a0and\u00a0memory\u00a0allocation\u00a0as\u00a0possible.\u00a0It\u00a0is\u00a0assumed,\u00a0that for best\u00a0performance,\u00a0we\u00a0favor\u00a0doing\u00a0fewer\u00a0but\u00a0larger\u00a0copy\u00a0operations\u00a0(rather\u00a0than\u00a0more\u00a0but\u00a0smaller).\u00a0\r\n\r\nSuch\u00a0a\u00a0pipeline\u00a0would\u00a0need\u00a0to\u00a0be\u00a0flexible\u00a0in\u00a0order\u00a0to\u00a0enable\u00a0high\u00a0performance\u00a0implementations\u00a0in\u00a0several\u00a0different\u00a0cases:\r\n (1)\u00a0In\u00a0some\u00a0cases,\u00a0such\u00a0as\u00a0plain-encoded\u00a0number\u00a0array,\u00a0it\u00a0might\u00a0even\u00a0be\u00a0possible\u00a0to\u00a0copy\u00a0/\u00a0create\u00a0the\u00a0array\u00a0from\u00a0a\u00a0single\u00a0contiguous\u00a0section from a page\u00a0buffer.\u00a0\r\n (2)\u00a0In\u00a0other\u00a0cases,\u00a0such\u00a0as\u00a0plain-encoded\u00a0string\u00a0array,\u00a0since values are encoded in non-contiguous\u00a0slices (where value bytes are separated by length bytes) in a\u00a0page\u00a0buffer\u00a0contains\u00a0multiple\u00a0values,\u00a0individual\u00a0values\u00a0will\u00a0have\u00a0to\u00a0be\u00a0copied\u00a0separately\u00a0and\u00a0it's\u00a0not\u00a0obvious\u00a0how\u00a0this\u00a0can\u00a0be\u00a0avoided.\r\n (3)\u00a0Finally,\u00a0in\u00a0the\u00a0case\u00a0of\u00a0bit-packing\u00a0encoding\u00a0and\u00a0smaller\u00a0numeric\u00a0values,\u00a0page\u00a0buffer\u00a0data\u00a0has\u00a0to\u00a0be\u00a0decoded\u00a0/\u00a0expanded\u00a0before\u00a0it\u00a0is\u00a0ready\u00a0to\u00a0copy\u00a0into\u00a0an\u00a0arrow\u00a0arrow,\u00a0so\u00a0a\u00a0`Vec<u8>`\u00a0will\u00a0have\u00a0to\u00a0be\u00a0returned\u00a0instead\u00a0of\u00a0a\u00a0slice\u00a0pointing\u00a0to\u00a0a\u00a0page\u00a0buffer.\r\n\r\nI propose that the implementation is split into three layers - (1) decoder, (2) column reader and (3) array converter layers (not too dissimilar from the current implementation, except it would be based on Iterators), as follows:\r\n\r\n**(1) Decoder layer:**\r\n\r\nA\u00a0decoder\u00a0output\u00a0abstraction\u00a0that\u00a0enables\u00a0all\u00a0of\u00a0the\u00a0above\u00a0cases\u00a0and\u00a0minimizes\u00a0intermediate\u00a0memory\u00a0allocation\u00a0is\u00a0`Iterator<Item\u00a0=\u00a0(count,\u00a0AsRef<[u8]>)>`.\r\n Then\u00a0in\u00a0case\u00a0(1)\u00a0above,\u00a0where\u00a0a\u00a0numeric\u00a0array\u00a0could\u00a0be\u00a0created\u00a0from\u00a0a\u00a0single\u00a0contiguous\u00a0byte\u00a0slice,\u00a0such\u00a0an\u00a0iterator\u00a0could\u00a0return\u00a0a\u00a0single\u00a0item\u00a0such\u00a0as\u00a0`(1024,\u00a0&[u8])`.\u00a0\r\n In\u00a0case\u00a0(2)\u00a0above,\u00a0where\u00a0each\u00a0string\u00a0value\u00a0is\u00a0encoded\u00a0as\u00a0an\u00a0individual\u00a0byte\u00a0slice,\u00a0but\u00a0it\u00a0is\u00a0still\u00a0possible\u00a0to\u00a0copy\u00a0directly\u00a0from\u00a0a\u00a0page\u00a0buffer,\u00a0a\u00a0decoder\u00a0iterator\u00a0could\u00a0return\u00a0a\u00a0sequence\u00a0of\u00a0items\u00a0such\u00a0as\u00a0`(1,\u00a0&[u8])`.\u00a0\r\n And\u00a0finally\u00a0in\u00a0case\u00a0(3)\u00a0above,\u00a0where\u00a0bit-packed\u00a0values\u00a0have\u00a0to\u00a0be\u00a0unpacked/expanded,\u00a0and\u00a0it's\u00a0NOT\u00a0possible\u00a0to\u00a0copy\u00a0value\u00a0bytes\u00a0directly\u00a0from\u00a0a\u00a0page\u00a0buffer,\u00a0a\u00a0decoder\u00a0iterator\u00a0could\u00a0return\u00a0items\u00a0representing\u00a0chunks\u00a0of\u00a0values\u00a0such\u00a0as\u00a0`(32,\u00a0Vec<u8>)` where bit-packed values have been unpacked and\u00a0 the\u00a0chunk\u00a0size\u00a0is\u00a0configured\u00a0for\u00a0best\u00a0performance.\r\n\r\nAnother\u00a0benefit\u00a0of\u00a0an\u00a0`Iterator`-based\u00a0abstraction\u00a0is\u00a0that\u00a0it\u00a0would\u00a0prepare\u00a0the\u00a0parquet\u00a0crate\u00a0for\u00a0 migration\u00a0to\u00a0`async`\u00a0`Stream`s\u00a0(my\u00a0understanding\u00a0is\u00a0that\u00a0a\u00a0`Stream`\u00a0is\u00a0effectively\u00a0an\u00a0async\u00a0`Iterator`).\r\n\r\n**(2) Column reader layer:**\r\n\r\nThen\u00a0a\u00a0higher\u00a0level\u00a0iterator\u00a0could\u00a0combine\u00a0a\u00a0value\u00a0iterator\u00a0and\u00a0a\u00a0(def)\u00a0level\u00a0iterator\u00a0to\u00a0produce\u00a0a\u00a0sequence\u00a0of\u00a0`ValueSequence(count,\u00a0AsRef<[u8]>)`\u00a0and\u00a0`NullSequence(count)`\u00a0items\u00a0from\u00a0which\u00a0an\u00a0arrow\u00a0array\u00a0can\u00a0be\u00a0created\u00a0efficiently.\r\n\r\nIn\u00a0future,\u00a0a\u00a0higher\u00a0level\u00a0iterator\u00a0(for\u00a0the\u00a0keys)\u00a0could\u00a0be\u00a0combined\u00a0with\u00a0a\u00a0dictionary\u00a0value\u00a0iterator\u00a0to\u00a0create\u00a0a\u00a0dictionary\u00a0array.\r\n\r\n**(3) Array converter layer:**\r\n\r\nFinally,\u00a0Arrow\u00a0arrays\u00a0would\u00a0be\u00a0created\u00a0from\u00a0a\u00a0(generic)\u00a0higher-level\u00a0iterator,\u00a0using\u00a0a\u00a0layer\u00a0of\u00a0array\u00a0converters\u00a0that\u00a0know\u00a0what\u00a0the\u00a0value\u00a0bytes\u00a0and\u00a0nulls\u00a0mean\u00a0for\u00a0each\u00a0type\u00a0of\u00a0array.\r\n\r\n\u00a0\r\n\r\n`[~nevime]` , `[~Dandandan]` , `[~jorgecarleitao]` let\u00a0me\u00a0know\u00a0what\u00a0you\u00a0think\r\n\r\nNext\u00a0steps:\r\n \\*\u00a0split\u00a0work\u00a0into\u00a0smaller\u00a0tasks\u00a0that\u00a0could\u00a0be\u00a0done\u00a0over\u00a0time",
        "created_at": "2021-03-06T19:43:33.000Z",
        "updated_at": "2021-04-26T12:49:30.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Rust",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-04-26T12:49:30.000Z"
    },
    "comments": [
        {
            "created_at": "2021-03-06T19:50:27.340Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17296636) by Jorge Leit\u00e3o (jorgecarleitao):*\nNot much to say other than AWESOME! Look very nuch forward to it!"
        },
        {
            "created_at": "2021-03-07T09:34:38.277Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17296808) by Neville Dipale (nevi_me):*\nThis sounds like a solid proposal, I also like the split that you suggest :)\r\n\r\n"
        },
        {
            "created_at": "2021-03-07T09:56:26.934Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17296818) by Dani\u00ebl Heres (Dandandan):*\nThat sounds like a cool idea. I like the idea of a very thin abstraction that doesn't sacrifice performance.\r\n\r\nFor the iterator type, I think the count might not be (always) necessary? As it can depend on the datatype, or will be always be the same (1 or 32 / etc) for the other types? Are there situations were we really need the count?"
        },
        {
            "created_at": "2021-03-07T11:00:25.271Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17296838) by Yordan Pavlov (yordan-pavlov):*\n`[~Dandandan]` \u00a0regarding the count values, yes you are right - in the case of string arrays, the generated count values will always equal 1. But the count values may\u00a0still be useful in cases where a primitive array is split across multiple non-contiguous slices, e.g. due to page boundaries. It may be possible to calculate the count values based on the data type (I have to think more about that), but at the moment I still like how they make the expected value count explicit. This could change during implementation though."
        },
        {
            "created_at": "2021-03-07T21:38:03.584Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17296986) by Dani\u00ebl Heres (Dandandan):*\n`[~yordan-pavlov]` makes sense, thanks!"
        },
        {
            "created_at": "2021-03-27T11:54:01.798Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17309934) by Dani\u00ebl Heres (Dandandan):*\n`[~yordan-pavlov]` just checking - any updates to share and could you use some help?\r\nAny idea yet how the work could be split into multiple issues / PRs?\r\nMaybe I could focus on a subtask if we can split the work.\r\n\r\nI think it would be amazing to have a faster Parquet reader, even if it \"only\" is 5-10% - as it's a large performance bottleneck now :).\r\n\r\nDo you have some WIP code & experiments that could use a review?"
        },
        {
            "created_at": "2021-03-28T23:04:23.361Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17310301) by Yordan Pavlov (yordan-pavlov):*\nHi `[~Dandandan]`, apologies I should have updated on my progress earlier, but I was busy trying things out.\r\n\r\nMy thinking so far has been in the lines of how to replace pretty much the entire path from parquet pages all the way into arrow arrays using iterators (because I am hoping that an iterator-based implementation would minimize unnecessary memory allocation). Something like this:\u00a0\r\nIterator<RowGroup> >> Iterator<(ColumnChunkContext,\u00a0Page)>\u00a0>>\u00a0Iterator<(ValueSliceIterator,\u00a0DefLevelIterator,\u00a0RepLevelIterator)>\r\n>>\u00a0(Iterator<ValueSliceIterator>,\u00a0Iterator<DefLevelIterator>,\u00a0Iterator<RepLevelIterator>)\r\nSo far I have implemented splitting an iterator into multiple (parallel) iterators based on <https://stackoverflow.com/questions/25586681/splitting-iteratora-b-into-iteratora-and-iteratorb#25588440>\r\n\r\nThis will be useful, as illustrated above, for splitting an iterator over pages into iterators over values, def levels and rep levels which can be consume independently (but usually in parallel).\r\n\r\nAlso, in the past week I have been working on an splitting an iterator of byte slices into iterators that return no more than batch_size items - I have almost figured out how to do this, I just have to make it a bit more generic and do some more benchmarking. I would also like to do some benchmarking with <https://docs.rs/hyper/0.14.4/hyper/body/struct.Bytes.html>\u00a0(which appears to be an alternative implementation of the ByteBufferPtr that already exists in the parquet crate).\r\n\r\nFiguring out exactly how the work will be split into different PRs is what I will focus on next, but I already have some ideas:\r\n\r\nI think would be to start small, by building on\u00a0PageIterator::next()\u00a0->\u00a0PageReader to produce an iterator of pages, something like:\r\n\r\n\u00a0\r\n//\u00a0create\u00a0iterator\u00a0of\u00a0(contiguous)\u00a0data\u00a0slices\u00a0across\u00a0all\u00a0pages\u00a0from\u00a0all\u00a0row\u00a0groups\r\nrow_group_iter\u00a0//\u00a0iter\u00a0of\u00a0PageReader\r\n\u00a0\u00a0//\u00a0add\u00a0row\u00a0group\u00a0context\u00a0using\u00a0the\u00a0scan()\u00a0operator\r\n\u00a0\u00a0.iter_mut().flat_map(|x|\u00a0{\r\n\u00a0 \u00a0 \u00a0 // the column chunk / row group context is used to store dictionaries for dictionary-encoded chunks\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0let\u00a0context\u00a0=\u00a0Rc::new(RefCell::new(IterContext::new()));\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x.map(move\u00a0|v|\u00a0(context.clone(),\u00a0v))\r\n\u00a0\u00a0})\u00a0//\u00a0iter\u00a0of\u00a0(mut\u00a0RowGroupContext,\u00a0Page)\r\n\u00a0\u00a0.map(|(c,\u00a0p)|\u00a0{\u00a0\r\n\u00a0\u00a0\u00a0\u00a0let\u00a0mut\u00a0context\u00a0=\u00a0c.borrow_mut();\r\n\u00a0\u00a0\u00a0\u00a0get_decoder(p)\r\n\u00a0\u00a0})\u00a0//\u00a0iter\u00a0of\u00a0AsRef<[u8]>\r\n\u00a0\u00a0.flatten()\r\n\u00a0\r\n\r\nIterating over pages is something that is implemented inconsistently for primitive and complex types, and I would like to ultimately merge the two implementations, so that there is no more primitive or complex array reader, just a single arrow array reader using adapters / converters for different types of arrays.\r\n\r\nAlso the decoding functionality implemented in each parquet type is only used by the plain decoder (and not used by any other decoder) and I would look to move this away from the types and into the plain decoder where it belongs.\r\n\r\nThen, I would look into implementing the Iterator<Item = AsRef<[u8]>> idea for the different decoders and also into how exactly the adaptors / converters for different types of arrays would work.\r\n\r\nI am open to suggestions on how we could collaborate better on this. Let me know what you think."
        },
        {
            "created_at": "2021-03-29T05:40:15.197Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17310433) by Jorge Leit\u00e3o (jorgecarleitao):*\nFWIW, I started going through the parquet crate and re-write some parts of it. There are many, many opportunities to improve performance there.\r\n\r\nI also agree with you that we should push the \"to arrow\" to the page level. Also, IMO we should scratch the \"DataType\" and instead implement a specific implementation for boolean, (i32, i64, float, double), byteArray, FixedByteArray.\r\n\r\nI am looking into the encodings, and IMO there is some work groundwork that we need to take before going for the arrow-specific problem.\r\n\r\nI am looking at the RLE encoding, and I think that it may not be correct atm. Parquet [expects a 4-byte length](https://github.com/apache/parquet-format/blob/master/Encodings.md#run-length-encoding--bit-packing-hybrid-rle--3), but we only take a 1-byte length (i.e. up to 255 values). I do not know how we can even read def, ref levels, and boolean values with our encoder atm.\r\n\r\nI also found [this crate](https://github.com/tantivy-search/bitpacking/issues)\u00a0that seems to be implementing the encoding we need, including ordered, with SIMD instructions. We could probably think about depending on it.\r\n\r\nWhat I did so far: created a new repo from scratch and started moving bits by bits things there, going through a full review of the code (my personal way of reading and understanding code).\r\n\r\nI think that the easiest way would be to have a call where we would align knowledge and priorities."
        },
        {
            "created_at": "2021-03-29T09:24:04.726Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17310522) by Yordan Pavlov (yordan-pavlov):*\n`[~jorgecarleitao]` \u00a0thank you for looking into the parquet encoding code; I was also looking into the RLE code, because I needed to understand how it would fit with an Iterator<AsRef<[u8]>> abstraction. I do agree that the RLE code needs improvement / simplification and it could also be made faster (e.g. using SIMD) and if a library can be used to do all that - great. I also agree that there are many improvement opportunities throughout the parquet crate and it will continue to be an area of focus for me for a while, but sadly I only have a couple of hours per day to spare.\u00a0\r\n\r\nWhen you said \"to have a call\" what did you have in mind in terms of frequency (e.g. weekly, bi-weekly, etc.) and channel (zoom, telegram, etc.) ?"
        },
        {
            "created_at": "2021-03-30T06:14:34.779Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17311165) by Jorge Leit\u00e3o (jorgecarleitao):*\nOk, so, just to give a heads up: I have been experimenting with the code, and here is the result so far: <https://github.com/jorgecarleitao/parquet2>\r\n\r\nI was able to read a parquet file with arbitrary parallelism (the IO-CPU tradeoff is delegated to downstream). The missing parts are decoding and deserialization, which IMO is what `[~yordan-pavlov]` is thinking about.\r\n\r\nI reduced the problem to: given an iterator of decompressed (but encoded) pages, convert it to an arrow Array. IMO when no encoding is used, we either use a back-to-back or similar (e.g. Int96 is special). When encoding is used, we should probably decode directly to buffers, so that we avoid an extra memcopy.\r\n\r\n`[~yordan-pavlov]`, do you use slack? There is an arrow-rust channel on the official Apache slack: <https://the-asf.slack.com/archives/C01QUFS30TD> We could sync there.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-03-30T08:54:56.298Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17311291) by Yordan Pavlov (yordan-pavlov):*\n`[~jorgecarleitao]` \u00a0I would be happy to have a chat in Slack, but it appears that an\u00a0@apache.org email address is necessary to join and I don't have one.\r\n\r\nAlso, I noticed that in your parquet2 repo, a separate page iterator is created for each row group, very similar to how it works currently. I was planning to wrap multiple row group page iterators into a single iterator returning a sequence of pages from multiple row groups (see the code snippet in my previous comment)."
        },
        {
            "created_at": "2021-03-30T10:12:09.111Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17311356) by Dani\u00ebl Heres (Dandandan):*\n`[~yordan-pavlov]` you can join the apache slack here: https://s.apache.org/slack-invite"
        },
        {
            "created_at": "2021-03-30T10:48:14.441Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17311401) by Jorge Leit\u00e3o (jorgecarleitao):*\nI see. To understand: is there a reason why this should be in [Parquet] instead of in [DataFusion]? I.e. why should we push a specific parallelism strategy to the library?\r\n\r\nAsking this because the way I see it, the parquet crate can't tell which use-case is being used on and provide an optimal strategy for (one record per page, per group or per file or per files?). For example, s3 vs hdfs vs local file-system typically require different parallelism strategies.\r\n\r\nMy hypothesis (which may be wrong!) is that the parquet crate should offer \"units of work\" that can be divided/parallelized according to IO (e.g. s3 vs filesystem), memory and CPU constraints that each consumer has, and allow consumers of the library (e.g. DataFusion, Polars, Ballista, s3 vs hdfs vs file-system) to design strategies that fit their constraints the best, by assembling these units according to their compute model."
        },
        {
            "created_at": "2021-04-05T18:51:49.565Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17315037) by Yordan Pavlov (yordan-pavlov):*\nUPDATE: after spending the past few weeks figuring out how different steps could be implemented on the way from page buffer to arrow array (such as create iterator of pages across row groups, share dictionary data between pages in the same row column chunk, split page buffer into different iterators for data, rep and def levels, and reading batches of values), my next step is going to be implementing this idea end-to-end for a particular type of array (StringArray). In this way the idea can be tested sooner (in terms of performance, etc.), reviewed and feedback collected, before expanding the implementation for more types. I hope to have an initial implementation in about a week."
        },
        {
            "created_at": "2021-04-13T21:51:19.494Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17320550) by Yordan Pavlov (yordan-pavlov):*\nI have finally been able to assemble enough code to demonstrate the core idea and have created a branch here <https://github.com/yordan-pavlov/arrow/commit/c62c5394726b79d428a93e2593d0c24da3c9d286#diff-dce1a37fc60ea0c8d13a61bf530abbf9f82aef43224597f31a7ba4d9fe7bd10dR258>\r\n\r\nThe test doesn't pass yet, but the code compiles and demonstrates how an iterator could be created over many pages from many row groups / column chunks, and then split into separate iterators for (values, def levels, rep levels) and then read in batches.\r\n\r\nThe iterator is created in\u00a0ArrowArrayReader::try_new and used in <ArrowArrayReader as\u00a0ArrayReader>::next_batch.\r\n\r\nMy plan is that\u00a0ArrowArrayReader will replace both\u00a0PrimitiveArrayReader and\u00a0ComplexObjectArrayReader when arrow array converters have been implemented for all types.\r\n\r\nFeedback is most welcome.\r\n\r\nNext steps are:\r\n \\* complete implementation to define arrow array converter interface\r\n \\* implement decoder iterator for def / rep levels\r\n \\* implement decoder iterator for plain encoding\r\n \\* implement StringArray converter\r\n \\* make unit test pass\r\n \\* attempt to replace\u00a0ComplexObjectArrayReader for StringArrays\r\n \\* benchmark performance\r\n \\* create initial PR\r\n\r\nAfter this initial PR, implementing arrow array converters for the remaining types could be done in separate PRs."
        },
        {
            "created_at": "2021-04-18T20:13:44.587Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17324597) by Yordan Pavlov (yordan-pavlov):*\nUPDATE: over the past few days I managed to finish the core implementation of the new\u00a0ArrowArrayReader with the key bits being:\r\n \\* the converters will only produce an all-value / no-null ArrayData instance - this simplifies the converter interface and keeps all other logic generic\r\n \\* if no def levels are available, this no-null ArrayData produced from the converter is simply converted to an array and returned without changes\r\n \\* if def levels are available, a BooleanArray is created from the def levels and used to efficiently determine how many values to read and also efficiently insert NULLs using\u00a0MutableArrayData (with an algorithm very similar to zip()) - this implementation re-uses as much of the existing arrow code as possible\r\n \\* the StringArray converter has been implemented as a function before moving to a converter in a later change\r\n\r\nNext steps are:\r\n \\* implement decoder iterator for def / rep levels\r\n \\* implement decoder iterator for plain encoding\r\n \\* make unit test pass\r\n \\* attempt to replace\u00a0ComplexObjectArrayReader for StringArrays\r\n \\* benchmark performance\r\n \\* create initial PR\r\n\r\nthe latest changes can be found here:\r\n\r\nhttps://github.com/yordan-pavlov/arrow/commit/7299f2a747cc52237c21b9d85df994a66097d731#diff-dce1a37fc60ea0c8d13a61bf530abbf9f82aef43224597f31a7ba4d9fe7bd10dR418"
        },
        {
            "created_at": "2021-04-21T21:49:50.476Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17326954) by Yordan Pavlov (yordan-pavlov):*\nUPDATE: I have now implemented the level decoder iterator and support for def and rep levels in the ArrowArrayReader here:\r\n\r\n<https://github.com/yordan-pavlov/arrow/commit/3a820c58747cf692efaf90b7bc3716d60b6ecb85>\r\n\r\nThis commit incudes a change to load def / rep levels into Int16Array which is used to efficiently calculate the null bitmap for values from def levels using\u00a0arrow::compute::eq_scalar."
        },
        {
            "created_at": "2021-04-25T20:58:50.034Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17331639) by Yordan Pavlov (yordan-pavlov):*\nUPDATE: I have added the ArrayConverter trait, implemented decoder iterators for plain encoding, and the string array test now passes;\r\n\r\nthe latest changes can be found here: <https://github.com/yordan-pavlov/arrow/commit/dc93466510c6be1c6a21a61b1e948a3fa7959a9a>\r\n\r\nNext steps are:\r\n \\* attempt to replace\u00a0ComplexObjectArrayReader for StringArrays\r\n \\* implement missing parts to make ArrowArrayReader work for StringArrays (likely RLE and dictionary encodings)\r\n \\* benchmark performance\r\n \\* create initial PR"
        },
        {
            "created_at": "2021-04-26T12:49:28.623Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11897?focusedCommentId=17332232) by Andrew Lamb (alamb):*\nMigrated to github: https://github.com/apache/arrow-rs/issues/200"
        }
    ]
}