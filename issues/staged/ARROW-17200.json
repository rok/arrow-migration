{
    "issue": {
        "title": "[Python][Parquet] support partitioning by Pandas DataFrame index",
        "body": "***Note**: This issue was originally created as [ARROW-17200](https://issues.apache.org/jira/browse/ARROW-17200). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn a Pandas `DataFrame` with a multi-index, with a slowly-varying \"outer\" index level, one might want to partition by that index level when saving the data frame to Parquet format. This is currently not possible; you need to manually reset the index before writing, and re-add the index after reading. It would be very useful if you could supply the name of an index level to `partition_cols` instead of (or ideally in addition to) a data column name.\r\n\r\nI originally posted this on the Pandas issue tracker (<https://github.com/pandas-dev/pandas/issues/47797>). Matthew Roeschke looked at the code and figured out that the partitioning functionality was implemented entirely in PyArrow, and that the change would need to happen within PyArrow itself.",
        "created_at": "2022-07-25T20:49:08.000Z",
        "updated_at": "2022-10-20T12:20:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-10-20T12:20:00.000Z"
    },
    "comments": [
        {
            "created_at": "2022-10-20T12:18:41.297Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17200?focusedCommentId=17621068) by Alenka Frim (alenka):*\nThis should be possible.\r\n\r\nWhen transforming pandas dataframe into Arrow table the multi-index is converted into columns. These columns can then be defined as `partition_cols` for writing parquet files into partitions. Also looking at the code in pandas codebase, the correct method is selected if `partition_cols` are supplied:\r\n\r\n<https://github.com/pandas-dev/pandas/blob/56d82a9bd654e91d14596e82e4d9c82215fa5bc8/pandas/io/parquet.py#L195-L209>\r\n\r\nwhich is `write_to_dataset`. A working example:\r\n\r\n```python\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Creating a dataframe with MultiIndex\r\narrays = [\r\n    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\r\n    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\r\n]\r\ntuples = list(zip(*arrays))\r\nindex = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\r\ndf = pd.DataFrame(data={'randn':  np.random.randn(8)}, index=index)\r\n\r\n# writing to a partitioned dataset\r\ndf.to_parquet(path='dataset_name', partition_cols=[\"first\", \"second\"])\r\n\r\n# inspecting the pieces\r\nimport pyarrow.parquet as pq\r\ndataset = pq.ParquetDataset('dataset_name', use_legacy_dataset=False)\r\ndataset.fragments\r\n# [<pyarrow.dataset.ParquetFileFragment path=dataset_name/first=bar/second=one/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=one, first=bar]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=bar/second=one/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=one, first=bar]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=bar/second=two/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=two, first=bar]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=bar/second=two/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=two, first=bar]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=baz/second=one/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=one, first=baz]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=baz/second=one/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=one, first=baz]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=baz/second=two/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=two, first=baz]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=baz/second=two/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=two, first=baz]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=foo/second=one/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=one, first=foo]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=foo/second=one/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=one, first=foo]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=foo/second=two/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=two, first=foo]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=foo/second=two/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=two, first=foo]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=qux/second=one/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=one, first=qux]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=qux/second=one/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=one, first=qux]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=qux/second=two/91796db98d874ef2b170f22a304a7c5e-0.parquet partition=[second=two, first=qux]>,\r\n#  <pyarrow.dataset.ParquetFileFragment path=dataset_name/first=qux/second=two/c1daafbac5334b9ea230821de383cb93-0.parquet partition=[second=two, first=qux]>]\r\n\r\n```"
        }
    ]
}