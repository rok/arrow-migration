{
    "issue": {
        "title": "[Python] Pyarrow fails to loads parquet file with long column names",
        "body": "***Note**: This issue was originally created as [ARROW-16546](https://issues.apache.org/jira/browse/ARROW-16546). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen loading parquet file \"OSError: Couldn't deserialize thrift: TProtocolException: Exceeded size limit\" is raised. This seems to be related to memory usage of table header. The issue may be coming from C code part. Also pyarrow 0.16 version is capable to read that parquet file.\r\n\r\nBelow is code snippet to reproduce the issue. Screenshot of jupyter-notebook with more details is in attachments.\r\n\r\nCode snippet creates 2 pandas dataframes which only differ in column names. One with short column names is stored and read without problem while the other dataframe with long column names is stored but raises Exception during reading.\r\n\r\n\r\n```java\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndata = np.random.randn(10, 250000)\r\nindex = range(10)\r\nshort_column_names = [f\"col_{i}\" for i in range(250000)]\r\nlong_column_names = [f\"some_really_long_column_name_ending_with_integer_number_{i}\" for i in range(250000)]\r\n\r\ndf_short_cols = pd.DataFrame(columns=short_column_names, data=data, index=index)\r\ndf_long_cols = pd.DataFrame(columns=long_column_names, data=data, index=index)# Identical dataframes only column names are different\r\n\r\n# Storing dataframe with long column names works OK but reading fails\r\ndf_long_cols.to_parquet(\"long_cols.parquet\", engine=\"pyarrow\") # Storing works\r\ndf_long_cols_loaded = pd.read_parquet(\"long_cols.parquet\", engine=\"pyarrow\") # <--- Fails here\n```\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2022-05-12T15:02:55.000Z",
        "updated_at": "2022-06-07T14:20:18.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-06-07T11:29:08.000Z"
    },
    "comments": [
        {
            "created_at": "2022-05-12T17:45:44.768Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16546?focusedCommentId=17536262) by Antoine Pitrou (apitrou):*\nThere are limits in place to avoid space/time bombs using Parquet files, but we need to make these limits configurable:\r\nhttps://github.com/apache/arrow/blob/90aac16761b7dbf5fe931bc8837cad5116939270/cpp/src/parquet/thrift_internal.h#L388-L394\r\n"
        },
        {
            "created_at": "2022-05-17T10:53:20.206Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16546?focusedCommentId=17538109) by Joris Van den Bossche (jorisvandenbossche):*\nWe had another issue in the past about too large metadata (ARROW-13655), but I suppose that was about the total message size"
        },
        {
            "created_at": "2022-06-07T11:29:08.757Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16546?focusedCommentId=17550946) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 13275\n<https://github.com/apache/arrow/pull/13275>"
        }
    ]
}