{
    "issue": {
        "title": "[C++][Parquet] 16MB limit on (nested) column chunk prevents tuning row_group_size",
        "body": "***Note**: This issue was originally created as [ARROW-4688](https://issues.apache.org/jira/browse/ARROW-4688). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWe working on parquet files that involve nested lists. At most they are multi-dimensional lists of simple types (never structs), but i understand, for Parquet, they're still nested columns and involve repetition levels.\u00a0\r\n\r\nSome of these columns\u00a0hold lists of rather large byte arrays (that dominate the overall size of the row). When we bump the `row_group_size` to above 16MB we see:\u00a0\r\n\r\n\u00a0\r\n```java\n\r\nFile \"pyarrow/_parquet.pyx\", line 700, in pyarrow._parquet.ParquetReader.read_row_group\r\n File \"pyarrow/error.pxi\", line 89, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\n```\r\n\u00a0\r\n\r\nI conclude it's [this](https://github.com/apache/arrow/blob/master/cpp/src/parquet/arrow/reader.cc#L848) bit complaining:\r\n\r\n\u00a0\r\n```java\n\r\ntemplate <typename ParquetType>\r\n\tStatus PrimitiveImpl::WrapIntoListArray(Datum* inout_array) {\r\n\tif (descr_->max_repetition_level() == 0) {\r\n\t  // Flat, no action\r\n\t  return Status::OK();\r\n\t}\r\n\t\r\n\tstd::shared_ptr<Array> flat_array;\r\n\t\r\n\t// ARROW-3762(wesm): If inout_array is a chunked array, we reject as this is\r\n\t// not yet implemented\r\n\tif (inout_array->kind() == Datum::CHUNKED_ARRAY) {\r\n\t  if (inout_array->chunked_array()->num_chunks() > 1) {\r\n\t    return Status::NotImplemented(\r\n\t      \"Nested data conversions not implemented for \"\r\n\t      \"chunked array outputs\");\n```\r\n\u00a0\r\n\r\nThis appears to happen\u00a0in\u00a0the callstack of\u00a0ColumnReader::ColumnReaderImpl::NextBatch\u00a0\r\nand it appears to be provoked by [this](https://github.com/apache/arrow/blob/de84293d9c93fe721cd127f1a27acc94fe290f3f/cpp/src/parquet/arrow/record_reader.cc#L604) constant:\r\n```java\n\r\ntemplate <>   \u00a0 \r\nvoid TypedRecordReader<ByteArrayType>::InitializeBuilder() {   \u00a0 \r\n  // Maximum of 16MB chunks   \u00a0 \r\n  constexpr int32_t kBinaryChunksize = 1 << 24;   \u00a0 \r\n  DCHECK_EQ(descr_->physical_type(), Type::BYTE_ARRAY);   \u00a0   \r\n  builder_.reset(\r\n    new::arrow::internal::ChunkedBinaryBuilder(kBinaryChunksize, pool_));  }   \n```\r\nWhich appears to imply\u00a0that the column chunk data, if larger than\u00a0kBinaryChunksize (hardcoded to 16MB), is returned as a Datum::CHUNKED_ARRAY of more than one (16MB) chunks. Which ultimatelly leads to the Status::NotImplemented error.\r\n\r\nWe have no influence over what data we ingest, we have some influence in how we flatten it and we need to tune our row_group_size to something sensibly larger than 16MB.\u00a0\r\n\r\nWe have see no obvious workaround for this and so we need to ask (1) if the above diagnosis appears to correct (2) do people see any sensible workarounds (3) is there an imminent intention to fix this in the Arrow community and if not, how difficult would it be to fix this (in case we can afford helping)",
        "created_at": "2019-02-26T20:11:30.000Z",
        "updated_at": "2019-03-28T22:15:52.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-03-25T01:24:42.000Z"
    },
    "comments": [
        {
            "created_at": "2019-03-14T14:51:45.973Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16792741) by Aaron Key (keyaaron):*\nI have experienced the same issue when reading in a large column which is an array of strings, rather than a nested array structure.\u00a0 This also appears to have been a regression since 0.11.1 as I am able to read the column with that release but not in 0.12.0"
        },
        {
            "created_at": "2019-03-14T15:15:59.526Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16792785) by Wes McKinney (wesm):*\nI see. Yes, I think this is a regression and we should fix for 0.13"
        },
        {
            "created_at": "2019-03-20T12:14:47.522Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16797103) by Uwe Korn (uwe):*\nCode to reproduce:\r\n```java\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame({\r\n\"a\": np.arange(30000)\r\n})\r\ndf['b'] = df['a'].apply(lambda x: list(map(str, range(x))))\r\ntable = pa.Table.from_pandas(df)\r\nbuf = pa.BufferOutputStream()\r\npq.write_table(table, buf)\r\npq.read_table(buf.getvalue())\n```"
        },
        {
            "created_at": "2019-03-20T14:02:48.421Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16797199) by Uwe Korn (uwe):*\nAs an understanding on how this bug actually occured:\r\n \\* Why is there a limit of 16MB in the chunking, wouldn't 2G be better? (i.e. less chunks)\r\n \\* The chunks are currently split on the lowest nesting level. As far as I can see, a more upper level list could be spread between chunks. Is this correct?"
        },
        {
            "created_at": "2019-03-21T22:12:37.763Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16798493) by Wes McKinney (wesm):*\nSmaller chunks with `ChunkedBinaryBuilder` actually yields better performance. I didn't think about splitting within a cell so it'll have to be 2GB splitting"
        },
        {
            "created_at": "2019-03-25T01:24:42.130Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4688?focusedCommentId=16800326) by Wes McKinney (wesm):*\nIssue resolved by pull request 4023\n<https://github.com/apache/arrow/pull/4023>"
        }
    ]
}