{
    "issue": {
        "title": "Python: pyarrow.Filesystem.read_parquet causing error if nthreads>1",
        "body": "***Note**: This issue was originally created as [ARROW-1039](https://issues.apache.org/jira/browse/ARROW-1039). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently I have the code:\n\n```Java\nclient = HdfsClient(\"hdfshost\", 8020, \"myuser\", driver='libhdfs3')\nparquet_file = '/my/parquet/file'\nparquet = client.read_parquet(parquet_file, nthreads=1)\ndf = parquet.to_pandas()\n```\n\nThis works as expected. If I make nthreads=2 however I get:\n\n```\n2017-05-16 02:59:36.414677, p116977, th140677336123136, ERROR InputStreamImpl: failed to read Block: [block pool ID: BP-1695827161-10.87.14.23-1472240973777 block ID 1080474497_6733673] file /my/parquet/file/part-00001-e32bec64-fdcc-4c42-a292-c837a081310b.snappy.parquet from Datanode: hdfshost1(1.2.3.4), \nRemoteBlockReader.cpp: 304: ChecksumException: RemoteBlockReader: checksum not match for Block: [block pool ID: BP-1695827161-10.87.14.23-1472240973777 block ID 1080474497_6733673], on Datanode: hdfshost1(1.2.3.4)\n        @       Hdfs::Internal::RemoteBlockReader::verifyChecksum(int)\n        @       Hdfs::Internal::RemoteBlockReader::readNextPacket()\n        @       Hdfs::Internal::RemoteBlockReader::read(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::readOneBlock(char*, int, bool)\n        @       Hdfs::Internal::InputStreamImpl::readInternal(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::read(char*, int)\n        @       hdfsRead\n        @       arrow::io::HdfsReadableFile::ReadAt(long, long, std::shared_ptr<arrow::Buffer>*)\n        @       parquet::ArrowInputFile::ReadAt(long, long)\n        @       parquet::InMemoryInputStream::InMemoryInputStream(parquet::RandomAccessSource*, long, long)\n        @       parquet::SerializedRowGroup::GetColumnPageReader(int)\n        @       parquet::RowGroupReader::Column(int)\n        @       parquet::arrow::AllRowGroupsIterator::Next()\n        @       parquet::arrow::ColumnReader::Impl::NextRowGroup()\n        @       parquet::arrow::ColumnReader::Impl::Impl(arrow::MemoryPool*, std::unique_ptr<parquet::arrow::FileColumnIterator, std::default_delete<parquet::arrow::FileColumnIterator> >)\n        @       parquet::arrow::FileReader::Impl::GetColumn(int, std::unique_ptr<parquet::arrow::ColumnReader, std::default_delete<parquet::arrow::ColumnReader> >*)\n        @       parquet::arrow::FileReader::Impl::ReadColumn(int, std::shared_ptr<arrow::Array>*)\n        @       parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}::operator()(int) const\n        @       std::thread::_Impl<std::_Bind_simple<arrow::Status parquet::arrow::ParallelFor<parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&>(int, int, parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&)::{lambda()#1} ()> >::_M_run()\n        @       Unknown\n        @       start_thread\n        @       clone\n, retry read again from another Datanode.\n2017-05-16 02:59:36.414858, p116977, th140677336123136, INFO IntputStreamImpl: Add invalid datanode hdfshost1(1.2.3.4) to failed datanodes and try another datanode again for file /my/parquet/file/part-00001-e32bec64-fdcc-4c42-a292-c837a081310b.snappy.parquet.\n2017-05-16 02:59:36.424118, p116977, th140677702768384, ERROR InputStreamImpl: failed to read Block: [block pool ID: BP-1695827161-10.87.14.23-1472240973777 block ID 1080474497_6733673] file /my/parquet/file/part-00001-e32bec64-fdcc-4c42-a292-c837a081310b.snappy.parquet from Datanode: hdfshost2(1.2.3.5), \nRemoteBlockReader.cpp: 205: HdfsIOException: RemoteBlockReader: failed to read block header for Block: [block pool ID: BP-1695827161-10.87.14.23-1472240973777 block ID 1080474497_6733673] from Datanode: hdfshost1(1.2.3.4) .\n        @       Hdfs::Internal::RemoteBlockReader::readPacketHeader()\n        @       Hdfs::Internal::RemoteBlockReader::readNextPacket()\n        @       Hdfs::Internal::RemoteBlockReader::read(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::readOneBlock(char*, int, bool)\n        @       Hdfs::Internal::InputStreamImpl::readInternal(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::read(char*, int)\n        @       hdfsRead\n        @       arrow::io::HdfsReadableFile::ReadAt(long, long, std::shared_ptr<arrow::Buffer>*)\n        @       parquet::ArrowInputFile::ReadAt(long, long)\n        @       parquet::InMemoryInputStream::InMemoryInputStream(parquet::RandomAccessSource*, long, long)\n        @       parquet::SerializedRowGroup::GetColumnPageReader(int)\n        @       parquet::RowGroupReader::Column(int)\n        @       parquet::arrow::AllRowGroupsIterator::Next()\n        @       parquet::arrow::ColumnReader::Impl::NextRowGroup()\n        @       parquet::arrow::ColumnReader::Impl::Impl(arrow::MemoryPool*, std::unique_ptr<parquet::arrow::FileColumnIterator, std::default_delete<parquet::arrow::FileColumnIterator> >)\n        @       parquet::arrow::FileReader::Impl::GetColumn(int, std::unique_ptr<parquet::arrow::ColumnReader, std::default_delete<parquet::arrow::ColumnReader> >*)\n        @       parquet::arrow::FileReader::Impl::ReadColumn(int, std::shared_ptr<arrow::Array>*)\n        @       parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}::operator()(int) const\n        @       std::thread::_Impl<std::_Bind_simple<arrow::Status parquet::arrow::ParallelFor<parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&>(int, int, parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&)::{lambda()#1} ()> >::_M_run()\n        @       Unknown\n        @       start_thread\n        @       clone\nCaused by\nPacketHeader.cpp: 109: HdfsIOException: Invalid PacketHeader, packetLen is 1228668939, protoLen is 1290, buf size is 31\n        @       Hdfs::Internal::PacketHeader::readFields(char const*, unsigned long)\n        @       Hdfs::Internal::RemoteBlockReader::readPacketHeader()\n        @       Hdfs::Internal::RemoteBlockReader::readNextPacket()\n        @       Hdfs::Internal::RemoteBlockReader::read(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::readOneBlock(char*, int, bool)\n        @       Hdfs::Internal::InputStreamImpl::readInternal(char*, int)\n        @       Hdfs::Internal::InputStreamImpl::read(char*, int)\n        @       hdfsRead\n        @       arrow::io::HdfsReadableFile::ReadAt(long, long, std::shared_ptr<arrow::Buffer>*)\n        @       parquet::ArrowInputFile::ReadAt(long, long)\n        @       parquet::InMemoryInputStream::InMemoryInputStream(parquet::RandomAccessSource*, long, long)\n        @       parquet::SerializedRowGroup::GetColumnPageReader(int)\n        @       parquet::RowGroupReader::Column(int)\n        @       parquet::arrow::AllRowGroupsIterator::Next()\n        @       parquet::arrow::ColumnReader::Impl::NextRowGroup()\n        @       parquet::arrow::ColumnReader::Impl::Impl(arrow::MemoryPool*, std::unique_ptr<parquet::arrow::FileColumnIterator, std::default_delete<parquet::arrow::FileColumnIterator> >)\n        @       parquet::arrow::FileReader::Impl::GetColumn(int, std::unique_ptr<parquet::arrow::ColumnReader, std::default_delete<parquet::arrow::ColumnReader> >*)\n        @       parquet::arrow::FileReader::Impl::ReadColumn(int, std::shared_ptr<arrow::Array>*)\n        @       parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}::operator()(int) const\n        @       std::thread::_Impl<std::_Bind_simple<arrow::Status parquet::arrow::ParallelFor<parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&>(int, int, parquet::arrow::FileReader::Impl::ReadTable(std::vector<int, std::allocator<int> > const&, std::shared_ptr<arrow::Table>*)::{lambda(int)#1}&)::{lambda()#1} ()> >::_M_run()\n        @       Unknown\n        @       start_thread\n        @       clone\n, retry read again from another Datanode.\n2017-05-16 02:59:36.424266, p116977, th140677702768384, INFO IntputStreamImpl: Add invalid datanode hdfshost2(1.2.3.5) to failed datanodes and try another datanode again for file /my/parquet/file/part-00001-e32bec64-fdcc-4c42-a292-c837a081310b.snappy.parquet.\nterminate called after throwing an instance of 'parquet::ParquetException'\n  what():  Unable to read column chunk data\nAborted\n```\n\nIf nthreads>2 I get:\n\n```\nSegmentation fault\n```\n\nI'm running this in a conda environment with:\n\n```\npyarrow                   0.3.0.post          np112py27_1    conda-forge\nparquet-cpp               1.1.0pre                      2    conda-forge\narrow-cpp                 0.3.0.post          np112py27_1    conda-forge\nlibhdfs3                  2.2.31                        1  \n```",
        "created_at": "2017-05-16T07:24:03.000Z",
        "updated_at": "2017-06-05T01:13:06.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-06-05T01:13:06.000Z"
    },
    "comments": [
        {
            "created_at": "2017-05-16T15:48:14.679Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1039?focusedCommentId=16012616) by Wes McKinney (wesm):*\nHdfsClient is not yet safe for multithreaded use, see ARROW-424. It would be great if someone could look into this. See patch here where I added threadsafe reads to normal files: https://github.com/apache/arrow/commit/2821030124eb3e884b0e48f09c38b54f00430b13#diff-ab3140126fb8bd048de1f68cc9ac93a9"
        },
        {
            "created_at": "2017-05-23T19:17:24.871Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1039?focusedCommentId=16021691) by Wes McKinney (wesm):*\nThis is fixed in https://github.com/apache/arrow/pull/712"
        },
        {
            "created_at": "2017-06-05T01:13:06.673Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1039?focusedCommentId=16036443) by Wes McKinney (wesm):*\nPlease let me know if this error resurfaces"
        }
    ]
}