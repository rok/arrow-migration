{
    "issue": {
        "title": "[Python] Can't read large file that pyarrow wrote",
        "body": "***Note**: This issue was originally created as [ARROW-3999](https://issues.apache.org/jira/browse/ARROW-3999). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI loaded a large Pandas DataFrame from a CSV and successfully wrote it to a Parquet file using the DataFrame's `to_parquet` method. However, reading that same file back results in an exception. The DataFrame consists of about 32 million rows with seven columns; four are ASCII text and three are booleans.\r\n\r\n\u00a0\r\n```java\n\r\n>>> source_df.shape\r\n(32070402, 7)\r\n\r\n>>> source_df.dtypes\r\nUrl Source object\r\nUrl Destination object\r\nAnchor text object\r\nFollow / No-Follow object\r\nLink No-Follow bool\r\nMeta No-Follow bool\r\nRobot No-Follow bool\r\ndtype: object\r\n\r\n>>> source_df.to_parquet('export.parq', compression='gzip',\r\n                         use_deprecated_int96_timestamps=True)\r\n\r\n>>> loaded_df = pd.read_parquet('export.parq')\r\nTraceback (most recent call last):\r\n File \"<stdin>\", line 1, in <module>\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py\", line 288, in read_parquet\r\n   return impl.read(path, columns=columns, **kwargs)\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/io/parquet.py\", line 131, in read\r\n   **kwargs).to_pandas()\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1074, in read_table\r\n   use_pandas_metadata=use_pandas_metadata)\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/filesystem.py\", line 184, in read_parquet\r\n   use_pandas_metadata=use_pandas_metadata)\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/parquet.py\", line 943, in read\r\n   use_pandas_metadata=use_pandas_metadata)\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/parquet.py\", line 500, in read\r\n   table = reader.read(**options)\r\n File \"/Users/tux/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pyarrow/parquet.py\", line 187, in read\r\n   use_threads=use_threads)\r\n File \"pyarrow/_parquet.pyx\", line 721, in pyarrow._parquet.ParquetReader.read_all\r\n File \"pyarrow/error.pxi\", line 83, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowIOError: Arrow error: Capacity error: BinaryArray cannot contain more than 2147483646 bytes, have 2147483685\r\n\r\nArrow error: Capacity error: BinaryArray cannot contain more than 2147483646 bytes, have 2147483685\r\n\u00a0\n```\r\n\u00a0\r\n\r\nOne would expect that if PyArrow can\u00a0write a file successfully, it can read it back as well.\u00a0Fortunately the\u00a0`fastparquet` library has no problem\u00a0reading this file, so we didn't lose any data, but the roundtripping problem was a bit of a surprise.",
        "created_at": "2018-12-11T19:29:52.000Z",
        "updated_at": "2018-12-11T19:44:50.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-12-11T19:44:50.000Z"
    },
    "comments": [
        {
            "created_at": "2018-12-11T19:44:50.750Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3999?focusedCommentId=16717872) by Wes McKinney (wesm):*\nDuplicate of ARROW-3762"
        }
    ]
}