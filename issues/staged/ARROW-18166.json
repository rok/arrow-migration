{
    "issue": {
        "title": "Allow ConvertOptions.timestamp_parsers for date types",
        "body": "***Note**: This issue was originally created as [ARROW-18166](https://issues.apache.org/jira/browse/ARROW-18166). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently, the timestamp_parsers option of the CSV reader only works for timestamp datatypes.\r\n\r\nIf one wants to immediately read dates as date32 objects (in my use case csv data is read and stored as parquet files with correct types), one has to cast the table to a schema with date32 types after the fact.\r\nThis snipped shows that loading the data fails when specifying the date type:\r\n```java\n\r\nimport pyarrow as pa\r\nfrom pyarrow import csv\r\n\r\ndef open_bytes(b, **kwargs):\r\n\u00a0 \u00a0 return csv.open_csv(pa.py_buffer(b), **kwargs)\r\ndef read_bytes(b, **kwargs):\r\n\u00a0 \u00a0 return open_bytes(b, **kwargs).read_all()\r\n\r\nrows = b\"a,b\\n1970/01/01,1980-01-01 00\\n1970/01/02,1980-01-02 00\\n\"\r\nschema = pa.schema([(\"a\", pa.timestamp(\"ms\")), (\"b\", pa.string())])\r\nopts = csv.ConvertOptions(column_types=schema, timestamp_parsers=[\"%Y/%m/%d\"])\r\ntable = read_bytes(rows, convert_options=opts)\r\nassert table.schema == schema # works\r\n\r\nschema = pa.schema([(\"a\", pa.date32()), (\"b\", pa.string())])\r\nopts = csv.ConvertOptions(column_types=schema, timestamp_parsers=[\"%Y/%m/%d\"])\r\ntable = read_bytes(rows, convert_options=opts) # error here\r\nassert table.schema == schema\r\n---------------------------------------------------------------------------\r\nArrowInvalid \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Traceback (most recent call last)\r\nInput In [134], in <cell line: 22>()\r\n\u00a0 \u00a0 \u00a020 schema = pa.schema([(\"a\", pa.date32()), (\"b\", pa.string())])\r\n\u00a0 \u00a0 \u00a021 opts = csv.ConvertOptions(column_types=schema, timestamp_parsers=[\"%Y/%m/%d\"])\r\n---> 22 table = read_bytes(rows, convert_options=opts)\r\n\u00a0 \u00a0 \u00a023 assert table.schema == schemaInput In [134], in read_bytes(b, **kwargs)\r\n\u00a0 \u00a0 \u00a0 9 def read_bytes(b, **kwargs):\r\n---> 10 \u00a0 \u00a0 return open_bytes(b, **kwargs).read_all()Input In [134], in open_bytes(b, **kwargs)\r\n\u00a0 \u00a0 \u00a0 5 def open_bytes(b, **kwargs):\r\n----> 6 \u00a0 \u00a0 return csv.open_csv(pa.py_buffer(b), **kwargs)File ~/.virtualenvs/ogi/lib/python3.9/site-packages/pyarrow/_csv.pyx:1273, in pyarrow._csv.open_csv()File ~/.virtualenvs/ogi/lib/python3.9/site-packages/pyarrow/_csv.pyx:1137, in pyarrow._csv.CSVStreamingReader._open()File ~/.virtualenvs/ogi/lib/python3.9/site-packages/pyarrow/error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()File ~/.virtualenvs/ogi/lib/python3.9/site-packages/pyarrow/error.pxi:100, in pyarrow.lib.check_status()ArrowInvalid: In CSV column #0: CSV conversion error to date32[day]: invalid value '1970/01/01'\r\n```\r\nIt would be useful to allow the timestamp_parsers for date types as well (or add an analogous argument for dates), such that such errors don't occur and the resulting table has the required datatypes without a casting step.\r\n\r\n\u00a0\r\n\r\nA little bit more context is in the comments of https://issues.apache.org/jira/browse/ARROW-10848 (26/Oct/22).",
        "created_at": "2022-10-26T12:16:08.000Z",
        "updated_at": "2022-10-26T14:11:35.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-10-26T14:00:51.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18166?focusedCommentId=17624484) by Rok Mihevc (rokm):*\nThanks for the extensive Jira `[~timlod]` ! I see this on Arrow 9.0.0 as well.\r\n\r\nIt seems we currently don't have a DateParser the way we have the [TimestampParser](https://github.com/apache/arrow/blob/master/cpp/src/arrow/util/value_parsing.cc#L49) and the return type will always be `timestamp(\"xs\")`. One solution here would be to add DateParser that would accept arbitrary format and return `date32` or `date64` (as ARROW-10847 suggests).\r\n\r\nNote: `date64` is defined as milliseconds since UNIX epoch so casting it to `timestamp(\"ms\") is just a metadata change. On the other hand casting `date32` to `timestamp(\"s\") requires multiplication with 86400 and the metadata change (which is computationally equivalent to what would proposed DateParser do).\r\n\r\n`[~timlod]` Is your main concern performance or the API?\r\n`[~apitrou]` What do you think about adding DateParser to handle the gap here?"
        },
        {
            "created_at": "2022-10-26T14:11:35.345Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18166?focusedCommentId=17624495) by Tim Loderhose (timlod):*\nMy main concern is the API - specifying the date types directly is just neater than having to specify an alternate schema that is used for loading (with timestamps), and then casting to a schema that is used for writing (with dates).\r\n\r\nI use date32 mainly because 32 bits are enough to store these dates, and there's a lot of data (so this could lead to using less storage).\r\n\r\nIf there's a solution that both simplifies usage and provides good performance, that would of course be best - I'm not educated enough about arrow's internals though to really contribute to this discussion beyond stating what would be useful.\r\n\r\nUsage is actually in pandas, so this data ironically gets converted back to timestamps when using the data.\r\n\r\n(I've spent way too much time coming up with a good solution here - the problem is that the csvs we receive are not validated, so there could be dates like 0001/01/01 which mess up timestamp conversions. That's why we settled on storing what we know should be the correct type for the given data)\r\n\r\nI did not notice too big of an impact in performance when using `date_as_object=False` in the `to_pandas()` API, so this is fine.\r\n\r\n\u00a0"
        }
    ]
}