{
    "issue": {
        "title": "Slow reading of partitioned parquet files from S3",
        "body": "***Note**: This issue was originally created as [ARROW-16982](https://issues.apache.org/jira/browse/ARROW-16982). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen reading partitioned files from S3 and using filters to select partitions, the reader will send list requests each time read_table() is called.\r\n```python\n\r\n# partitioning: s3://bucket/year=xxxx/month=y/day=z\r\n\r\nfrom pyarrow import parquet\r\nparquet.read_table('s3://bucket', filters=[('day', '=', 1)]) # lists s3 bucket\r\nparquet.read_table('s3://bucket', filters=[('day', '=', 2)]) # lists again\n```\r\nThis is not a problem if done once, but repeated calls to select different partitions lead to a large amount of (slow and potentially expensive) S3 list requests.\r\n\r\nCurrent workaround is to list and filter partition structure manually, however this is not nearly as convenient as using filters.\r\n\r\nIf we know that the S3 prefixes did not change, it should be possible to do recursive list only once and load different data multiple times (using only S3 get requests). I suppose this should be possible by using ParquetDataset, however current implementation only allows filters in constructor and not in the read() method.",
        "created_at": "2022-07-05T18:29:42.000Z",
        "updated_at": "2022-07-05T18:29:42.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}