{
    "issue": {
        "title": "[Python] Pyarrow using considerable more memory when reading partitioned Parquet file",
        "body": "***Note**: This issue was originally created as [ARROW-4470](https://issues.apache.org/jira/browse/ARROW-4470). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nI have a partitioned Parquet table in Impala in HDFS, using Hive metastore, with the following structure:\r\n\r\n`/data/myparquettable/year=2016``/data/myparquettable/year=2016/myfile_1.prt`\r\n\r\n`/data/myparquettable/year=2016/myfile_2.prt`\r\n\r\n`/data/myparquettable/year=2016/myfile_3.prt`\r\n\r\n`/data/myparquettable/year=2017`\r\n\r\n`/data/myparquettable/year=2017/myfile_1.prt`\r\n\r\n`/data/myparquettable/year=2017/myfile_2.prt`\r\n\r\n`/data/myparquettable/year=2017/myfile_3.prt`\r\n\r\nand so on. I need to work with one partition, so I copied one partition to a local filesystem:\r\n\r\n`hdfs fs -get /data/myparquettable/year=2017 /local/`\r\n\r\nso now I have some data on the local disk:\r\n\r\n`/local/year=2017/myfile_1.prt ``/local/year=2017/myfile_2.prt `\r\n\r\netc.I tried to read it using Pyarrow:\r\n\r\n`import pyarrow.parquet as pq``pq.read_parquet('/local/year=2017')`\r\n\r\nand it starts reading. The problem is that the local Parquet files are around 15GB total, and I blew up my machine memory a couple of times because when reading these files, Pyarrow is using more than 60GB of RAM, and I'm not sure how much it will take because it never finishes. Is this expected? Is there a workaround?\r\n\r\n\u00a0",
        "created_at": "2019-02-04T10:54:39.000Z",
        "updated_at": "2019-08-21T23:21:15.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-08-21T23:21:15.000Z"
    },
    "comments": [
        {
            "created_at": "2019-02-04T13:37:11.376Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4470?focusedCommentId=16759849) by Uwe Korn (uwe):*\n`[~ispmarin]` Parquet files are even less than 10% of the size of the data in RAM. So this blow-up could be the expected RAM usage. Please have a look at a single file and its memory size as Pandas DataFrame as a comparison whether the RAM usage is realistic. (Call `df.memory_usage(deep=True)` to get the memory usage of a pandas DataFrame)"
        },
        {
            "created_at": "2019-02-04T13:55:27.502Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4470?focusedCommentId=16759861) by Wes McKinney (wesm):*\nEventually we need to implement the ability to write an Arrow IPC stream to disk while reading a Parquet file (or any dataset source):\r\n\r\n- Read a RecordBatch/Table from the file in one thread\n- Write to IPC stream/file on disk in another thread\n  \n  Once the read is completed we can memory map the IPC file so the memory use will stay low"
        },
        {
            "created_at": "2019-08-21T23:21:15.839Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4470?focusedCommentId=16912782) by Wes McKinney (wesm):*\nIf you can provide a reproduction of the issue we can provide more advice how to reduce memory use (or determine whether there are bugs). Note that pyarrow 0.14.1 has a memory use bug ARROW-6060 that is fixed in master"
        }
    ]
}