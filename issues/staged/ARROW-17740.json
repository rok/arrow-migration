{
    "issue": {
        "title": "[c++][compute]Is there any other way to use Join besides Acero\uff1f",
        "body": "***Note**: This issue was originally created as [ARROW-17740](https://issues.apache.org/jira/browse/ARROW-17740). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nAcero performs poorly, and coredump occurs frequently\uff01\r\n\u00a0\r\nIn the scenario I'm working on, I'll read one Parquet file and then several other Parquet files. These files will have the same column name (UUID). I need to join (by UUID), project (remove UUID), and filter (some custom filtering) the results of the two reads. I found that Acero could only be used to do join, but when I tested it, Acero performance was very poor and very unstable, coredump often happened. Is there another way? Or just another way to do a join!\r\n\u00a0\r\nmy project commit: [\u94fe\u63a5](https://github.com/LinGeLin/io/commit/9b1b06d8d74154f0768bf5258cc3eaa2b9e20701)\r\n\r\ntensorflow ==2.6.2\r\nyou can build tfio as follows:\r\n./configure.sh\r\nbazel build ~~s~~ \u00a0-verbose_failures $BAZEL_OPTIMIZATION //tensorflow_io/... //tensorflow_io_gcs_filesystem/... --compilation_mode=opt --copt=-msse4.2 --copt=-mfma --copt=-mavx2\u00a0\r\npython setup.py bdist_wheel --data bazel-bin\r\npip install dist/tensorflow_io-0.21.0-cp38-cp38-linux_x86_64.whl --force-reinstall --no-deps\r\n\u00a0\r\nrun v4test.py to test the dataset\r\n\u00a0\r\nData.zip contains several parquet files, which are stored on S3 in my scenario.\r\n\r\nI have copied some of the code into test.cpp and can only see the general flow, not compiled\r\n\r\n\u00a0",
        "created_at": "2022-09-15T14:25:07.000Z",
        "updated_at": "2022-10-05T19:59:21.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-09-16T13:30:30.876Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17605827) by Aldrin Montana (octalene):*\nis test.cpp just a snippet?\r\n\r\n\u00a0\r\n```cpp\n\r\n      ...\r\n      // auto arrow_status = tr.ReadNext(&batch);\r\n      auto record_batchs_result = tr.ToRecordBatches();\r\n      if (!record_batchs_result.ok()) {\r\n        res = arrow::Status::Invalid(\"response table ToRecordBatches failed: \" +\r\n                                                   record_batchs_result.status().ToString());\r\n        break;\r\n      }\r\n      if (background) {\r\n        next_record_batches_ = record_batchs_result.ValueOrDie();\r\n\r\n      } else {\r\n        record_batches_ = record_batchs_result.ValueOrDie();\r\n      }\r\n    } while (0);\r\n    ...\r\n  }\r\n```\r\n\r\nThere is 1 instance of `record_batches_` and there's a member variable `record_batchs_`. It's probably an unrelated error, but still seems supsicious.\r\n\r\nI also cannot view the project commit. not sure if it's a private fork?"
        },
        {
            "created_at": "2022-09-16T13:41:35.949Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17605837) by LinGeLin (LinGeLin):*\nno private fork, the commit info changed.I have changed it to the correct link"
        },
        {
            "created_at": "2022-09-16T13:43:14.336Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17605840) by LinGeLin (LinGeLin):*\njust snippet"
        },
        {
            "created_at": "2022-09-20T12:08:33.790Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17607148) by LinGeLin (LinGeLin):*\nI re-uploaded a copy of the code that can be compiled and run in join_test.zip, including cmakelists.txt, the test data files and the Python code that generated the test files. There is also Python code to view the data files. You will need to compile Arrow 9.0 yourself."
        },
        {
            "created_at": "2022-09-21T00:41:37.823Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17607435) by Weston Pace (westonpace):*\nThanks for uploading the new example.  I was able to get it to compile and play around with it.  There is only one thing that jumped out at me that could cause segmentation fault.  When you run the plan:\r\n\r\n```\n\r\n  // validate the Execplan\r\n  plan->Validate();\r\n  // start the Execplan\r\n  plan->StartProducing();\r\n  std::shared_ptr<arrow::Table> response_table;\r\n  response_table =\r\n      arrow::Table::FromRecordBatchReader(sink_reader.get()).ValueOrDie();\r\n  std::cout << \"Result: \" << response_table->ToString() << std::endl;\r\n  plan->StopProducing();\r\n  plan->finished();\r\n```\r\n\r\nThe call to `plan->finished()` actually returns a `Future<>` object. You are not waiting for this future to complete.  The `ExecPlan` can not be safely destroyed until this future has completed.  If this were getting destroyed early I would expect you would see an error logged to stderr:\r\n\r\n```\n\r\nPlan was destroyed before finishing\r\n```\r\n\r\nInstead you probably want...\r\n\r\n```\n\r\nplan->finished().result().ValueOrDie();\r\n```\r\n\r\nThere are also a few calls like `plan->StartProducing()` and `plan->Validate()` which return `arrow::Status` but you are not inspecting the result and so you might miss errors.\r\n\r\nRegarding performance.  You are currently reading each file into a table and then using a table_source node to read from the in-memory table.  Acero can work this way but it really shines when your sources are files.  This is what the \"scan\" node is designed for.  The scan node has some advantages:\r\n\r\n \\* The join will start processing while the I/O is happening (e.g. we can join on batch N while reading batch N-1 from disk)\r\n \\* The scan node will read in parallel and optimize I/O for full-file reading with pre-buffering, etc.\r\n\r\nI'm uploading a new version of the test_join.cpp program which demonstrates using a scanner. [test_join.cpp](test_join.cpp) \r\n"
        },
        {
            "created_at": "2022-09-21T14:20:42.058Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17607779) by LinGeLin (LinGeLin):*\nThank you very much, your reply is very helpful. I have one more question to ask. Since our data is actually stored in S3, I would like to ask if we can set project during SCAN. My understanding is that we will only get the columns we need from S3 instead of scanning the entire file. This will greatly reduce the network bandwidth usage. Or did I misunderstand that even if I do project after SCAN, it will also only read the required columns?\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-09-21T14:28:28.480Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17607787) by LinGeLin (LinGeLin):*\nUnfortunately, when the size of Parquet files reaches about 500M, the performance still cannot meet our requirements. I generated a CSV file with 1,000 columns and 100,000 rows using gen_csv.py and then converted it to a Parquet file as a left file. A Parquet file with 1,000 columns and 80,000 rows is also generated as the right file. Only one column (S1, B1) is read from a file. The test results are as follows:\r\n\r\nduration: 7.41882 s\r\nrows: 80000\r\nspeed: 10783.4rows/s\r\n\r\n\r\nIn fact, in our scenario, we use Parquet to store the features of the recommendation system. The right file will be used to store the features of all the recommendation algorithm groups, and it will have about 7,000 columns. Left file stores preprocessing characteristics for a specific group. There are about 350 columns so far, but about 30 columns are arrays of length 30."
        },
        {
            "created_at": "2022-09-22T01:42:56.033Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17608025) by Weston Pace (westonpace):*\n> I would like to ask if we can set project during SCAN\r\n\r\nYou can do this with:\r\n\r\n```\n\r\nscanner_builder->Project(column_names);\r\n```\r\n\r\n> Only one column (S1, B1) is read from a file.\r\n\r\nIs this with setting projection on the scanner as mentioned above?  We've had bottlenecks on wide files (e.g. many columns) before so it's possible there is some low hanging fruit here.  Also, is this with a release build?"
        },
        {
            "created_at": "2022-09-22T02:32:18.767Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17608035) by LinGeLin (LinGeLin):*\nyes\uff0crelease build. I also did a project at SCAN\r\n```java\n\r\n//\u4ee3\u7801\u5360\u4f4d\u7b26\r\n// when left file, column_names = {\"uuid\", \"s1\"}\r\n// when right file, column_names = {\"uuid\", \"b1\"}\r\nscanner_build->Project(column_names);\n```\r\nIs too many columns affecting performance? Each of my new test data files has a thousand columns"
        },
        {
            "created_at": "2022-09-27T09:49:09.874Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17609940) by LinGeLin (LinGeLin):*\n<https://drive.google.com/file/d/1VAx_LMpIoQ2pF8bYguuWc_hJpjTY02LP/view?usp=sharing>\r\n\r\nnew test data with a thousand columns and code file. you can download the data.zip from google drive\r\n\r\nJust checking to make sure Acero can handle files of this size efficiently? Even more columns. If not, we must consider other implementations"
        },
        {
            "created_at": "2022-09-30T06:32:50.756Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17611381) by LinGeLin (LinGeLin):*\nLooks like there's something even more deadly. The following error is reported even if the column used for the join and ultimately the column to be selected is not of type List.\r\n\r\n![image-2022-09-30-14-32-48-405.png](https://issues.apache.org/jira/secure/attachment/13049962/image-2022-09-30-14-32-48-405.png)"
        },
        {
            "created_at": "2022-09-30T07:21:29.697Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17611404) by LinGeLin (LinGeLin):*\nI'm giving up on Acero\u3002I have also tested DuckDB and DuckDB also performs poorly when reading many columns. Any other suggestions? Otherwise, I'm gonna have to tank."
        },
        {
            "created_at": "2022-10-05T19:59:21.368Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17740?focusedCommentId=17613157) by Weston Pace (westonpace):*\n> Looks like there's something even more deadly. The following error is reported even if the column used for the join and ultimately the column to be selected is not of type List.\r\n\r\nCan you clarify?  Is the column part of the join payload or not?  If it is not part of the payload at all then that is a new issue.  If it is part of the payload then I think that is ARROW-17216\r\n\r\n> I'm giving up on Acero\u3002I have also tested DuckDB and DuckDB also performs poorly when reading many columns. Any other suggestions? Otherwise, I'm gonna have to tank.\r\n\r\nI am not aware of anyone actively working on performance for the many-columns case."
        }
    ]
}