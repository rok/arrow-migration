{
    "issue": {
        "title": "[C++] If a projected_schema is not supplied but a bound projection expression is then we should use that to infer the projected_schema",
        "body": "***Note**: This issue was originally created as [ARROW-15726](https://issues.apache.org/jira/browse/ARROW-15726). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe following query should read a single column from the target parquet file.\r\n\r\n```\n\r\nopen_dataset(\"lineitem.parquet\") %>% select(l_tax) %>% filter(l_tax < 0.01) %>% collect()\r\n```\r\n\r\nFurthermore, it should apply a pushdown filter to the source node allowing parquet row groups to potentially filter out target data.\r\n\r\nAt the moment it creates the following exec plan:\r\n\r\n```\n\r\n3:SinkNode{}\r\n  2:ProjectNode{projection=[l_tax]}\r\n    1:FilterNode{filter=(l_tax < 0.01)}\r\n      0:SourceNode{}\r\n```\r\n\r\nThere is no projection or filter in the source node.  As a result we end up reading much more data from disk (the entire file) than we need to (at most a single column).\r\n\r\nThis _could_ be fixed via heuristics in the dplyr code.  However, it may quickly get complex (for example, the project comes after the filter, so you need to make sure you push down a projection that includes both the columns accessed by the filter and the columns accessed by the projection OR can you push down the projection through a join [yes you can], how do you know which columns to apply to which source node).\r\n\r\nA more complete fix would be to call into some kind of 3rd party optimizer (e.g. calcite) after the plan has been created by dplyr but before it is passed to the execution engine.",
        "created_at": "2022-02-18T03:21:17.000Z",
        "updated_at": "2022-02-24T12:11:43.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-02-23T18:31:25.000Z"
    },
    "comments": [
        {
            "created_at": "2022-02-18T13:00:44.012Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15726?focusedCommentId=17494586) by Neal Richardson (npr):*\nWe pass filter and columns for predicate pushdown to create the scan node: https://github.com/apache/arrow/blob/master/r/R/query-engine.R#L92\r\n\r\nhttps://github.com/apache/arrow/blob/master/r/src/compute-exec.cpp#L142-L158\r\n\r\nSo what's not working correctly here?"
        },
        {
            "created_at": "2022-02-18T18:02:22.416Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15726?focusedCommentId=17494767) by Jonathan Keane (jonkeane):*\nThis might be coincidence (though I suspect now...) Our dataset benchmarks are suddenly failing (with at least some of the failures being caused by attempting to read data that shouldn't be being read [1])\r\n\r\nElena has helped narrow down the range of possible commits that this could have happened in:\r\n\r\nThe first commit this might have happened in is https://github.com/apache/arrow/commit/a935c81b595d24179e115d64cda944efa93aa0e0\r\nand https://github.com/apache/arrow/commit/afaa92e7e4289d6e4f302cc91810368794e8092b it for sure happens in, so it was that commit or before.\r\n\r\nHere's an example buildkite log: https://buildkite.com/apache-arrow/arrow-bci-benchmark-on-ursa-i9-9960x/builds/145#ebd7ea7a-3fad-4865-9e73-49200d89ddd6/6-3230\r\n\r\n[1] this is a bit in the weeds, so bear with me: The dataset we use for these benchmarks includes data in an early year that causes a schema failure `Unsupported cast from string to null using function cast_null`. The benchmarks that we wrote cleverly avoid selecting anything from this first year (so if filter pushdown is working, we don't get the error). It _has_ been working (for a while now! even with exec nodes), but suddenly about three days ago, that has actually stopped working and the benchmarks started failing"
        },
        {
            "created_at": "2022-02-18T18:49:18.837Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15726?focusedCommentId=17494780) by Weston Pace (westonpace):*\nOk.  The projection problem is my fault due to a change I introduced in https://github.com/apache/arrow/commit/a935c81b595d24179e115d64cda944efa93aa0e0 to make it easier to scan.  The change created a default behavior if the user did not specify a projection or projected_schema.  Unfortunately, if the user supplies a bound projection function but not a projected_schema (which is what R does) then we could (and were) infer the projected_schema from the arguments of the projection expression.\r\n\r\nThe filter problem wasn't actually a problem but an issue with printing.  We do not print the pushdown filter when we print the scan node so I had assumed it wasn't specified but it was, in fact, there."
        },
        {
            "created_at": "2022-02-23T18:31:25.758Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15726?focusedCommentId=17496980) by Weston Pace (westonpace):*\nIssue resolved by pull request 12466\n<https://github.com/apache/arrow/pull/12466>"
        }
    ]
}