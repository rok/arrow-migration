{
    "issue": {
        "title": "[Python] Load HdfsClient default options from core-site.xml or hdfs-site.xml, if available",
        "body": "***Note**: This issue was originally created as [ARROW-448](https://issues.apache.org/jira/browse/ARROW-448). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis will yield a nicer user experience for some users",
        "created_at": "2016-12-29T12:09:17.000Z",
        "updated_at": "2021-06-22T12:47:34.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-06-22T12:47:34.000Z"
    },
    "comments": [
        {
            "created_at": "2017-10-04T01:57:00.716Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-448?focusedCommentId=16190677) by Wes McKinney (wesm):*\nCould use some help with this"
        },
        {
            "created_at": "2018-01-25T22:35:09.247Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-448?focusedCommentId=16340219) by Jim Crist (jim.crist):*\nIs this necessary? Per the lihbdfs docs, if no host is provided, the default namenode is automatically determined via the xml files. This works fine on my system with no need to specify the host directly. What more is there to do?"
        },
        {
            "created_at": "2018-01-25T23:17:09.672Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-448?focusedCommentId=16340273) by Wes McKinney (wesm):*\nDoes this rely on any environment variables to work (maybe `HADOOP_HOME`?) I think perhaps we only need to document what configuration is necessary to make it connect automatically. I wasn't sure if there was any automagic configuration from e.g. <https://github.com/dask/hdfs3/blob/master/hdfs3/conf.py>\u00a0that would be useful to replicate here."
        },
        {
            "created_at": "2018-02-03T00:13:26.449Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-448?focusedCommentId=16351100) by Jim Crist (jim.crist):*\n> Does this rely on any environment variables to work (maybe `HADOOP_HOME`?)\r\n\r\nThe documentation on this is a bit unclear, but I believe it just relies on the classpath being properly set to include the conf directory (see \"Common Problems\" section here <https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/LibHdfs.html>). For example, the first path in my `hadoop classpath --glob` is `/etc/hadoop/conf`, which is where the configuration files live. Since pyarrow automatically sets this using `hadoop` then everything works fine if you're using the libhdfs driver.\r\n\r\n\u00a0\r\n\r\nHowever, libhdfs3 doesn't work that way. From reading the source, it looks like it looks for \"dfs.default.uri\" in the path specified by the `LIBHDFS3_CONF` environment variable, and falls back to `localhost:8020` ([https://github.com/ContinuumIO/libhdfs3-downstream/blob/master/libhdfs3/src/common/SessionConfig.cpp#L164](https://github.com/ContinuumIO/libhdfs3-downstream/blob/master/libhdfs3/src/common/SessionConfig.cpp#L164).)). This is why hdfs3 needed to implement the auto-configuration stuff itself, as the underlying libhdfs3 library doesn't do autoconfiguration the same way.\r\n\r\n\u00a0\r\n\r\nIn my experience the auto-configuration in hdfs3 has been unreliable, but could be emulated in simple cases. The exact ordering of setting overrides isn't clear to me, but the order of preference seems to be `dfs.nameservices`, `dfs.namenode.rpc-address`, `fs.defaultFS`, which are found in a combination of `core-site.xml` and `hdfs-site.xml`."
        }
    ]
}