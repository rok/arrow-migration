{
    "issue": {
        "title": "[C++]Memory leak while reading parquet file",
        "body": "***Note**: This issue was originally created as [ARROW-14987](https://issues.apache.org/jira/browse/ARROW-14987). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen I used parquet to access data, I found that the memory usage was still high after the function ended. I reproduced this problem in the example. code show as below:\r\n\r\n\u00a0\r\n```c++\n\r\n#include <arrow/api.h>\r\n#include <arrow/io/api.h>\r\n#include <parquet/arrow/reader.h>\r\n#include <parquet/arrow/writer.h>\r\n#include <parquet/exception.h>\r\n#include <unistd.h>\r\n#include <iostream>\r\n\r\nstd::shared_ptr<arrow::Table> generate_table() {\r\n  arrow::Int64Builder i64builder;\r\n  for (int i=0;i<320000;i++){\r\n\t  i64builder.Append(i);\r\n  }\r\n  std::shared_ptr<arrow::Array> i64array;\r\n  PARQUET_THROW_NOT_OK(i64builder.Finish(&i64array));\r\n\r\n  std::shared_ptr<arrow::Schema> schema = arrow::schema(\r\n      {arrow::field(\"int\", arrow::int64())});\r\n\r\n  return arrow::Table::Make(schema, {i64array});\r\n}\r\n\r\nvoid write_parquet_file(const arrow::Table& table) {\r\n  std::shared_ptr<arrow::io::FileOutputStream> outfile;\r\n  PARQUET_ASSIGN_OR_THROW(\r\n      outfile, arrow::io::FileOutputStream::Open(\"parquet-arrow-example.parquet\"));\r\n  PARQUET_THROW_NOT_OK(\r\n      parquet::arrow::WriteTable(table, arrow::default_memory_pool(), outfile, 3));\r\n}\r\n\r\nvoid read_whole_file() {\r\n  std::cout << \"Reading parquet-arrow-example.parquet at once\" << std::endl;\r\n  std::shared_ptr<arrow::io::ReadableFile> infile;\r\n  PARQUET_ASSIGN_OR_THROW(infile,\r\n                          arrow::io::ReadableFile::Open(\"parquet-arrow-example.parquet\",\r\n                                                        arrow::default_memory_pool()));\r\n\r\n  std::unique_ptr<parquet::arrow::FileReader> reader;\r\n  PARQUET_THROW_NOT_OK(\r\n      parquet::arrow::OpenFile(infile, arrow::default_memory_pool(), &reader));\r\n  std::shared_ptr<arrow::Table> table;\r\n  PARQUET_THROW_NOT_OK(reader->ReadTable(&table));\r\n  std::cout << \"Loaded \" << table->num_rows() << \" rows in \" << table->num_columns()\r\n            << \" columns.\" << std::endl;\r\n}\r\n\r\nint main(int argc, char** argv) {\r\n  std::shared_ptr<arrow::Table> table = generate_table();\r\n  write_parquet_file(*table);\r\n  std::cout << \"start \" <<std::endl;\r\n  read_whole_file();\r\n  std::cout << \"end \" <<std::endl;\r\n  sleep(100);\r\n}\r\n\r\n```\r\nAfter the end, during sleep, the memory usage is still more than 100M and has not dropped. When I increase the data volume by 5 times, the memory usage is about 500M, and it will not drop.\r\nI want to know whether this part of the data is cached by the memory pool, or whether it is a memory leak problem. If there is no memory leak, how to set memory pool size or release memory?",
        "created_at": "2021-12-06T07:52:57.000Z",
        "updated_at": "2022-09-28T16:50:23.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-12-07T03:41:31.746Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14987?focusedCommentId=17454354) by Weston Pace (westonpace):*\n**TL:DR; A chunk_size of 3 is way too low.**\r\n\r\nThank you so much for the detailed reproduction.\r\n\r\n### Some notes\r\n\r\nFirst, I used 5 times the amount of data that you were working with.  This works out to 12.5MB of int64_t \"data\"\r\n\r\nSecond, you are not releasing the variable named \"table\" in your main method.  This holds on to 12.5MB of RAM.  I added table.reset() before the sleep to take care of this.\r\n\r\nThird, a chunk size of 3 is pathologically small. This means parquet is going to have to write row group metadata after every 3 rows of data.  As a result, the parquet file, which only contains 12.5MB of real data, requires 169MB.  This means there is ~157MB of metadata.  A chunk size should, at a minimum, be in the tens of thousands, and often is in the millions.\r\n\r\n**When I run this test I end up with nearly 1GB of memory usage!  Even given the erroneously large parquet file this seems like way too much**\r\n\r\n### Figuring out Arrow memory pool usage\r\n\r\nOne helpful tool when determining how much RAM Arrow is using is to print out how many bytes Arrow thinks it is holding onto.  To do this you can add...\r\n\r\n```\n\r\nstd::cout << arrow::default_memory_pool()->bytes_allocated() << \" bytes_allocated\" << std::endl;\r\n```\r\n\r\nAssuming you add the \"table.reset()\" call this should print \"0 bytes allocated\" which means that Arrow is not holding on to any memory.\r\n\r\nThe second common thing to get blamed is jemalloc.  Arrow uses jemalloc (or possibly mimalloc) internally in its memory pools and these allocators sometimes over-allocate and sometimes hold onto memory for a little while.  However, this seems unlikely because jemalloc is configured by default by Arrow to release over-allocated memory every 1 second.\r\n\r\nTo verify I built an instrumented version of Arrow to print stats for its internal jemalloc pool after 5 seconds of being idle.  I got:\r\n\r\n```\n\r\nAllocated: 29000, active: 45056, metadata: 6581448 (n_thp 0), resident: 6606848, mapped: 12627968, retained: 125259776\r\n```\r\n\r\nThis means Arrow has 29KB of data actively allocated (this is curious, given bytes_allocated is 0, and worth investigation at a later date, but certainly not the culprit here).\r\n\r\nThat 29KB of active data spans 45.056KB of pages (this is what people refer to when they talk about fragmentation).  There is also 6.58MB of jemalloc metadata.  I'm pretty sure this is rather independent of the workload and not something to worry too much about.\r\n\r\nCombined, this 45.056KB of data and 6.58MB of metadata is occupying 6.61MB of RSS.  So far so good.\r\n\r\n### Figuring out the rest of the memory usage\r\n\r\nThere is only one other place the remaining memory usage can be, which is the application's global system allocator.  To debug this further I built my test application with jemalloc (a different jemalloc instance than the one running Arrow).  This means Arrow's memory pool will use one instance of jemalloc and everything else will use my own instance of jemalloc.  Printing stats I get:\r\n\r\n```\n\r\nAllocated: 257904, active: 569344, metadata: 15162288 (n_thp 0), resident: 950906880, mapped: 958836736, retained: 648630272\r\n```\r\n\r\nNow we have found our culprit.  There is about 258KB allocated and it occupied 569KB worth of pages and 15MB of jemalloc metadata.  This is pretty reasonable and makes sense (this is memory used by shared pointers and various metadata objects.  It seems pretty appropriate.\r\n\r\n_However, this ~15MB of data is occupying nearly 1GB of RSS!_\r\n\r\nTo debug further I used jemalloc's memory profiling to track where all of these allocations were happening.  It turns out most of these allocations were in the parquet reader itself.  While the table built will eventually be constructed in Arrow's memory pool the parquet reader does not use the memory pool for the various allocations needed to operate the reader itself.\r\n\r\nSo, putting this all together into a hypothesis...\r\n\r\nThe chunk size of 3 means we have a ton of metadata.  This metadata gets allocated by the parquet reader in lots of very small allocations.  These allocations have terrible fragmentation and the system allocator ends up scattering this information across a wide swath of RSS and results in a large amount of over-allocation.\r\n\r\n### Fixes\r\n\r\n#### Fix 1: Use more jemalloc\r\n\r\nSince my test was already using jemalloc I can configure jemalloc the same way Arrow does by enabling the background thread and setting it to purge on a 1 second interval.  Now, running my test, after 5 seconds of inactivity I get the following from the global jemalloc:\r\n\r\n```\n\r\nAllocated: 246608, active: 544768, metadata: 15155760 (n_thp 0), resident: 15675392, mapped: 23613440, retained: 1382526976\r\n```\r\n\r\nWe now see that same ~15MB of data and jemalloc metadata is now spread across 15.6MB of RSS (pretty great fragmentation support).  I can confirm this by looking at the RSS of the process which reports 25MB (most of which is explained by the two jemalloc instance's metadata) which is a massive improvement over 1GB.\r\n\r\n#### Fix 2: Use a sane chunk size\r\n\r\nIf I change the chunk size to 100,000 then suddenly parquet is not making so many tiny allocations (my program runs much faster) and I get the following stats for the global jemalloc instance:\r\n\r\n```\n\r\nAllocated: 1756168, active: 2027520, metadata: 4492600 (n_thp 0), resident: 6496256, mapped: 8318976, retained: 64557056\r\n```\r\n\r\nAnd I see only 18.5MB of RSS usage."
        },
        {
            "created_at": "2021-12-08T03:47:25.901Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14987?focusedCommentId=17454944) by Qingxiang Chen (chenqx303):*\nThank you for your answer, the scheme of increasing the chunk size really helped me a lot. I have two more questions.\r\n1. What are the advantages of jemalloc compared to the default malloc, and why it reduces the memory usage so much. Should I replace my system malloc with jemalloc to achieve optimization 1?\r\n2. How do you use jemalloc to view the allocated memory? If I want to analyze related memory problems in my system, what should I do\uff1f"
        },
        {
            "created_at": "2021-12-10T18:45:55.843Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14987?focusedCommentId=17457321) by Weston Pace (westonpace):*\n1. I can't really answer this, sorry.  This is up to you and I don't think we make any recommendations on what allocator should be used.  I was only using jemalloc for debugging purposes.\r\n\r\n2. If you have jemalloc enabled you can use malloc_stats_print.  Interpreting this information is rather difficult.  Jemalloc also has a profiler that is sometimes handy for tracking large swathes of allocations.\r\n\r\nThat being said, these questions kind of fall outside the scope of Arrow so I'm not sure how much help I can really give."
        },
        {
            "created_at": "2022-06-29T18:02:55.335Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14987?focusedCommentId=17560618) by &res (0x26dres):*\n`[~westonpace]` first of all thanks for the above explanation it is very interesting and helpful\r\n\r\nI'm having similar issues where the overall memory usage of my program doesn't match what I would expect from arrow.\r\n\r\nIf I compare the size of the arrow.Tables (using nbytes) with the real memory usage of my program there are very big discrepancies.\r\n\r\nSo taking a similar example, where I create a table, split it in many small tables and put them back together in one big table:\r\n\u00a0\r\n```python\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\n\r\ndef recombine(table: pa.Table) -> pa.Table:\r\n    tables = []\r\n    for i in range(table.num_rows):\r\n        tables.append(table[i: i + 1])\r\n    return pa.concat_tables(tables).combine_chunks()\r\n\r\n\r\ndef create_table() -> pa.Table:\r\n    return pa.Table.from_pandas(\r\n        pd.DataFrame(np.random.rand(1000000, 3), columns=['col1', 'col2', 'col3'])\r\n    )\r\n\r\n\r\ndef play_with_memory():\r\n    for i in range(5):\r\n        print(i)\r\n        recombine(create_table())\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    play_with_memory()\r\n```\r\nIn this example, the table uses 24mb. When I look at the memory_pool stats I can see that bytes_allocated is around 24MB and max_memory is at 48MB. So it all makes sense.\r\n\r\nBut after the first pass, the memory usage of my process goes to 1.7gb, and this memory never gets released.\r\n\r\n\u00a0\r\n\r\nI suspect that when creating the array of tables, this create a lot of small objects. These allocations may get fragmented and increase the memory usage of the process.\r\n\r\nWhilst I understand this pathologically bad, I can imagine long running services may accumulate a lot of these type of small object allocation. Or another example would be a batch job which loads fragmented parquet dataset and puts them back together.\r\n\r\n\u00a0\r\n\r\nI've written an instrumented version of this code:\r\n\r\n\u00a0\r\n```python\n\r\nimport gc\r\nimport os\r\nimport sys\r\n\r\nimport guppy\r\nimport humanize\r\nimport numpy as np\r\nimport pandas as pd\r\nimport psutil\r\nimport pyarrow as pa\r\n\r\n\r\ndef recombine(table: pa.Table, position, process, h) -> pa.Table:\r\n    tables = []\r\n    for i in range(table.num_rows):\r\n        tables.append(table[i: i + 1])\r\n    print(\r\n        {\r\n            \"position\": position,\r\n            \"bytes_allocated\": humanize.naturalsize(pa.default_memory_pool().bytes_allocated()),\r\n            \"max_memory\": humanize.naturalsize(pa.default_memory_pool().max_memory()),\r\n            \"process_memory\": humanize.naturalsize(process.memory_info().rss),\r\n            \"heap\": humanize.naturalsize(h.heap().size)\r\n        }\r\n    )\r\n    return pa.concat_tables(tables).combine_chunks()\r\n\r\n\r\ndef create_table() -> pa.Table:\r\n    return pa.Table.from_pandas(\r\n        pd.DataFrame(np.random.rand(1000000, 3), columns=['col1', 'col2', 'col3'])\r\n    )\r\n\r\n\r\ndef play_with_memory():\r\n    process = psutil.Process(os.getpid())\r\n    h = guppy.hpy()\r\n    print({\r\n        \"python\": sys.version_info,\r\n        \"pyarrow\": pa.__version__,\r\n        \"memory_pool\": pa.default_memory_pool().backend_name,\r\n    })\r\n    print(\r\n        {\r\n            \"position\": \"before\",\r\n            \"bytes_allocated\": humanize.naturalsize(pa.default_memory_pool().bytes_allocated()),\r\n            \"max_memory\": humanize.naturalsize(pa.default_memory_pool().max_memory()),\r\n            \"process_memory\": humanize.naturalsize(process.memory_info().rss),\r\n            \"heap\": humanize.naturalsize(h.heap().size)\r\n        }\r\n    )\r\n    for i in range(5):\r\n        recombine(create_table(), f\"during_{i}\", process, h)\r\n        gc.collect()\r\n        print(\r\n            {\r\n                \"position\": f\"after_{i}\",\r\n                \"bytes_allocated\": humanize.naturalsize(pa.default_memory_pool().bytes_allocated()),\r\n                \"max_memory\": humanize.naturalsize(pa.default_memory_pool().max_memory()),\r\n                \"process_memory\": humanize.naturalsize(process.memory_info().rss),\r\n                \"heap\": humanize.naturalsize(h.heap().size)\r\n            }\r\n        )\r\n    print(\r\n        {\r\n            \"position\": \"after\",\r\n            \"bytes_allocated\": humanize.naturalsize(pa.default_memory_pool().bytes_allocated()),\r\n            \"max_memory\": humanize.naturalsize(pa.default_memory_pool().max_memory()),\r\n            \"process_memory\": humanize.naturalsize(process.memory_info().rss),\r\n            \"heap\": humanize.naturalsize(h.heap().size)\r\n        }\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    play_with_memory()\r\n\r\n```\r\nWhen running it (using docker) I see these results:\r\n```python\n\r\n{'python': sys.version_info(major=3, minor=8, micro=13, releaselevel='final', serial=0), 'pyarrow': '8.0.0', 'memory_pool': 'jemalloc'}\r\n{'position': 'before', 'bytes_allocated': '0 Bytes', 'max_memory': '0 Bytes', 'process_memory': '178.3 MB', 'heap': '31.0 MB'}\r\n{'position': 'during_0', 'bytes_allocated': '24.0 MB', 'max_memory': '24.1 MB', 'process_memory': '1.7 GB', 'heap': '144.1 MB'}\r\n{'position': 'after_0', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n{'position': 'during_1', 'bytes_allocated': '24.0 MB', 'max_memory': '48.0 MB', 'process_memory': '2.0 GB', 'heap': '144.1 MB'}\r\n{'position': 'after_1', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n{'position': 'during_2', 'bytes_allocated': '24.0 MB', 'max_memory': '48.0 MB', 'process_memory': '2.0 GB', 'heap': '144.1 MB'}\r\n{'position': 'after_2', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n{'position': 'during_3', 'bytes_allocated': '24.0 MB', 'max_memory': '48.0 MB', 'process_memory': '2.0 GB', 'heap': '144.1 MB'}\r\n{'position': 'after_3', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n{'position': 'during_4', 'bytes_allocated': '24.0 MB', 'max_memory': '48.0 MB', 'process_memory': '2.0 GB', 'heap': '144.1 MB'}\r\n{'position': 'after_4', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n{'position': 'after', 'bytes_allocated': '0 Bytes', 'max_memory': '48.0 MB', 'process_memory': '1.9 GB', 'heap': '31.4 MB'}\r\n\r\n```\r\n\u00a0\r\n\r\nAs you can see, the heap and the max_memory are within the bond of what one would expect, the the process memory isn't.\r\n\r\nHere's the docker file I used to build:\r\n```java\n\r\nFROM python:3.8-slim-bullseye\r\n\r\nRUN --mount=type=cache,target=/root/.cache/pip pip install --upgrade pip\r\nRUN --mount=type=cache,target=/root/.cache/pip pip install pyarrow humanize pandas numpy psutil guppy3\r\n\r\nENV PYTHONPATH=/opt/leak\r\nENV PYTHONUNBUFFERED=1\r\n\r\nWORKDIR /opt/leak\r\nADD memory_leak.py ./\r\n\r\nENTRYPOINT [\"python\", \"memory_leak.py\"]\r\n\r\n```\r\n\u00a0\r\n\r\nI'm not really sure how to solve this issue. I would like to be able to do this:\r\n> \"Since my test was already using jemalloc I can configure jemalloc the same way Arrow does by enabling the background thread and setting it to purge on a 1 second interval. Now, running my test, after 5 seconds of inactivity I get the following from the global jemalloc\"\r\nBut I'm not sure how to do it in python."
        },
        {
            "created_at": "2022-09-28T16:50:23.348Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14987?focusedCommentId=17610651) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned per [project policy](https://arrow.apache.org/docs/dev/developers/bug_reports.html#issue-assignment). Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}