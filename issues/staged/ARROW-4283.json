{
    "issue": {
        "title": "[Python] Should RecordBatchStreamReader/Writer be AsyncIterable?",
        "body": "***Note**: This issue was originally created as [ARROW-4283](https://issues.apache.org/jira/browse/ARROW-4283). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFiling this issue after a discussion today with `[~xhochy]` about how to implement streaming pyarrow http services. I had attempted to use both Flask and [aiohttp](https://aiohttp.readthedocs.io/en/stable/streams.html)'s streaming interfaces because they seemed familiar, but no dice. I have no idea how hard this would be to add \u2013 supporting all the asynciterable primitives in JS was non-trivial.",
        "created_at": "2019-01-17T15:09:59.000Z",
        "updated_at": "2019-05-31T02:00:54.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2019-01-17T15:16:46.280Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4283?focusedCommentId=16745184) by Antoine Pitrou (apitrou):*\nThere are two cases here:\r\n \\* RecordBatchStreamReader: this is too high-level; you need to read from your data source in Python (using `await something.read()`) then construct a record batch out of the data (perhaps with a BufferReader)\r\n \\* RecordBatchStreamWriter: conversely, you probably want to write your record batch first into a BufferOutputStream, then write the resulting data in Python (using `await something.write()`)"
        },
        {
            "created_at": "2019-01-20T02:55:51.392Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4283?focusedCommentId=16747321) by Paul Taylor (paul.e.taylor):*\n`[~pitrou]` Thanks for the feedback.\r\n\r\nI want to clarify: my Python skills aren't sharp, I'm not familiar with the pyarrow API or Python's asyncio/async-iterable primitives, so filter my comments through the lens of a beginner.\r\n\r\nThe little experience I do have is using the RecordBatchStreamReader to read from stdin (via `sys.stdin.buffer`) and named file descriptors (via `os.fdopen()`). Since Python's so friendly (and I have no idea how the Python IO primitives work), I thought maybe I could pass aiohttp's `Request.stream` to the RecordBatchStreamReader constructor, and quickly learned that no, I can't ;).\r\n\r\nIn the JS implementation we have two main entry points for reading RecordBatch streams:\r\n1. a static [`RecordBatchReader.from(source)`](https://github.com/apache/arrow/blob/cc1ce6194b905768b1a6d9f0e209270f62dc558a/js/src/ipc/reader.ts#L142), which accepts heterogeneous source types and returns a RecordBatchReader for the underlying Arrow type (file, stream, or JSON) and conforms to sync/async semantics of the source input type\n1. methods that create [through/transform streams](https://github.com/apache/arrow/blob/cc1ce6194b905768b1a6d9f0e209270f62dc558a/js/bin/file-to-stream.js#L33) from the RecordBatchReader and RecordBatchWriter, for use with node's native stream primitives\n   \n   Each link in the streaming pipeline is a sort of transform stream, and a significant amount of effort went into supporting all the different node/browser IO primitives, so I understand if that's too much to ask at this point.\n   \n   As an alternative, would it be possible to add a method that accepts a Python byte stream, and returns a zero-copy AsyncIterable of RecordBatches? Or maybe\u00a0add an an example in the [python/ipc](https://arrow.apache.org/docs/python/ipc.html#writing-and-reading-streams) docs page of how to do that?"
        }
    ]
}