{
    "issue": {
        "title": "[Python][Dataset] The first table schema becomes a common schema for the full Dataset",
        "body": "***Note**: This issue was originally created as [ARROW-12080](https://issues.apache.org/jira/browse/ARROW-12080). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe first table schema becomes a common schema for the full Dataset. It could cause problems with sparse data.\r\n\r\nConsider example below, when first chunks is full of NA, pyarrow ignores dtypes from pandas for a whole dataset:\r\n```java\n\r\n# get dataset\r\n!wget https://physionet.org/files/mimiciii-demo/1.4/D_ITEMS.csv\r\n\r\n\r\nimport pandas as pd \r\nimport pyarrow.parquet as pq\r\nimport pyarrow as pa\r\nimport pyarrow.dataset as ds\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\n\r\ndef foo(input_csv='D_ITEMS.csv', output='tmp.parquet', chunksize=1000):\r\n    if Path(output).exists():\r\n        shutil.rmtree(output)    # write dataset\r\n    d_items = pd.read_csv(input_csv, index_col='row_id',\r\n                      usecols=['row_id', 'itemid', 'label', 'dbsource', 'category', 'param_type'],\r\n                      dtype={'row_id': int, 'itemid': int, 'label': str, 'dbsource': str,\r\n                             'category': str, 'param_type': str}, chunksize=chunksize)    for i, chunk in enumerate(d_items):\r\n        table = pa.Table.from_pandas(chunk)\r\n        if i == 0:\r\n            schema1 = pa.Schema.from_pandas(chunk)\r\n            schema2 = table.schema\r\n#         print(table.field('param_type'))\r\n        pq.write_to_dataset(table, root_path=output)\r\n    \r\n# read dataset\r\n    dataset = ds.dataset(output)\r\n    \r\n# compare schemas\r\n    print('Schemas are equal: ', dataset.schema == schema1 == schema2)\r\n    print(dataset.schema.types)\r\n    print('Should be string', dataset.schema.field('param_type'))    \r\n    return dataset\r\n```\r\n```java\n\r\ndataset = foo()\r\ndataset.to_table()\r\n\r\n>>>Schemas are equal:  False\r\n[DataType(int64), DataType(string), DataType(string), DataType(null), DataType(null), DataType(int64)]\r\nShould be string pyarrow.Field<param_type: null>\r\n---------------------------------------------------------------------------\r\nArrowTypeError: fields had matching names but differing types. From: category: string To: category: null\n```\r\nIf you do schemas listing, you'll see that almost all parquet files ignored pandas dtypes:\r\n```java\n\r\nimport os\r\n\r\nfor i in os.listdir('tmp.parquet/'):\r\n    print(ds.dataset(os.path.join('tmp.parquet/', i)).schema.field('param_type'))\r\n\r\n>>>pyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: string>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: string>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: string>\r\npyarrow.Field<param_type: string>\r\npyarrow.Field<param_type: null>\r\npyarrow.Field<param_type: null>\r\n```\r\nBut if we will get bigger chunk of data, that contains non NA values, everything is OK:\r\n```java\n\r\ndataset = foo(chunksize=10000)\r\ndataset.to_table()\r\n\r\n>>>Schemas are equal:  True\r\n[DataType(int64), DataType(string), DataType(string), DataType(string), DataType(string), DataType(int64)]\r\nShould be string pyarrow.Field<param_type: string>\r\npyarrow.Table\r\nitemid: int64\r\nlabel: string\r\ndbsource: string\r\ncategory: string\r\nparam_type: string\r\nrow_id: int64\r\n```\r\nCheck NA in data:\r\n```java\n\r\npd.read_csv('D_ITEMS.csv', nrows=1000)['param_type'].unique()\r\n>>>array([nan])\r\n\r\npd.read_csv('D_ITEMS.csv', nrows=10000)['param_type'].unique()\r\n>>>array([nan, 'Numeric', 'Text', 'Date time', 'Solution', 'Process',\r\n       'Checkbox'], dtype=object)\r\n```\r\n\u00a0\r\n\r\n\u00a0PS: switching issues reporting from github to Jira is outstanding move\r\n\r\n\u00a0",
        "created_at": "2021-03-24T21:31:51.000Z",
        "updated_at": "2021-04-13T11:34:48.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-03-24T21:49:06.843Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12080?focusedCommentId=17308196) by Borys Kabakov (banderlog):*\nProbably things even worse, because the code snippet below crashes at random:\r\n\r\n\u00a0\r\n```java\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as ds\r\nimport pandas as pd\r\nimport numpy as np\r\nimport shutil\r\nfrom pathlib import Path\r\n\r\n\r\noutput='./tmp.parquet'\r\nif Path(output).exists():\r\n    shutil.rmtree(output)\r\n\r\ndf = pd.DataFrame({'A': [np.nan, np.nan, '3', np.nan],\r\n                   'B': ['1', '2', '3', np.nan]},\r\n                  dtype=str).to_csv('tmp.csv', index=False)\r\n\r\nfor i, chunk in enumerate(pd.read_csv('tmp.csv', dtype={'A': str, 'B': str}, chunksize=1)):\r\n    table = pa.Table.from_pandas(chunk)\r\n    print(table.schema.types)\r\n    pq.write_to_dataset(table, root_path=output)\r\n\r\ndataset = ds.dataset('./tmp.parquet')\r\ndataset.to_table().to_pandas()\r\n```\r\nExamples of output:\r\n\r\n\u00a0\r\n```java\n\r\n# first example\r\n[DataType(null), DataType(string)]\r\n[DataType(null), DataType(string)]\r\n[DataType(string), DataType(string)]\r\n[DataType(null), DataType(null)]\r\n      A     B\r\n0     3     3\r\n1  None     2\r\n2  None  None\r\n3  None     1\r\n\r\n```\r\n```java\n\r\n# second example\r\n[DataType(null), DataType(string)]\r\n[DataType(null), DataType(string)]\r\n[DataType(string), DataType(string)]\r\n[DataType(null), DataType(null)]\r\n-----------------------------\r\nArrowTypeError: fields had matching names but differing types. From: A: string To: A: null\r\n```"
        },
        {
            "created_at": "2021-03-24T22:17:15.612Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12080?focusedCommentId=17308207) by Borys Kabakov (banderlog):*\nIt affects not only datasets, but writing to a single file too:\r\n```java\n\r\nimport pandas as pd \r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nfrom pathlib import Path\r\n\r\n\r\ndef bar(input_csv='D_ITEMS.csv', output='tmp.parquet', chunksize=1000):\r\n    Path(output).unlink(missing_ok=True)\r\n    pqwriter = None\r\n    \r\n    if Path(output).exists():\r\n        shutil.rmtree(output)    # write file\r\n    d_items = pd.read_csv(input_csv, index_col='row_id',\r\n                      usecols=['row_id', 'itemid', 'label', 'dbsource', 'category', 'param_type'],\r\n                      dtype={'row_id': int, 'itemid': int, 'label': str, 'dbsource': str,\r\n                             'category': str, 'param_type': str}, chunksize=chunksize)    for i, chunk in enumerate(d_items):\r\n        table = pa.Table.from_pandas(chunk)\r\n        if i == 0:\r\n# create a parquet write object giving it an output file\r\n            pqwriter = pq.ParquetWriter(output, table.schema)\r\n        pqwriter.write_table(table)\r\n        \r\n# close the parquet writer\r\n    if pqwriter:\r\n        pqwriter.close()\r\n        \r\n    df = pd.read_parquet(output)\r\n    return df\r\n\r\n\r\n# all will be fine\r\n# returned dataframe equal to 'D_ITEMS.csv'\r\ndf = bar(chunksize=10000)\r\n\r\n# it will crash\r\n# same reason as before -- only NAs in the first chunk\r\ndf = bar(chunksize=1000)\r\n\r\n>>>\r\n---------------------------------------------------------------------------\r\nValueError: Table schema does not match schema used to create file: \r\ntable:\r\nitemid: int64\r\nlabel: string\r\ndbsource: string\r\ncategory: null\r\nparam_type: null\r\nrow_id: int64\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [\"row_id\"], \"column_indexes\": [{\"name\": null, ' + 877 vs. \r\nfile:\r\nitemid: int64\r\nlabel: string\r\ndbsource: string\r\ncategory: string\r\nparam_type: null\r\nrow_id: int64\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [\"row_id\"], \"column_indexes\": [{\"name\": null, ' + 879\r\n```"
        },
        {
            "created_at": "2021-04-13T11:34:48.878Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12080?focusedCommentId=17320095) by Joris Van den Bossche (jorisvandenbossche):*\n`[~banderlog]` sorry for the slow response. \r\n\r\n> The first table schema becomes a common schema for the full Dataset\r\n\r\nThat's indeed the current behaviour (but I see that this should be documented better). See ARROW-8221 for a general issue about expanding this (to eg unifying the schema across all files). \r\n\r\nA workaround for now is to manually specify the schema (of course, in case of CSV you actually need to parse the data to get the schema ..). You could read once a bigger chunk to get the proper schema, and then use that schema to pass to `ds.dataset(..)`. Or if you know the schema of the file, you can create it manually with `pa.schema(..)` (similarly as you pass a dict of types to pandas.read_csv).\r\n\r\nIn your specific case, you can actually already specify the scheme in `table = pa.Table.from_pandas(chunk)` before writing to parquet. By doing that, you can ensure that the parquet files have the proper types, and then subsequent reading of the Parquet dataset will work fine without needing to specify the schema manually."
        }
    ]
}