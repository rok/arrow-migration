{
    "issue": {
        "title": "Sigbus when using mmap in multiprocessing env over netapp",
        "body": "***Note**: This issue was originally created as [ARROW-12547](https://issues.apache.org/jira/browse/ARROW-12547). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWe have noticed a condition where using arrow to read parquet files that reside on our netapp from slurm (over python) raise an occasional signal 7.\r\n\r\nWe haven\u2019t yet tried disabling memory mapping yet, although we do expect that turning memory mapping off in read_table will resolve the issue.\r\n\r\nThis seems to occur when we read a file that has just been written, even though we do write parquet files to a transient location and then swap the file in using os.rename\r\n\r\n\u00a0\r\n\r\nAll that said, we were not sure if this was known issue or if team pyarrow is interested in the stack trace.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nThread 1 (Thread 0x7fafa7dff700 (LWP 44408)):\r\n\r\n#0\u00a0 __memcpy_avx_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S:238\r\n\r\n#1\u00a0 0x00007fafb9c40aba in snappy::RawUncompress(snappy::Source\\*, char\\*) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/**libarrow.so**.300\r\n\r\n#2\u00a0 0x00007fafb9c41131 in snappy::RawUncompress(char const\\*, unsigned long, char\\*) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libarrow.so.300\r\n\r\n#3\u00a0 0x00007fafb942abbe in arrow::util::internal::(anonymous namespace)::SnappyCodec::Decompress(long, unsigned char const\\*, long, unsigned char\\*) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libarrow.so.300\r\n\r\n#4\u00a0 0x00007fafb4d0965e in parquet::(anonymous namespace)::SerializedPageReader::DecompressIfNeeded(std::shared_ptr<arrow::Buffer>, int, int, int) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/**libparquet.so**.300\r\n\r\n#5\u00a0 0x00007fafb4d2bc2d in parquet::(anonymous namespace)::SerializedPageReader::NextPage() () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#6\u00a0 0x00007fafb4d330c3 in parquet::(anonymous namespace)::ColumnReaderImplBase<parquet::PhysicalType<(parquet::Type::type)5> >::HasNextInternal() [clone .part.0] () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#7\u00a0 0x00007fafb4d33eb8 in parquet::internal::(anonymous namespace)::TypedRecordReader<parquet::PhysicalType<(parquet::Type::type)5> >::ReadRecords(long) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#8\u00a0 0x00007fafb4d21bb8 in parquet::arrow::(anonymous namespace)::LeafReader::LoadBatch(long) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#9\u00a0 0x00007fafb4d489c8 in parquet::arrow::ColumnReaderImpl::NextBatch(long, std::shared_ptr<arrow::ChunkedArray>\\*) () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#10 0x00007fafb4d32db9 in arrow::internal::FnOnce<void ()>::FnImpl<std::_Bind<arrow::detail::ContinueFuture (arrow::Future<arrow::detail::Empty>, parquet::arrow::(anonymous namespace)::FileReaderImpl::GetRecordBatchReader(std::vector<int, std::allocator<int> > const&, std::vector<int, std::allocator<int> > const&, std::unique_ptr<arrow::RecordBatchReader, std::default_delete<arrow::RecordBatchReader> >\\*)::\\{lambda()#1}::operator()()::\\{lambda(int)#1}, int)> >::invoke() () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libparquet.so.300\r\n\r\n#11 0x00007fafb9444ddd in std::thread::_State_impl<std::thread::_Invoker<std::tuple<arrow::internal::ThreadPool::LaunchWorkersUnlocked(int)::\\{lambda()#1}> > >::_M_run() () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libarrow.so.300\r\n\r\n#12 0x00007fafb9dd3580 in execute_native_thread_routine () from /home/svc_backtest/portfolio_analytics/prod/pyenv/lib/python3.7/site-packages/pyarrow/libarrow.so.300\r\n\r\n#13 0x00007fafefcdc6ba in start_thread (arg=0x7fafa7dff700) at pthread_create.c:333\r\n\r\n#14 0x00007fafefa1241d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109",
        "created_at": "2021-04-26T17:24:38.000Z",
        "updated_at": "2021-05-07T13:28:27.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-05-07T13:28:27.000Z"
    },
    "comments": [
        {
            "created_at": "2021-04-26T17:59:45.347Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12547?focusedCommentId=17332653) by Maarten Breddels (maartenbreddels):*\nI recommend trying without memory mapping. If IO failures occur during mmaped reading or writing, the cause is usually hidden because it's during a memory read of write handling and no error handling/reporting exists for that AFAIK (just a SIGBUS is raised). The SIGBUS errors I've seen in my life were due to disk failure (corrupt SSD) or when running out of disk space. Using regular io API's improved error msg'es."
        },
        {
            "created_at": "2021-04-27T01:39:36.922Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12547?focusedCommentId=17332830) by Jay Baywatch (Baywatch):*\nI wrote a test and was unexpectedly able to replicate this using Pyarrow 3.0 and pandas 0.25.2.\r\n\r\n\r\n\r\nEDIT: upon further review, it was a bus error, but not exactly the same. Could be something environmental to us, although it's happened on 3 separate hosts now.\r\n\r\nI'll keep digging."
        },
        {
            "created_at": "2021-04-27T19:57:06.378Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12547?focusedCommentId=17333519) by Jay Baywatch (Baywatch):*\nI can consistently reproduce this but seem like its more of a netapp over NFSv3 issue than arrow.\r\n\r\n\u00a0\r\n\r\nThe background is that we have batch ETL jobs that run every 10 minutes that replace parquet files on a r/o netapp volume.\u00a0 We write parquet to tmp and then rename to the production path. Netapp should handle keeping the inode handles for processes that are currently reading when that happens.\r\n\r\nIt seems that if a client is reading when a bunch of updates come in, something weird happens during the page in process and we get a bus error. The files are all about 200 MB, and this only seems to happen when mmaping is enabled in pa.read_table and there are multiple updates to files in the same directory, even if we are not reading them\r\n\r\n\u00a0\r\n\r\nit breaks here:\r\n```java\n\r\ndata = pq.read_table(file_name, \r\n          columns=columns, \r\n          use_pandas_metadata=False,\r\n          memory_map=True,\r\n          use_legacy_dataset=False)\r\n\u00a0\r\n```\r\n\u00a0\r\n\r\nFatal Python error: Bus error\r\n\r\nThread 0x00007fc9d75cd700 (most recent call first):\r\n\r\n\u00a0 File \"/home/baywatch/gitlab/xref_collider/pyenv/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1582 in read\r\n\r\n\u00a0 File \"/home/baywatch/gitlab/xref_collider/pyenv/lib/python3.7/site-packages/pyarrow/parquet.py\", line 1704 in read_table\r\n\r\n\u00a0 File \"cache_writer.py\", line 69 in read\r\n\r\n\u00a0 File \"cache_writer.py\", line 101 in write_test\r\n\r\n\u00a0 File \"cache_writer.py\", line 113 in <module>\r\n\r\nBus error (core dumped)\r\n\r\n\u00a0\r\n\r\nI am not sure if you want to delete this issue or not, it sure feels more of a netapp issue than an arrow issue. Maybe mmap on NFS is just a bad idea in general.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-05-07T13:26:40.169Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12547?focusedCommentId=17340828) by Jay Baywatch (Baywatch):*\nI am unable to reproduce with mmap disabled in read_table. I don't consider this to be a bug in pyarrow."
        },
        {
            "created_at": "2021-05-07T13:28:27.687Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12547?focusedCommentId=17340832) by Jay Baywatch (Baywatch):*\nThis isn't an arrow issue. I can reproduce with other types of memory-mapped files on NFS and am unable to reproduce with arrow when mmap is turned off.\u00a0"
        }
    ]
}