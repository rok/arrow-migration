{
    "issue": {
        "title": "[Python] Pretty printing very large ChunkedArray objects can use unbounded memory",
        "body": "***Note**: This issue was originally created as [ARROW-4099](https://issues.apache.org/jira/browse/ARROW-4099). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn working on ARROW-2970, I have the following dataset:\r\n\r\n```Java\n\r\nvalues = [b'x'] + [\r\n    b'x' * (1 << 20)\r\n] * 2 * (1 << 10)\r\n\r\narr = np.array(values)\r\n\r\narrow_arr = pa.array(arr)\r\n```\r\n\r\nThe object `arrow_arr` has 129 chunks, each element of which is 1MB of binary. The repr for this object is over 600MB:\r\n\r\n```Java\n\r\nIn [10]: rep = repr(arrow_arr)\r\n\r\nIn [11]: len(rep)\r\nOut[11]: 637536258\r\n```\r\n\r\nThere's probably a number of failsafes we can implement to avoid badness in these pathological cases (which may not happen often, but given the kinds of bug reports we are seeing, people do have datasets that look like this)",
        "created_at": "2018-12-21T05:21:23.000Z",
        "updated_at": "2022-11-17T22:23:23.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2019-06-24T02:25:42.388Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-4099?focusedCommentId=16870756) by Wes McKinney (wesm):*\nWhat we probably need to do is implement a global size bound on the output of `PrettyPrint` so that we bail out early when we hit a particular limit (e.g. around a megabyte or so). This is a pretty significant refactor of `src/arrow/pretty_print.cc` since there are many functions that write directly into `std::ostream` without any size book-keeping. This isn't causing enough of a user problem to require us to fix it right now"
        }
    ]
}