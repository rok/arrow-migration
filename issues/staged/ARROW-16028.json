{
    "issue": {
        "title": "Memory leak in `fragment.to_table`",
        "body": "***Note**: This issue was originally created as [ARROW-16028](https://issues.apache.org/jira/browse/ARROW-16028). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis \"pseudo\" code ends with OOM.\r\n\r\n\u00a0\r\n```java\n\r\nimport fsspec\r\nimport pyarrow\r\nimport pyarrow.parquet as pq\r\n\r\nfs = fsspec.filesystem(\r\n\u00a0 \u00a0 \"s3\",\r\n\u00a0 \u00a0 default_cache_type=\"none\",\r\n\u00a0 \u00a0 default_fill_cache=False,\r\n\u00a0 \u00a0 **our_storage_options,\r\n)\r\ndataset = pq.ParquetDataset(\r\n\u00a0 \u00a0 \"path in bucket\",\r\n\u00a0 \u00a0 filesystem=fs,\r\n\u00a0 \u00a0 filters=some_filters,\r\n\u00a0 \u00a0 use_legacy_dataset=False,\r\n)\r\n\r\n# this ends with OOM\r\ndataset.read(columns=columns_to_read)\r\n\r\n# and this too\r\ntables = []\r\nfor fragment in dataset.fragments:\r\n   tables.append(fragment.to_table(columns=columns_to_read))\r\nall_data = pyarrow.lib.concat_tables(tables) \n```\r\nWhat is really weird is if we put a debug point in the loop and **load** just {**}one fragment{**}. It loads, but something **keeps eating memory after load** until there is no left.\r\n\r\nWe are trying to read a parquet table that has several files under desired partitions. Each fragment has tens of columns and tens of millions of rows.\r\n\r\n\u00a0",
        "created_at": "2022-03-25T06:56:43.000Z",
        "updated_at": "2022-03-29T16:57:31.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-03-25T18:25:12.727Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17512531) by Will Jones (willjones127):*\nAre you sure the data you are trying to load isn't just too big for the memory on your machine?\u00a0\r\n> What is really weird is if we put a debug point in the loop and\u00a0**load**\u00a0just\u00a0{**}one fragment{**}.\r\nFYI in the API you are using, `dataset.fragments` [returns the materialized list of fragments](https://github.com/apache/arrow/blob/5a5f4ce326194750422ef6f053469ed1912ce69f/python/pyarrow/parquet.py#L1806-L1808), not an iterator, so you are actually loading all the fragments in that call, not just one. Instead, you should try using the newer datasets API and the associated `dataset.get_fragments()` method, which does return an iterator:\r\n```python\n\r\nimport pyarrow.dataset as ds\r\ndataset = ds.dataset(\"path in bucket\", filesystem=fs)\r\nfor fragment in dataset.get_fragments(filters=some_filters):\r\n# do something with fragment\n```\r\n> It loads, but something\u00a0**keeps eating memory after load**\u00a0until there is no left.\r\nHow are you measuring memory usage? Many tools, like Activity Monitor or Task Manager have a certain lag, so it's normal to see them register increases in memory **after** a memory hungry operation occurs."
        },
        {
            "created_at": "2022-03-28T07:38:49.554Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17513210) by ondrej metelka (ometelka):*\n<q>Are you sure the data you are trying to load isn't just too big for the memory on your machine?\u00a0</q>\r\nYes, we are able to load it with old version of dataset.\r\n\r\nThe `dataset.get_fragments()` helped with OOM, but it is not entirely compatible with the approach.\r\n\r\nThis code seems to be working to some extent\r\n```java\n\r\nimport pyarrow.dataset as ds\r\ntables = []\r\ndataset = ds.dataset(self.full_path(path), filesystem=self.fs)\r\nfor fragment in dataset.get_fragments(filter=filters):\r\n   sc = fragment.scanner(columns=columns)\r\n   tables.append(sc.to_table())\r\nall_data = pyarrow.lib.concat_tables(tables)\r\nresult_df = all_data.to_pandas()\r\n```\r\n\r\nBut we can't use filters on partitions this way (or at least I didn't figure it out). Our bucket has this structure \"BUCKET/some/path/date=2021-09-28/hour=22/file-2.parquet\". We are doing filtering with `filters=[(\"date\", \"==\", \"2021-09-28\")]`. But the `get_fragments` expects an `Expression`. Which is very hard to grasp.\r\n\r\nI can see that `dataset.files` contains all files within provided path. Is there a way how to simply apply filter on partitions same as is with `ParquetDataset`? `ParquetDatasetV2` doesn't have get_fragments method.\r\n"
        },
        {
            "created_at": "2022-03-28T15:15:12.268Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17513450) by Will Jones (willjones127):*\n> Yes, we are able to load it with old version of dataset.\r\nOkay, then it's likely you are running into a memory performance issue in Parquet. See `[~westonpace]` 's comment here: <https://github.com/apache/arrow/issues/12653#issuecomment-1071923418>\r\n\r\n> But the `get_fragments` expects an `Expression`. Which is very hard to grasp.\r\n\u00a0Have you looked at our [dataset docs](https://arrow.apache.org/docs/python/dataset.html#filtering-data)? The equivalent expression is:\r\n```python\n\r\nimport pyarrow.dataset as ds\r\nfrom datetime import date\r\n\r\nfilters=ds.field(\"date\") == \"2021-09-28\"\r\n# This also works\r\nfilters=ds.field(\"date\") == date(2021, 9, 28)\r\n```"
        },
        {
            "created_at": "2022-03-29T06:08:27.677Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17513826) by ondrej metelka (ometelka):*\nThanks for pointing out the memory issue. We need to stick with `use_legacy_dataset=True` until version 8.\r\n\r\nThe expression seems to be doing filtering based on rows, while the previous version filters partitions/parquet full paths.\r\nI think I've seen some comments in the code about expressions that if can't be applied on partitions, there is a fallback for row filtering.\r\nAnyway, I wasn't able to apply it to the dataset to get only files/fragments that match selected partitions. The new dataset works differently. In the old version, the filters are applied right away to get matching pieces. The new one seems to be applying those on rows when reading."
        },
        {
            "created_at": "2022-03-29T14:47:02.120Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17514135) by Will Jones (willjones127):*\n> Anyway, I wasn't able to apply it to the dataset to get only files/fragments that match selected partitions. The new dataset works differently. In the old version, the filters are applied right away to get matching pieces. The new one seems to be applying those on rows when reading.\r\nThe `filter` parameter in `to_table` will filter out rows, but the `filter` parameter in [`get_fragments`](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.FileSystemDataset.html#pyarrow.dataset.FileSystemDataset.get_fragments) should apply just to fragments."
        },
        {
            "created_at": "2022-03-29T16:57:31.039Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16028?focusedCommentId=17514193) by Weston Pace (westonpace):*\nIf you have filters they should be supplied as part of the `to_table` call in addition to any `get_fragments` call.  Filtering is applied at three levels today.\r\n\r\n- We will filter out fragments that don't match based on the partitioning columns but I don't think you are supplying any partitioning information.  For example, if you have hive style partitioning and your filenames look like `x=7/y=3/part-0.parquet` and your filter is `x>10` then we would skip that file.\n- We will filter out entire row groups if the parquet file has row group statistics and the statistics can eliminate the row group from consideration.  Again, if your filter was `x>10` and the row group statistics had the `-30 <= x <= 5` then we would skip the row group.\n- We will finally filter out rows that don't match using an in-memory evaluator.\n  \n  > What is really weird is if we put a debug point in the loop and load just one fragment. It loads, but something keeps eating memory after load until there is no left.\n  \n  This is surprising to me.  I would expect this if you were using `to_batches` as we do readahead in that case.  However, when using `to_table` we should be finished when we return and memory usage should not increase."
        }
    ]
}