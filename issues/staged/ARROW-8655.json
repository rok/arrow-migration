{
    "issue": {
        "title": "[C++][Dataset][Python][R] Preserve partitioning information for a discovered Dataset",
        "body": "***Note**: This issue was originally created as [ARROW-8655](https://issues.apache.org/jira/browse/ARROW-8655). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently, we have the `HivePartitioning` and `DirectoryPartitioning` classes that describe a partitioning used in the discovery phase. But once a dataset object is created, it doesn't know any more about this, it just has partition expressions for the fragments. And the partition keys are added to the schema, but you can't directly know which columns of the schema originated from the partitions.\r\n\r\nHowever, there can be use cases where it would be useful that a dataset still \"knows\" from what kind of partitioning it was created:\r\n\r\n- The \"read CSV write back Parquet\" use case, where the CSV was already partitioned and you want to automatically preserve that partitioning for parquet (kind of roundtripping the partitioning on read/write)\n- To convert the dataset to other representation, eg conversion to pandas, it can be useful to know what columns were partition columns (eg for pandas, those columns might be good candidates to be set as the index of the pandas/dask DataFrame). I can imagine conversions to other systems can use similar information.\n",
        "created_at": "2020-04-30T16:09:29.000Z",
        "updated_at": "2021-07-15T10:28:45.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-07-13T18:58:53.000Z"
    },
    "comments": [
        {
            "created_at": "2020-07-08T14:47:14.072Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17153663) by Rick Zamora (rjzamora):*\nAn additional use case here is in dask.dataframe/dask_cudf.\r\n\r\nFor dask.dataframe, we have used the list of partition names and the partition \"keys\" in the past, but can likely get by without the explicit information.\u00a0 For dask_cudf, it is a bit more critical to have access to the partition names (and keys), because we need to use libcudf for the actual I/O.\u00a0 That is, we use pyarrow for the \"planning/discovery\" stage of `read_parquet`, but want to use GPU-accelerated I/O (cannot convert fragments into arrow tables to translate the partitioning information).\r\n\r\nRef to what dask_cudf is currently doing with the partitioning information from ParquetDataset:\u00a0https://github.com/rapidsai/cudf/blob/754f292cce161ef46b077056342e3d263ca80be3/python/dask_cudf/dask_cudf/io/parquet.py#L75"
        },
        {
            "created_at": "2020-07-09T13:14:39.819Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17154550) by Joris Van den Bossche (jorisvandenbossche):*\n`[~rjzamora]` thanks for the additional context! (and indeed for dask_cudf it is more crucial)\r\n\r\nI don't think we will be able to fix this one \"properly\" before 1.0, but maybe at least we should provide a \"band-aid\" to get the information from the partition expression of a Fragment.\r\n\r\nThis typically looks like a combination of \"equality\" expressions:\r\n\r\n```Java\n\r\nIn [26]: list(dataset.get_fragments())[0].partition_expression                                                                                                                                                     \r\nOut[26]: <pyarrow.dataset.Expression (part == A:string)>\r\n```\r\n\r\nIf we provide a helper to get the `(\"part\", \"A\")` information from it, that might at least enable a temporary workaround in dask_cudf."
        },
        {
            "created_at": "2020-07-09T20:15:46.811Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17154901) by Ben Kietzman (bkietz):*\nIssue resolved by pull request 7691\n<https://github.com/apache/arrow/pull/7691>"
        },
        {
            "created_at": "2020-12-11T11:14:23.460Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17247851) by Joris Van den Bossche (jorisvandenbossche):*\nWhile we added the \"band aid\" solution (private-ish helper function to get key/values from partition expression), I think we still should consider adding a more official and user friendly API for this.\r\n\r\n(eg also vaex started using the helper function -> https://github.com/vaexio/vaex/pull/1094)"
        },
        {
            "created_at": "2021-07-06T08:30:06.735Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17375352) by Joris Van den Bossche (jorisvandenbossche):*\n(this has come up again in https://github.com/dask/dask/pull/7557 (cc `[~fjetter]`), and I also wondered about while deprecating similar functionality in ParquetDataset in ARROW-13074 / https://github.com/apache/arrow/pull/10549, so trying to revive this issue)\r\n\r\nTrying to think through what information could be useful to expose, I think there are two levels to potentially expose information: the dataset and the fragment.\r\n\r\nFor the full Dataset:\r\n\r\n- The names of the partition fields (in correct order), and maybe full schema (i.e. including the type)\n- All possible values a specific partition field can take?\n  \n  For the individual fragments:\n  \n- The actual field values for each of the partition fields/keys, such as the mapping that is currently returned by `ds._get_partition_keys(fragment.partition_expression` (assuming this would preserve order, is there anything more needed here? Or \"just\" a more official (public) method to get this information?)\n  \n  For the dataset-level, we could maybe simply expose the \"finished\" Partitioning object that is created while creating the FileSystemDataset through the factory method. Currently, this is Partitioning object is discarded, but we could pass it through to the FileSystemDataset to preserve the partitioning object from which it was created. "
        },
        {
            "created_at": "2021-07-06T10:56:56.097Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17375477) by Joris Van den Bossche (jorisvandenbossche):*\nLooking at what information is available in the legacy ParquetDataset:\r\n\r\n```python\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as ds\r\n\r\ndf = pd.DataFrame({\"year\": [2020, 2020, 2021, 2021], \"month\":[1, 2, 1, 2], \"values\": [1, 2, 3, 4]})\r\ndf.to_parquet(\"test_partitioned\", partition_cols=[\"year\", \"month\"], engine=\"pyarrow\")\r\n```\r\n\r\n```python\n\r\nIn [2]: d1 = pq.ParquetDataset(\"test_partitioned/\")\r\n\r\nIn [4]: piece = d1.pieces[0]\r\n\r\n# a single piece has information in \"partition_keys\"\r\nIn [5]: piece\r\nOut[5]: ParquetDatasetPiece('test_partitioned//year=2020/month=1/1b27b290ffdf4cccbf21d2d2feef79f4.parquet', row_group=None, partition_keys=[('year', 0), ('month', 0)])\r\n\r\n# note: the values are the *indices* into the unique values per partition field, not the actual value\r\nIn [6]: piece.partition_keys\r\nOut[6]: [('year', 0), ('month', 0)]\r\n\r\n# In addition, the dataset object has a \"partitions\" attribute\r\nIn [7]: d1.partitions\r\nOut[7]: <pyarrow.parquet.ParquetPartitions at 0x7fe4caea22e0>\r\n\r\n# the partition field names (note: also not ordered here)\r\nIn [8]: d1.partitions.partition_names\r\nOut[8]: {'month', 'year'}\r\n\r\n# more information about each partition field\r\nIn [9]: d1.partitions.levels\r\nOut[9]: \r\n[<pyarrow.parquet.PartitionSet at 0x7fe4c9ea8760>,\r\n <pyarrow.parquet.PartitionSet at 0x7fe4cae94e20>]\r\n\r\nIn [10]: d1.partitions.levels[0].dictionary\r\nOut[10]: \r\n<pyarrow.lib.Int64Array object at 0x7fe481dfafa0>\r\n[\r\n  2020,\r\n  2021\r\n]\r\n\r\nIn [11]: d1.partitions.levels[0].keys\r\nOut[11]: ['2020', '2021']\r\n\r\nIn [12]: d1.partitions.levels[0].key_indices\r\nOut[12]: {'2020': 0, '2021': 1}\r\n\r\nIn [13]: d1.partitions.levels[0].name\r\nOut[13]: 'year'\r\n```\r\n\r\nI am working on a draft PR to preserve the `Partitioning` object from which the `FileSystemDataset` was created in the new API. This could look like:\r\n\r\n```python\n\r\nIn [16]: d2 = ds.dataset(\"test_partitioned/\", partitioning=\"hive\")\r\n\r\nIn [17]: d2\r\nOut[17]: <pyarrow._dataset.FileSystemDataset at 0x7fe481b6e270>\r\n\r\nIn [18]: d2.partitioning\r\nOut[18]: <pyarrow._dataset.HivePartitioning at 0x7fe48162c7f0>\r\n\r\nIn [19]: d2.partitioning.schema\r\nOut[19]: \r\nyear: int32\r\nmonth: int32\r\n\r\nIn [20]: d2.partitioning.schema.names\r\nOut[20]: ['year', 'month']\r\n\r\nIn [21]: d2.partitioning.dictionaries\r\nOut[21]: \r\n[<pyarrow.lib.Int32Array object at 0x7fe480fd9fa0>\r\n [\r\n   2020,\r\n   2021\r\n ],\r\n <pyarrow.lib.Int32Array object at 0x7fe480fd9b80>\r\n [\r\n   1,\r\n   2\r\n ]]\r\n```\r\n\r\nI think this should provide all necessary information?\r\n\r\nIn addition, in the new API, we have the `Fragments`, which currently already allow the following:\r\n\r\n```python\n\r\nIn [22]: fragments = list(d2.get_fragments())\r\n\r\nIn [23]: fragments[0].partition_expression\r\nOut[23]: <pyarrow.dataset.Expression ((year == 2020) and (month == 1))>\r\n\r\nIn [24]: ds._get_partition_keys(fragments[0].partition_expression)\r\nOut[24]: {'month': 1, 'year': 2020}\r\n```\r\n\r\nWe could add a public property on the `Fragment` to expose this, for example a `FileFragment.partition_values` (or other name if there are suggestions).\r\n\r\nFeedback on whether this would be useful / sufficient for dask (or other use cases) would be very welcome!"
        },
        {
            "created_at": "2021-07-06T17:15:10.488Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17375871) by Weston Pace (westonpace):*\nI think the only thing tricky here is the unique values.\u00a0 In the general case a dataset may not know all possible values.\u00a0 The \"partition_expression\" of a fragment is not required to be an equality expression (or even several ANDed together).\u00a0 Technically there is nothing against creating a union dataset, perhaps composed of a CSV dataset (where all data has timestamp < 2020) and a parquet dataset (where all data has timestamp > 2020) because the company changed how they stored data at some point.\r\n\r\nScanning for all values currently used is something that happens in the factory->dataset part (which I suppose is kind of hidden in the current python implementation).\u00a0 Maybe there is something we can add to the dataset factory so that calling Finish (or perhaps adding a new property that can be accessed after calling Finish) could return dictionaries of everything it discovered."
        },
        {
            "created_at": "2021-07-06T17:44:57.935Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17375905) by Joris Van den Bossche (jorisvandenbossche):*\nI should maybe have been more explicit, but I think this is fine if the above (eg the `FileSystemDataset.partitioning` attribute I proposed in the PR) only works for datasets that were created through the factory function, since that covers many typical use cases (and specifically the use case of dask) and is indeed also the only use case where this information can reliably be known. I think it is fine that this will not work (i.e. return None) for eg union datasets. \r\n\r\nSimilarly for the \"partition_expression\": if it's created through discovery with a Directory/HivePartitioning, we know that the partition expression will always only include equalities. \r\nIndeed in general this will not be true, but again I think that is fine (although that's maybe a reason to not make this an attribute on the FileFragment, but keep it as a function extracting the information from the partition expression).\r\n\r\n> Maybe there is something we can add to the dataset factory so that calling Finish (or perhaps adding a new property that can be accessed after calling Finish) could return dictionaries of everything it discovered.\r\n\r\nCurrently those dictionaries are accessible from the `Partitioning` object inside the `Finish()` call, but indeed after calling `Finish()` you can't access this because the `Partitioning` object is not stored in either the returned dataset or on the original factory object. \r\nMaking it available on the FileSystemDatasetFactory instead of attaching it to the returned FileSystemDataset (as I am doing in the PR -> https://github.com/apache/arrow/pull/10661) is an option as well, and then the Python layer could handle it (and attaching it to the cython Dataset class). `[~westonpace]` that's maybe something to comment on the PR if you prefer that way."
        },
        {
            "created_at": "2021-07-13T18:58:53.149Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8655?focusedCommentId=17380115) by Ben Kietzman (bkietz):*\nIssue resolved by pull request 10661\n<https://github.com/apache/arrow/pull/10661>"
        }
    ]
}