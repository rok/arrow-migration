{
    "issue": {
        "title": "[C++] Do not error in Table::CombineChunks for BinaryArray types that overflow 2GB limit",
        "body": "***Note**: This issue was originally created as [ARROW-5744](https://issues.apache.org/jira/browse/ARROW-5744). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nDiscovered during ARROW-5635 code review",
        "created_at": "2019-06-26T15:51:44.000Z",
        "updated_at": "2020-06-19T03:24:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-06-19T03:24:38.000Z"
    },
    "comments": [
        {
            "created_at": "2019-06-26T19:47:26.088Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=16873625) by Ben Kietzman (bkietz):*\n`[~wesmckinn]` `[~brillsp]`\r\n\r\nCould you clarify what should happen? Concatenate already raises an error if concatenation of BinaryArrays (or ListArrays) would result in an offset over INT_MAX: \r\n\r\nhttps://github.com/apache/arrow/blob/master/cpp/src/arrow/array/concatenate-test.cc#L215-L224\r\n\r\n\r\nWRT https://github.com/apache/arrow/pull/4598#issuecomment-504120539\r\n\r\nThe 2GB limit is a limitation of resulting from using int32_t offsets to delimit the strings in a character buffer, so even if concatenate were to produce a character buffer larger than 2GB offsets for viewing that much string data could not be represented in int32_t"
        },
        {
            "created_at": "2019-06-26T19:54:27.242Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=16873630) by Wes McKinney (wesm):*\nAh, it wasn't obvious from reading the source code, I found it now. \r\n\r\nI updated the JIRA title. The behavior should be to eagerly concatenate chunks to produce as large chunks as possible. So if you have 10GB of total data you might end up with 5 or so concatenated chunks. \r\n\r\nI'm moving this out of 0.14.0 since it's not urgent"
        },
        {
            "created_at": "2019-07-01T20:17:40.937Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=16876466) by Ben Kietzman (bkietz):*\nWhen resolving this issue, the presence of the offset overflow error should probably be added to Concatenate's doccomment"
        },
        {
            "created_at": "2020-06-08T16:35:47.293Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=17128433) by Wes McKinney (wesm):*\n`[~bkietz]` this is still an issue right?"
        },
        {
            "created_at": "2020-06-16T15:39:25.563Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=17136749) by Ben Kietzman (bkietz):*\n`[~wesm]` I'm not sure what the resolution should be;\r\n\r\n> The behavior should be to eagerly concatenate chunks to produce as large chunks as possible.\r\n\r\nThis isn't possible since the output is a single Array rather than a ChunkedArray or Datum. If we moved all of this to a VectorFunction then we could fall back on chunked output when buffers would overflow. In the context of Concatenate as ArrayVector -> Array it seems the resolution is adding a comment indicating that the function may error if buffers would overflow."
        },
        {
            "created_at": "2020-06-16T15:41:32.742Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=17136752) by Ben Kietzman (bkietz):*\nSorry, it'd been a while since I looked at this issue. I'll proceed by detecting potential overflow in Table::CombineChunks. This is a band-aid fix, though. For example if the table has a struct column with a string field then that could overflow in the same way"
        },
        {
            "created_at": "2020-06-19T03:24:38.843Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5744?focusedCommentId=17140161) by Wes McKinney (wesm):*\nIssue resolved by pull request 7454\n<https://github.com/apache/arrow/pull/7454>"
        }
    ]
}