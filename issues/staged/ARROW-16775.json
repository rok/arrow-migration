{
    "issue": {
        "title": "[Python] pyarrow's read_table is way slower than iter_batches",
        "body": "***Note**: This issue was originally created as [ARROW-16775](https://issues.apache.org/jira/browse/ARROW-16775). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi!\r\n\r\nLoading a table created from DataFrame \u00a0`pyarrow.parquet.read_table()` is taking 3x \u00a0much time as loading it as batches\r\n\r\n\u00a0\r\n```java\n\r\npyarrow.Table.from_batches(\r\n    list(pyarrow.parquet.ParquetFile.iter_batches()\r\n)\n```\r\n\u00a0\r\n#### Minimal example\r\n\r\n\u00a0\r\n```java\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ndf = pd.DataFrame(\r\n\u00a0 \u00a0 {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"a\": np.random.random(10**9),\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 \"b\": np.random.random(10**9)\r\n\u00a0 \u00a0 }\r\n)\r\n\r\ndf.to_parquet(\"file.parquet\")\r\n\r\ntable_of_whole_file = pq.read_table(\"file.parquet\")\r\n\r\ntable_of_batches = pa.Table.from_batches(\r\n\u00a0 \u00a0 list(\r\n\u00a0 \u00a0 \u00a0 \u00a0 pq.ParquetFile(\"file.parquet\").iter_batches()\r\n\u00a0 \u00a0 )\r\n)\r\n\r\ntable_of_one_batch = pa.Table.from_batches(\r\n\u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 next(pq.ParquetFile(\"file.parquet\")\r\n\u00a0 \u00a0 \u00a0 \u00a0 .iter_batches(batch_size=10**9))\r\n\u00a0 \u00a0 ]\r\n)\n```\r\n\u00a0\r\n\r\n_table_of_batches_\u00a0reading time is 11.5 seconds,\u00a0_table_of_whole_file_ read time is 33.2s.\r\n\r\nAlso loading table as one batch _table_of_one_batch_\u00a0is slightly faster: 9.8s.\r\n#### Parquet file metadata\r\n\r\n\u00a0\r\n```java\n\r\n<pyarrow._parquet.FileMetaData object at 0x129ab83b0>\r\n  created_by: parquet-cpp-arrow version 8.0.0\r\n  num_columns: 2\r\n  num_rows: 1000000000\r\n  num_row_groups: 15\r\n  format_version: 1.0\r\n  serialized_size: 5680 \n```\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2022-06-07T13:34:58.000Z",
        "updated_at": "2022-07-11T13:49:10.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-06-08T13:12:28.127Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17551620) by Alessandro Molina (amol-):*\nThis probably deserves to be critical, the \"suggested method\" in the documentation is actually the worst performing one. It's something impacting the majority of our users."
        },
        {
            "created_at": "2022-06-08T14:38:00.153Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17551652) by Weston Pace (westonpace):*\nWhat results do you get at a slightly smaller scale (e.g. `10 * * 8`)?  I get an out-of-memory error at 10\\*\\*9 (this is ~16GB of data which is the limit on my system).  At `10 * * 8` I get the following timings:\r\n\r\n```\n\r\ntable_of_whole_file: 0.6154186725616455\r\ntable_of_batches: 0.8553369045257568\r\ntable_of_one_batch: 0.6191871166229248\r\n```\r\n\r\nI wonder if the problem is that we are hitting swap and `table_of_whole_file` performs poorly when using swap.  I'm not sure how much we want to optimize in that case vs. suggesting the data be consumed iteratively."
        },
        {
            "created_at": "2022-06-15T20:06:36.129Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17554778) by Satoshi Nakamoto (lain):*\n`[~westonpace]` \u00a0for `2 \\* 10\\***8` I get following results. Difference on this size isn't so critical. But my dataframes are more like 10**\\*9 sized version so it bothers me.\r\n\r\n![image-2022-06-16-03-04-25-158.png](https://issues.apache.org/jira/secure/attachment/13045144/image-2022-06-16-03-04-25-158.png)"
        },
        {
            "created_at": "2022-06-29T17:45:07.805Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17560600) by Weston Pace (westonpace):*\nI'm still not having any luck reproducing this.  read_table is consistently faster than iter_batches for me.  Here is the script I'm running:\r\n\r\n```\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport time\r\nimport math\r\nimport psutil\r\n\r\nfor multiplier in [0.5, 1, 1.5, 2, 2.5, 3]:\r\n  batch_size = multiplier * (10**8)\r\n  df = pd.DataFrame(\r\n      {\r\n          \"a\": np.random.random(math.ceil(batch_size)),\r\n          \"b\": np.random.random(math.ceil(batch_size))\r\n      }\r\n   )\r\n\r\n  df.to_parquet(\"file.parquet\")\r\n\r\n  print(f'Multiplier: {multiplier}')\r\n  start = time.time()\r\n  table_of_whole_file = pq.read_table(\"file.parquet\")\r\n  table_of_whole_file_time = time.time() - start\r\n  print(f'  table_of_whole_file: {table_of_whole_file_time}')\r\n\r\n  start = time.time()\r\n  table_of_batches = pa.Table.from_batches(\r\n      list(\r\n          pq.ParquetFile(\"file.parquet\").iter_batches(batch_size=10**9)\r\n      )\r\n   )\r\n  table_of_batches_time = time.time() - start\r\n  print(f'  table_of_batches: {table_of_batches_time}')\r\n\r\n  print(f'  difference: {table_of_batches_time - table_of_whole_file_time}')\r\n  virt_mem = psutil.virtual_memory()\r\n  print(f'  {virt_mem}')\r\n```\r\n\r\n...and sample results...\r\n\r\n```\n\r\nMultiplier: 0.5\r\n  table_of_whole_file: 0.36187195777893066\r\n  table_of_batches: 0.48351168632507324\r\n  difference: 0.12163972854614258\r\n  svmem(total=33526784000, available=22198300672, percent=33.8, used=10641412096, free=20502384640, active=1426874368, inactive=10684268544, buffers=184336384, cached=2198650880, shared=202821632, slab=481918976)\r\nMultiplier: 1\r\n  table_of_whole_file: 0.6138112545013428\r\n  table_of_batches: 0.8580827713012695\r\n  difference: 0.24427151679992676\r\n  svmem(total=33526784000, available=18544037888, percent=44.7, used=14295646208, free=16026877952, active=2184822784, inactive=14370017280, buffers=184406016, cached=3019853824, shared=202821632, slab=502353920)\r\nMultiplier: 1.5\r\n  table_of_whole_file: 0.84134840965271\r\n  table_of_batches: 1.1889660358428955\r\n  difference: 0.34761762619018555\r\n  svmem(total=33526784000, available=14790352896, percent=55.9, used=19936378880, free=9563262976, active=2951610368, inactive=20027539456, buffers=184483840, cached=3842658304, shared=202821632, slab=523534336)\r\nMultiplier: 2\r\n  table_of_whole_file: 1.1325435638427734\r\n  table_of_batches: 1.5530588626861572\r\n  difference: 0.4205152988433838\r\n  svmem(total=33526784000, available=11027169280, percent=67.1, used=21811699712, free=6840262656, active=3731607552, inactive=21917872128, buffers=185122816, cached=4689698816, shared=203624448, slab=549113856)\r\nMultiplier: 2.5\r\n  table_of_whole_file: 1.3614778518676758\r\n  table_of_batches: 1.9443469047546387\r\n  difference: 0.5828690528869629\r\n  svmem(total=33526784000, available=7724249088, percent=77.0, used=25151610880, free=2676121600, active=4467277824, inactive=25309102080, buffers=185188352, cached=5513863168, shared=203624448, slab=574017536)\r\nMultiplier: 3\r\n  table_of_whole_file: 1.8207118511199951\r\n  table_of_batches: 27.461324453353882\r\n  difference: 25.640612602233887\r\n  svmem(total=33526784000, available=5755801600, percent=82.8, used=27084144640, free=615559168, active=2758205440, inactive=29104529408, buffers=62664704, cached=5764415488, shared=203624448, slab=517799936)\r\n```\r\n\r\nI will try and find someone with an M1 to test this for me.  I'll also try with timeit to see if running in a loop is important."
        },
        {
            "created_at": "2022-07-08T12:01:56.075Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17564252) by Satoshi Nakamoto (lain):*\n`[~westonpace]` \u00a0here are results on my linux server machine with python3.9\r\n```java\n\r\nMultiplier: 0.5\r\n\u00a0 table_of_whole_file: 0.48801302909851074\r\n\u00a0 table_of_batches: 0.2905139923095703\r\n\u00a0 difference: -0.19749903678894043\r\n\u00a0 svmem(total=34359738368, available=15275065344, percent=55.5, used=16666951680, free=1070137344, active=14219853824, inactive=14177533952, wired=2447097856)\r\nMultiplier: 1\r\n\u00a0 table_of_whole_file: 0.7520689964294434\r\n\u00a0 table_of_batches: 0.6367530822753906\r\n\u00a0 difference: -0.11531591415405273\r\n\u00a0 svmem(total=34359738368, available=15246065664, percent=55.6, used=15617949696, free=2043740160, active=13216202752, inactive=13194248192, wired=2401746944)\r\nMultiplier: 1.5\r\n\u00a0 table_of_whole_file: 0.7303309440612793\r\n\u00a0 table_of_batches: 0.7717642784118652\r\n\u00a0 difference: 0.04143333435058594\r\n\u00a0 svmem(total=34359738368, available=15102951424, percent=56.0, used=14017642496, free=3168894976, active=11958730752, inactive=11807539200, wired=2058911744)\r\nMultiplier: 2\r\n\u00a0 table_of_whole_file: 1.0269582271575928\r\n\u00a0 table_of_batches: 1.0449450016021729\r\n\u00a0 difference: 0.017986774444580078\r\n\u00a0 svmem(total=34359738368, available=14586134528, percent=57.5, used=11820531712, free=4926554112, active=9668657152, inactive=9565945856, wired=2151874560)\r\nMultiplier: 2.5\r\n\u00a0 table_of_whole_file: 2.2550718784332275\r\n\u00a0 table_of_batches: 1.280627965927124\r\n\u00a0 difference: -0.9744439125061035\r\n\u00a0 svmem(total=34359738368, available=13351124992, percent=61.1, used=9824157696, free=5161664512, active=7733215232, inactive=8085258240, wired=2090942464)\r\nMultiplier: 3\r\n\u00a0 table_of_whole_file: 3.1672561168670654\r\n\u00a0 table_of_batches: 1.7320072650909424\r\n\u00a0 difference: -1.435248851776123\r\n\u00a0 svmem(total=34359738368, available=13742899200, percent=60.0, used=9232990208, free=5884755968, active=7187152896, inactive=7759429632, wired=2045837312) \n```\r\nAs you can see, `batches` consistently are faster than `{}whole_file{`}, especially with higher multipliers. Multipler of 3 has x1.5 difference. As I said earlier, on multiplier of 10+ the difference is x3+.\r\n\r\nWhich env were you testing on?"
        },
        {
            "created_at": "2022-07-11T13:49:10.940Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16775?focusedCommentId=17565014) by Weston Pace (westonpace):*\nI was testing on my Linux desktop.  An i7-10700K (8 physical cores + hyperthreading for 2 threads/core) with 32GB of RAM and about 20GBps RAM bandwidth.\r\n\r\nI did get a chance to test on an M1 briefly last week and I was able to reproduce your numbers.  I'll try and take a further look at this performance issue this week."
        }
    ]
}