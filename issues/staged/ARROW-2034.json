{
    "issue": {
        "title": "[C++] Filesystem implementation for Azure Blob Storage",
        "body": "***Note**: This issue was originally created as [ARROW-2034](https://issues.apache.org/jira/browse/ARROW-2034). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n",
        "created_at": "2018-01-25T14:55:42.000Z",
        "updated_at": "2022-10-13T16:10:20.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-05-06T00:39:04.628Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17100361) by Wes McKinney (wesm):*\nI see that TileDB (MIT license) has built a C++ wrapper for Azure\r\n\r\nhttps://github.com/TileDB-Inc/TileDB/blob/dev/tiledb/sm/filesystem/azure.cc\r\n\r\nNo this is not moved to fsspec, this is a C++ ticket"
        },
        {
            "created_at": "2021-01-28T16:57:25.345Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273854) by Antoine Pitrou (apitrou):*\nI'm not sure I understand the relationship between Blob Store and Data Lake. Is Data Lake a higher-level layer above Blob Store? Or are they two different services that would need separate filesystem implementations?"
        },
        {
            "created_at": "2021-01-28T16:59:04.631Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273855) by Antoine Pitrou (apitrou):*\nAccording to <https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction> ,\r\n> Data Lake Storage Gen2 converges the capabilities of [Azure Data Lake Storage Gen1](https://docs.microsoft.com/en-us/azure/data-lake-store/) with Azure Blob storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you'll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\r\nI'm not sure this means the same C++ API can be used to access both, though."
        },
        {
            "created_at": "2021-01-28T17:03:36.068Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273858) by Uwe Korn (uwe):*\nIt is as confusing in reality, here is what they all are (I'm though already 1 year outdated on this):\r\n\r\n- Blob Store: Like S3, simple but limited API\n- Data Lake Gen 1: HDFS-like deployment with different but more user-friendly API / attributes\n- Data Lake Gen 2: Some improvements were made to the Blob Store so that there is no need for a special (more expensive) Data Lake service anymore, everything is now on the Blob Store. A new set of APIs was though released that exposes some nice features that the initial Blob Store API didn't have, probably for marketing purposes this was named Data Lake Gen 2 although technically Blob Store Gen 2 would have been more appropriate."
        },
        {
            "created_at": "2021-01-28T17:10:00.236Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273861) by Antoine Pitrou (apitrou):*\nHa, and here are some interesting resources:\r\n \\* <https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues>\u00a0: \"Blob APIs and Data Lake Storage Gen2 APIs can operate on the same data.\" _but_  \"You cannot use blob API and Data Lake Storage APIs to write to the same instance of a file\" (sounds ok?)\r\n \\* <https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs> : three different kinds of blobs. \"Block blobs\" sound most useful, though \"append blobs\" may work too.\r\n \\* <https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace> : a Data Lake admin can configure a it with a hierarchical filesystem semantics.\u00a0 Apparently it may enable different APIs (??).\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-01-28T17:19:21.000Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273878) by Yesh (madras):*\nThere is also https://github.com/Azure/azure-sdk-for-cpp which I\u2019ve tested against adl gen2 ."
        },
        {
            "created_at": "2021-01-28T17:48:21.416Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17273897) by Antoine Pitrou (apitrou):*\nVarious pointers about the Azure C++ SDK:\r\n\r\n[https://devblogs.microsoft.com/azure-sdk/cppintro/#example-code-using-the-c-storage-blob-client-libraryhttps://devblogs.microsoft.com/azure-sdk/cppintro/](https://devblogs.microsoft.com/azure-sdk/cppintro/)\r\n\r\n<https://github.com/Azure/azure-sdk-for-cpp>\r\n\r\n<https://github.com/Azure/azure-sdk-for-cpp/blob/master/sdk/storage/azure-storage-blobs/sample/blob_getting_started.cpp>\r\n\r\n<https://azure.github.io/azure-sdk-for-cpp/> (note the API docs are segregated per component, with separate land pages for Core, Storage, Blog Storage, etc.)\r\n\r\nNote that the SDK **requires C++14**.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-12-02T18:28:57.114Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17452547) by Tom Augspurger (TomAugspurger):*\nDoes Arrow support C\\+\\14 features now (or more specifically, is the SDK being C\\\\14 a problem?) From https://issues.apache.org/jira/browse/ARROW-13744 it seems like C\\\\14 is at least tested, but https://github.com/apache/arrow/blame/master/docs/source/developers/cpp/building.rst#L40 says a \"A C\\\\+11-enabled compiler. \" is required."
        },
        {
            "created_at": "2021-12-02T18:44:38.937Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17452556) by Neal Richardson (npr):*\nI'm not an expert here, but I think we could use C+14 if required and if the compiler supports it. If the compiler doesn't support C 14, we wouldn't be able to build the azure sdk. So the line would be that Arrow requires C 11 at a minimum, and some features are only available with C+14."
        },
        {
            "created_at": "2021-12-02T18:48:29.030Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17452557) by Antoine Pitrou (apitrou):*\nWe require only C+11 in the codebase. We might add C+14-requiring optional components if desired, but that will add complication to the build setup."
        },
        {
            "created_at": "2021-12-21T15:29:30.434Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17463316) by Shashanka Balakuntala Srinivasa (balakuntala):*\nhi `[~apitrou]` , we were looking into implementing this feature from our side. I did try to compile the whole arrow code base with c+14 and ran unit tests as well. Everything is passing in local and as mentioned before : [ARROW-13744] [CI] c+14 and 17 nightly job fails - ASF JIRA (apache.org) ticket mentions we have daily build run for validation which are passing.\u00a0\r\n\r\nSince the azure sdks depend on c+14 features, and since we have the code compiled in c 14, can we look into upgrading the c+ version to 14?\u00a0\r\nLet me know if there are any issues. I will be happy to take those and work on them."
        },
        {
            "created_at": "2021-12-21T15:31:27.855Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17463318) by Antoine Pitrou (apitrou):*\n`[~balakuntala]` We can probably require C+\\14 for the Azure filesystem implementation only, but the rest of Arrow should remain C\\+11-compatible."
        },
        {
            "created_at": "2022-09-06T16:12:27.434Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17600865) by Dean MacGregor (deanm0000):*\nIf someone wants to work on this but doesn't have an Azure account let me know.\u00a0 I can make a storage account for this development/testing"
        },
        {
            "created_at": "2022-10-05T14:57:27.724Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17613032) by Bipin Mathew (mathewb001):*\nI have a rudimentary implementation of this that supports import and export to Azure. I tried to align as closely as possible to the s3 implementation, however as I needed it only for a specific use case and have not had a chance to implement all the same methods. Depending on bandwidth, I could probably build out the implementation further. I have never contributed to this project. Do all endpoints need to be implemented before it can be merged into the code base? Can it be built out over time? I attach what I have so far here.\r\n\r\n\r\n[azfs.h](azfs.h)[azfs.cc](azfs.cc)"
        },
        {
            "created_at": "2022-10-05T16:27:51.286Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17613061) by Antoine Pitrou (apitrou):*\n`[~mathewb001]` Well, there's a Github PR open already, did you take a look?"
        },
        {
            "created_at": "2022-10-05T16:46:00.026Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2034?focusedCommentId=17613070) by Bipin Mathew (mathewb001):*\nOh this is much further along than what I have offered. Looking forward to\nits release so I can cut over to it.\n\n\n"
        }
    ]
}