{
    "issue": {
        "title": "[C++/Python] Fix decimal precision inference",
        "body": "***Note**: This issue was originally created as [ARROW-2160](https://issues.apache.org/jira/browse/ARROW-2160). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n```Java\n\r\nimport pyarrow as pa\r\nimport pandas as pd\r\nimport decimal\r\n\r\ndf = pd.DataFrame({'a': [decimal.Decimal('0.1'), decimal.Decimal('0.01')]})\r\npa.Table.from_pandas(df)\r\n```\r\n\r\nraises:\r\n```Java\n\r\npyarrow.lib.ArrowInvalid: Decimal type with precision 2 does not fit into precision inferred from first array element: 1\r\n```\r\n\r\nLooks arrow is inferring the highest precision for given column based on the first cell and expecting the rest fits in. I understand this is by design but from the point of view of pandas-arrow compatibility this is quite painful as pandas is more flexible (as demonstrated).\r\n\r\nWhat this means is that user trying to pass pandas `DataFrame` with `Decimal` column(s) to arrow `Table` would always have to first:\r\n1. Find the highest precision used in (each of) that column(s)\n1. Adjust the first cell of (each of) that column(s) so that it explicitly uses the highest precision of that column(s)\n1. Only then pass such `DataFrame` to `Table.from_pandas()`\n   \n   So given this unavoidable procedure (and assuming arrow needs to be strict about the highest precision for a column) - shouldn't some similar logic be part of the `Table.from_pandas()` directly to make this transparent?",
        "created_at": "2018-02-14T23:09:30.000Z",
        "updated_at": "2018-03-01T22:28:14.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-03-01T22:28:14.000Z"
    },
    "comments": [
        {
            "created_at": "2018-02-14T23:20:06.028Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2160?focusedCommentId=16364934) by Phillip Cloud (cpcloud):*\nThanks for the report. I will make sure this goes into 0.9.0."
        },
        {
            "created_at": "2018-02-15T00:20:59.158Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2160?focusedCommentId=16364996) by Phillip Cloud (cpcloud):*\nThis is actually a bug, since we _do_ perform the inference if the user does not pass a type in. The case of all leading zeros isn't handled."
        },
        {
            "created_at": "2018-02-15T02:12:45.670Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2160?focusedCommentId=16365056) by Julian Hyde (julianhyde):*\nAs the resident SQL curmudgeon let me point out that the SQL world has a different definition of precision, apparently, than the Python world. In SQL, precision is the total number of digits (left and right of the point). The number 1234.56 has precision 6 and scale 2.\r\n\r\nI'm not claiming that SQL's terminology is \"right\", but since the terminologies are at odds, let's be clear when we talk about precision."
        },
        {
            "created_at": "2018-02-15T02:48:01.667Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2160?focusedCommentId=16365074) by Phillip Cloud (cpcloud):*\nThe definitions are the same, modulo possibly the case of all leading zeros, which is what's in question here. Does SQL have a consistent definition of precision with respect to leading zeros?"
        },
        {
            "created_at": "2018-03-01T22:28:14.322Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2160?focusedCommentId=16382773) by Wes McKinney (wesm):*\nResolved as part of ARROW-2145 https://github.com/apache/arrow/commit/bfac60dd73bffa5f7bcefc890486268036182278"
        }
    ]
}