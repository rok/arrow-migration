{
    "issue": {
        "title": "[Java] jdbcToArrowVectors / sqlToArrowVectorIterator fails to handle variable decimal precision / scale",
        "body": "***Note**: This issue was originally created as [ARROW-16427](https://issues.apache.org/jira/browse/ARROW-16427). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen a JDBC driver returns a Numeric type that doesn't exactly align with what is in the JDBC metadata, jdbcToArrowVectors / sqlToArrowVectorIterator fails to process the result (failing on serializing the the value into the BigDecimalVector).\u00a0\r\n\r\nIt appears as though this is because JDBC drivers can return BigDecimal / Numeric values that are different between the metadata and not consistent between each of the rows.\u00a0\r\n\r\nIs there a recommended course of action to represent a variable precision / scale decimal vector? In any case it does not seem possible to convert JDBC data with the built in utilities that uses these numeric types when they come in this form.\u00a0\r\n\r\nIt seems like both the Oracle and the Postgres JDBC driver also returns metadata with a 0,0 precision / scale when values in the result set have different (and varied) precision / scale.\u00a0\r\n\r\nAn example:\u00a0\r\n\r\nAgainst postgres, running a simple SQL query that produces numeric types can lead to a JDBC result set with BigDecimal values with variable decimal precision/scale.\u00a0\r\n```java\n\r\nSELECT value FROM (\r\n  SELECT 1000000000000000.01 AS \"value\" \r\n  UNION SELECT 1000000000300.0000001\r\n) a \n```\r\n\u00a0\r\n\r\nThe postgres JDBC adapter produces a result set that looks like the following:\u00a0\r\n\r\n\u00a0\r\n\n|\u00a0|value|precision|scale|\r|\n|-|-|-|-|-|\n|metadata|N/A|0|0|\r|\n|row 1|1000000000000000.01|18|2|\r|\n|row 2|1000000000300.0000001|20|7|\r<br>\r<br>\u00a0\r<br>\r<br>Even a result set that returns a single value may Numeric values with precision / scale that do not match the precision / scale in the ResultSetMetadata.\u00a0\r<br>\r<br>\u00a0\r<br>```java<br>\r<br>SELECT AVG(one) from (\r<br>  SELECT 1000000000000000.01 as \"one\" \r<br>  UNION select 1000000000300.0000001\r<br>) a <br>```\r<br>produces a result set that looks like this\r<br>\r<br>\u00a0\r|\n|\u00a0|value|precision|scale|\r|\n|metadata|N/A|0|0|\r|\n|row 1|500500000000150.0050001|22|7|\r<br>\r<br>\u00a0\r<br>\r<br>When processing the result set using the simple jdbcToArrowVectors (or sqlToArrowVectorIterator) this fails to set the values extracted from the result set into the the DecimalVector\r<br>\r<br>\u00a0\r<br>```java<br>\r<br>val calendar = JdbcToArrowUtils.getUtcCalendar()\r<br>val schema = JdbcToArrowUtils.jdbcToArrowSchema(rs.metaData, calendar)\r<br>val root = VectorSchemaRoot.create(schema, RootAllocator())\r<br>val vectors = JdbcToArrowUtils.jdbcToArrowVectors(rs, root, calendar) <br>```\r<br>Error:\r<br>\r<br>\u00a0\r<br>```java<br>\r<br>Exception in thread \"main\" java.lang.IndexOutOfBoundsException: index: 0, length: 1 (expected: range(0, 0))\r<br>\u00a0 \u00a0 at org.apache.arrow.memory.ArrowBuf.checkIndexD(ArrowBuf.java:318)\r<br>\u00a0 \u00a0 at org.apache.arrow.memory.ArrowBuf.chk(ArrowBuf.java:305)\r<br>\u00a0 \u00a0 at org.apache.arrow.memory.ArrowBuf.getByte(ArrowBuf.java:507)\r<br>\u00a0 \u00a0 at org.apache.arrow.vector.BitVectorHelper.setBit(BitVectorHelper.java:85)\r<br>\u00a0 \u00a0 at org.apache.arrow.vector.DecimalVector.set(DecimalVector.java:354)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.consumer.DecimalConsumer$NullableDecimalConsumer.consume(DecimalConsumer.java:61)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.consumer.CompositeJdbcConsumer.consume(CompositeJdbcConsumer.java:46)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.JdbcToArrowUtils.jdbcToArrowVectors(JdbcToArrowUtils.java:369)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.JdbcToArrowUtils.jdbcToArrowVectors(JdbcToArrowUtils.java:321) <br>```\r<br>\u00a0\r<br>\r<br>using `sqlToArrowVectorIterator` also fails with an error trying to set data into the vector: (requires a little bit of trickery to force creation of the package private configuration)\r<br>\r<br>\u00a0\r<br>```java<br>\r<br>Exception in thread \"main\" java.lang.RuntimeException: Error occurred while getting next schema root.\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.next(ArrowVectorIterator.java:179)\r<br>\u00a0 \u00a0 at com.acme.dataformat.ArrowResultSetProcessor.processResultSet(ArrowResultSetProcessor.kt:31)\r<br>\u00a0 \u00a0 at com.acme.AppKt.main(App.kt:54)\r<br>\u00a0 \u00a0 at com.acme.AppKt.main(App.kt)\r<br>Caused by: java.lang.RuntimeException: Error occurred while consuming data.\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.consumeData(ArrowVectorIterator.java:121)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.load(ArrowVectorIterator.java:153)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.next(ArrowVectorIterator.java:175)\r<br>\u00a0 \u00a0 ... 3 more\r<br>Caused by: java.lang.UnsupportedOperationException: BigDecimal scale must equal that in the Arrow vector: 7 != 0\r<br>\u00a0 \u00a0 at org.apache.arrow.vector.util.DecimalUtility.checkPrecisionAndScale(DecimalUtility.java:95)\r<br>\u00a0 \u00a0 at org.apache.arrow.vector.DecimalVector.set(DecimalVector.java:355)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.consumer.DecimalConsumer$NullableDecimalConsumer.consume(DecimalConsumer.java:61)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.consumer.CompositeJdbcConsumer.consume(CompositeJdbcConsumer.java:46)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.consumeData(ArrowVectorIterator.java:113)\r<br>\u00a0 \u00a0 ... 5 more <br>```\r<br>\u00a0\r<br>\r<br>\u00a0|\n",
        "created_at": "2022-04-30T21:19:21.000Z",
        "updated_at": "2022-05-18T22:12:25.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Java",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-05-18T17:52:32.000Z"
    },
    "comments": [
        {
            "created_at": "2022-05-11T23:46:33.694Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17535140) by @toddfarmer:*\nThanks for reporting this, `[~jswenson]` !\u00a0 I've verified the behavior as you've described.\u00a0 In the face of unreliable ResultSetMetaData from the JDBC vendor, it seems likely we will need to supply a fallback configuration option to allow explicit definition of mapping.\u00a0 I don't believe the scale and precision of the resulting Arrow vectors can be resized dynamically based on values read in after allocation/creation.\r\n\r\nThere is a JdbcFieldInfo object that allows explicit definition of precision and scale, but this seems to be configurable only for applying to elements of list/array types."
        },
        {
            "created_at": "2022-05-12T07:33:02.257Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17535926) by Antoine Pitrou (apitrou):*\ncc `[~rpimike1022]` \u00a0"
        },
        {
            "created_at": "2022-05-12T07:34:45.199Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17535928) by Antoine Pitrou (apitrou):*\nSo if the JDBC-to-Arrow interface is streaming, it seems a configuration option is the only way to go about this.\r\n\r\nAlso cc `[~lidavidm]` as that's an interesting JDBC (and presumably ODBC) gotcha."
        },
        {
            "created_at": "2022-05-12T12:08:12.516Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17536073) by David Li (lidavidm):*\nThe other choice could be a canonical extension type (presumably something like Struct { decimal: FixedSizeBinary(16), precision: UInt32, scale: Int32 }). I agree the configuration option is probably the only way to handle this right now."
        },
        {
            "created_at": "2022-05-12T21:30:59.577Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17536352) by Jonathan Swenson (jswenson):*\nIt appears as though there is some capabilities in configuring the type mapping and the like by passing a type mapping function when constructing, `JdbcToArrowConfig` but from the looks of things, that path is currently unsupported (the <font color=\"#505f79\">`JdbcToArrowConfig`</font> constructor is package private).\u00a0\r\n\r\nHowever, even if that were supported (mapping `decimal(0,0)` to the struct type from `[~lidavidm]`'s comment), there would not be a corresponding way to configure the a JdbcConsumer for more complex types like this as `{}ArrowVectorIterator{`}'s initialize method calls the static `JdbcToArrowUtils.getConsumer`<font color=\"#172b4d\"> method.\u00a0</font>\r\n\r\nI don't know how many other databases do things like this, but from a [stackoverflow post](https://stackoverflow.com/questions/1410267/oracle-resultsetmetadata-getprecision-getscale?rq=1) I found that perhaps Oracle does something similar.\u00a0\r\n\r\nIn general I'm interested in any plans for additional configurability of this module to allow for supporting more complex JDBC types. postgres: timestamp with timezone, intervals, UUIDs, etc / bigquery: nested and repeated fields. I've considered implementing something similar to work around some of these issues, but so much already exists in this system today that it would be hard to walk away from.\r\n\r\nI'm also interested in this configuration class as it appears as though it also supports re-use of the `VectorSchemaRoot` which would make writing out a binary payload using `ArrowStreamWriter` (which takes a single `{}VectorSchemaRoot{`}) upon instantiation.\u00a0"
        },
        {
            "created_at": "2022-05-13T16:28:16.922Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17536755) by @toddfarmer:*\nI've been working on enabling configuration of explicit type mapping of top-level columns (not just array sub-elements), and found that there's still a problem as it relates to inconsistent scales.\u00a0 From the example provided:\r\n\n|\u00a0|value|precision|scale|\r|\n|-|-|-|-|-|\n|metadata|N/A|0|0|\r|\n|row 1|1000000000000000.01|18|2|\r|\n|row 2|1000000000300.0000001|20|7|\r<br>\r<br>Even if configured to provision a vector with an initial scale of 7, the first row fails to be converted because of the mismatch in scale:\r<br>```java<br>\r<br>Caused by: java.lang.RuntimeException: Error occurred while consuming data.\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.consumeData(ArrowVectorIterator.java:129)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.load(ArrowVectorIterator.java:161)\r<br>\u00a0 \u00a0 at org.apache.arrow.adapter.jdbc.ArrowVectorIterator.next(ArrowVectorIterator.java:186)\r<br>\u00a0 \u00a0 ... 54 more\r<br>Caused by: java.lang.UnsupportedOperationException: BigDecimal scale must equal that in the Arrow vector: 2 != 7<br>```\r<br>The value could have the scale implicitly increased from 1000000000000000.01 to 1000000000000000.0100000 - but that could be a lossy conversion.\u00a0 The actual original value with a scale of 7 could actually have been 1000000000000000.0114236, rounded down to 1000000000000000.01 when the scale of 2 was applied.\u00a0 I wonder whether these are edge cases - e.g., results synthesized from non-tabular data.\u00a0 If that's the case, and we can _generally_ assume consistency in precision and scale within a single column of a resultset, the configuration option makes sense.\u00a0 If that's the case, I'd propose we defer work on scale conversion to support inconsistent scale within a single column to a separate task.|\n"
        },
        {
            "created_at": "2022-05-13T17:38:56.850Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17536788) by Jonathan Swenson (jswenson):*\nIn postgres it is possible to do this with data directly from a table.\u00a0\r\n\r\nIf the table is declared as type numeric it will be created with the defaults: 0 precision 0 scale.\u00a0\r\n\r\nFrom the [postgres docs](https://www.postgresql.org/docs/current/datatype-numeric.html#DATATYPE-NUMERIC-DECIMAL):\u00a0\r\n> Specifying:\n> NUMERIC\n> without any precision or scale creates an\u00a0\u201cunconstrained numeric\u201d\u00a0column in which numeric values of any length can be stored, up to the implementation limits. A column of this kind will not coerce input values to any particular scale, whereas\u00a0`numeric`\u00a0columns with a declared scale will coerce input values to that scale. (The\u00a0SQL\u00a0standard requires a default scale of 0, i.e., coercion to integer precision. We find this a bit useless. If you're concerned about portability, always specify the precision and scale explicitly.)\r\nIf you set up a table with an \"unconstrained\" numeric column like this:\u00a0\r\n\r\n\u00a0\r\n```java\n\r\ntest=# create table numeric_test(num numeric);\r\nCREATE TABLE\r\n\r\ntest=# select column_name, numeric_precision, numeric_scale from information_schema.columns where table_name = 'numeric_test';\r\n\u00a0column_name | numeric_precision | numeric_scale\u00a0\r\n-------------+-------------------+---------------\r\n\u00a0num \u00a0 \u00a0 \u00a0 \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n(1 row)\r\n\r\ntest=# insert into numeric_test values (1000000000000000.01);\r\nINSERT 0 1\r\n\r\ntest=# insert into numeric_test values (1000000000300.0000001);\r\nINSERT 0 1\r\n\r\ntest=# select * from numeric_test;\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n-----------------------\r\n\u00a0\u00a0 1000000000000000.01\r\n\u00a01000000000300.0000001\r\n(2 rows) \n```\r\n\u00a0\r\n\r\nThe result come back in JDBC as described in the issue.\u00a0\r\n\n|\u00a0|value|precision|scale|\r|||||||||||||\n|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|\n|meta|N/A|0|0|\r|\n|row 1|100000000000000.01|18|2|\r|\n|row 2|1000000000300.0000001|20|7|\r<br>\r<br>\u00a0\r<br>----\r<br>Another example is when you use the average function over integers. While this SQL statement is contrived, it comes from a usecase of taking the average of a table in a demo dataset I use.\u00a0\r<br>```java<br>\r<br>select x, avg(y), count(*) from (\r<br>  select 1 as x, 2 as y union select 1, 3 union select 1, 5 union \r<br>  select 2, 6 union select 2, 9 union select 2, 10 union \r<br>  select 3, 11 union select 3, 24 union select 3, 19 union select 3, 11 union select 3, 29 union \r<br>  select 3, 18 union select 3, 22 union select 3, 11 union select 3, 21 union \r<br>  select 4, 1 union select 4, 2\r<br>) a group by x; <br>```\r<br>\u00a0\r<br>\r<br>This query returns the data as follows via JDBC:\r|\n|\u00a0|value|precision|scale|\r|\n|meta|N/A|0|0|\r|\n|1|1.5000000000000000|17|16|\r|\n|2|20.5714285714285714|18|16|\r|\n|3|3.3333333333333333|17|16|\r|\n|4|8.3333333333333333|17|16|\r<br>\r<br>In this case, the scale is at least consistent in the result set, but the precision is not.\u00a0\r<br>\r<br>If you were to create a table from this query with CTAS\u00a0\r<br>{noformat}\r<br>test=# CREATE TABLE example AS SELECT ...\r<br>SELECT 4\r<br>\r<br>test=# select column_name, numeric_precision, numeric_scale from information_schema.columns where table_name = 'example';\r<br>\u00a0column_name|numeric_precision|numeric_scale\u00a0\r<br>------------~~+~~-----------------~~+~~--------------\r<br>\u00a0x \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 32|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0\r<br>\u00a0avg \u00a0 \u00a0 \u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r<br>\u00a0count \u00a0 \u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 64|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0\r<br>(3 rows){noformat}\r<br>Notably the avg column created has a null (unconstrained) precision and scale. The results of selecting from this table via JDBC come back identical to the simple select query with a precision / scale of zero and a constant scale (16) and variable precision (18).\r<br>\r<br>\u00a0\r<br>----\r<br>If you set up a table using a specified precision / scale:\u00a0\r<br>\r<br>\u00a0\r<br>{code:java}\r<br>test=# create table numeric_seven_test(num numeric(22, 7));\r<br>CREATE TABLE\r<br>\r<br>test=# insert into numeric_seven_test values (1000000000300.0000001);\r<br>INSERT 0 1\r<br>\r<br>test=# insert into numeric_seven_test values (100000000000000.01);\r<br>INSERT 0 1\r<br>\r<br>test=# select column_name, numeric_precision, numeric_scale from information_schema.columns where table_name = 'numeric_seven_test';\r<br>\u00a0column_name|numeric_precision|numeric_scale\u00a0\r<br>------------~~+~~-----------------~~+~~--------------\r<br>\u00a0num \u00a0 \u00a0 \u00a0 \u00a0|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 22|\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 7\r<br>(1 row)\r<br> \u00a0\r<br>test=# select \\* from numeric_seven_test;\r<br>\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r<br>-------------------------\r<br>\u00a0\u00a0 1000000000300.0000001\r<br>\u00a0100000000000000.0100000\r<br>(2 rows){code}\r<br>\u00a0\r<br>\r<br>I'm not exactly sure why, but in order to contain the values I specified, I needed to increase the precision to 22 (rather than 20) in the above examples.\r<br>\r<br>When I pull these values back with JDBC, I get the following.\u00a0\r<br>\r<br>\u00a0\r|\n|\u00a0|value|precision|scale|\r|\n|meta|N/A|22|7|\r|\n|row 1|1000000000300.0000001|20|7|\r|\n|row 2|100000000000000.0100000|22|7|\r<br>\r<br>\u00a0\r<br>\r<br>\u00a0|\n"
        },
        {
            "created_at": "2022-05-13T20:07:05.868Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17536860) by @toddfarmer:*\nThe code evaluates precision and scale differently, requiring only that the target precision be _at least_ the same precision as the value being mapped to the vector.\u00a0 This differs from handling of scale, which requires exact matching:\r\n```java\n\r\n/**\r\n * Check that the BigDecimal scale equals the vectorScale and that the BigDecimal precision is\r\n * less than or equal to the vectorPrecision. If not, then an UnsupportedOperationException is\r\n * thrown, otherwise returns true.\r\n */\r\npublic static boolean checkPrecisionAndScale(BigDecimal value, int vectorPrecision, int vectorScale) {\r\n  if (value.scale() != vectorScale) {\r\n    throw new UnsupportedOperationException(\"BigDecimal scale must equal that in the Arrow vector: \" +\r\n        value.scale() + \" != \" + vectorScale);\r\n  }\r\n  if (value.precision() > vectorPrecision) {\r\n    throw new UnsupportedOperationException(\"BigDecimal precision can not be greater than that in the Arrow \" +\r\n      \"vector: \" + value.precision() + \" > \" + vectorPrecision);\r\n  }\r\n  return true;\r\n} \n```\r\nIn that context, configuration will support the use cases except for the first, where postgres returns rows with differing scale values for the same column.\u00a0 If we're comfortable separating these concerns, I'll propose a PR to enable configurable mapping, and open a new issue to track handling the rows with differing scale values problem."
        },
        {
            "created_at": "2022-05-16T17:05:53.148Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17537665) by Jonathan Swenson (jswenson):*\n@toddfarmer \u2013 I'm unsure what the implications of the configurable mapping would be. Would the idea be to force the scale (and min precision) in configuration? And then coerce the JDBC BigDecimal values to that (regardless of what they come back from the JDBC driver as)?\u00a0\r\n\r\nMaybe I'm not understanding the nuance. I think it is likely reasonable to break these into two efforts from a development standpoint. I'm mostly just curious how I might be able to utilize the first effort here.\u00a0"
        },
        {
            "created_at": "2022-05-16T17:22:09.665Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17537676) by @toddfarmer:*\n`[~jswenson]` - That is my thinking, yes.\u00a0 It's also likely that future code to enable coercion should rely on explicit configuration.\u00a0 At least one of the sample data sets you supplied earlier could make use of the proposed configuration (no coercion) code:\r\n\n|\u00a0|value|precision|scale|\r|\n|-|-|-|-|-|\n|meta|N/A|0|0|\r|\n|1|1.5000000000000000|17|16|\r|\n|2|20.5714285714285714|18|16|\r|\n|3|3.3333333333333333|17|16|\r|\n|4|8.3333333333333333|17|16|\r<br>\r<br>In such a case, the scale and precision are constant (enough) across the actual data, but are not reliably represented in the ResultSetMetaData.\u00a0 The ability to configure mapping will provide relief in such scenarios.|\n"
        },
        {
            "created_at": "2022-05-16T18:44:35.028Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17537722) by Jonathan Swenson (jswenson):*\nDue to the dynamic nature of the way that the system I'm using writes / generates queries, I don't believe that I'll be able to \"guess\" the correct precision and scale for a query, but I can see how this could be useful for queries that are more static / known ahead of time.\r\n\r\nI can't think of any great ways to select the correct configuration dynamically (other than perhaps looking at the first row ahead of time or something \u2013 which isn't really possible with ForwardOnlyResultSets). But I could see how this step goes in the right direction.\u00a0"
        },
        {
            "created_at": "2022-05-17T19:42:59.358Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17538431) by @toddfarmer:*\nOpened ARROW-16600 to track related work allowing configurable coercion of BigDecimal scale."
        },
        {
            "created_at": "2022-05-18T17:52:32.447Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16427?focusedCommentId=17538988) by David Li (lidavidm):*\nIssue resolved by pull request 13166\n<https://github.com/apache/arrow/pull/13166>"
        }
    ]
}