{
    "issue": {
        "title": "[Python][Parquet]Pandas datetime columns not correctly roundtripping with fastparquet(0.7.0) and pyarrow ",
        "body": "***Note**: This issue was originally created as [ARROW-13471](https://issues.apache.org/jira/browse/ARROW-13471). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen trying to roundtrip data with pandas.read_parquet, datetime64[ns] columns are not round-tripped correctly if the data is written with fastparquet and read in with pyarrow. The data appears to be read in correctly, but the dtypes are incorrect.\r\n\r\nNote: This works correctly if the engine used to read and write is fastparquet.\r\n\r\nI asked this on the fastparquet bug tracker and they said that it was a pyarrow bug.\r\n\r\nxref\u00a0[Broken compat between fastparquet(0.7.0) and pyarrow \u00b7 Issue #650 \u00b7 dask/fastparquet (github.com)](https://github.com/dask/fastparquet/issues/650)\r\n```java\n\r\nimport pandas as pd\r\ns = pd.DataFrame({\"a\":pd.date_range(\"20130101\", periods=3)})\r\ns.dtypes\r\n# datetime64[ns] \r\ns.to_parquet(\"test.parquet\", engine=\"fastparquet\")\r\npd.read_parquet(\"test.parquet\", engine=\"pyarrow\").dtypes \r\n# datetime64[ns, UTC]\r\n```\r\n\u00a0",
        "created_at": "2021-07-27T23:25:54.000Z",
        "updated_at": "2021-08-02T13:43:01.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-08-02T13:43:01.000Z"
    },
    "comments": [
        {
            "created_at": "2021-08-02T13:42:33.921Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13471?focusedCommentId=17391599) by Joris Van den Bossche (jorisvandenbossche):*\n`[~lithomas1]` thanks for the report! I answered in the fastparquet issue (I think it's a fastparquet issue). Let's continue the discussion there first, so closing this for now."
        }
    ]
}