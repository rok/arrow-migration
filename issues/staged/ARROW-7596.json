{
    "issue": {
        "title": "[Python] Only apply zero-copy DataFrame block optimizations when split_blocks=True",
        "body": "***Note**: This issue was originally created as [ARROW-7596](https://issues.apache.org/jira/browse/ARROW-7596). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFollow up to ARROW-3789 since there is downstream code that assumes that the DataFrame produced always has all mutable blocks",
        "created_at": "2020-01-16T22:49:40.000Z",
        "updated_at": "2020-01-24T23:05:02.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-01-22T12:46:31.000Z"
    },
    "comments": [
        {
            "created_at": "2020-01-20T05:11:25.072Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7596?focusedCommentId=17019233) by Wes McKinney (wesm):*\nI'll take care of this tomorrow"
        },
        {
            "created_at": "2020-01-22T12:46:31.170Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7596?focusedCommentId=17021018) by Krisztian Szucs (kszucs):*\nIssue resolved by pull request 6250\n<https://github.com/apache/arrow/pull/6250>"
        },
        {
            "created_at": "2020-01-24T18:22:22.312Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7596?focusedCommentId=17023156) by Bryan Cutler (bryanc):*\nLinking some discussion on the mailing list regarding pyspark behavior with this option: <https://www.mail-archive.com/dev@arrow.apache.org/msg17008.html>\r\n```\n\r\nJoris Van den Bossche\r\nFri, 24 Jan 2020 02:11:05 -0800\r\n\r\nHi Bryan,\r\n\r\nFor the case that the column is no timestamp and was not modified: I don't\r\nthink it will take copies of the full dataframe by assigning columns in a\r\nloop like that. But it is still doing work (it will copy data for that\r\ncolumn into the array holding those data for 2D blocks), and which can\r\neasily be avoided I think by only assigning back when the column was\r\nactually modified (eg by moving the is_datetime64tz_dtype inline in the\r\nloop iterating through all columns, so you can only write back if actually\r\nhaving tz-aware data).Further, even if you do the above to avoid writing back to the dataframe\r\nwhen not needed, I am not sure you should directly try to use the new\r\nzero-copy feature of the Table.to_pandas conversion (with\r\nsplit_blocks=True). It depends very much on what further happens with the\r\nconverted dataframe. Once you do some operations in pandas, those splitted\r\nblocks will get combined (resulting in a memory copy then), and it also\r\nmeans you can't modify the dataframe (if this dataframe is used in python\r\nUDFs, it might limit what can be done in those UDFs. Just guessing here, I\r\ndon't know the pyspark code well enough).\r\n\r\nJoris\r\n\r\n\r\nOn Thu, 23 Jan 2020 at 21:03, Bryan Cutler <cutl...@gmail.com> wrote:\r\n\r\n> Thanks for investigating this and the quick fix Joris and Wes!  I just have\r\n> a couple questions about the behavior observed here.  The pyspark code\r\n> assigns either the same series back to the pandas.DataFrame or makes some\r\n> modifications if it is a timestamp. In the case there are no timestamps, is\r\n> this potentially making extra copies or will it be unable to take advantage\r\n> of new zero-copy features in pyarrow? For the case of having timestamp\r\n> columns that need to be modified, is there a more efficient way to create a\r\n> new dataframe with only copies of the modified series?  Thanks!\r\n>\r\n> Bryan \n```\r\n\u00a0"
        }
    ]
}