{
    "issue": {
        "title": "[R] Substantial RAM use increase in 9.0.0 release on write_dataset()",
        "body": "***Note**: This issue was originally created as [ARROW-17541](https://issues.apache.org/jira/browse/ARROW-17541). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nConsider the following example of opening a remote dataset (a single 4 GB parquet file) and streaming it to disk. Consider this reprex:\r\n\r\n\u00a0\r\n```java\n\r\n\r\ns3 <- arrow::s3_bucket(\"data\", endpoint_override = \"minio3.ecoforecast.org\", anonymous=TRUE)\r\ndf <- arrow::open_dataset(s3$path(\"waq_test\"))\r\narrow::write_dataset(df, tempfile())\r\n\u00a0\n```\r\nIn 8.0.0, this operation peaks at about ~10 GB RAM use, which is already surprisingly high (when the whole file is 4 GB when on disk), but on arrow 9.0.0 RAM use for the same operation approximately doubles, which is large enough to trigger the OOM killer on the task in several of our active production workflows.\u00a0\r\n\r\n\u00a0\r\n\r\nCan this large RAM use increase introduced in 9.0 be avoided?\u00a0 Is it possible for this operation to use even less RAM than it does in 8.0 release?\u00a0 Is there something about this particular parquet file that should be responsible for the large RAM use?\u00a0\r\n\r\n\u00a0\r\n\r\nArrow's impressively fast performance on large data on remote hosts is really game-changing for us.\u00a0 Still, the OOM errors are a bit unexpected at this scale (i.e. single 4GB parquet file), as R users we really depend on arrow's out-of-band operations to work with larger-than-RAM data.\r\n\r\n\u00a0",
        "created_at": "2022-08-26T22:49:35.000Z",
        "updated_at": "2022-10-31T11:11:39.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-08-26T23:21:23.643Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17585635) by Weston Pace (westonpace):*\nAre you writing the file locally?  How are you measuring RAM usage?\r\n\r\nIs that 8-10GB in the process' RSS space?  If so, we probably need to expose controls in R to lower the readahead amount (there are some other long-term options but this would be a short-term fix that would let you tradeoff RAM vs. I/O throughput)\r\n\r\nOn the other hand, if the process' RSS space is lower, but the system available memory is still low (e.g. output from the `free` command) then it's probably because your input throughput (e.g. downloading from S3) is faster than your disk's write speed (not too surprising with an HDD or even some SSDs).  What happens in this case is the writes are simply memcpy'ing data from RSS into the kernel page cache and marking the pages dirty.  The write doesn't actually persist to the disk until sometime later (even possibly after the process has ended).  If your write is slower than your read then the kernel's page cache will fill up and clobber all other memory, pushing it to swap or even potentially invoking the oom killer.  On the bright side, we do have a [PR in progress](https://github.com/apache/arrow/pull/13640) which should alleviate this problem, at least on Linux.\r\n\r\nCan you do some investigation to try and figure out which of these possibilities we are encountering?"
        },
        {
            "created_at": "2022-08-26T23:37:24.162Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17585636) by Weston Pace (westonpace):*\n{blockquote}\r\nCan this large RAM use increase introduced in 9.0 be avoided?  Is it possible for this operation to use even less RAM than it does in 8.0 release?  Is there something about this particular parquet file that should be responsible for the large RAM use? \r\n{blockquote}\r\n\r\nHmm...I'm not sure why this would increase from 8.0 to 9.0 either.  I'll try your reprex and see if I can reproduce this on Monday."
        },
        {
            "created_at": "2022-08-26T23:44:09.662Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17585638) by Carl Boettiger (cboettig):*\nThanks Weston for the pointers here!\r\n\r\n\u00a0\r\n\r\nYes, tempfile() in the example above returns a local path, though I see the unexpectedly high RAM use in a variety of settings:\r\n \\* In the example described above, tempfile() for me is a local NVMe disk, while the reading is from a remote server, so I would not expect the read-in to be faster than the write-out in this case.\r\n \\* in our original case, we are streaming out to another S3 bucket where we see the crash, rather than writing to a local tempfile()\r\n\r\nIn both settings I'm testing on a Linux platform.\u00a0 On Arrow 9.0 I see peak RAM use at nearly 30 GB in \"RES\" category (e.g. in glances/top) or as \"used\" if I run `free -h` from bash.\u00a0 The RStudio interface shows this same number (though benchmarking software like bench::bench_memory() does not see this RAM use, reporting it as only a few hundred KB). \u00a0\r\n\r\nSorry I don't know my way around the situations better.\u00a0 Can you successfully run the code above?\u00a0 (It doesn't require any authentication, should be a reprex, I've run it in a lot of places).\u00a0 In every platform in which I run those three lines, I see RAM use 2-3x higher in arrow 9.0 and always way higher than the 4GB file size.\u00a0 Is that expected?"
        },
        {
            "created_at": "2022-08-26T23:48:37.445Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17585640) by Weston Pace (westonpace):*\nThanks, this should be enough for me to investigate further."
        },
        {
            "created_at": "2022-08-30T03:15:53.526Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17597495) by Weston Pace (westonpace):*\nI looked at this a bit today but RAM usage didn't get past 6GB and I saw similar behavior between 8 and 9.  Even that seems high and I still have a few theories (e.g. I was only using debug builds) so I plan to keep exploring this but wanted to give an update.  If it's not too much trouble can you explain how you are obtaining or building the arrow package?"
        },
        {
            "created_at": "2022-08-30T03:19:05.231Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17597497) by Carl Boettiger (cboettig):*\nWeird.\u00a0 I'm running on ubuntu linux (rocker/geospatial container), installing arrow as a prebuilt binary from RStudio package manager"
        },
        {
            "created_at": "2022-08-30T21:28:24.867Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17598082) by Weston Pace (westonpace):*\nOk.  There is definitely something strange going on here.  I plotted memory usage today during a run via R and then made the same run via python.  Notable observations:\r\n\r\n \\* R does not appear to be releasing batches that have been written but the python version is doing so :(\r\n \\* S3 download speeds, when run through python, are half as fast as the R version.\r\n\r\nNaively I want to say backpressure is the culprit in both cases.  A lack of backpressure leading to excess memory in R and an excess of backpressure throttling downloads in python.  However, the R behavior is not what I would expect from a lack of backpressure.  Even without backpressure we should be freeing data as it is written.\r\n\r\n ![Screenshot 2022-08-30 at 14-23-20 Online Graph Maker ](Screenshot 2022-08-30 at 14-23-20 Online Graph Maker ) "
        },
        {
            "created_at": "2022-08-31T00:38:21.890Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17598117) by Weston Pace (westonpace):*\nSo I'm pretty sure that the real problem is that R's garbage collector is somehow holding onto pool memory.  This wasn't new in 9.0.0.  As you said yourself, it was already using a pretty excessive amount of RAM in 8.0.0.  Notice that, in python, downloading this file uses less than 1GB of RAM.  I'm not going to worry too much about the fact that RAM rate doubled between 8.0.0 and 9.0.0.  I suspect that may just be that we are more aggressive with readahead on these sorts of files in 9.0.0 (in 8.0.0 we always read ahead 8 batches, in 9.0.0 the readahead is based on # of rows which leads to 20 batches on this file).\r\n\r\nR's garbage collector is not running for two reasons:\r\n\r\n1. R is not aware there is any memory pressure, because it doesn't see the RAM used by Arrow pool memory.  As far as it is concerned it is only holding onto 80MB and in reality it is holding onto multiple GB of RAM.\r\n\r\n2. We are executing a single (admittedly long running) C statement with `write_dataset`.  R's garbage collector will not (I think) run mid-execution.\r\n\r\nWe could investigate the above two issues but there is a third, more concerning problem:\r\n\r\n3. R is holding onto memory when it isn't clear to me it should even be able to see the memory.\r\n\r\nThe allocations we are making in Arrow come from the memory pool, they are owned by record batch objects, and those record batch objects are never (as far as I know) converted to R.  Perhaps they are being converted to R somewhere (we are scanning and then writing, do we scan into R before we send to the write node?  I wouldn't think so but I could be wrong).  Or perhaps R's memory allocator works in some strange way I'm not aware of.\r\n\r\nI'm going to have to step back from this investigation as I've hit my limit for the week and there is other work I am on the hook for.  So if any R aficionados want to investigate I'd be grateful."
        },
        {
            "created_at": "2022-08-31T01:27:28.472Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17598124) by Dewey Dunnington (paleolimbot):*\nI'm working on two PRs that touch some of those parts of the code and will investigate at some point in the next two weeks. A few thoughts:\r\n\r\n> R's garbage collector will not (I think) run mid-execution.\r\n\r\nI believe that's true, although in theory R is also not allocating any memory either. I don't know of any way to allocate R memory without (potentially) running the garbage collector.\r\n\r\n> R is holding onto memory when it isn't clear to me it should even be able to see the memory.\r\n\r\nWould a more precise way to say this be that there is some shared pointer (potentially held by an R6 object that is still in scope and not being destroyed) that is keeping the record batches from being freed? We do have an R reference to the exec plan and the final node of the exec plan (which would be the penultimate node in the dataset write, which is probably the scan node). (It still makes no sense to me why the batches aren't getting released)."
        },
        {
            "created_at": "2022-08-31T14:26:57.719Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17598406) by Weston Pace (westonpace):*\n> Would a more precise way to say this be that there is some shared pointer (potentially held by an R6 object that is still in scope and not being destroyed) that is keeping the record batches from being freed? We do have an R reference to the exec plan and the final node of the exec plan (which would be the penultimate node in the dataset write, which is probably the scan node). (It still makes no sense to me why the batches aren't getting released).\r\n\r\nYes, I think that is a more precise way.  Holding onto the ExecPlan (which owns the nodes too) should be ok (indeed, desirable)."
        },
        {
            "created_at": "2022-10-27T12:37:43.337Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17625079) by Dewey Dunnington (paleolimbot):*\nThis may or may not be related, but we have a report of \"leaked memory\" from a dataset collect here:\r\n\r\nhttps://stackoverflow.com/questions/74221492/r-arrow-open-dataset-selectmyvars-collect-causing-memory-leak"
        },
        {
            "created_at": "2022-10-28T12:58:37.123Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17541?focusedCommentId=17625688) by Dewey Dunnington (paleolimbot):*\nAlso cataloging `[~assignUser]`'s observation that this only seems to happen with datasets with a particular schema!"
        }
    ]
}