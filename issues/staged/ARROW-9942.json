{
    "issue": {
        "title": "[Python] Schema Evolution - Add new Field",
        "body": "***Note**: This issue was originally created as [ARROW-9942](https://issues.apache.org/jira/browse/ARROW-9942). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWe are trying to leverage the new Dataset implementation and specifically rely on the schema evolution feature there. However when adding a new field in a later parquet file, the schemas don't seem to be merged and the new field is not available.\u00a0\r\n\r\nSimple example:\r\n```python\n\r\nimport pandas as pd\r\nfrom pyarrow import parquet as pq\r\nfrom pyarrow import dataset as ds\r\nimport pyarrow as pa\r\n\r\npath = \"data/sample/\"\r\n\r\ndf1 = pd.DataFrame({\"field1\": [\"a\", \"b\", \"c\"]})\r\ndf2 = pd.DataFrame({\"field1\": [\"d\", \"e\", \"f\"],\r\n                    \"field2\": [\"x\", \"y\", \"z\"]})\r\n\r\ndf1.to_parquet(path + \"df1.parquet\", coerce_timestamps=None, version=\"2.0\", index=False)\r\ndf2.to_parquet(path + \"df2.parquet\", coerce_timestamps=None, version=\"2.0\", index=False)\r\n\r\n# read via pandas\r\ndf = pd.read_parquet(path)\r\nprint(df.head())\r\nprint(df.info())\r\n```\r\nOutput:\r\n```\n\r\n  field1\r\n0      a\r\n1      b\r\n2      c\r\n3      d\r\n4      e\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 6 entries, 0 to 5\r\nData columns (total 1 columns):\r\n#   Column  Non-Null Count  Dtype \r\n---  ------  --------------  ----- \r\n 0   field1  6 non-null      object\r\ndtypes: object(1)\r\nmemory usage: 176.0+ bytes\r\nNone\r\n```\r\nMy expectation was to get the field2 as well based on what I have understood with the new Dataset implementation from ARROW-8039.\r\n\r\nWhen using the Dataset API with a schema created from the second dataframe I'm able to read the field2:\r\n```python\n\r\n# write metadata\r\nschema = pa.Schema.from_pandas(df2, preserve_index=False)\r\npq.write_metadata(schema, path + \"_common_metadata\", version=\"2.0\", coerce_timestamps=None)\r\n\r\n# read with new dataset and schema\r\nschema = pq.read_schema(path + \"_common_metadata\")\r\ndf = ds.dataset(path, schema=schema, format=\"parquet\").to_table().to_pandas()\r\nprint(df.head())\r\nprint(df.info())\r\n```\r\nOutput:\r\n```\n\r\n  field1 field2\r\n0      a   None\r\n1      b   None\r\n2      c   None\r\n3      d      x\r\n4      e      y\r\n<class 'pandas.core.frame.DataFrame'>\r\nRangeIndex: 6 entries, 0 to 5\r\nData columns (total 2 columns):\r\n#   Column  Non-Null Count  Dtype \r\n---  ------  --------------  ----- \r\n 0   field1  6 non-null      object\r\n 1   field2  3 non-null      object\r\ndtypes: object(2)\r\nmemory usage: 224.0+ bytes\r\nNone\r\n```\r\nThis works, however I want to avoid to write a `_common_metadata` file if possible. Is there a way to get the schema merge without passing an explicit schema? Or is this this yet to be implemented?",
        "created_at": "2020-09-08T13:33:41.000Z",
        "updated_at": "2020-12-22T09:52:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}