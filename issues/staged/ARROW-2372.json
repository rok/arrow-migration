{
    "issue": {
        "title": "[Python] ArrowIOError: Invalid argument when reading Parquet file",
        "body": "***Note**: This issue was originally created as [ARROW-2372](https://issues.apache.org/jira/browse/ARROW-2372). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI get an ArrowIOError when reading a specific file that was also written by pyarrow. Specifically, the traceback is:\r\n```python\n\r\n>>> import pyarrow.parquet as pq\r\n>>> pq.ParquetFile('gaz2016zcta5distancemiles.parquet')\r\n ---------------------------------------------------------------------------\r\n ArrowIOError Traceback (most recent call last)\r\n <ipython-input-18-149f11bf68a5> in <module>()\r\n ----> 1 pf = pq.ParquetFile('gaz2016zcta5distancemiles.parquet')\r\n~/local/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py in _init_(self, source, metadata, common_metadata)\r\n 62 self.reader = ParquetReader()\r\n 63 source = _ensure_file(source)\r\n ---> 64 self.reader.open(source, metadata=metadata)\r\n 65 self.common_metadata = common_metadata\r\n 66 self._nested_paths_by_prefix = self._build_nested_paths()\r\n_parquet.pyx in pyarrow._parquet.ParquetReader.open()\r\nerror.pxi in pyarrow.lib.check_status()\r\nArrowIOError: Arrow error: IOError: [Errno 22] Invalid argument\r\n```\r\nHere's a reproducible example with the specific file I'm working with. I'm converting a 34 GB csv file to parquet in chunks of roughly 2GB each. To get the source data:\r\n```bash\n\r\nwget https://www.nber.org/distance/2016/gaz/zcta5/gaz2016zcta5distancemiles.csv.zip\r\nunzip gaz2016zcta5distancemiles.csv.zip\n```\r\nThen the basic idea from the [pyarrow Parquet documentation](https://arrow.apache.org/docs/python/parquet.html#finer-grained-reading-and-writing)\u00a0is instantiating the writer class; looping over chunks of the csv and writing them to parquet; then closing the writer object.\r\n\r\n\u00a0\r\n```python\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nfrom pathlib import Path\r\n\r\nzcta_file = Path('gaz2016zcta5distancemiles.csv')\r\nitr = pd.read_csv(\r\n    zcta_file,\r\n    header=0,\r\n    dtype={'zip1': str, 'zip2': str, 'mi_to_zcta5': np.float64},\r\n    engine='c',\r\n    chunksize=64617153)\r\n\r\nschema = pa.schema([\r\n    pa.field('zip1', pa.string()),\r\n    pa.field('zip2', pa.string()),\r\n    pa.field('mi_to_zcta5', pa.float64())])\r\n\r\nwriter = pq.ParquetWriter('gaz2016zcta5distancemiles.parquet', schema=schema)\r\nprint(f'Starting conversion')\r\n\r\ni = 0\r\nfor df in itr:\r\n    i += 1\r\n    print(f'Finished reading csv block {i}')\r\n\r\n    table = pa.Table.from_pandas(df, preserve_index=False, nthreads=3)\r\n    writer.write_table(table)\r\n\r\n    print(f'Finished writing parquet block {i}')\r\n\r\nwriter.close()\r\n```\r\nThen running this python script produces the file\u00a0\r\n```java\n\r\ngaz2016zcta5distancemiles.parquet\n```\r\n, but just attempting to read the metadata with `pq.ParquetFile()` produces the above exception.\r\n\r\nI tested this with pyarrow 0.8 and pyarrow 0.9. I assume that pandas would complain on import of the csv if the columns in the data were not `string`, `string`, and `float64`, so I think creating the Parquet schema in that way should be fine.",
        "created_at": "2018-03-30T21:57:54.000Z",
        "updated_at": "2018-07-27T18:58:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-06-04T16:03:03.000Z"
    },
    "comments": [
        {
            "created_at": "2018-03-30T21:58:58.650Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16420984) by Kyle Barron (kylebarron):*\nTo make sure that the schema creation wasn't the issue, I rewrote the loop to be:\r\n\r\n```python\n\r\ni = 0\r\nfor df in itr:\r\n    i += 1\r\n    print(f'Finished reading csv block {i}')\r\n\r\n    table = pa.Table.from_pandas(df, preserve_index=False, nthreads=3)\r\n    if i == 1:\r\n        writer = pq.ParquetWriter('gaz2016zcta5distancemiles.parquet', schema=table.schema)\r\n\r\n    writer.write_table(table)\r\n\r\n    print(f'Finished writing parquet block {i}')\r\n\r\nwriter.close()\r\n\r\n```\r\n\r\nbut I still get the same exception when trying to read the metadata as above."
        },
        {
            "created_at": "2018-03-31T10:13:40.074Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16421269) by Justin Tan (jtan):*\nI'm experiencing the same issue with conversion of HDF5 files, writing to parquet format succeeds if the output filesize is sufficiently small (< ~5 GB), but fails otherwise, have you experimented with reducing the csv size and seeing if you get the same results?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2018-04-02T14:44:21.588Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16422572) by Kyle Barron (kylebarron):*\nI edited my code to the script below, which, I believe, writes a parquet file with just the first 2GB csv chunk, then with the first two, and so on, checking each time that it can open the output. Here's the traceback first, which suggests that it was able to open the Parquet file representing around 6GB of csv data, but not the Parquet file representing about 8GB of csv data.\r\n```java\n\r\nStarting conversion, up to iteration 0\r\n\t0.12 minutes\r\nFinished reading csv block 0\r\n\t0.43 minutes\r\nFinished writing parquet block 0\r\n\t1.80 minutes\r\nStarting conversion, up to iteration 1\r\n\t1.80 minutes\r\nFinished reading csv block 0\r\n\t2.12 minutes\r\nFinished writing parquet block 0\r\n\t3.49 minutes\r\nFinished reading csv block 1\r\n\t3.80 minutes\r\nFinished writing parquet block 1\r\n\t5.19 minutes\r\nStarting conversion, up to iteration 2\r\n\t5.20 minutes\r\nFinished reading csv block 0\r\n\t5.52 minutes\r\nFinished writing parquet block 0\r\n\t6.91 minutes\r\nFinished reading csv block 1\r\n\t7.22 minutes\r\nFinished writing parquet block 1\r\n\t8.59 minutes\r\nFinished reading csv block 2\r\n\t8.92 minutes\r\nFinished writing parquet block 2\r\n\t10.29 minutes\r\nStarting conversion, up to iteration 3\r\n\t10.29 minutes\r\nFinished reading csv block 0\r\n\t10.60 minutes\r\nFinished writing parquet block 0\r\n\t11.98 minutes\r\nFinished reading csv block 1\r\n\t12.30 minutes\r\nFinished writing parquet block 1\r\n\t13.66 minutes\r\nFinished reading csv block 2\r\n\t13.98 minutes\r\nFinished writing parquet block 2\r\n\t15.35 minutes\r\nFinished reading csv block 3\r\n\t15.68 minutes\r\nFinished writing parquet block 3\r\n\t17.05 minutes\r\n---------------------------------------------------------------------------\r\nArrowIOError                              Traceback (most recent call last)\r\n<ipython-input-10-2fadd2a47023> in <module>()\r\n     29         if j == i:\r\n     30             writer.close()\r\n---> 31             pf = pq.ParquetFile(f'gaz2016zcta5distancemiles_{i}.parquet')\r\n     32             pfs_dict[i] = pf\r\n     33             break\r\n\r\n~/local/anaconda3/lib/python3.6/site-packages/pyarrow/parquet.py in __init__(self, source, metadata, common_metadata)\r\n     62         self.reader = ParquetReader()\r\n     63         source = _ensure_file(source)\r\n---> 64         self.reader.open(source, metadata=metadata)\r\n     65         self.common_metadata = common_metadata\r\n     66         self._nested_paths_by_prefix = self._build_nested_paths()\r\n\r\n_parquet.pyx in pyarrow._parquet.ParquetReader.open()\r\n\r\nerror.pxi in pyarrow.lib.check_status()\r\n\r\nArrowIOError: Arrow error: IOError: [Errno 22] Invalid argument\r\n```\r\nAnd the source code:\r\n```Java\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nfrom pathlib import Path\r\nfrom time import time\r\n\r\nt0 = time()\r\n\r\nzcta_file = Path('gaz2016zcta5distancemiles.csv')\r\n\r\npfs_dict = {}\r\n\r\nfor i in range(17):\r\n    itr = pd.read_csv(\r\n        zcta_file,\r\n        header=0,\r\n        dtype={'zip1': str, 'zip2': str, 'mi_to_zcta5': np.float64},\r\n        engine='c',\r\n        chunksize=64617153)  # previously determined to be about 2GB of csv data\r\n\r\n    msg = f'Starting conversion, up to iteration {i}'\r\n    msg += f'\\n\\t{(time() - t0) / 60:.2f} minutes'\r\n    print(msg)\r\n\r\n    j = 0\r\n    for df in itr:\r\n        msg = f'Finished reading csv block {j}'\r\n        msg += f'\\n\\t{(time() - t0) / 60:.2f} minutes'\r\n        print(msg)\r\n\r\n        table = pa.Table.from_pandas(df, preserve_index=False, nthreads=3)\r\n        if j == 0:\r\n            writer = pq.ParquetWriter(f'gaz2016zcta5distancemiles_{i}.parquet', schema=table.schema)\r\n\r\n        writer.write_table(table)\r\n\r\n        msg = f'Finished writing parquet block {j}'\r\n        msg += f'\\n\\t{(time() - t0) / 60:.2f} minutes'\r\n        print(msg)\r\n\r\n        if j == i:\r\n            writer.close()\r\n            pf = pq.ParquetFile(f'gaz2016zcta5distancemiles_{i}.parquet')\r\n            pfs_dict[i] = pf\r\n            break\r\n\r\n        j += 1\r\n```"
        },
        {
            "created_at": "2018-04-16T15:45:36.744Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16439603) by Antoine Pitrou (apitrou):*\nThis may have been fixed with ARROW-2369. Is there a possibility for you to test with Arrow git master?"
        },
        {
            "created_at": "2018-04-16T16:12:16.528Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16439646) by Kyle Barron (kylebarron):*\nSorry, I couldn't figure out how build Arrow and Parquet. I tried to follow <https://github.com/apache/arrow/blob/master/python/doc/source/development.rst>\u00a0with Conda\u00a0exactly, but I get errors. Specifically I think it's trying to use gcc 7.2.0 instead of 4.9. I might just have to wait for 9.1."
        },
        {
            "created_at": "2018-04-16T19:07:53.817Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16439884) by Antoine Pitrou (apitrou):*\nOk, I have downloaded the dataset and confirms that it works on git master."
        },
        {
            "created_at": "2018-04-16T19:08:54.680Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16439885) by Kyle Barron (kylebarron):*\nAwesome thanks!"
        },
        {
            "created_at": "2018-06-19T09:32:27.078Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16516868) by bnu (beanu):*\nI got the same issue. Trying to run it with the Arrow git master it returns an error using:\u00a0\r\n\r\n**pip install git+<https://github.com/apache/arrow.git>**\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\nUsers\nuser\nAppData\nLocal\nTemp\npip-req-build-x2lu_5ci\nsetup.py'\r\n\r\nAm I missing something? thanks"
        },
        {
            "created_at": "2018-06-19T09:55:45.667Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16516896) by Wes McKinney (wesm):*\nThe Arrow Python library cannot be installed that way. Refer to the Python documentation about instructions to build from source "
        },
        {
            "created_at": "2018-06-28T15:40:59.505Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16526444) by Ravi (rshrivas):*\nHi Is there a date when the new version of pyarrow would be released with the above fix ? We are facing same problem and using the 0.9.0. Thank you."
        },
        {
            "created_at": "2018-06-29T11:15:51.136Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2372?focusedCommentId=16527479) by Wes McKinney (wesm):*\nYou can build your own wheels from git master by following the instructions in https://github.com/apache/arrow/tree/master/python/manylinux1\r\n\r\nI hope we will be able to release Arrow 0.10.0 by the end of July"
        }
    ]
}