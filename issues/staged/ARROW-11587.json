{
    "issue": {
        "title": "[C++] Implement a fixed-width file reader ",
        "body": "***Note**: This issue was originally created as [ARROW-11587](https://issues.apache.org/jira/browse/ARROW-11587). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFixed-width files are a common data provisioning format for (very) large, administrative data files. We have been converting provisioned fwf files to `.parquet` and then leveraging `arrow::open_dataset()` with good success. However, we still run into RAM issues with the read-in step and are keen to try new approaches to this in-memory RAM issue (ideally without chunking files etc).\r\n\r\nA simple, example workflow looks like this:\r\n```java\n\r\n sample_data <- \"https://github.com/bcgov/dipr/raw/master/inst/extdata/starwars-fwf.dat.gz\"\r\nvroom::vroom_fwf(sample_data,\r\n\u00a0\u00a0 col_positions = vroom::fwf_positions(\r\n\u00a0\u00a0\u00a0\u00a0 c(1, 22, 25, 31),\r\n\u00a0\u00a0\u00a0\u00a0 c(21, 24, 30, 35),\r\n\u00a0\u00a0\u00a0\u00a0 c(\"name\", \"height\", \"mass\", \"has_hair\")\r\n\u00a0\u00a0\u00a0\u00a0 ),\r\n\u00a0\u00a0\u00a0\u00a0 col_types = (\"cnnl\")\r\n) %>%\r\n\u00a0\u00a0\u00a0 dplyr::group_by(has_hair) %>%\r\n\u00a0\u00a0\u00a0 arrow::write_dataset(path = \"starwars_parquet\",\r\n\u00a0\u00a0\u00a0\u00a0 format = \"parquet\")\n```\r\n\u00a0\r\n\r\nWith an \\{arrow} fixed-width reader, we could perhaps leverage `arrow::open_dataset(as_data_frame = FALSE)` directly on a large fwf file and then convert to partitioned `.parquet` files with arrow::write_dataset()?",
        "created_at": "2021-02-10T18:57:03.000Z",
        "updated_at": "2022-11-03T11:18:03.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}