{
    "issue": {
        "title": "[Python] ArrowInvalid error on reading partitioned parquet files with fsspec.adlfs (pyarrow-7.0.0) due to removed '/' in the ls of path",
        "body": "***Note**: This issue was originally created as [ARROW-16077](https://issues.apache.org/jira/browse/ARROW-16077). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nReading a partitioned parquet from adlfs with pyarrow through pandas will throw unnecessary exceptions on not matching forward slashes in the listed files returned from adlfs, ie:\r\n\r\n\u00a0\r\n```python\n\r\nimport pandas as pd\r\n\r\npd.read_parquet(\"adl://resource/path/to/parquet/files\")\n```\r\nresults in exception of the form\r\n```bash\n\r\npyarrow.lib.ArrowInvalid: GetFileInfo() yielded path 'path/to/parquet/files/part-0001.parquet', which is outside base dir '/path/to/parquet/files/'\n```\r\n\u00a0\r\n\r\nand testing with modifying the adlfs method to prepend slashes to all returned files, we still end up with an error on file paths that would otherwise be handled correctly where there is a double slash in a location where there should be one, ie:\r\n\r\n\r\n\r\n\u00a0\r\n```python\n\r\nimport pandas as pd\r\n\r\npd.read_parquet(\"adl://resource/path/to//parquet/files\") \n```\r\nwould throw\r\n```bash\n\r\npyarrow.lib.ArrowInvalid: GetFileInfo() yielded path '/path/to/parquet/files/part-0001.parquet', which is outside base dir '/path/to//parquet/files/' \n```\r\nIn both cases the ls has returned correctly from adlfs, given it's discovered the file part-0001.parquet but the pyarrow exception stops what could otherwise be successful processing.\u00a0",
        "created_at": "2022-03-30T20:16:57.000Z",
        "updated_at": "2022-04-05T22:57:25.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-04-05T13:41:29.773Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16077?focusedCommentId=17517444) by Joris Van den Bossche (jorisvandenbossche):*\n`[~jon-rosenberg-env]` thanks for the report. That looks like an annoying issue! \r\n\r\nI am not very familiar with ADL myself (or access to it for testing. Do they have public datasets that can be used to test without an account like you can have public S3 buckets?), so can't directly help with diagnosing this issue. But a few questions:\r\n\r\nCan you try passing a `adlfs` filesystem object manually? Something like\r\n\r\n```Java\n\r\nimport adlfs\r\nimport pyarrow.parquet as pq\r\n\r\nadl = adlfs.AzureDatalakeFileSystem(...)\r\npq.read_table(\"...\", filesystem=adl)\r\n```\r\n\r\nWe have had previous reports related to Azure Data Lake, so while there have been issues before, that also indicates it was at least possible to read from that to a certain extent. cc `[~ldacey]` did you ever run into this specific issue?\r\n\r\n"
        },
        {
            "created_at": "2022-04-05T14:25:47.361Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16077?focusedCommentId=17517478) by Lance Dacey (ldacey):*\nI am not sure about any public datasets. Locally, I use [azurite](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azurite?tabs=visual-studio) for testing which can be installed or run as a Docker container. Note that I only use Azure Blob and not Azure Data Lake, so there might be some differences I am not aware of.\r\n\r\nI use pyarrow ds.dataset() or pq.read_table() with a filesystem to read parquet data from Azure. I did a couple of tests with double slashes in the path. Perhaps I misunderstood what the original issue was, but it looks like I can read the data with pq.read_table and with pandas using fs.open() and storage_options. I pasted my quick tests below.\r\n\r\n\r\n\r\n```python\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pytest\r\nfrom adlfs import AzureBlobFileSystem\r\nfrom pandas.testing import assert_frame_equal\r\n\r\n\r\nURL = \"http://127.0.0.1:10000\"\r\nACCOUNT_NAME = \"devstoreaccount1\"\r\nKEY = \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\"\r\nCONN_STR = f\"DefaultEndpointsProtocol=http;AccountName={ACCOUNT_NAME};AccountKey={KEY};BlobEndpoint={URL}/{ACCOUNT_NAME};\"\r\n\r\n\r\n@pytest.fixture\r\ndef example_data():\r\n    return {\r\n        \"date_id\": [20210114, 20210811],\r\n        \"id\": [1, 2],\r\n        \"created_at\": [\r\n            \"2021-01-14 16:45:18\",\r\n            \"2021-08-11 15:10:00\",\r\n        ],\r\n        \"updated_at\": [\r\n            \"2021-01-14 16:45:18\",\r\n            \"2021-08-11 15:10:00\",\r\n        ],\r\n        \"category\": [\"cow\", \"sheep\"],\r\n        \"value\": [0, 99],\r\n    }\r\n\r\n\r\ndef test_double_slashes(example_data):\r\n    fs = AzureBlobFileSystem(account_name=ACCOUNT_NAME, connection_string=CONN_STR)\r\n    fs.mkdir(\"resource\")\r\n    path = \"resource/path/to//parquet/files/part-001.parquet\"\r\n    table = pa.table(example_data)\r\n    pq.write_table(table, where=path, filesystem=fs)\r\n\r\n# use pq.read_table() with filesystem\r\n    new = pq.read_table(source=path, filesystem=fs)\r\n    assert new == table\r\n\r\n# use adlfs filesystem.open()\r\n    df = pd.read_parquet(fs.open(path, mode=\"rb\"))\r\n    dataframe_table = pa.Table.from_pandas(df)\r\n    assert table == dataframe_table\r\n\r\n# use abfs path with storage options\r\n    df2 = pd.read_parquet(f\"abfs://{path}\", storage_options={\"connection_string\": CONN_STR})\r\n    assert_frame_equal(df, df2)\r\n\r\n```\r\n\r\n\r\n"
        },
        {
            "created_at": "2022-04-05T15:55:50.696Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16077?focusedCommentId=17517534) by Jon Rosenberg (jon-rosenberg-env):*\nI'm not sure about public paths, I'll see if I can get something more specific to running inside the azurite image later today, but am seeing that the test code here is slightly different in:\r\n\r\n1. I'm just specifying the full datalake path, and not specifying filesystem or storage option in my pandas read, but with my environment variables with azure credentials, and using the scheme of the passed url, pandas is not having an issue connecting to the lake. My hunch is this usage detail shouldn't affect my issue, but I'll verify when testing later.\r\n\r\n\r\n2. I'm passing in the path to the partitioned files, not any file itself. That is, instead of\r\n```java\n\r\nabfs://resource/path/to//parquet/files/part-001.parquet\n```\r\nI would be passing\r\n```java\n\r\nabfs://resource/path/to//parquet/files \n```\r\nwhich requires an ls from adlfs to retrieve the parquet files to concatenate, and the ls is performed successfully returning the list of files EXCEPT in the returned list of the directory files from adlfs the double slash is not included in the paths, returning:\r\n```java\n\r\nresource/path/to/parquet/files/part-001.parquet \n```\r\nNOT\r\n```java\n\r\nresource/path/to//parquet/files/part-001.parquet \n```\r\nand thus PyArrow was throwing an exception for me on being outside\u00a0\r\n```java\n\r\nresource/path/to//parquet/files \n```\r\ndespite otherwise being able to proceed with the read if not for this check."
        },
        {
            "created_at": "2022-04-05T22:57:25.587Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16077?focusedCommentId=17517741) by Jon Rosenberg (jon-rosenberg-env):*\nOK, I had to do some separate testing since azurite is for blob storage and not adl, but it does seem there is a difference between how the two behave.\r\n\r\nIt appears that in blob storage\r\n```java\n\r\nresource/path/to//parquet/files  \n```\r\nis a valid and distinct path from\r\n```java\n\r\nresource/path/to/parquet/files  \n```\r\nChanging the write in your test to write to a path with only one slash but keeping the double slash in the read tests caused a failure for me, but it appeared to be due to reading an empty location.\r\n\r\nIn the data lake however any double slash path is interpreted the same as a single slash, which is what my error is arising out of. I unfortunately still don't have a public datalake path however but will look around for such a reproduction."
        }
    ]
}