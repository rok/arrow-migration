{
    "issue": {
        "title": "[Python][C++] pyarrow version 1.0.1 throws Out Of Memory exception while reading large number of files using ParquetDataset",
        "body": "***Note**: This issue was originally created as [ARROW-9974](https://issues.apache.org/jira/browse/ARROW-9974). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n<https://stackoverflow.com/questions/63792849/pyarrow-version-1-0-bug-throws-out-of-memory-exception-while-reading-large-numbe>\r\n\r\n\r\nI have a dataframe split and stored in more than 5000 files. I use ParquetDataset(fnames).read() to load all files. I updated the pyarrow to latest version 1.0.1 from 0.13.0 and it has started throwing \"OSError: Out of memory: malloc of size 131072 failed\". The same code on the same machine still works with older version. My machine has 256Gb memory way more than enough to load the data which requires < 10Gb. You can use below code to generate the issue on your side.\r\n\r\n```Java\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pyarrow.parquet as pq\r\n\r\ndef generate():\r\n# create a big dataframe\r\n\r\n    df = pd.DataFrame({'A': np.arange(50000000)})\r\n    df['F1'] = np.random.randn(50000000) * 100\r\n    df['F2'] = np.random.randn(50000000) * 100\r\n    df['F3'] = np.random.randn(50000000) * 100\r\n    df['F4'] = np.random.randn(50000000) * 100\r\n    df['F5'] = np.random.randn(50000000) * 100\r\n    df['F6'] = np.random.randn(50000000) * 100\r\n    df['F7'] = np.random.randn(50000000) * 100\r\n    df['F8'] = np.random.randn(50000000) * 100\r\n    df['F9'] = 'ABCDEFGH'\r\n    df['F10'] = 'ABCDEFGH'\r\n    df['F11'] = 'ABCDEFGH'\r\n    df['F12'] = 'ABCDEFGH01234'\r\n    df['F13'] = 'ABCDEFGH01234'\r\n    df['F14'] = 'ABCDEFGH01234'\r\n    df['F15'] = 'ABCDEFGH01234567'\r\n    df['F16'] = 'ABCDEFGH01234567'\r\n    df['F17'] = 'ABCDEFGH01234567'\r\n\r\n# split and save data to 5000 files\r\n    for i in range(5000):\r\n        df.iloc[i*10000:(i+1)*10000].to_parquet(f'{i}.parquet', index=False)\r\n\r\ndef read_works():\r\n# below code works to read\r\n    df = []\r\n    for i in range(5000):\r\n        df.append(pd.read_parquet(f'{i}.parquet'))\r\n\r\n    df = pd.concat(df)\r\n\r\ndef read_errors():\r\n# below code crashes with memory error in pyarrow 1.0/1.0.1 (works fine with version 0.13.0)\r\n# tried use_legacy_dataset=False, same issue\r\n\r\n    fnames = []\r\n    for i in range(5000):\r\n        fnames.append(f'{i}.parquet')\r\n\r\n    len(fnames)\r\n\r\n    df = pq.ParquetDataset(fnames).read(use_threads=False)\r\n\u00a0\r\n\r\n\u00a0\n```",
        "created_at": "2020-09-11T10:24:28.000Z",
        "updated_at": "2021-01-12T20:46:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-01-12T20:46:26.000Z"
    },
    "comments": [
        {
            "created_at": "2020-09-11T10:26:12.480Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17194203) by Ashish Gupta (kgashish):*\nApologies sample code hasn't formatted nicely, please refer to the stackoverflow."
        },
        {
            "created_at": "2020-09-11T14:41:34.777Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17194299) by Wes McKinney (wesm):*\ncc `[~bkietz]` `[~jorisvandenbossche]`"
        },
        {
            "created_at": "2020-09-11T19:41:04.986Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17194504) by Joris Van den Bossche (jorisvandenbossche):*\n`[~kgashish]` thanks for opening the issue here!\r\n\r\nAs mentioned on StackOverflow, I can't reproduce the issue locally (I tried with 10x smaller data because I don't have enough RAM otherwise, but then don't get the error), so some follow up questions: 1) Can you show the full traceback for both the case with `use_legacy_dataset` set to True or False? 2) Do you still get the error if you eg comment out adding columns F15 to F17 ? \r\n\r\n"
        },
        {
            "created_at": "2020-09-12T15:58:43.968Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17194764) by Ashish Gupta (kgashish):*\n1) Please find attached full traceback of both cases\u00a0[legacy_false.txt](legacy_false.txt)\r\n\r\n\u00a0\r\n\r\n<sup>2) I started removing columns one by one and below is the smallest dataframe where adding the last column F11 causes both scenarios use_legacy_dataset set to True and False to throw error. Interestingly, use_legacy_dataset=True starts crashing after first 5 columns. If you have more than 8Gb ram you should be able to test it.</sup>\r\n```java\n\r\n# create a big dataframe\r\n    import pandas as pd\r\n    import numpy as np    df = pd.DataFrame({'A': np.arange(50000000)})\r\n    df['F1'] = np.random.randn(50000000)\r\n    df['F2'] = np.random.randn(50000000)\r\n    df['F3'] = np.random.randn(50000000)\r\n    df['F4'] = 'ABCDEFGH'\r\n    df['F5'] = 'ABCDEFGH'\r\n    df['F6'] = 'ABCDEFGH'\r\n    df['F7'] = 'ABCDEFGH'\r\n    df['F8'] = 'ABCDEFGH'\r\n    df['F9'] = 'ABCDEFGH'\r\n    df['F10'] = 'ABCDEFGH'\r\n# df['F11'] = 'ABCDEFGH'\r\n\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-09-29T12:58:02.620Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17203912) by Krisztian Szucs (kszucs):*\nI was also unable to reproduce the error, tried with both of your examples on master and 1.0.1. Could you show the backtrace from the coredump?"
        },
        {
            "created_at": "2020-09-29T15:01:14.483Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17204007) by Ashish Gupta (kgashish):*\nIt seems it has something to do with the operating system. The code is crashing on a machine running linux centos 8. I tried the same code with the same versions of conda/pandas/pyarrow on a Windows pc and it worked. Would it be possible for you to test it on centos-8\r\n\r\ncat /etc/os-release\r\nNAME=\"CentOS Linux\"\r\nVERSION=\"8 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"8\"\r\nPLATFORM_ID=\"platform:el8\"\r\nPRETTY_NAME=\"CentOS Linux 8 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:centos:centos:8\"\r\nHOME_URL=\"https://www.centos.org/\"\r\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\r\n\r\n\u00a0\r\n\r\nI am not sure how to get backtrace from the coredump, if you can advise I will try to provide it to you.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-09-29T15:37:42.247Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17204038) by Krisztian Szucs (kszucs):*\nSure! \r\n\r\nFirst you need to enable coredumps on the system, usually by running \"\"ulimit -c unlimited\". \r\nThen trigger the segfault again which should generate a file named \"core\" usually in the same directory. \r\nOnce the core file is available you can examine it with \"gdb path/to/executable path/to/core\", in this case \"gdb python ./core\" if you are located where the core file is.\r\n\r\nInside gdb running \"bt\" command should show you the backtrace of the segfault."
        },
        {
            "created_at": "2020-09-29T17:45:01.673Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17204167) by Ashish Gupta (kgashish):*\nI have test.py as below\r\n```java\n\r\n\u00a0\n```\r\n\u00a0\r\n```java\n\r\nimport pyarrow.parquet as pq\r\nfnames = []\r\nfor i in range(5000):\r\n fnames.append(f'{i}.parquet')\r\nlen(fnames)\r\ndf = pq.ParquetDataset(fnames, use_legacy_dataset=True).read(use_threads=False)\r\n```\r\n\u00a0\r\n\r\nwith use_legacy_dataset=True, there is no core dump, just the below error\r\n\r\n\u00a0\r\n```java\n\r\nTraceback (most recent call last):\r\n File \"test.py\", line 9, in <module>\r\n df = pq.ParquetDataset(fnames, use_legacy_dataset=True).read(use_threads=False)\r\n File \"/data/install/anaconda3/lib/python3.8/site-packages/pyarrow/parquet.py\", line 1271, in read\r\n table = piece.read(columns=columns, use_threads=use_threads,\r\n File \"/data/install/anaconda3/lib/python3.8/site-packages/pyarrow/parquet.py\", line 718, in read\r\n table = reader.read(**options)\r\n File \"/data/install/anaconda3/lib/python3.8/site-packages/pyarrow/parquet.py\", line 326, in read\r\n return self.reader.read_all(column_indices=column_indices,\r\n File \"pyarrow/_parquet.pyx\", line 1125, in pyarrow._parquet.ParquetReader.read_all\r\n File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\nOSError: Out of memory: realloc of size 65600 failed\r\ncannot allocate memory for thread-local data: ABORT\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nwith use_legacy_dataset=False, there is a core dump but I think there is some issue reading the coredump file.\r\n\r\n\u00a0\r\n```java\n\r\nReading symbols from /data/install/anaconda3/bin/python...done.\r\nBFD: warning: /data/dump/temp/core.python.862529738.cd627b19559c40969f42d3bb01c5e03d.739784.1601400696000000 is truncated: expected core file size >= 4752101376, found: 2147483648\r\nwarning: core file may not match specified executable file.\r\n[New LWP 739784]\r\n[New LWP 739791]\r\n[New LWP 739795]\r\n[New LWP 739788]\r\n[New LWP 739785]\r\n[New LWP 739792]\r\n[New LWP 739790]\r\n[New LWP 739794]\r\n[New LWP 739793]\r\n[New LWP 739789]\r\nCannot access memory at address 0x7f86e6283128\r\nCannot access memory at address 0x7f86e6283120\r\nFailed to read a valid object file image from memory.\r\nCore was generated by `/data/install/anaconda3/bin/python test.py'.\r\nProgram terminated with signal SIGABRT, Aborted.\r\n#0 0x00007f86e5aae70f in ?? ()\r\n[Current thread is 1 (LWP 739784)]\r\n(gdb) bt\r\n#0 0x00007f86e5aae70f in ?? ()\r\nBacktrace stopped: Cannot access memory at address 0x7ffcf1fb8c80\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nWould you be able to run the example on a machine with linux centos 8?\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-09-29T22:36:32.458Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17204338) by Krisztian Szucs (kszucs):*\nI have not had the time to test it on centos 8 yet, but will try to reproduce it during the week."
        },
        {
            "created_at": "2020-10-05T16:26:33.918Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17208167) by Antoine Pitrou (apitrou):*\nThe error message (\"OSError: Out of memory: malloc of size 131072 failed\") tells us that the failure is returned by the glibc memory allocator, not by the jemalloc allocator which is used by Arrow for array data. Also, the failed allocation is tiny (128 kB). This hints at a possible heap fragmentation problem.\r\n\r\nI'll recommend you try playing with the glibc malloc tunables, especially the `MALLOC_MMAP_THRESHOLD_` environment variable (note trailing underscore). For example `MALLOC_MMAP_THRESHOLD_=65536`. See <https://www.gnu.org/software/libc/manual/html_node/Malloc-Tunable-Parameters.html> for reference."
        },
        {
            "created_at": "2020-10-06T19:05:38.290Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17209047) by Ben Kietzman (bkietz):*\nI'm moving this out of the 2.0 release since we don't have a clean reproducer and it's evidently centos8-only. If we find a consistent reproducer then we can pull it back into 2.0"
        },
        {
            "created_at": "2020-10-07T16:45:40.780Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17209671) by Ashish Gupta (kgashish):*\nAnyone tried to reproduce on centos-8?"
        },
        {
            "created_at": "2020-10-07T16:50:06.729Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17209676) by Antoine Pitrou (apitrou):*\n`[~kgashish]` Can you try what I suggested above?"
        },
        {
            "created_at": "2020-10-07T17:17:00.562Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17209705) by Ashish Gupta (kgashish):*\nTried...\r\n\r\nexport MALLOC_MMAP_THRESHOLD_=65536\r\n\r\nsame error \"OSError: Out of memory: malloc of size 131072 failed\"\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-18T00:50:58.527Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17251419) by Weston Pace (westonpace):*\nI attempted to reproduce this on centos-8 and was not successful.\u00a0 I used an Amazon EC2 M4.large instance with the following AMI image (<https://aws.amazon.com/marketplace/pp/B08KYLN2CG)>\r\n\r\nSince the image only contains 8GB of RAM I used the smaller dataset example you posted.\r\n\r\nIronically the `read_works` method failed at `pd.concat` due to an out of RAM error (a legitimate one as pandas needed an additional 2.1 GB which was not available).\r\n\r\nThe `read_errors` method succeeded with `use_legacy_dataset` set to true or false.\r\n\r\nIt appears the core file you generated ran into some kind of 2GB limit.\u00a0 Since you have 256GB on the machine your core file could be quite large.\u00a0 Try following the advice here (<https://stackoverflow.com/questions/43341954/is-2g-the-limit-size-of-coredump-file-on-linux)> to see if you are able to make any further progress.\r\n\r\n\u00a0\r\n\r\n==Details of the test machine==\r\n\r\n\u00a0\r\n\r\n[centos@ip-172-30-0-34 ~]$ cat /etc/redhat-release \r\nCentOS Linux release 8.2.2004 (Core) \r\n[centos@ip-172-30-0-34 ~]$ uname -a\r\nLinux ip-172-30-0-34.ec2.internal 4.18.0-193.19.1.el8_2.x86_64 #1 SMP Mon Sep 14 14:37:00 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n[centos@ip-172-30-0-34 ~]$ cat /etc/os-release \r\nNAME=\"CentOS Linux\"\r\nVERSION=\"8 (Core)\"\r\nID=\"centos\"\r\nID_LIKE=\"rhel fedora\"\r\nVERSION_ID=\"8\"\r\nPLATFORM_ID=\"platform:el8\"\r\nPRETTY_NAME=\"CentOS Linux 8 (Core)\"\r\nANSI_COLOR=\"0;31\"\r\nCPE_NAME=\"cpe:/o:centos:centos:8\"\r\nHOME_URL=\"https://www.centos.org/\"\r\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\r\n\r\nCENTOS_MANTISBT_PROJECT=\"CentOS-8\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"8\"\r\nREDHAT_SUPPORT_PRODUCT=\"centos\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"8\"\r\n\r\n[centos@ip-172-30-0-34 ~]$ python3 -mpip freeze\r\nasn1crypto==0.24.0\r\nBabel==2.5.1\r\ncffi==1.11.5\r\nchardet==3.0.4\r\ncloud-init==19.4\r\nconfigobj==5.0.6\r\ncryptography==2.3\r\ndbus-python==1.2.4\r\ndecorator==4.2.1\r\ngpg==1.10.0\r\nidna==2.5\r\nJinja2==2.10.1\r\njsonpatch==1.21\r\njsonpointer==1.10\r\njsonschema==2.6.0\r\nMarkupSafe==0.23\r\nnetifaces==0.10.6\r\nnumpy==1.19.4\r\noauthlib==2.1.0\r\npandas==1.1.5\r\npciutils==2.3.6\r\nperf==0.1\r\nply==3.9\r\nprettytable==0.7.2\r\npyarrow==1.0.1\r\npycairo==1.16.3\r\npycparser==2.14\r\npygobject==3.28.3\r\nPyJWT==1.6.1\r\npyOpenSSL==18.0.0\r\npyserial==3.1.1\r\nPySocks==1.6.8\r\npython-dateutil==2.8.1\r\npython-dmidecode==3.12.2\r\npython-linux-procfs==0.6\r\npytz==2017.2\r\npyudev==0.21.0\r\nPyYAML==3.12\r\nrequests==2.20.0\r\nrhnlib==2.8.6\r\nrpm==4.14.2\r\nschedutils==0.6\r\nselinux==2.9\r\nsepolicy==1.1\r\nsetools==4.2.2\r\nsetroubleshoot==1.1\r\nsix==1.11.0\r\nslip==0.6.4\r\nslip.dbus==0.6.4\r\nsyspurpose==1.26.20\r\nsystemd-python==234\r\nurllib3==1.24.2"
        },
        {
            "created_at": "2020-12-21T09:37:06.836Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17252725) by Ashish Gupta (kgashish):*\nThanks for looking into this. I suspect the dataframe size is too small to show the error. Did you try gradually increasing the size by say 5% and try read_error?\u00a0"
        },
        {
            "created_at": "2020-12-23T01:27:23.603Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17253825) by Weston Pace (westonpace):*\nI was able to reproduce memory issues at 65000000 rows.\u00a0 However, this is always about the point where I would expect to see issues (since I was nearing the 8GB limit).\u00a0 I did not see the errors you were seeing because the system was overcomitting and triggering the OOM killer instead of hard stopping the allocation.\u00a0 Once I changed the overcommit mode to 2 (no overcommit) then I started getting malloc/realloc errors.\u00a0 I'm wondering if this may be related to the issues you are seeing.\u00a0 Can you share with me some more information?\r\n\r\nIs this running on a VM or a dedicated server?\r\n\r\nWhat is the output of `sysctl vm.overcommit_memory`?\r\n\r\nWhat is the output of `cat /proc/meminfo`?\r\n\r\nWhat is the output of `free`?"
        },
        {
            "created_at": "2020-12-23T10:04:04.503Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17254007) by Ashish Gupta (kgashish):*\nThis is a dedicated physical server.\r\n\r\nsysctl vm.overcommit_memory\r\nvm.overcommit_memory = 2\r\n\r\n\u00a0\r\n\r\ncat /proc/meminfo\r\nMemTotal: 263518320 kB\r\nMemFree: 34640640 kB\r\nMemAvailable: 247394700 kB\r\nBuffers: 52 kB\r\nCached: 217424924 kB\r\nSwapCached: 5308 kB\r\nActive: 175441652 kB\r\nInactive: 46026880 kB\r\nActive(anon): 3637200 kB\r\nInactive(anon): 540420 kB\r\nActive(file): 171804452 kB\r\nInactive(file): 45486460 kB\r\nUnevictable: 0 kB\r\nMlocked: 0 kB\r\nSwapTotal: 4194300 kB\r\nSwapFree: 3900668 kB\r\nDirty: 8 kB\r\nWriteback: 0 kB\r\nAnonPages: 4025572 kB\r\nMapped: 350944 kB\r\nShmem: 185972 kB\r\nKReclaimable: 3498500 kB\r\nSlab: 5042144 kB\r\nSReclaimable: 3498500 kB\r\nSUnreclaim: 1543644 kB\r\nKernelStack: 19744 kB\r\nPageTables: 27404 kB\r\nNFS_Unstable: 0 kB\r\nBounce: 0 kB\r\nWritebackTmp: 0 kB\r\nCommitLimit: 135953460 kB\r\nCommitted_AS: 6058728 kB\r\nVmallocTotal: 34359738367 kB\r\nVmallocUsed: 0 kB\r\nVmallocChunk: 0 kB\r\nPercpu: 82240 kB\r\nHardwareCorrupted: 0 kB\r\nAnonHugePages: 548864 kB\r\nShmemHugePages: 0 kB\r\nShmemPmdMapped: 0 kB\r\nHugePages_Total: 0\r\nHugePages_Free: 0\r\nHugePages_Rsvd: 0\r\nHugePages_Surp: 0\r\nHugepagesize: 2048 kB\r\nHugetlb: 0 kB\r\nDirectMap4k: 8881248 kB\r\nDirectMap2M: 137568256 kB\r\nDirectMap1G: 121634816 kB\r\n\r\n\u00a0\r\n\r\nfree\r\n total used free shared buff/cache available\r\nMem: 263518320 10488492 31968144 185980 221061684 244854292\r\nSwap: 4194300 293632 3900668\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-23T18:06:36.655Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17254171) by Weston Pace (westonpace):*\nI believe what is happening is that the `ParquetDataset` approach is using more memory.\u00a0 That is because the pyarrow.Table in-memory representation is larger than the pandas dataframe in-memory representation (in this case).\u00a0 Specifically for strings.\u00a0 Arrow is going to store each string as an array of bytes + 4 bytes so each instance of 'ABCDEFGH' is going to occupy 12 bytes of RAM.\u00a0 On the other hand, pandas is going to store an 8 byte pointer to an 8 byte string.\u00a0 If all the strings were different this would be mean 16 bytes per string but since they are all the same the string instance is shared so it is more or less 8 bytes.\r\n\r\nSo for the smaller table, arrow is using ~5.8GB while pandas is using ~4.4GB.\u00a0 This explains why it fails in 1.0.1 but does not explain...\r\n\r\n> \"256Gb memory way more than enough to load the data which requires < 10Gb\"\r\n\r\nFor this I think the problem is simply that your system is not allowing each process to use 256GB.\u00a0 With \"vm.overcommit_memory = 2\" the OS is going to avoid overcomitting entirely.\u00a0 In addition, a large portion of RAM (~120 GB) is reserved for the kernel (this is tunable and you might want to consider tuning it since this is a rather large amount to reserve for the kernel).\u00a0 The remaining 135953460KB (seen as CommitLimit in the meminfo) is shared across all processes.\u00a0 Since overcomitting is disabled this is tracking the reserved (not used) RAM from all processes.\r\n\r\nTo confirm all of this I suggest two tests.\r\n\r\n1) Confirm how much RAM is actually in use by python / pyarrow\r\n\r\nChange the last lines of your script to...\r\n\r\n\u00a0\r\n```java\n\r\ntry:\r\n    read_errors()\r\nexcept:\r\n    max_bytes = pa.default_memory_pool().max_memory()\r\n    input(\"Press Enter to continue...\")\r\n    print(f'Arrow bytes in use: {max_bytes}')\r\n    raise\r\n```\r\nThis will pause the program at the crash and allow you to inspect the memory.\u00a0 You can do this by looking up the process...\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```java\n\r\n(base) [centos@ip-172-30-0-34 ~]$ ps -eaf | grep -i python\r\nroot         880       1  0 Dec18 ?        00:00:41 /usr/libexec/platform-python -Es /usr/sbin/tuned -l -P\r\ncentos    228668  225417 90 17:45 pts/0    00:00:16 python experiment.py\r\ncentos    228680  228186  0 17:45 pts/1    00:00:00 grep --color=auto -i python\r\n```\r\n...and then lookup the RAM usage of the process.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```java\n\r\n(base) [centos@ip-172-30-0-34 ~]$ cat /proc/228668/status | grep -i vmdata\r\nVmData:\t 3445600 kB\r\n```\r\nIn addition, the experiment will print how many bytes arrow was using (to help distinguish from RAM used by the python heap and RAM reserved but not in use)...\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```java\n\r\nPress Enter to continue...^[[A\r\nArrow bytes in use: 2601828992\r\nTraceback (most recent call last):\r\n```\r\nSo, even though my system has 8GB of RAM because 4GB is reserved for the kernel, ~0.5 GB is in use by other processes, ~1GB is in use by python, only ~2.6GB remain for arrow.\r\n\r\n\u00a0\r\n\r\n2) Confirm how much RAM your system is currently allowing to be allocated.\r\n\r\nCompile and run the following simple program...\r\n```java\n\r\n#include <stdio.h>   \r\n#include <stdlib.h>int MBYTE = 1024*1024;int main(void) {  int mbytes_allocated = 0;\r\n  int ** pointers = malloc(MBYTE);\r\n  while(mbytes_allocated < MBYTE) {\r\n    int * pointer = malloc(MBYTE);\r\n    if (pointer == 0) {\r\n      for (int i = 0; i < mbytes_allocated; i++) {\r\n\tfree(pointers[i]);\r\n      }\r\n      break;\r\n    }\r\n    mbytes_allocated++;\r\n  }\r\n  printf(\"Allocated %d megabytes before failing\\n\", mbytes_allocated);\r\n}\r\n```\r\n```java\n\r\n\u00a0(base) [centos@ip-172-30-0-34 ~]$ gcc -o allocator allocator.c\r\n (base) [centos@ip-172-30-0-34 ~]$ ./allocator\r\n Allocated 3395 megabytes before failing\r\n```\r\nThis matches pretty closely with what we were seeing in python.\r\n\r\nAs for a fix, in the new datasets API (available starting in 1.1 but more fully in 2.0) you can use [scan](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html#pyarrow.dataset.Dataset.scan) which will allow you to convert to pandas incrementally and should have similar RAM usage to the first approach you had.\u00a0 You may also want to tune your OS as you are reserving quite a bit for the kernel and that might be too much.\u00a0 vm.overcommit_ratio defaults to 0.5 and that is often too aggressive for systems with large amounts of RAM.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-24T19:34:46.270Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17254642) by Ashish Gupta (kgashish):*\nIf the system memory limit is the issue, would it have worked on the same machine with the older version of pyarrow? My code was working perfectly fine with 0.13.0.\u00a0 Regarding tests you asked me...\r\n\r\n1) Confirm how much RAM is actually in use by python / pyarrow\r\n\r\n\u00a0 read_error crashes with a core dump, so I am not able to use try/except block.\r\n```java\n\r\npython test.py\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n what(): std::bad_alloc\r\nAborted (core dumped)\n```\r\n\u00a0\r\n\r\nHowever I was observed in top command that memory usage was around 5Gb when it crashed.\r\n\r\n\u00a0\r\n\r\n2)\u00a0./allocator\r\nAllocated 110641 megabytes before failing\r\n\r\n\u00a0\r\n\r\nWith read_works I checked the max memory required for my example is 15Gb. So given that 110 Gb is available and there is nothing else running it doesn't make sense.\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-25T02:09:57.017Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17254703) by Weston Pace (westonpace):*\n> If the system memory limit is the issue, would it have worked on the same machine with the older version of pyarrow? My code was working perfectly fine with 0.13.0.\r\n\r\nI understand, my theory was that this new version was using more RAM which was causing the issue.\u00a0 Right now, I would like to narrow down the problem between something on the system limiting your allocation and some bug in pyarrow causing a large spike in allocation and pushing it over the limit.\r\n\r\nSo I think it is important to know exactly how much RAM the process was using when it failed (for example, if it is exactly or very close to 4GB then that gives us a potential limit to look for.\u00a0 If there is some loop getting stuck and allocating memory really quickly then we'd see 110GB and it might not show in top because it happens so quick).\r\n\r\nIt sounds like your process crashes in a couple of different ways.\u00a0 If you get an OSError then you should be able to catch it with the python code I shared.\u00a0 If you are now consistently getting std::bad_alloc then you can still catch it using gdb.\u00a0 Unfortunately, gdb won't catch the OSError so it might be a bit of trial and error.\u00a0 It also sounds like I am not quite reproducing the same behavior you are seeing.\r\n\r\nI will continue to look into possibilities after the holiday.\u00a0 In the meantime, if you are able to figure out exactly how much RAM the process is using when it crashes it could be helpful."
        },
        {
            "created_at": "2020-12-28T00:20:59.235Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255332) by Weston Pace (westonpace):*\nOk.\u00a0 I think I've really tracked it down now.\u00a0 It appears the root cause is a combination of jemalloc, aggressive muzzy decay, and disabling overcommit.\u00a0 Jemalloc is tracking the issue [[https://github.com/jemalloc/jemalloc/issues/1328]|https://github.com/jemalloc/jemalloc/issues/1328].]\r\n\r\nJemmalloc ends up creating many many small mmaps and eventually the vm.max_map_count is reached.\u00a0 You can see the value of this limit here.\r\n\r\n\u00a0\r\n```java\n\r\n[centos@ip-172-30-0-42 ~]$ sysctl vm.max_map_count\r\nvm.max_map_count = 65530\r\n```\r\nTo confirm this is indeed your issue you will need to pause it at crash time (either using gdb or python's except/input as discussed above) and count the number of maps...\r\n\r\n\u00a0\r\n```java\n\r\n[centos@ip-172-30-0-42 ~]$ cat /proc/1829/maps | wc -l\r\n65532\r\n```\r\n(note, this approach will give an approximation of the # of maps and not the exact count, but it shouldn't be anywhere close to the limit under normal operation).\r\n\r\n\u00a0\r\n\r\n\u2014\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nThe preferred workaround per the jemalloc issue is to enable overcommit.\u00a0 You can configure the system to prioritize killing the process using arrow if the oom killer is too unpredictable.\r\n\r\n\u00a0\r\n\r\nIf overcommit must be disabled, for whatever reason, then you could always compile arrow without jemalloc.\r\n\r\n\u00a0\r\n\r\nFinally, in the issue listed above there is some configuration that is suggested, basically reducing the rate at which jemalloc returns pages to the OS.\u00a0 This jemalloc configuration would not be universally applicable however so it doesn't make sense for Arrow to change these defaults.\u00a0 These settings are also not configurable at the moment so this option isn't really possible given the current code."
        },
        {
            "created_at": "2020-12-28T00:23:03.463Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255333) by Weston Pace (westonpace):*\nAlso, this behavior was introduced between 0.13.0 and 1.0.1 because arrow changed how it configures jemalloc so that pages are returned to the OS more aggressively.\u00a0 For more details see ARROW-6994 (which was fixed in Arrow 0.16.0)."
        },
        {
            "created_at": "2020-12-28T10:32:48.749Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255514) by Ashish Gupta (kgashish):*\nLooks like that's the issue. Thanks!\r\n```java\n\r\ncat /proc/377933/maps | wc -l\r\n65532\n```\r\nI am able to pause read_errors with\u00a0use_legacy_dataset=True (throws an OSError), while\u00a0use_legacy_dataset=False crashes.\r\n\r\nSo if overcommit is enabled\u00a0vm.max_map_count will not increase that quickly?\r\n\r\nWhy this is not an issue on windows 10, read somewhere windows 10 doesn't overcommit memory?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-28T16:47:03.325Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255642) by Antoine Pitrou (apitrou):*\n> Why this is not an issue on windows 10, read somewhere windows 10 doesn't overcommit memory?\r\n\r\nIIRC, jemalloc isn't enabled on Windows, so we either use mimalloc or the system allocator there.\r\n\r\n> So if overcommit is enabled vm.max_map_count will not increase that quickly?\r\n\r\nIf I'm reading correctly, yes.\r\n\r\nNote that Arrow can use other memory allocators, in two different ways:\r\n\r\n1) you can disable jemalloc at compile-time by passing `-DARROW_JEMALLOC=OFF` to CMake\r\n\r\n2) you can pass custom memory pools at runtime: see <https://arrow.apache.org/docs/cpp/api/memory.html?highlight=mimalloc#memory-pools>\r\n\r\nRelated: ARROW-11009."
        },
        {
            "created_at": "2020-12-28T17:27:19.905Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255667) by Ashish Gupta (kgashish):*\nI will wait for ARROW-11009 to be resolved so I can switch memory allocators from an environment variable.\r\n\r\nMy knowledge of cpp is very limited, anything I can change in pyarrow installation or run time call to\u00a0use another memory allocators?\r\n\r\nThis is great analysis, a lot of learning. Thanks."
        },
        {
            "created_at": "2020-12-28T17:36:25.945Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255671) by Weston Pace (westonpace):*\nYou could try adding...\r\n\r\npa.jemalloc_set_decay_ms(10000)\r\n\r\nto the top of your script.\u00a0 I don't have time to test it today and I don't know that this would be a complete fix or if it would just postpone the issue but it should move behavior back to closer to what it was in 0.13.0."
        },
        {
            "created_at": "2020-12-28T18:15:59.974Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17255684) by Ashish Gupta (kgashish):*\nTried, didn't work."
        },
        {
            "created_at": "2021-01-12T20:23:00.669Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17263669) by Weston Pace (westonpace):*\nNow that ARROW-11049\u00a0 is finished I tried this out with the latest from master.\u00a0 I found that both the system memory allocator and the jemalloc memory allocator (the default) encountered this problem with the mmap limit.\r\n\r\nHowever, the mimalloc allocator does not encounter this issue.\u00a0 This means that you will need to install a version of pyarrow that has mimalloc enabled and you will need to add this to the top of your program, preferably before you do anything with pyarrow.\r\n```java\n\r\npa.set_memory_pool(pa.mimalloc_memory_pool())\r\n```"
        },
        {
            "created_at": "2021-01-12T20:45:54.565Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9974?focusedCommentId=17263685) by Antoine Pitrou (apitrou):*\nOk, there's probably no need to keep this issue open. In ARROW-11228 we'll add more tuning possibilities for the cases where our default jemalloc configuration isn't adequate."
        }
    ]
}