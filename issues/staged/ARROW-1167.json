{
    "issue": {
        "title": "[Python] Create chunked BinaryArray in Table.from_pandas when a column's data exceeds 2GB",
        "body": "***Note**: This issue was originally created as [ARROW-1167](https://issues.apache.org/jira/browse/ARROW-1167). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen writing a pyarrow Table (instantiated from a Pandas dataframe reading in a ~5GB CSV file) to a parquet file, the interpreter cores with the following stack trace from gdb:\n\n```Java\n#0  __memmove_avx_unaligned () at ../sysdeps/x86_64/multiarch/memcpy-avx-unaligned.S:181\n#1  0x00007fbaa5c779f1 in parquet::InMemoryOutputStream::Write(unsigned char const*, long) () from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#2  0x00007fbaa5c0ce97 in parquet::PlainEncoder<parquet::DataType<(parquet::Type::type)6> >::Put(parquet::ByteArray const*, int) ()\n   from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#3  0x00007fbaa5c18855 in parquet::TypedColumnWriter<parquet::DataType<(parquet::Type::type)6> >::WriteMiniBatch(long, short const*, short const*, parquet::ByteArray const*) ()\n   from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#4  0x00007fbaa5c189d5 in parquet::TypedColumnWriter<parquet::DataType<(parquet::Type::type)6> >::WriteBatch(long, short const*, short const*, parquet::ByteArray const*) ()\n   from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#5  0x00007fbaa5be0900 in arrow::Status parquet::arrow::FileWriter::Impl::TypedWriteBatch<parquet::DataType<(parquet::Type::type)6>, arrow::BinaryType>(parquet::ColumnWriter*, std::shared_ptr<arrow::Array> const&, long, short const*, short const*) () from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#6  0x00007fbaa5be171d in parquet::arrow::FileWriter::Impl::WriteColumnChunk(arrow::Array const&) () from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#7  0x00007fbaa5be1dad in parquet::arrow::FileWriter::WriteColumnChunk(arrow::Array const&) () from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#8  0x00007fbaa5be2047 in parquet::arrow::FileWriter::WriteTable(arrow::Table const&, long) () from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/libparquet.so.1\n#9  0x00007fbaa51e1f53 in __pyx_pw_7pyarrow_8_parquet_13ParquetWriter_5write_table(_object*, _object*, _object*) ()\n   from /home/ubuntu/.local/lib/python3.5/site-packages/pyarrow/_parquet.cpython-35m-x86_64-linux-gnu.so\n#10 0x00000000004e9bc7 in PyCFunction_Call () at ../Objects/methodobject.c:98\n#11 0x0000000000529885 in do_call (nk=<optimized out>, na=<optimized out>, pp_stack=0x7ffe6510a6c0, func=<optimized out>) at ../Python/ceval.c:4933\n#12 call_function (oparg=<optimized out>, pp_stack=0x7ffe6510a6c0) at ../Python/ceval.c:4732\n#13 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\n#14 0x000000000052d2e3 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\n#15 0x0000000000528eee in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe6510a8d0, func=<optimized out>) at ../Python/ceval.c:4813\n#16 call_function (oparg=<optimized out>, pp_stack=0x7ffe6510a8d0) at ../Python/ceval.c:4730\n#17 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\n#18 0x000000000052d2e3 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\n#19 0x0000000000528eee in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe6510aae0, func=<optimized out>) at ../Python/ceval.c:4813\n#20 call_function (oparg=<optimized out>, pp_stack=0x7ffe6510aae0) at ../Python/ceval.c:4730\n#21 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\n#22 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe6510ac10, func=<optimized out>) at ../Python/ceval.c:4803\n#23 call_function (oparg=<optimized out>, pp_stack=0x7ffe6510ac10) at ../Python/ceval.c:4730\n#24 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\n#25 0x0000000000528814 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=0x7ffe6510ad40, func=<optimized out>) at ../Python/ceval.c:4803\n#26 call_function (oparg=<optimized out>, pp_stack=0x7ffe6510ad40) at ../Python/ceval.c:4730\n#27 PyEval_EvalFrameEx () at ../Python/ceval.c:3236\n#28 0x000000000052d2e3 in _PyEval_EvalCodeWithName () at ../Python/ceval.c:4018\n#29 0x000000000052dfdf in PyEval_EvalCodeEx () at ../Python/ceval.c:4039\n#30 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:777\n#31 0x00000000005fd2c2 in run_mod () at ../Python/pythonrun.c:976\n#32 0x00000000005ff76a in PyRun_FileExFlags () at ../Python/pythonrun.c:929\n#33 0x00000000005ff95c in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:396\n#34 0x000000000063e7d6 in run_file (p_cf=0x7ffe6510afb0, filename=0x2161260 L\"scripts/parquet_export.py\", fp=0x226fde0) at ../Modules/main.c:318\n#35 Py_Main () at ../Modules/main.c:768\n#36 0x00000000004cfe41 in main () at ../Programs/python.c:65\n#37 0x00007fbadf0db830 in __libc_start_main (main=0x4cfd60 <main>, argc=2, argv=0x7ffe6510b1c8, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffe6510b1b8)\n    at ../csu/libc-start.c:291\n#38 0x00000000005d5f29 in _start ()\n```\n\nThis is occurring in a pretty vanilla call to `pq.write_table(table, output)`. Before the crash, I'm able to print out the table's schema and it looks a little odd (all columns are explicitly specified in `pandas.read_csv()` to be strings...\n\n```Java\n_id: string\nref_id: string\nref_no: string\nstage: string\nstage2_ref_id: string\norg_id: string\nclassification: string\nsolicitation_no: string\nnotice_type: string\nbusiness_category: string\nprocurement_mode: string\nfunding_instrument: string\nfunding_source: string\napproved_budget: string\npublish_date: string\nclosing_date: string\ncontract_duration: string\ncalendar_type: string\ntrade_agreement: string\npre_bid_date: string\npre_bid_venue: string\nprocuring_entity_org_id: string\nprocuring_entity_org: string\nclient_agency_org_id: string\nclient_agency_org: string\ncontact_person: string\ncontact_person_address: string\ntender_title: string\ndescription: string\nother_info: string\nreason: string\ncreated_by: string\ncreation_date: string\nmodified_date: string\nspecial_instruction: string\ncollection_contact: string\ntender_status: string\ncollection_point: string\ndate_available: string\nserialid: string\n__index_level_0__: int64\n-- metadata --\npandas: {\"index_columns\": [\"__index_level_0__\"], \"columns\": [{\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"_id\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"ref_id\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"ref_no\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"stage\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"stage2_ref_id\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"org_id\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"classification\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"solicitation_no\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"notice_type\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"business_category\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"procurement_mode\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"funding_instrument\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"funding_source\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"approved_budget\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"publish_date\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"closing_date\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"contract_duration\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"calendar_type\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"trade_agreement\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"pre_bid_date\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"pre_bid_venue\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"procuring_entity_org_id\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"procuring_entity_org\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"client_agency_org_id\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"client_agency_org\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"contact_person\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"contact_person_address\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"tender_title\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"description\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"other_info\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"reason\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"created_by\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"creation_date\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"modified_date\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"special_instruction\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"collection_contact\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"tender_status\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"collection_point\"}, {\"pandas_type\": \"mixed\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"date_available\"}, {\"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null, \"name\": \"serialid\"}, {\"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null, \"name\": \"__index_level_0__\"}], \"pandas_version\": \"0.19.2\"}\nSegmentation fault (core dumped)\n```",
        "created_at": "2017-06-29T16:38:16.000Z",
        "updated_at": "2017-07-19T17:36:22.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-07-19T13:17:30.000Z"
    },
    "comments": [
        {
            "created_at": "2017-06-29T17:35:55.154Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068672) by Wes McKinney (wesm):*\nWhat version of the software is this? Can you see if you can reproduce the failure with a debug build? A backtrace with debug symbols enabled would be helpful. If there's any way that one of us can repro the issues ourselves that would be very helpful"
        },
        {
            "created_at": "2017-06-29T17:37:31.041Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068674) by Wes McKinney (wesm):*\nAlso could you clarify how the schema is \"odd\"? It looks like some columns have both unicode and bytes objects. "
        },
        {
            "created_at": "2017-06-29T18:02:23.243Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068718) by Wes McKinney (wesm):*\nIt seems like this could be a manifestation of https://github.com/apache/parquet-cpp/pull/195; if so then one of the DCHECKs will get triggered in a debug build, and then we can try to figure out the underlying cause"
        },
        {
            "created_at": "2017-06-29T18:23:24.253Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068742) by Jeff Knupp (jeffknupp):*\nYeah, I thought the same thing re: https://github.com/apache/parquet-cpp/pull/195. It's \"odd\" because there shouldn't be/aren't any bytes objects. It's a plain-text CSV and I'd be shocked if there were even any non-ascii values in there. This is from the stock pyarrow 0.4.1 install from PyPI. I can do a debug build of the latest version and report what I find."
        },
        {
            "created_at": "2017-06-29T19:24:22.514Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068821) by Wes McKinney (wesm):*\nThere's another bug here (\"mixed\" is not a valid pandas_type in the metadata), reported in ARROW-1168"
        },
        {
            "created_at": "2017-06-29T20:35:45.054Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068944) by Phillip Cloud (cpcloud):*\n`[~jeffknupp]` Can you upload all or part of that CSV file?"
        },
        {
            "created_at": "2017-06-29T20:40:03.607Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16068952) by Jeff Knupp (jeff.knupp@enigma.com):*\nI can't upload the whole thing (it's > 5GB) but I can certainly upload a\nportion of it. Let me grab it and see if I can reproduce on a small portion\nof the file.\n\nOn Thu, Jun 29, 2017 at 4:36 PM, Phillip Cloud (JIRA) <jira@apache.org>\n\n"
        },
        {
            "created_at": "2017-06-30T18:51:57.506Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16070576) by Jeff Knupp (jeffknupp):*\nSmallest amount I could get it to core reliably on is 500,000 lines (1 GB uncompressed). Here is a link to a bzip2 compressed version: https://www.dropbox.com/s/hguhamz0gdv2uzv/test_data.csv.bz2?dl=0\n\nMD5 of uncompressed file is listed below:\n\nMD5 (test_data.csv) = 9a66139195677008b4fcb56468e19234"
        },
        {
            "created_at": "2017-07-01T00:15:36.615Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16070891) by Wes McKinney (wesm):*\nWith that data file I'm running the following code with master branches in a debug build and no core dump:\n\n```Java\nimport os\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nDATA_PATH = os.path.expanduser('~/Downloads/test_data.csv.bz2')\n\ndtypes = {'_id': str,\n          'approved_budget': str,\n          'business_category': str,\n          'calendar_type': str,\n          'classification': str,\n          'client_agency_org': str,\n          'client_agency_org_id': str,\n          'closing_date': str,\n          'collection_contact': str,\n          'collection_point': str,\n          'contact_person': str,\n          'contact_person_address': str,\n          'contract_duration': str,\n          'created_by': str,\n          'creation_date': str,\n          'date_available': str,\n          'description': str,\n          'funding_instrument': str,\n          'funding_source': str,\n          'modified_date': str,\n          'notice_type': str,\n          'org_id': str,\n          'other_info': str,\n          'pre_bid_date': str,\n          'pre_bid_venue': str,\n          'procurement_mode': str,\n          'procuring_entity_org': str,\n          'procuring_entity_org_id': str,\n          'publish_date': str,\n          'reason': str,\n          'ref_id': str,\n          'ref_no': str,\n          'serialid': str,\n          'solicitation_no': str,\n          'special_instruction': str,\n          'stage': str,\n          'stage2_ref_id': str,\n          'tender_status': str,\n          'tender_title': str,\n          'trade_agreement': str}\n\n\ndf = pd.read_csv(DATA_PATH, dtype=dtypes)\ntable = pa.Table.from_pandas(df)\n\npq.write_table(table, 'test.parquet')\n```\n\nI'm using pandas 0.18.1; I will try latest pandas version / release builds / pyarrow 0.4.1 later. Let me know if I should use different code"
        },
        {
            "created_at": "2017-07-02T02:47:51.201Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16071476) by Jeff Knupp (jeffknupp):*\nAh, it may be that the file isn't hitting whatever limit/line was causing the error, since I was actually building the file to aid in recreating https://github.com/pandas-dev/pandas/issues/16798. I'll post a link to the smallest file I can create that reproduces the error (though it may require most of the 3+ GB file)."
        },
        {
            "created_at": "2017-07-02T12:03:07.723Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16071612) by Jeff Knupp (jeffknupp):*\n`[~wesmckinn]` I've attached a link to a bzip2d version of the source file that reliably reproduces the issue. I tried to see if I could reproduce it with a subset of the file's data, but I was only able to get it to crash (stepping in increments of 1,000,000 lines) at 18,000,000 out of ~18,800,000 lines, so I am just posting the original file in its entirety. Let me know if you have issues reproducing.\n\nLink: [test_data.csv.bz2](https://www.dropbox.com/s/hguhamz0gdv2uzv/test_data.csv.bz2?dl=0)\n\nMD5 for uncompressed file:\n\n> MD5 (./test_data.csv) = 9f92942dab60d1fde04773d57759fce2"
        },
        {
            "created_at": "2017-07-02T22:01:10.138Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16071802) by Wes McKinney (wesm):*\nThanks. I'm able to reproduce; I will dig in and try to figure out the root cause"
        },
        {
            "created_at": "2017-07-02T22:15:10.667Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16071805) by Wes McKinney (wesm):*\nOK, I believe the root cause is that one of the columns in this dataset has over 2GB of string data in it, which is causing an undetected overflow in the int32 offsets in the underlying `BinaryArray` object. So there's a bunch of things that need to happen:\n\n- Detecting int32 overflow in BinaryBuilder (so constructing a malformed BinaryArray like this isn't possible)\n- Making sure such overflows are raised properly out of Table.from_pandas\n- Providing for chunked table construction in {{Table.from_pandas]} (which will help you fix this problem)\n\ncc `[~xhochy]`"
        },
        {
            "created_at": "2017-07-06T16:21:39.321Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16076822) by Jeff Knupp (jeffknupp):*\nSo `[~wesmckinn]`, pandas has the exact same bug (a bit easier to trigger) as reported here: https://github.com/pandas-dev/pandas/issues/16798. I tracked down where the allocation that triggers the issue is occurring and unsurprisingly it's when growing the buffer to accommodate the size of the data. I've confirmed that this, also, results in an integer overflow for the size to be allocated.\n\nNow, that's all well and good, but I'd actually like to fix all of these issues in the two projects. **Does it make sense to move to int64 to track buffer sizes**? We can still check for overflow, but this solves the underlying issue as well.\n\nLet me know what you think."
        },
        {
            "created_at": "2017-07-06T16:28:59.879Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16076837) by Wes McKinney (wesm):*\nWhat do you mean by \"Does it make sense to move to int64 to track buffer sizes?\" ? The problem in Arrow is different, I think \u2013 the variable length offsets are overflowing, the underlying memory buffers all use 64-bit integers. There is ARROW-750 to add string/binary types with 64-bit offsets, but in the meantime the easier route is to create a chunked array in Table.from_pandas rather than one huge array"
        },
        {
            "created_at": "2017-07-06T16:33:20.268Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16076847) by Jeff Knupp (jeffknupp):*\nAh, OK. I misunderstood. The pandas problem is somewhat similar but that one _is_ caused by the size of the type used to calculate memory (re)allocations. Nevermind! I'll ask this question on the pandas PR. I'll also look into implementing a chunked array in `Table.from_pandas`."
        },
        {
            "created_at": "2017-07-16T22:46:32.012Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16089168) by Wes McKinney (wesm):*\nMoving this to 0.5.0. Added the overflow checks in ARROW-1177 https://github.com/apache/arrow/pull/853 but I think we can resolve this temporarily by chunking the binary column when it gets too large in Table.from_pandas"
        },
        {
            "created_at": "2017-07-18T20:35:32.221Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16092142) by Wes McKinney (wesm):*\nWe thankfully aren't dumping core anymore after https://github.com/apache/arrow/commit/1541a08c795f03b2ce5884a00fc83d1fb79a448e\n\n```Java\n$ python dump.py \nTraceback (most recent call last):\n  File \"dump.py\", line 52, in <module>\n    table = pa.Table.from_pandas(df, timestamps_to_ms=True)\n  File \"pyarrow/table.pxi\", line 741, in pyarrow.lib.Table.from_pandas (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:33966)\n    names, arrays, metadata = _dataframe_to_arrays(\n  File \"pyarrow/table.pxi\", line 341, in pyarrow.lib._dataframe_to_arrays (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:30351)\n    array = Array.from_pandas(\n  File \"pyarrow/array.pxi\", line 180, in pyarrow.lib.Array.from_pandas (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:22099)\n    check_status(PandasObjectsToArrow(\n  File \"pyarrow/error.pxi\", line 58, in pyarrow.lib.check_status (/home/wesm/code/arrow/python/build/temp.linux-x86_64-3.5/lib.cxx:7392)\n    raise ArrowInvalid(message)\npyarrow.lib.ArrowInvalid: Invalid: BinaryArray cannot contain more than 2147483646 bytes, have 2147486096\n```\n\nI am going to change this JIRA's scope to include creating a chunked binary array"
        },
        {
            "created_at": "2017-07-19T03:33:16.629Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16092520) by Wes McKinney (wesm):*\nPR: https://github.com/apache/arrow/pull/867"
        },
        {
            "created_at": "2017-07-19T13:17:30.482Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16093053) by Wes McKinney (wesm):*\nIssue resolved by pull request 867\n<https://github.com/apache/arrow/pull/867>"
        },
        {
            "created_at": "2017-07-19T17:36:22.795Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1167?focusedCommentId=16093472) by Jeff Knupp (jeffknupp):*\nFantastic! Thanks very much `[~wesmckinn]`."
        }
    ]
}