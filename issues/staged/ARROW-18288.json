{
    "issue": {
        "title": "[GO]: pqarrow (github.com/apache/arrow/go/v9/parquet/pqarrow) cannot handle arrow's DICTIONARY field",
        "body": "***Note**: This issue was originally created as [ARROW-18288](https://issues.apache.org/jira/browse/ARROW-18288). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHey, Arrow Go Dev:\r\n\u00a0\r\nI was trying to save some arrow tables out to parquet files, with the help of\u00a0the \"[github.com/apache/arrow/go/v9/parquet/pqarrow](http://github.com/apache/arrow/go/v9/parquet/pqarrow)\" package. btw, it's generally a great design (of Arrow) and a great Go implementation.\u00a0\r\n\r\n\u00a0\r\nHowever, one issue sticks out: in my original arrow Table I have some DICTIONARY fields, which pqarrow does NOT currently support.\r\n\u00a0\r\nI would assume supporting them will be quite straightward: just \"denormalize\" the DICTIONARY value into corresponding values (string, Timestamp, etc), and it's up to the Parquet to do the right trick (using DICTIONARY encoding, etc).\u00a0\r\n\u00a0\r\nI would have done this conversion\u00a0on-the-fly by myself, by converting each DICTIONARY field into underlying values. However, the arrow table schema is dynamic and outside my control, and I need to iterate through fields\u00a0(maybe structs) to locate those) -> it would be much better if pqarrow can support this natively.\u00a0\r\n\u00a0\r\nCan anyone help? thanks!",
        "created_at": "2022-11-09T00:13:17.000Z",
        "updated_at": "2022-11-14T22:38:17.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Go",
            "Component: Parquet",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-11-14T22:38:17.499Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-18288?focusedCommentId=17634070) by Matthew Topol (zeroshade):*\nThis isn't quite as straightforward as denormalizing the values in order to ensure proper statistics handling and efficient propagation. (Sure, you could just naively denormalize and then write, but that could cause unnecessary copies and other inefficient handling). I've started working on this but ran into a couple snags where I will need to utilize and enhance the compute package. I'll have a few things up for this soon as I work it out as this is going to require:\r\n\r\n- Enabling proper casting from Dictionary types to values (unpacking dictionaries)\n- One of the following:\n  - Implementing hash kernels for the Compute module to efficiently perform a `unique` operation on the dictionary indexes to find the min/max for stats\n  - Implementing aggregation kernels to implement using MinMax to find the min-max on the dictionary array directly (more efficient than hash for uniqueness but will take longer / harder to do)."
        }
    ]
}