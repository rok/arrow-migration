{
    "issue": {
        "title": "[C++] Arrow type large_string cannot be written to Parquet type column descriptor",
        "body": "***Note**: This issue was originally created as [ARROW-10426](https://issues.apache.org/jira/browse/ARROW-10426). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen trying to write a dataset in parquet format, arrow errors with the message: \"Arrow type large_string cannot be written to Parquet type column descriptor\"\r\n```java\n\r\narrow::write_dataset(\r\n dataframe,\r\n \"/directory/\",\r\n \"parquet\",\r\n \"partitioning\" = c(\"col1\", \"col2\")\r\n)\r\n```\r\nThe dataframe in question is very large with one column containing the text of message board posts encoded in HTML.",
        "created_at": "2020-10-29T21:49:30.000Z",
        "updated_at": "2020-11-23T13:54:24.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: R",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-11-23T13:54:24.000Z"
    },
    "comments": [
        {
            "created_at": "2020-10-30T02:42:09.365Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10426?focusedCommentId=17223354) by Neal Richardson (npr):*\nDoing some digging in the source, the error message comes from https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc#L1504-L1508, and specifically it sounds like it's being thrown from https://github.com/apache/arrow/blob/master/cpp/src/parquet/column_writer.cc#L1797. Basically the C++ Parquet writer doesn't support Large\\* types, only the regular string/binary.\r\n\r\nIs this just not yet implemented, or is this something that Parquet does not support? [~emkornfield@gmail.com] `[~emkornfield]` `[~wesm]`\r\n\r\nFrom the R side, I suspect that ARROW-9293 will make this go away. If the data.frame -> Table conversion chunked the input, then this large_string type column would probably end up as regular string type and thus wouldn't fail to write to Parquet.\r\n\r\n"
        },
        {
            "created_at": "2020-10-30T03:22:50.580Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10426?focusedCommentId=17223386) by Micah Kornfield (emkornfield):*\nI think this is just an oversight.  There is a limit of 4 bytes for encoding the length of any one string"
        },
        {
            "created_at": "2020-10-30T18:44:48.997Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10426?focusedCommentId=17223842) by Wes McKinney (wesm):*\nI think this just needs to be implemented (with the caveat that very large values over 2GB cannot be written of course)"
        },
        {
            "created_at": "2020-11-23T13:54:24.301Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10426?focusedCommentId=17237383) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 8632\n<https://github.com/apache/arrow/pull/8632>"
        }
    ]
}