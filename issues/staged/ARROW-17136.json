{
    "issue": {
        "title": "[C++] HadoopFileSystem open_append_stream throwing an error if file does not exists",
        "body": "***Note**: This issue was originally created as [ARROW-17136](https://issues.apache.org/jira/browse/ARROW-17136). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nas per the document method, open_append_stream will create the file if does not exists. But when I try to append to the file in hdfs it is throwing an error like file, not found.\r\n\r\nhdfsOpenFile(/tmp/xyz.json): FileSystem#append((Lorg/apache/hadoop/fs/Path;)Lorg/apache/hadoop/fs/FSDataOutputStream;) error:\r\nRemoteException: Failed to append to non-existent file /tmp/xyz.json for client\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2639)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:487)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\njava.io.FileNotFoundException: Failed to append to non-existent file /tmp/xyz.json for client x.x.x.x\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2639)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:487)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1367)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1424)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1394)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem$5.doCall(DistributedFileSystem.java:423)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem$5.doCall(DistributedFileSystem.java:419)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:431)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:400)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1386)\r\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): Failed to append to non-existent file /tmp/xyz.json for client 10.128.8.11\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSDirAppendOp.appendFile(FSDirAppendOp.java:104)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2639)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:805)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:487)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.security.AccessController.doPrivileged(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at javax.security.auth.Subject.doAs(Subject.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1562)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Client.call(Client.java:1508)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.Client.call(Client.java:1405)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.sun.proxy.$Proxy9.append(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.append(ClientNamenodeProtocolTranslatorPB.java:403)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.reflect.Method.invoke(Method.java:498)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at com.sun.proxy.$Proxy10.append(Unknown Source)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1333)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1355)\r\n\u00a0 \u00a0 \u00a0 \u00a0 ... 8 more\r\nTraceback (most recent call last):\r\n\u00a0 File \"<stdin>\", line 1, in <module>\r\n\u00a0 File \"pyarrow/_fs.pyx\", line 738, in pyarrow._fs.FileSystem.open_append_stream\r\n\u00a0 File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n\u00a0 File \"pyarrow/error.pxi\", line 113, in pyarrow.lib.check_status\r\nFileNotFoundError: [Errno 2] Opening HDFS file '/tmp/xyz.json' failed. Detail: [errno 2] No such file or directory",
        "created_at": "2022-07-20T06:49:00.000Z",
        "updated_at": "2022-11-18T09:44:24.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-11-17T11:52:51.889Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17136?focusedCommentId=17635315) by Alenka Frim (alenka):*\nThank you for reporting the issue `[~asagarshinde]` .\r\n\r\nYes, there seems to be inconsistency between HadoopFileSystem implementation and its [base class FileSystem](https://arrow.apache.org/docs/dev/python/generated/pyarrow.fs.FileSystem.html#pyarrow.fs.FileSystem.open_append_stream), from where the docstring for `open_append_stream` is coming from. HadoopFileSystem doesn't create an empty file if the target doesn't exist.\r\n\r\nThis behaviour is coming form the external implementation of Hadoop. What could be done on our side is to adapt the same behaviour on the HadoopFileSystem implementation as in the base FileSystem class with changing the C++ `open_append_stream` check so that it would, in case of non-existent target, create a new empty file.\r\n\r\nI don't think this is a priority for now, but contributions are much welcomed.\r\n\r\n\u00a0"
        }
    ]
}