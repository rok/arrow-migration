{
    "issue": {
        "title": "[Python] Unable to read single child field of struct column from Parquet",
        "body": "***Note**: This issue was originally created as [ARROW-13151](https://issues.apache.org/jira/browse/ARROW-13151). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nGiven the following table\r\n```java\n\r\ndata = {\"root\": [[{\"addr\": {\"this\": 3, \"that\": 3}}]]}\r\ntable = pa.Table.from_pydict(data)\r\n```\r\nreading the nested column leads to an `pyarrow.lib.ArrowInvalid` error:\r\n```java\n\r\npq.write_table(table, \"/tmp/table.parquet\")\r\nfile = pq.ParquetFile(\"/tmp/table.parquet\")\r\narray = file.read([\"root.list.item.addr.that\"])\r\n```\r\nTraceback:\r\n```java\n\r\nTraceback (most recent call last):\r\n  File \"....\", line 21, in <module>\r\n    array = file.read([\"root.list.item.addr.that\"])\r\n  File \"/home/angus/.mambaforge/envs/awkward/lib/python3.9/site-packages/pyarrow/parquet.py\", line 383, in read\r\n    return self.reader.read_all(column_indices=column_indices,\r\n  File \"pyarrow/_parquet.pyx\", line 1097, in pyarrow._parquet.ParquetReader.read_all\r\n  File \"pyarrow/error.pxi\", line 97, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: List child array invalid: Invalid: Struct child array #0 does not match type field: struct<that: int64> vs struct<that: int64, this: int64>\r\n```\r\nIt's possible that I don't quite understand this properly - am I doing something wrong?",
        "created_at": "2021-06-23T17:52:23.000Z",
        "updated_at": "2021-10-15T08:56:29.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-10-15T08:56:29.000Z"
    },
    "comments": [
        {
            "created_at": "2021-06-24T14:20:46.976Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17368879) by Joris Van den Bossche (jorisvandenbossche):*\nReading the file itself seems to work OK:\r\n\r\n```Java\n\r\nIn [13]: file.read()\r\nOut[13]: \r\npyarrow.Table\r\nroot: list<item: struct<addr: struct<that: int64, this: int64>>>\r\n  child 0, item: struct<addr: struct<that: int64, this: int64>>\r\n      child 0, addr: struct<that: int64, this: int64>\r\n          child 0, that: int64\r\n          child 1, this: int64\r\n```\r\n\r\nBut it's the reading of only a specific child field that seems to be failing (I can reproduce the failure). \r\n\r\nI am not fully sure what is currently expected to be supported (whether we support reading a single field of a struct column).  cc `[~emkornfield]`\r\n\r\nThanks for the report!"
        },
        {
            "created_at": "2021-06-24T16:04:51.659Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17368946) by Micah Kornfield (emkornfield):*\nThis seems like a legitimate bug, I would guess this might occur when applying arrow schema back to the data. I forget if we have a flag exposed to turn that off as a work around."
        },
        {
            "created_at": "2021-06-24T16:17:28.032Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17368952) by Micah Kornfield (emkornfield):*\nAs an aside it is ugly that we need to include \"list.item\" in the path to read the single column."
        },
        {
            "created_at": "2021-06-25T14:52:56.444Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17369517) by Jim Pivarski (jpivarski):*\nI hope reading a single field of a struct column is supported! It's an important use-case for us.\r\n\r\nIn particle physics, our data consist of many collision events, each with a variable-length number of particles, and each particle is a struct with many fields. Often, there's even deeper structure than that, but this is the basic structure. These structs are very wide, with as many as a hundred fields, because the same dataset is used by 3000 authors, all doing different analyses on the same input dataset. Most individual data analysts don't access more than 10% of these struct fields.\r\n\r\nTherefore, it's important to be able to read the data lazily (in interactive analysis) or at least selectively (in high-throughput applications). Reading and decompressing data are often bottlenecks, so restricting data-loading to just the data we use is by itself a 10\u00d7 improvement. We have a custom file format (ROOT) that is designed to provide exactly this selective reading, but we've been looking at Parquet as a more cross-language and non-domain-specific alternative.\r\n\r\nThe bug that Angus reported arose in a framework that provides lazy-reading, Awkward Array's [ak.from_parquet](https://awkward-array.readthedocs.io/en/latest/_auto/ak.from_parquet.html) function, which uses pyarrow.parquet.ParquetFile to read the data and convert it to Arrow, then converts the Arrow into Awkward Arrays (which are highly interchangeable with Arrow Arrays; conversion in both directions is usually zero-copy). [This whole feature](https://github.com/scikit-hep/awkward-1.0/blob/1ecfc3e29aaf1b79cd7e0e8fa1598452f3827c64/src/awkward/operations/convert.py#L3122-L3959) was designed around the idea that you can read individual struct fields, just as you can read individual columns. Just today, I found out that's not true, even in our basic case that does not trigger errors like Angus's:\r\n\r\n>>> pq.write_table(pa.Table.from_pydict(\\{\"events\": [{\"muons\": [{\"pt\": 10.5, \"eta\": -1.5, \"phi\": 0.1}]}]}), \"/tmp/testy.parquet\")\r\n\r\n>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"events.muons.list.item.pt\"])\u00a0 \u00a0# reads all three\r\npyarrow.Table\r\nevents: struct<muons: list<item: struct<eta: double, phi: double, pt: double>>>\r\n\u00a0 child 0, muons: list<item: struct<eta: double, phi: double, pt: double>>\r\n\u00a0 \u00a0 child 0, item: struct<eta: double, phi: double, pt: double>\r\n\u00a0 \u00a0 \u00a0 child 0, eta: double\r\n\u00a0 \u00a0 \u00a0 child 1, phi: double\r\n\u00a0 \u00a0 \u00a0 child 2, pt: double\r\n>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"events.muons.list.item.eta\"])\u00a0 \u00a0# reads all three\r\npyarrow.Table\r\nevents: struct<muons: list<item: struct<eta: double, phi: double, pt: double>>>\r\n\u00a0 child 0, muons: list<item: struct<eta: double, phi: double, pt: double>>\r\n\u00a0 \u00a0 child 0, item: struct<eta: double, phi: double, pt: double>\r\n\u00a0 \u00a0 \u00a0 child 0, eta: double\r\n\u00a0 \u00a0 \u00a0 child 1, phi: double\r\n\u00a0 \u00a0 \u00a0 child 2, pt: double\r\n>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"events.muons.list.item.phi\"])\u00a0 \u00a0# reads all three\r\npyarrow.Table\r\nevents: struct<muons: list<item: struct<eta: double, phi: double, pt: double>>>\r\n\u00a0 child 0, muons: list<item: struct<eta: double, phi: double, pt: double>>\r\n\u00a0 \u00a0 child 0, item: struct<eta: double, phi: double, pt: double>\r\n\u00a0 \u00a0 \u00a0 child 0, eta: double\r\n\u00a0 \u00a0 \u00a0 child 1, phi: double\r\n\u00a0 \u00a0 \u00a0 child 2, pt: double\r\n\r\nI hadn't realized that our attempts to read only \"muon pt\" or only \"muon eta\" were, in fact, reading all muon fields. (In the real datasets, muons have 32 fields, electrons have 47, taus have 37, jets have 30, photons have 27...)\r\n\r\nWe could try to rearrange data to something shallower:\r\n\r\n`>>> pq.write_table(pa.Table.from_pydict(\\{\"muons\": [{\"pt\": 10.5, \"eta\": -1.5, \"phi\": 0.1}]}), \"/tmp/testy.parquet\")`\r\n`>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"muons.pt\"])`\r\n`pyarrow.Table`\r\n`muons: struct<pt: double>`\r\n`\u00a0 child 0, pt: double`\r\n`>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"muons.eta\"])`\r\n`pyarrow.Table`\r\n`muons: struct<eta: double>`\r\n`\u00a0 child 0, eta: double`\r\n`>>> pq.ParquetFile(\"/tmp/testy.parquet\").read([\"muons.phi\"])`\r\n`pyarrow.Table`\r\n`muons: struct<phi: double>`\r\n`\u00a0 child 0, phi: double`\r\n\r\nbut that puts a hard-to-predict constraint on data structures. In the above, aren't we \"reading a single column of a struct column?\" (I probably saw this behavior and assumed that it would continue to deeper structures, which is how I never noticed that they sometimes read all struct fields.)\r\n\r\nAs a real-world case, here's a dataset that naturally has a structure that suffers from over-reading. It's not physics-related: it's a translation of the [Million Song Dataset](http://millionsongdataset.com/) into Parquet (side-note: it's losslessly 3\u00d7 smaller than the original HDF5 files because of all the variable-length data): s3://pivarski-princeton/millionsongs/ . Lazily loading it has odd performance characteristics that I hadn't measured in detail until now:\r\n\r\nIn [1]: import awkward as ak\r\n\r\nIn [2]: songs = ak.from_parquet(\"/home/jpivarski/storage/data/million-song-datas\r\n ...: et/full/millionsongs/millionsongs-A-zstd.parquet\", lazy=True)\r\n\r\nIn [3]: %time songs.analysis.segments.loudness_start\r\nCPU times: user 19.1 ms, sys: 0 ns, total: 19.1 ms\r\nWall time: 18.8 ms\r\nOut[3]: <Array [[-60, -22.7, -23, ... -38, -37.5]] type='39100 \\* var \\* float64'>\r\n\r\nIn [4]: %time songs.analysis.segments.loudness_max\r\nCPU times: user 3.97 ms, sys: 14 \u00b5s, total: 3.98 ms\r\nWall time: 4.03 ms\r\nOut[4]: <Array [[-20.6, -21.1, ... -35.6, -35.6]] type='39100 \\* var \\* float64'>\r\n\r\nIn [5]: %time songs.analysis.segments.pitches\r\nCPU times: user 4.2 ms, sys: 0 ns, total: 4.2 ms\r\nWall time: 4.2 ms\r\nOut[5]: <Array [[[0.294, 0.158, ... 0.437, 0.065]]] type='39100 \\* var \\* var \\* float64'>\r\n\r\nIn [6]: %time songs.analysis.segments.timbre\r\nCPU times: user 4.05 ms, sys: 53 \u00b5s, total: 4.1 ms\r\nWall time: 4.11 ms\r\nOut[6]: <Array [[[18, 71.3, 193, ... -34.8, 2.66]]] type='39100 \\* var \\* var \\* float64'>\r\n\r\nThe \"pitches\" and \"timbre\" fields are much larger than the \"loudness_start\" and \"loudness_max\" (note the deeper nesting), but the \"loudness_start\" takes 4\u00d7 longer to load because that is triggering a read of everything in the whole \"segments\" struct. This depth of nestedness is necessary because \"segments\" have to be differentiated from \"bars,\" \"beats,\" \"sections,\" \"tatums,\" and the \"analysis\" level has to be differentiated from \"metadata\"\u2014song names, artist names, tags\u2014all of which are nested with different cardinalities. Restructuring this to have individually readable fields be separate top-level table columns would mean managing a lot of relationships using naming conventions, rather than the tree structure that more naturally fits the data.\r\n\r\nSo\u2014the point of this long comment is that reading individual struct fields would be a very, very, very nice feature to have. Without it, we'll have to detect the unsupported cases and tell our users to rearrange their data to fit the supported ones through naming conventions."
        },
        {
            "created_at": "2021-06-25T15:59:57.144Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17369562) by Micah Kornfield (emkornfield):*\nIt should be very much supported.\u00a0 Like I said this is a bug.\u00a0 It will take some tracing to figure out why it is occurring.\u00a0\u00a0"
        },
        {
            "created_at": "2021-06-25T16:35:45.544Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17369582) by Jim Pivarski (jpivarski):*\nGreat, thank you! I see now that your calling it a \"bug\" was commenting on\u00a0Joris's question about whether it ought to be supported, and that's what I was responding to.\r\n\r\nWhen this is fixed, it will be a new minimum version of Arrow for us because of its importance in our work.\r\n\r\n(As a side-note, if you do change the ugly \"list.item\" access, we'll have to adjust because of course we're generating column names to request them like that. So if that changes, we'll definitely need to pin a minimum Arrow version because the new names would be incompatible. I'd prefer it not to; and after all, it's what's in the Parquet schema. Maybe \"synonyms\" could hide that feature from high-level users, though that complicates the interface.)"
        },
        {
            "created_at": "2021-07-05T16:57:06.048Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17374935) by Micah Kornfield (emkornfield):*\n> As a side-note, if you do change the ugly \"list.item\" access\r\nYeah this would be a seperate change, and I think we could make it be backwards compatible.\u00a0 \u00a0 This would be a separate issue.\u00a0 The reason why removing list.item makes sense is that there are legacy encodings that encode lists in different ways, it would be nice to have a uniform access to these different types (i.e. schema evolution), I'll open up a separate item to track this and we can have more discussion there, I don't expect anyone would take this up any time soon."
        },
        {
            "created_at": "2021-09-26T05:32:08.060Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17420211) by Micah Kornfield (emkornfield):*\nLooking at this the problem is we do not propagate filtered fields through lists or nested structs (only one level of structs)."
        },
        {
            "created_at": "2021-10-07T05:45:02.164Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17425351) by Micah Kornfield (emkornfield):*\nWith the PR that is up this now seems to work:\r\n```java\n\r\n\r\n>>> data = {\"root\": [[\\{\"addr\": {\"this\": 3, \"that\": 3}}]]}\r\n >>> table = pa.Table.from_pydict(data)\r\n >>> pq.write_table(table, \"/tmp/table.parquet\")\r\n >>> file = pq.ParquetFile(\"/tmp/table.parquet\")\r\n >>> array = file.read([\"root.list.item.addr.that\"])\r\n >>> array\r\n pyarrow.Table\r\n root: list<item: struct<addr: struct<that: int64>>>\r\n child 0, item: struct<addr: struct<that: int64>>\r\n child 0, addr: struct<that: int64>\r\n child 0, that: int64\r\n----\r\nroot: [[ \u2013 is_valid: all not null\r\n \u2013 child 0 type: struct<that: int64>\r\n \u2013 is_valid: all not null\r\n \u2013 child 0 type: int64\r\n [\r\n 3\r\n ]]]\r\n\u00a0\n```\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-10-14T13:10:12.188Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17428799) by Antoine Pitrou (apitrou):*\nIt seems the PR was merged, does this issue still need to remain unresolved?"
        },
        {
            "created_at": "2021-10-15T08:56:14.958Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13151?focusedCommentId=17429192) by Joris Van den Bossche (jorisvandenbossche):*\nYes, the PR fixed this. Not sure why the JIRA was not automatically resolved on merge."
        }
    ]
}