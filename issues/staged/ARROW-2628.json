{
    "issue": {
        "title": "[Python] parquet.write_to_dataset is memory-hungry on large DataFrames",
        "body": "***Note**: This issue was originally created as [ARROW-2628](https://issues.apache.org/jira/browse/ARROW-2628). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nSee discussion in https://github.com/apache/arrow/issues/1749. We should consider strategies for writing very large tables to a partitioned directory scheme. ",
        "created_at": "2018-05-22T00:39:41.000Z",
        "updated_at": "2020-09-09T12:57:36.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2019-04-29T12:18:26.078Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2628?focusedCommentId=16829198) by Joris Van den Bossche (jorisvandenbossche):*\nSimilar report in ARROW-2709, which had a PR linked: https://github.com/apache/arrow/pull/3344. This PR is closed (without merging), but contains some relevant discussion.\r\n\r\nThe current implementation of `pyarrow.parquet.write_to_dataset` converts the pyarrow Table to a pandas DataFrame, and then uses pandas' groupby method to split it in multiple dataframes (after dropping the partition columns from the dataframe, which makes yet another data copy). Each subset pandas DataFrame is then converted back to a pyarrow Table. \r\nIn addition, when using this functionality from pandas' `to_parquet`, you get an additional initial conversion of the pandas DataFrame to arrow Table. \r\n\r\nThis clearly is less than optimal. It might be that some of the copies could be avoided (e.g. it is not clear to me if `Table.from_pandas` always copies data). The closed PR tried to circumvent this by using arrow's dictionary encoding instead of pandas' groupby, and then reconstructing the subset Tables based on those indices). But ideally, a more arrow-native solution is used instead of those work-arounds.\r\n\r\nTo quote `[~wesmckinn]` from the PR ([github comment](https://github.com/apache/arrow/pull/3344#issuecomment-462093173)):\r\n\r\n> I want the proper C++ work completed instead. Here are the steps:\n> \n> - Struct hash: ARROW-3978\n> - Integer argsort part of ARROW-1566\n> - Take function ARROW-772\n> \n> These are the three things you need to do the groupby-split operation natively against an Arrow table. There is a slight complication which is implementing \"take\" against chunked arrays. This could be mitigated by doing the groupby-split at the contiguous record batch level\r\n"
        },
        {
            "created_at": "2020-03-16T16:38:46.799Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2628?focusedCommentId=17060363) by Wes McKinney (wesm):*\ncc `[~jorisvandenbossche]` \u2013 this will be an important dataset use case"
        }
    ]
}