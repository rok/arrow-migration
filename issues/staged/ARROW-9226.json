{
    "issue": {
        "title": "[Python] pyarrow.fs.HadoopFileSystem - retrieve options from core-site.xml or hdfs-site.xml if available",
        "body": "***Note**: This issue was originally created as [ARROW-9226](https://issues.apache.org/jira/browse/ARROW-9226). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n'Legacy' pyarrow.hdfs.connect was somehow able to get the namenode info from the hadoop configuration files.\r\n\r\nThe new pyarrow.fs.HadoopFileSystem requires the host to be specified.\r\n\r\nInferring this info from \"the environment\" makes it easier to deploy pipelines.\r\n\r\nBut more important, for HA namenodes it is almost impossible to know for sure what to specify.\u00a0If a rolling restart is ongoing, the namenode is changing. There is no guarantee on which will be active in a HA setup.\r\n\r\nI tried connecting to the standby namenode. The connection gets established, but when writing a file an error is raised that standby namenodes are not allowed to write to.\r\n\r\n\u00a0",
        "created_at": "2020-06-25T11:50:29.000Z",
        "updated_at": "2021-08-12T18:29:44.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-08-12T18:29:34.000Z"
    },
    "comments": [
        {
            "created_at": "2020-06-25T16:23:33.797Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17145070) by Antoine Pitrou (apitrou):*\n> 'Legacy' pyarrow.hdfs.connect was somehow able to get the namenode info from the hadoop configuration files.\r\n\r\nCan you elaborate? Did it happen automatically or did you have to pass an option?"
        },
        {
            "created_at": "2020-06-30T13:41:34.478Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17148682) by Bruno Quinart (bquinart):*\nNo explicit options. However following 4 environment variables need to be set (showing the values for our Cloudera CDH5 setup):\r\n \\* HADOOP_HOME =\u00a0/opt/cloudera/parcels/CDH\r\n \\* CLASSPATH = output of 'hadoop classpath --glob'\r\n \\* ARROW_LIBHDFS_DIR =\u00a0/opt/cloudera/parcels/CDH/lib64\r\n \\* JAVA_HOME =\u00a0/usr/java/default\r\n\r\nI am not sure if it is the first or the second that allows finding the 'default' hdfs info.\r\n\r\nThe python docs mention this default for host (refer to\u00a0<https://arrow.apache.org/docs/python/generated/pyarrow.hdfs.connect.html>\u00a0-\u00a0_Set to \"default\" for fs.defaultFS from core-site.xml._)\r\n\r\nThis allows a simple\u00a0fs = pa.hdfs.connect() statement."
        },
        {
            "created_at": "2020-07-11T22:44:03.990Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17156158) by Wes McKinney (wesm):*\nThis issue isn't assigned so unfortunately I think it's going to miss the 1.0.0 release"
        },
        {
            "created_at": "2020-10-29T15:55:00.010Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17222981) by Duan Shiqiang (duanshiqiang):*\nHaving the new FileSystem support connect to HDFS without explicitly specify host and port and more importantly supports HA namenodes would be really helpful in a lot of\u00a0scenarios."
        },
        {
            "created_at": "2021-03-17T02:33:28.384Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17303037) by wondertx (wondertx):*\nIf HA support is not supported by `fs.HadoopFileSystem`, `pyarrow.hdfs.connect` cannot be simply replaced"
        },
        {
            "created_at": "2021-03-17T08:53:34.469Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17303201) by Antoine Pitrou (apitrou):*\n`[~wondertx]` I don't think anybody on the current Arrow team has enough HDFS expertise for this, so it won't happen unless someone contributes the code.\r\n\r\nDo you want to submit a PR ? https://arrow.apache.org/docs/developers/contributing.html"
        },
        {
            "created_at": "2021-06-22T12:46:59.328Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17367293) by Antoine Pitrou (apitrou):*\n`[~icook]` Your advice would be appreciated."
        },
        {
            "created_at": "2021-06-22T14:33:57.929Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17367395) by Antoine Pitrou (apitrou):*\ncc `[~aucahuasi]` you may be interested too."
        },
        {
            "created_at": "2021-08-10T19:14:00.094Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17396846) by Itamar Turner-Trauring (itamarst):*\nDigging through the code, it doesn't seem like this logic was ever implemented in Arrow itself; deep down enough, it's logic from `libhdfs`/`libhdfs3`. If I read this correctly, since the new API still uses those underneath, it's probably just a matter of (re)exposing the low-level logic in the Arrow wrapper."
        },
        {
            "created_at": "2021-08-10T19:16:53.859Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17396847) by Antoine Pitrou (apitrou):*\n`[~itamarst]` Could you explain how this logic can be exposed? Most of us here don't have any deep HDFS knowledge."
        },
        {
            "created_at": "2021-08-10T19:42:38.386Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17396856) by Itamar Turner-Trauring (itamarst):*\nLooking through the code\u2014\r\n\r\n\\***The deprecated API:**\\*\r\n\r\n1. `pyarrow.hdfs` imports from `_hdfsio.pyx`.\r\n2. This is thin wrapper around `CIOHadoopFileSystem` and `HdfsConnectionConfig`.\r\n3. The former is wrapper around `arrow::io::HadoopFileSystem` (see `libarrow.pxd`).\r\n \r\n\\***The new API:**\\*\r\n\r\n1. `pyarrow.fs` imports from `_hdfs.pyx`\r\n2. This builds on Cython classes `CHdfsOptions` and `CHadoopFileSystem`, with very small amount of wrapper code.\r\n3. These are synonyms for C++ classes `arrow::fs::HdfsOptions` and `arrow::fs::HadoopFilesystem` (see `libarrow_fs.pxd`).\r\n\r\n\u2014\r\n\r\nLooking through the old code, the connection code path is in `cpp/src/arrow/io/hdfs.cc`, and mostly interacts with the driver, which is in either from `libhdfs` or `libhdfs3` via a shim (https://github.com/apache/arrow/blob/7b66f97330215fe020ec536671ee50f41aa1af35/cpp/src/arrow/io/hdfs_internal.h), which `dlopen()`s the underlying `libhdfs` library at least.\r\n\r\n... and `libhdfs` then calls into Java implementation.\r\n\r\nFurther digging suggests that the new code path (`arrow::fs::HadoopFilesystem`) still uses the `libhdfs/libhdfs3` drivers from `arrow::io`:\r\n\r\nhttps://github.com/apache/arrow/blob/7b66f97330215fe020ec536671ee50f41aa1af35/cpp/src/arrow/filesystem/hdfs.cc#L56\r\n\r\nGiven all the heavy lifting seems to be done by the underlying libraries, this suggests this functionality could be exposed again, and the issue is less implementing the logic, and more just exposing the underlying API again.\r\n\r\nI am probably going to try to do this."
        },
        {
            "created_at": "2021-08-11T18:38:59.071Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17397553) by Itamar Turner-Trauring (itamarst):*\nAs you can see in the PR, the fix did turn out to be pretty simple: https://github.com/apache/arrow/pull/10917"
        },
        {
            "created_at": "2021-08-12T18:29:34.833Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9226?focusedCommentId=17398219) by Antoine Pitrou (apitrou):*\nIssue resolved by pull request 10917\n<https://github.com/apache/arrow/pull/10917>"
        }
    ]
}