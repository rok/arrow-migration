{
    "issue": {
        "title": "Certain moderately-sized (~100MB) default-Snappy-compressed Parquet files take enormous memory and long time to load by pyarrow.parquet.read_table",
        "body": "***Note**: This issue was originally created as [ARROW-6796](https://issues.apache.org/jira/browse/ARROW-6796). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nMy Spark workloads produce small-to-moderately-sized Parquet files with typical on-disk sizes in the order of 100-300MB, and I use PyArrow to process these files further.\r\n\r\nSurprisingly, I find that similarly-sized Parquet files sometimes take vastly different amounts of memory and time to load using pyarrow.parquet.read_table. For illustration, I've uploaded 2 such parquet files to\u00a0s3://public-parquet-test-data/fast.snappy.parquet and\u00a0s3://public-parquet-test-data/slow.snappy.parquet.\r\n\r\nBoth files have about 1.2 million rows and 450 columns and occupy 100-120MB on disk. But when they are loaded by read_table:\r\n \\* fast.snappy.parquet takes 10-15GB of memory and 5-8s to load\r\n \\* slow.snappy.parquet takes up to 300GB (!!) of memory and 45-60s to load\r\n\r\nSince I have been using the default Snappy compression in all my Spark jobs, it is unlikely that the files differ in the their compression levels. That the on-disk sizes are similar suggests that they are similarly compressed. So the fact that slow.snappy.parquet takes 10-20x amounts of resources to read is very surprising.\r\n\r\nMy benchmarking code snippet is below. I'd appreciate your help to troubleshoot this matter.\r\n\r\n```{python}\r\nfrom pyarrow.parquet import read_metadata, read_table\r\nfrom time import time\r\nfrom tqdm import tqdm\r\n\u200b\r\n\u200b\r\nFAST_PARQUET_TMP_PATH = '/tmp/fast.snappy.parquet'\r\nSLOW_PARQUET_TMP_PATH = '/tmp/slow.snappy.parquet'\r\n\u200b\r\n\u200b\r\nfast_parquet_metadata = read_metadata(FAST_PARQUET_TMP_PATH)\r\nprint('Fast Parquet Metadata: {}\\n'.format(fast_parquet_metadata))\r\n\u200b\r\ndurations = []\r\nfor _ in tqdm(range(3)):\r\n    tic = time()\r\n    tbl = read_table(\r\n            source=FAST_PARQUET_TMP_PATH,\r\n            columns=None,\r\n            use_threads=True,\r\n            metadata=None,\r\n            use_pandas_metadata=False,\r\n            memory_map=False,\r\n            filesystem=None,\r\n            filters=None)\r\n    toc = time()\r\n    durations.append(toc-tic)\r\nprint('Fast Parquet READ_TABLE(...) Durations: {}\\n'\r\n      .format(', '.join('{:.0f}s'.format(duration) for duration in durations)))\r\n\u200b\r\n\u200b\r\nslow_parquet_metadata = read_metadata(SLOW_PARQUET_TMP_PATH)\r\nprint('Slow Parquet Metadata: {}\\n'.format(slow_parquet_metadata))\r\n\u200b\r\ndurations = []\r\nfor _ in tqdm(range(3)):\r\n    tic = time()\r\n    tbl = read_table(\r\n            source=SLOW_PARQUET_TMP_PATH,\r\n            columns=None,\r\n            use_threads=True,\r\n            metadata=None,\r\n            use_pandas_metadata=False,\r\n            memory_map=False,\r\n            filesystem=None,\r\n            filters=None)\r\n    toc = time()\r\n    durations.append(toc - tic)\r\nprint('Slow Parquet READ_TABLE(...) Durations: {}\\n'\r\n      .format(', '.join('{:.0f}s'.format(duration) for duration in durations)))\r\n```",
        "created_at": "2019-10-05T07:00:28.000Z",
        "updated_at": "2019-10-06T20:22:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-10-06T20:22:38.000Z"
    },
    "comments": [
        {
            "created_at": "2019-10-05T13:36:38.802Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6796?focusedCommentId=16945053) by Micah Kornfield (emkornfield@gmail.com):*\nThank\u00a0 for the report.\u00a0 There was at least one known memory issues in 0.14.1 (ARROW-6060).\u00a0 \u00a00.15.0 is being released now (available on pypi).\u00a0 Can you check if this is still an issue with the new version?"
        },
        {
            "created_at": "2019-10-06T14:55:18.581Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6796?focusedCommentId=16945365) by V Luong (MBALearnsToCode):*\n[~emkornfield@gmail.com] thank you very much. Yes, indeed this particular problem seems to have been resolved in 0.15 :)"
        },
        {
            "created_at": "2019-10-06T14:55:59.687Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6796?focusedCommentId=16945366) by V Luong (MBALearnsToCode):*\nResolved in 0.15.0, which has fixed a number of memory-related problems"
        },
        {
            "created_at": "2019-10-06T20:22:38.922Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6796?focusedCommentId=16945440) by Wes McKinney (wesm):*\nDup of ARROW-6060"
        }
    ]
}