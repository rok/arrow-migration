{
    "issue": {
        "title": "[C++] Change parquet::arrow::FileReader::ReadRowGroups to read into contiguous arrays",
        "body": "***Note**: This issue was originally created as [ARROW-3774](https://issues.apache.org/jira/browse/ARROW-3774). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nInstead of creating a chunk per RowGroup, we should read at least for primitive type into a single, pre-allocated Array. This needs some new functionality in the Record reader classes and thus should be done after https://github.com/apache/parquet-cpp/pull/462 is merged.",
        "created_at": "2018-08-19T14:27:27.000Z",
        "updated_at": "2018-12-13T03:19:49.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2018-12-13T03:18:14.000Z"
    },
    "comments": [
        {
            "created_at": "2018-12-13T03:18:14.688Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3774?focusedCommentId=16719704) by Wes McKinney (wesm):*\nI'm not sure this is really useful. Closing"
        },
        {
            "created_at": "2018-12-13T03:19:49.809Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3774?focusedCommentId=16719705) by Wes McKinney (wesm):*\nThe main use case would be for pandas (where things need to be contiguous), but there memory will have to be copied in general when calling `pyarrow.Table.to_pandas`, so the benefits of this optimization would be minimal, if any. Producing large contiguous arrays could even be more expensive than the current behavior of creating chunked arrays"
        }
    ]
}