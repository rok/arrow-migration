{
    "issue": {
        "title": "[C++][Parquet] Reading int96 timestamps out-of-bounds for ns resolution wrap around",
        "body": "***Note**: This issue was originally created as [ARROW-14891](https://issues.apache.org/jira/browse/ARROW-14891). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nGiven a parquet file with a int96 date 9999-12-31 on it (which does not fit in an i64 ns) is read as a \"wrapped\", resulting in the date 1816-03-29 05:56:08.066277376.\r\n\r\nSpark seems to discard the nanoseconds and only read int96 to micros, which gives them a 1000x of dates (which happens to cover the 9999, but not others). There is a long discussion over this issue here: https://github.com/apache/arrow-rs/issues/982 including a MWE for pyarrow.\r\n",
        "created_at": "2021-11-26T15:46:39.000Z",
        "updated_at": "2021-12-16T17:12:25.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-11-29T13:51:05.333Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14891?focusedCommentId=17450470) by Joris Van den Bossche (jorisvandenbossche):*\nA small reproducer (without relying on the provided file):\r\n\r\n```python\n\r\nimport datetime\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\n\r\ntable = pa.table({'col': [datetime.datetime(9999, 12, 31)]})\r\npq.write_table(table, \"test_int96.parquet\", use_deprecated_int96_timestamps=True)\r\n\r\nIn [5]: pq.read_table(\"test_int96.parquet\")\r\nOut[5]: \r\npyarrow.Table\r\ncol: timestamp[ns]\r\n----\r\ncol: [[1816-03-29 05:56:08.066277376]]\r\n```"
        }
    ]
}