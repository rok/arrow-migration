{
    "issue": {
        "title": "PyArrow SIGSEGV error when using UnionDatasets",
        "body": "***Note**: This issue was originally created as [ARROW-15045](https://issues.apache.org/jira/browse/ARROW-15045). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n### The context:\r\nI am using PyArrow to read a folder structured as `exchange/symbol/date.parquet`. The folder contains multiple exchanges, multiple symbols and multiple files. At the time I am writing the folder is about 30GB/1.85M files.\r\n\r\nIf I use a single PyArrow Dataset to read/manage the entire folder, the simplest process with just the dataset defined will occupy 2.3GB of RAM. The problem is, I am instanciating this dataset on multiple processes but since every process only needs some exchanges (typically just one), I don't need to read all folders and files in every single process.\r\n\r\nSo I tried to use a UnionDataset composed of single exchange Dataset. In this way, every process just loads the required folder/files as a dataset. By a simple test, by doing so every process now occupy just 868MB of RAM, -63%.\r\n\r\n### The problem:\r\nWhen using a single Dataset for the entire folder/files, I have no problem at all. I can read filtered data without problems and it's fast as duck.\r\n\r\nBut when I read the UnionDataset filtered data, I always get `Process finished with exit code 139 (interrupted by signal 11: SIGSEGV` error. So after looking every single source of the problem, I noticed that if I create a dummy folder with multiple exchanges but just some symbols, in order to limit the files amout to read, I don't get that error and it works normally. If I then copy new symbols folders (any) I get again that error.\r\n\r\nI came up thinking that the problem is not about my code, but linked instead to the amout of files that the UnionDataset is able to manage.\r\n\r\nAm I correct or am I doing something wrong? Thank you all, have a nice day and good work.",
        "created_at": "2021-12-09T16:27:47.000Z",
        "updated_at": "2022-06-29T16:32:11.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-12-13T09:33:56.841Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15045?focusedCommentId=17458240) by Joris Van den Bossche (jorisvandenbossche):*\n>  At the time I am writing the folder is about 30GB/1.85M files.\r\n\r\nDo I read this correctly that the folder contains 1.8 million files? Which means that on average every file is only around 18kb? \r\n(we should of course still look into the segfault, but as a general comment if the above is correct: I would recommend creating fewer and larger files)\r\n\r\n> So I tried to use a UnionDataset composed of single exchange Dataset. \r\n\r\nIt's not really clear to me how you are using a UnionDataset to limit the number of folders/files in it? (it's a union of which other datasets? What's the \"single exchange Dataset\")   \r\nCould you maybe provide some example code to illustrate the different workflows?\r\n\r\n\r\n\r\n"
        },
        {
            "created_at": "2021-12-13T11:49:25.908Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15045?focusedCommentId=17458340) by Thomas Cercato (ilpomo):*\nThis is an example tree of my data folder:\r\n\r\ndata_dir\r\n---- exchange_0_dir\r\n-------- symbol_0_dir\r\n------------ 2017-01-01.parquet\r\n------------ 2017-01-02.parquet\r\n------------ date-n.parquet\r\n-------- symbol_1_dir\r\n------------ 2017-01-01.parquet\r\n------------ 2017-01-02.parquet\r\n------------ date-n.parquet\r\n---- exchange_1_dir\r\n-------~~symbol_0_dir\r\n-~~---------- 2017-01-01.parquet\r\n------------ 2017-01-02.parquet\r\n------------ date-n.parquet\r\n-------- symbol_1_dir\r\n------------ 2017-01-01.parquet\r\n------------ 2017-01-02.parquet\r\n------------ date-n.parquet\r\n\r\nIf I create a dataset as `dataset(source='path/to/data_dir/', format='parquet', partitioning=partitioning(field_names=['exchange', 'asset'])`, it reads all the exchange directories with their content and since I have tons of files, that simple instance occupy 2.3GB in memory per process.\r\nSo I tried to create an UnionDataset as `dataset(source=[dataset(source=exchange, format='parquet', partitioning=partitioning(field_names=['asset'])) for exchange in [exchange_0_dir, exchange_6_dir, exchange_9_dir]])` and it returs that SIGSEGV error.\r\n\r\nI just checked the data folder details, it's **28.3GB for 880219 files**, so yeah, sorry for that mistake."
        },
        {
            "created_at": "2021-12-13T12:54:24.995Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15045?focusedCommentId=17458366) by Joris Van den Bossche (jorisvandenbossche):*\nThanks for that clarification! Now it is clearer what you exactly did with the union dataset.\r\n\r\nSince this is not that trivial to reproduce locally, would you be able to try to run your code with a debugger (`gdb`, to see if you can get a stack trace of when it is crashing (see eg https://stackoverflow.com/a/49414907/653364). That might give useful information to understand the cause of the crash.\r\n\r\n> I just checked the data folder details, it's 28.3GB for 880219 files, so yeah, sorry for that mistake.\r\n\r\nAside, this still means you only have on average files of around 30kb if I calculated that correctly. Generally, that's considered very small, and certainly for Parquet files (given the metadata overhead for parquet files). I am not fully sure if we have an option to disable reading Parquet metadata to reduce the memory usage. "
        },
        {
            "created_at": "2021-12-13T13:12:41.074Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15045?focusedCommentId=17458380) by Thomas Cercato (ilpomo):*\nCurrently the workstation is running a long computation, so I can't run the code with gdb. Is gdb any different from the default debugger available in PyCharm? Because I am not that familiar with UNIX, gdb and debuggers in general, but I find built in tools from JetBrains easy to understand.\r\n\r\nI am creating date/files because it's easier to manage them when fetching data from the exchanges. If I discover corrupted or incomplete data for a particular date, I have to fetch the data for just a single date, that on average is equivalent to 3 calls to the exchange. This give me control over the entire process of fetch/store/load/evaluate. Monthly files are bigger and if I have to fetch just part of them then the entire file must be loaded in memory, modified and stored again. With yearly files this is even worse. And no, weeks are excluded, they are a pain in the ass to manage on year change. I hate weeks."
        }
    ]
}