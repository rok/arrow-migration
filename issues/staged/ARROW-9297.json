{
    "issue": {
        "title": "[C++][Dataset] Dataset scanner cannot handle large binary column (> 2 GB)",
        "body": "***Note**: This issue was originally created as [ARROW-9297](https://issues.apache.org/jira/browse/ARROW-9297). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nRelated to ARROW-3762 (the parquet issue which has been solved), and discovered in ARROW-9139.\r\n\r\nWhen creating a Parquet file with a large binary column (larger than BinaryArray capacity):\r\n\r\n```Java\n\r\n# code from the test_parquet.py::test_binary_array_overflow_to_chunked test\r\nvalues = [b'x'] + [ \r\n    b'x' * (1 << 20) \r\n] * 2 * (1 << 10)                                                                                                                                                                                     \r\n\r\ntable = pa.table({'byte_col': values})                                                                                                                                                                    \r\npq.write_table(table, \"test_large_binary.parquet\")                                                                                                                                                        \r\n```\r\n\r\nthen reading this with the parquet API works (fixed by ARROW-3762):\r\n\r\n```Java\n\r\nIn [3]: pq.read_table(\"test_large_binary.parquet\")                                                                                                                                        \r\nOut[3]: \r\npyarrow.Table\r\nbyte_col: binary\r\n```\r\n\r\nbut with the Datasets API this still fails:\r\n\r\n```Java\n\r\nIn [1]: import pyarrow.dataset as ds                                                                                                                                                                               \r\n\r\nIn [2]: dataset = ds.dataset(\"test_large_binary.parquet\", format=\"parquet\")                                                                                                                                        \r\n\r\nIn [4]: dataset.to_table()                                                                                                                                                                                         \r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n<ipython-input-4-6fb0d79c4511> in <module>\r\n----> 1 dataset.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: This class cannot yet iterate chunked arrays\r\n\r\n```",
        "created_at": "2020-07-02T12:05:36.000Z",
        "updated_at": "2020-07-13T15:02:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-07-12T21:54:03.000Z"
    },
    "comments": [
        {
            "created_at": "2020-07-02T15:06:33.820Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9297?focusedCommentId=17150352) by Neal Richardson (npr):*\nIs the issue that pyarrow makes a Binary ChunkedArray instead of a LargeBinary Array when it gets a big binary column?"
        },
        {
            "created_at": "2020-07-02T15:42:45.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9297?focusedCommentId=17150395) by Joris Van den Bossche (jorisvandenbossche):*\nYes. And the `RowGroupRecordBatchReader::ReadNext` (https://github.com/apache/arrow/blob/e3c7dbfa934a971f004ef54eaafb0378453a52dd/cpp/src/parquet/arrow/reader.cc#L344) apparently cannot deal with this chunked array (maybe because a RecordBatch requires all single arrays, not chunked arrays). While the code path in the existing `parquet.read_table` can handle this.\r\n\r\nThe difference might be that `parquet.read_table` -> `ParquetFile.read` uses `FileReader::ReadTable`, while the datasets API uses `FileReader::GetRecordBatchReader`. \r\n\r\nAnd I am not sure how to deal with a chunked array if you expect to return record batches (so this might not be that easy to solve, although some general we need to solve for the Datasets API, not just for being able to use it in `parquet.read_table`)."
        },
        {
            "created_at": "2020-07-02T15:51:51.379Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9297?focusedCommentId=17150402) by Neal Richardson (npr):*\nYeah, this is why in ARROW-3308 in the R bindings I opted to make a LargeString Array rather than a ChunkedArray: RecordBatches must contain Arrays."
        },
        {
            "created_at": "2020-07-12T21:54:04.015Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9297?focusedCommentId=17156396) by Wes McKinney (wesm):*\nIssue resolved by pull request 7704\n<https://github.com/apache/arrow/pull/7704>"
        }
    ]
}