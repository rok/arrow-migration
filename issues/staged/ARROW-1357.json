{
    "issue": {
        "title": "[Python] Data corruption in reading multi-file parquet dataset",
        "body": "***Note**: This issue was originally created as [ARROW-1357](https://issues.apache.org/jira/browse/ARROW-1357). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI generated a parquet dataset in Spark that has two files. PyArrow corrupts the data of the second file if I read them both in using pyarrow's parquet directory loading mode.\n\n$ ls -l data\ntotal 28608\n-rw-rw-r-- 1 jarno jarno 14651449 Aug 15 09:30 part-00000-fd2ff92f-201b-4ffc-b0b4-275e06a7fa02.snappy.parquet\n-rw-rw-r-- 1 jarno jarno 14636502 Aug 15 09:30 part-00001-fd2ff92f-201b-4ffc-b0b4-275e06a7fa02.snappy.parquet\n\nimport pyarrow.parquet as pq\n\ntab1 = pq.read_table('data')\ndf1 = tab1.to_pandas()\ndf1[df1.account_id == 38658373328].legal_labels.tolist()\n1. [array([ 2,  3,  5,  8, 10, 11, 13, 14, 17, 18, 19, 21, 22, 31, 60, 61, 63,\n1.        64, 65, 66, 69, 70, 74, 75, 77, 82,  0,  1,  2,  3,  5,  8, 10, 11,\n1.        13, 14, 17, 18, 19, 21, 22])]\n\n\ntab2 = pq.read_table('data/part-00001-fd2ff92f-201b-4ffc-b0b4-275e06a7fa02.snappy.parquet')\ndf2 = tab2.to_pandas()\ndf2[df2.account_id == 38658373328].legal_labels.tolist()\n1. [array([ 0,  1,  2,  3,  5,  8, 10, 11, 13, 14, 17, 18, 19, 21, 22, 24, 28,\n1.        30, 31, 36, 38, 39, 40, 41, 43, 49, 60, 61, 62, 63, 64, 65, 66, 67,\n1.        69, 70, 74, 75, 77, 82, 90])]\n\nUnfortunately I cannot share the data files, and I was not able to create a dummy data file pair that would have triggered the bug. I'm sending this bug report in the hope that it is still useful without a minimal repro example.",
        "created_at": "2017-08-16T09:01:28.000Z",
        "updated_at": "2017-08-20T17:49:54.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-08-20T17:49:54.000Z"
    },
    "comments": [
        {
            "created_at": "2017-08-16T13:29:37.883Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16128799) by Wes McKinney (wesm):*\n`[~jseppanen]` could you clarify when you say \"corrupt\". Are the files on disk altered, or are the in-memory results only different? \n\nSecondly, can you confirm that the Arrow type of legal_labels is `list<int64>` or some other list type (show `tab1.schema` for legal_labels if you can)? It looks like there is a bug somewhere in the list handling code. We are working on publishing packages for the 0.6.0 release, I'll ping you to test with 0.6.0 to see if it's been fixed. The fact that the bug only occurs with multi-file reads will help me track down the issue faster.\n\ncc `[~xhochy]` `[~fjetter]`"
        },
        {
            "created_at": "2017-08-17T11:13:10.179Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16130263) by Jarno Seppanen (jseppanen):*\nsorry for my terseness, by \"corruption\" I meant that the data is loaded incorrectly into a pandas dataframe in memory. Specifically:\n1. when loading just single file, \"part-00001....parquet\", into memory, and querying with pandas, the contents are correct, which is identical to what I wrote from spark, and also how I can read it back in spark\n2. however, when loading two files as a directory, containing \"part-00000...\" and \"part-00001...parquet\" files, into memory and querying a row that belongs to the second file, its contents are not the same as how they were when just loading \"part-00001...\" alone\n\n>>> tab1.schema\naccount_id: int64\ncard_id: int64\ndate: date32[day]\nlabel: int64\nlegal_labels: list<element: int64>\nfeatures_type: int32\nfeatures_size: int32\nfeatures_indices: list<element: int32>\nfeatures_values: list<element: double>\n\u2013 metadata \u2013\norg.apache.spark.sql.parquet.row.metadata: {\"type\":\"struct\",\"fields\":[{\"name\":\"account_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"card_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"date\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"label\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"legal_labels\",\"type\":{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"features_type\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"features_size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"features_indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"features_values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}\n\n>>> tab2.schema\naccount_id: int64\ncard_id: int64\ndate: date32[day]\nlabel: int64\nlegal_labels: list<element: int64>\nfeatures_type: int32\nfeatures_size: int32\nfeatures_indices: list<element: int32>\nfeatures_values: list<element: double>\n\u2013 metadata \u2013\norg.apache.spark.sql.parquet.row.metadata: {\"type\":\"struct\",\"fields\":[{\"name\":\"account_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"card_id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"date\",\"type\":\"date\",\"nullable\":true,\"metadata\":{}},{\"name\":\"label\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"legal_labels\",\"type\":{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"features_type\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"features_size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"features_indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}},{\"name\":\"features_values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":true},\"nullable\":true,\"metadata\":{}}]}\n\n"
        },
        {
            "created_at": "2017-08-17T13:40:56.399Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16130406) by Wes McKinney (wesm):*\nThank you, I apologize for the trouble. I think this is enough information to sort out the problem. If you could take a look at the 0.6.0 release on PyPI and let us know if the problem still exists that would be helpful"
        },
        {
            "created_at": "2017-08-18T08:25:18.339Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16131896) by Jarno Seppanen (jseppanen):*\nTested on 0.6.0 from pypi, problem still exists\n\nFWIW, the first file is loaded correctly but the second file is mostly incorrect, when doing the multi-file load. New example:\n\n```Java\nimport sys\nimport pyarrow\nimport pyarrow.parquet as pq\nimport pandas as pd\n\nsys.version\n# '3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]'\npyarrow.__version__\n# '0.6.0'\npd.__version__\n# '0.19.0'\n\npart0 = pq.read_table('data/part-00000-fd2ff92f-201b-4ffc-b0b4-275e06a7fa02.snappy.parquet').to_pandas()\npart1 = pq.read_table('data/part-00001-fd2ff92f-201b-4ffc-b0b4-275e06a7fa02.snappy.parquet').to_pandas()\nboth_bad = pq.read_table('data').to_pandas()\nboth_good = pd.concat([part0, part1])\n\n# first part of multi-file load gets loaded correctly:\nall(all(x==y)\n    for x,y in zip(both_bad[:len(part0)].legal_labels,\n                   part0.legal_labels))\n# True\n\n# second part is loaded as corrupt in memory:\nall(all(x==y)\n    for x,y in zip(both_bad[len(part0):].legal_labels,\n                   part1.legal_labels))\n# False\n\n# most of the rows in the second part are corrupt\nsum(all(x==y)\n    for x,y in zip(both_bad[len(part0):].legal_labels,\n                   part1.legal_labels))\n# 151\n\nlen(part1)\n# 21447\n\n# for comparison, the same queries for the manually concatenated data frame:\nall(all(x==y)\n    for x,y in zip(both_good[:len(part0)].legal_labels,\n                   part0.legal_labels))\n# True\n\nall(all(x==y)\n    for x,y in zip(both_good[len(part0):].legal_labels,\n                   part1.legal_labels))\n# True\n```"
        },
        {
            "created_at": "2017-08-19T19:00:25.724Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16134230) by Wes McKinney (wesm):*\nThanks. I think I found the problem in the code. I will dig in and get this fixed "
        },
        {
            "created_at": "2017-08-20T16:44:00.256Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16134500) by Wes McKinney (wesm):*\nPR: https://github.com/apache/arrow/pull/979"
        },
        {
            "created_at": "2017-08-20T17:49:54.410Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1357?focusedCommentId=16134513) by Wes McKinney (wesm):*\nIssue resolved by pull request 979\n<https://github.com/apache/arrow/pull/979>"
        }
    ]
}