{
    "issue": {
        "title": "[Python] StructScalar Timestamp using .to_pandas() loses/converts type",
        "body": "***Note**: This issue was originally created as [ARROW-12680](https://issues.apache.org/jira/browse/ARROW-12680). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nWe're noticing an issue where we lose type and formatting on conversion to a pandas dataframe for a particular dataset we house, which contains a struct, and the underlying type of the child is Timestamp rather than datetime.datetime (which we believed synonymous from Pandas documentation).\r\n\r\n\u00a0\r\n\r\nInside the StructArray we can see nicely formatted timestamp values, but when we call .to_pandas() on it, we end up with epoch stamps for the date child.\r\n```java\n\r\nimport pyarrow.parquet as pq\r\n\r\ntbl=pq.read_table(\"part-00009-47f62157-cb6f-41a8-9ad6-ace65df94c6e-c000.snappy.parquet\")\r\n\r\ntbl.column(\"observations\").chunk(0).values\u00a0pyarrow.lib.StructArray object at 0x7fc8eb0cab40>\r\n\u2013 is_valid: all not null\r\n\u2013 child 0 type: timestamp[ns]\r\n[\r\n2000-01-01 00:00:00.000000000,\r\n2001-01-01 00:00:00.000000000,\r\n2002-01-01 00:00:00.000000000,\r\n2003-01-01 00:00:00.000000000,\r\n2004-01-01 00:00:00.000000000,\r\n2005-01-01 00:00:00.000000000,\r\n2006-01-01 00:00:00.000000000,\r\n2007-01-01 00:00:00.000000000,\r\n2008-01-01 00:00:00.000000000,\r\n2009-01-01 00:00:00.000000000,\r\n...\r\n2018-07-01 00:00:00.000000000,\r\n2018-10-01 00:00:00.000000000,\r\n2019-01-01 00:00:00.000000000,\r\n2019-04-01 00:00:00.000000000,\r\n2019-07-01 00:00:00.000000000,\r\n2019-10-01 00:00:00.000000000,\r\n2020-01-01 00:00:00.000000000,\r\n2020-04-01 00:00:00.000000000,\r\n2020-07-01 00:00:00.000000000,\r\n2020-10-01 00:00:00.000000000\r\n]\r\n\u2013 child 1 type: double\r\n[\r\n-2.69685,\r\n9.27988,\r\n7.26902,\r\n-7.55753,\r\n-1.62137,\r\n6.84773,\r\n-8.21204,\r\n-8.97041,\r\n-1.14405,\r\n-0.710153,\r\n...\r\n2.1658,\r\n3.05588,\r\n2.3868,\r\n2.10805,\r\n2.39984,\r\n2.54855,\r\n-7.26804,\r\n-2.35179,\r\n-0.867518,\r\n0.150593\r\n]\r\n```\r\n```java\n\r\n\u00a0\r\ntbl.to_pandas()['observations']\u00a0\r\n\r\n[{'date': 946684800000000000, 'value': -2.6968... 1 [{'date': 946684800000000000, 'value': 57.9608... 2 [{'date': 1483228800000000000, 'value': 95.904... 3 [{'date': 1214870400000000000, 'value': 19.021... 4 [{'date': 1199145600000000000, 'value': 1.2011... ... 636 [\\{'date': 1072915200000000000, 'value': 5.418}... 637 [{'date': 946684800000000000, 'value': 110.695... 638 [{'date': 1009843200000000000, 'value': 3.0094... 639 [{'date': 1222819200000000000, 'value': 48.365... 640 [{'date': 1199145600000000000, 'value': 1.5600... Name: observations, Length: 641, dtype: object\r\n\r\nIn [12]: tbl.to_pandas()[\"observations\"].iloc[0][0]\r\nOut[12]: {'date': 1041379200000000000, 'value': 249.523242}\r\n# date is now type Int\n```\r\n\u00a0\r\n\r\nWe notice that if we take the same table, save it back out to a file first, and then check the chunk(0).values as above, the underlying type changes from **Timestamp** to **datetime.datetime**, and that will now convert .to_pandas() correctly.\r\n```java\n\r\npq.write_table(tbl, \"output.parquet\")\r\n\r\ntbl2=pq.read_table(\"output.parquet\")\r\n\r\ntbl2.column(\"observations\").chunk(0).values[0]\r\nOut[17]: <pyarrow.StructScalar: {'date': datetime.datetime(2003, 1, 1, 0, 0), 'value': 249.523242}>\r\n\r\ntbl2.column(\"observations\").chunk(0).to_pandas()\r\nOut[18]: \r\n0        [{'date': 2003-01-01 00:00:00, 'value': 249.52...\r\n1        [{'date': 2008-01-01 00:00:00, 'value': 29.741...\r\n2        [{'date': 2000-01-01 00:00:00, 'value': 2.3454...\r\n3        [{'date': 2006-01-01 00:00:00, 'value': 1.2048...\r\n4        [{'date': 2008-01-01 00:00:00, 'value': 196546...\r\n                               ...                        \r\n29489    [{'date': 2010-01-01 00:00:00, 'value': 19.155...\r\n29490    [{'date': 2012-04-30 00:00:00, 'value': 0.0}, ...\r\n29491    [{'date': 2012-04-30 00:00:00, 'value': 0.0}, ...\r\n29492    [{'date': 2012-04-30 00:00:00, 'value': 0.0}, ...\r\n29493    [{'date': 2012-04-30 00:00:00, 'value': 10.0},...\r\nLength: 29494, dtype: object\r\n\r\ntbl2.to_pandas()[\"observations\"].iloc[0][0]\r\nOut[8]: {'date': datetime.datetime(2003, 1, 1, 0, 0), 'value': 249.523242}\r\n\r\n# date remains as datetime.datetime\n```\r\n\u00a0\r\n\r\nThanks in advance, and apologies if I have not followed Issue protocol on this board.\r\n\r\nIf there is a parameter that we just need to pass into .to_pandas for this to take place (I can see there is date_as_object/timestamp_as_object, but these have no effect), we would like to know.",
        "created_at": "2021-05-07T11:02:44.000Z",
        "updated_at": "2021-05-17T09:41:28.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-05-07T18:45:43.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12680?focusedCommentId=17341003) by Weston Pace (westonpace):*\nI think there is indeed a bug here.\u00a0 Let me try and demystify some of what is going on.\u00a0 There are 5+ temporal types in pyarrow but everything you are doing is currently related to just one, the timestamp type.\u00a0 The timestamp type represents seconds, milliseconds, microseconds, or nanoseconds from the epoch.\u00a0 In addition there may or may not be a time zone string.\u00a0 Finally, these types may or may not be in a struct (which shouldn't matter but does here...which is the bug).\r\n\r\nIn pandas there are 3+ ways to represent temporal information.\u00a0 The datetime.datetime object,\u00a0 A pandas.Timestamp, and an integer.\r\n\r\n\u00a0\r\n\r\nWhen you first read in your table you are getting a struct where the 'date' field is a timestamp with \\***nanosecond**\\* resolution.\r\n\r\nWhen you save your table and then reload it the timestamp is being truncated.\u00a0 This is because pq.write_table with version==1.0 (the default in pyarrow 3) will truncate nanosecond timestamps down to microseconds.\r\n\r\nSo when you next read in your table you are getting a struct where the 'date' field is a timestamp with \\***microsecond**\\* resolution.\r\n\r\nFinally, It seems this may be a regression of https://issues.apache.org/jira/browse/ARROW-7723\r\n\r\n\u00a0\r\n```java\n\r\nimport pyarrow as pa\r\nimport datetime\r\npylist = [datetime.datetime.now()]\r\narr1 = pa.array(pylist, pa.timestamp(unit='ms'))\r\narr2 = pa.array(pylist, pa.timestamp(unit='ns'))\r\nsarr = pa.StructArray.from_arrays([arr1, arr2], names=['ms', 'ns'])\r\ntable = pa.Table.from_arrays([arr1, arr2, sarr], ['ms', 'ns', 'struct'])\r\nprint(table.to_pandas())\r\n```\r\n\u00a0\r\n```java\n\r\n                       ms                         ns                                             struct\r\n0 2021-05-07 08:46:15.898 2021-05-07 08:46:15.898716  {'ms': 2021-05-07 08:46:15.898000, 'ns': 16203...\r\n\r\n```\r\n\u00a0\r\n\r\nAs for workarounds...if your schema is reliable you could cast from nanosecond resolution to us resolution (struct casting isn't working quite right (ARROW-1888) so it's a bit clunky):\r\n\r\n\u00a0\r\n```java\n\r\nimport pyarrow as pa\r\nimport pyarrow.compute as pc\r\n\r\ndates = pa.array([datetime.datetime.now()], pa.timestamp(unit='ns'))\r\nvalues = pa.array([200.37], pa.float64())\r\nobservations = pa.StructArray.from_arrays([dates, values], names=['dates', 'values'])\r\ndesired_type = pa.struct([pa.field('dates', pa.timestamp(unit='us')), pa.field('values', pa.float64())])\r\ntbl = pa.Table.from_arrays([observations], ['observations'])\r\nprint(tbl.to_pandas())\r\n\r\nbad_observations = tbl.column('observations').chunks\r\nvalues = [chunk.field('values') for chunk in bad_observations]\r\nbad_dates = [chunk.field('dates') for chunk in bad_observations]\r\ngood_dates = [pc.cast(bad_dates_chunk, pa.timestamp(unit='us')) for bad_dates_chunk in bad_dates]\r\ngood_observations_chunks = []\r\nfor i in range(len(good_dates)):\r\n    good_observations_chunks.append(pa.StructArray.from_arrays([good_dates[i], values[i]], names=['dates', 'values']))\r\ngood_observations = pa.chunked_array(good_observations_chunks)\r\ntbl = tbl.set_column(0, 'observations', good_observations)\r\nprint(tbl.to_pandas())\r\n```\r\n\u00a0"
        },
        {
            "created_at": "2021-05-08T09:49:14.196Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12680?focusedCommentId=17341262) by Tim Ryles (selyramit):*\nThanks ever so much `[~westonpace]` \u00a0for your response and even more so for the comprehensive explanation and workaround.\r\n\r\nI'm humbled by the fact I was assuming blithely timestamp = timestamp.\r\n\r\n\u00a0\r\n\r\nSo is this because all Timestamp are resolved to an underlying datetime.datetime, if their precision is greater than nanosecond as per ARROW-7723, and then, nanosecond timestamps are truncated to datetime.datetime as well IF the table contains a nanosecond field that is written out with such defaults \u2013 i.e. so we need to be aware if we ever just write out the parquet as an interim step (which we may do) too?\r\n\r\nTo prevent this truncation we could then use version='2.0'\u00a0and allow_truncated_timestamps=True\u00a0\\*\\*\u00a0for .write_table - are there any further implications of using version=2.0 in your opinion?\r\n\r\nI think in places we may have Hive/Impala so do not want to cause a finicky parquet issue by 'correcting' the drop in precision we weren't aware of, but I'm also curious why then version is defaulted to 1.0 in pyarrow 3.0.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nSecondly, right now at least then, it would be a poor decision for us to shape our data like this, if we do indeed require nanosecond precision -\u00a0I'm not sure I'll be permitted to just truncate to 'us' as our down-streams are usually interested in this, but I enjoyed your workaround!\r\n\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-05-10T19:04:23.357Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12680?focusedCommentId=17342091) by Weston Pace (westonpace):*\n> but I'm also curious why then version is defaulted to 1.0 in pyarrow 3.0.\r\n\r\nFrom the pyarrow docs it appears the primary reason is compatibility with older readers.\u00a0 However, there was also some (relatively) recent discussion on the topic in the ML and the consensus of those that participated was that making 2.0 the default was probably ok: https://lists.apache.org/thread.html/rf1a377c66990ae5ac0693119d416c93a7e19228d3eaaea8bd90acb17%40%3Cdev.arrow.apache.org%3E\r\n\r\nI've personally gone down the same path you did.\u00a0 I was not aware of the truncation, then ended up in a situation where fixing the truncation broke everything :).\u00a0 My project at the time was small enough I was able to push through however (and I didn't need to support other parquet readers)."
        },
        {
            "created_at": "2021-05-17T09:37:54.772Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12680?focusedCommentId=17346020) by Joris Van den Bossche (jorisvandenbossche):*\nReading the comments in https://github.com/apache/arrow/pull/6322 again, I _think_ the fact that for nanosecond timestamps inside a struct, we currently return integer epoch is kind of deliberate? (because of the lack of a better alternative, since `datetime.datetime` cannot represent nanoseconds)\r\n\r\nAlthough for struct _scalars_, we actually use pandas.Timestamp for nanosecond resolution columns:\r\n\r\n```Java\n\r\nIn [42]: sarr[0]\r\nOut[42]: <pyarrow.StructScalar: {'ms': datetime.datetime(2021, 5, 17, 11, 3, 58, 947000), 'ns': Timestamp('2021-05-17 11:03:58.947224')}>\r\n\r\nIn [43]: sarr[0].as_py()\r\nOut[43]: \r\n{'ms': datetime.datetime(2021, 5, 17, 11, 3, 58, 947000),\r\n 'ns': Timestamp('2021-05-17 11:03:58.947224')}\r\n```\r\n\r\nOf course, that is only possible if pandas is installed. And so maybe that's the reason that for array conversion we simply always use the \"safe\" integer epoch option. But it's certainly somewhat inconsistent."
        },
        {
            "created_at": "2021-05-17T09:41:28.023Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12680?focusedCommentId=17346022) by Joris Van den Bossche (jorisvandenbossche):*\n(in any case, we also need to document this better, to not each time have to look into old discussions / guess from the behaviour and source code, when such a question comes up ..)"
        }
    ]
}