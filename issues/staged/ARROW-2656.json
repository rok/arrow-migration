{
    "issue": {
        "title": "[Python] Improve ParquetManifest creation time ",
        "body": "***Note**: This issue was originally created as [ARROW-2656](https://issues.apache.org/jira/browse/ARROW-2656). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen a parquet dataset is highly partitioned, the time to call the constructor for\u00a0[ParquetManifest](https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L588)\u00a0takes a significant amount of time since it serially visits directories to find all parquet files. In a dataset with thousands of partition values this can take several minutes from a personal laptop.\r\n\r\nA quick win to vastly improve this performance would be to use a ThreadPool to have calls to\u00a0`_visit_level`\u00a0happen concurrently to prevent wasting a ton of time waiting on I/O.\r\n\r\nAn even faster option could be to allow for optional indexing of dataset metadata in something like the\u00a0`common_metadata`. This could contain all files in the manifest and their row_group information. This would also allow for [split_row_groups](https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L746)\u00a0to be implemented efficiently without needing to open every parquet file in the dataset to retrieve the metadata which is quite time consuming for large datasets. The main problem with the indexing approach are it requires immutability of the dataset, which doesn't seem too unreasonable. This specific implementation seems related to https://issues.apache.org/jira/browse/ARROW-1983\u00a0however that only covers the write portion.",
        "created_at": "2018-05-31T20:45:18.000Z",
        "updated_at": "2018-07-24T09:05:05.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2018-07-24T09:05:05.000Z"
    },
    "comments": [
        {
            "created_at": "2018-05-31T20:46:38.496Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2656?focusedCommentId=16497147) by Robbie Gruener (rgruener):*\nI will\u00a0attempt to get some code as a benchmark however (I believe) you cannot create partitioned parquet datasets through pyarrow right now which will make it necessary to do so through spark."
        },
        {
            "created_at": "2018-06-27T16:11:29.428Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2656?focusedCommentId=16525236) by Robbie Gruener (rgruener):*\nI have opened <https://github.com/apache/arrow/pull/2185>\u00a0which is a quick win that gives a rather large performance boost for creating a parquet manifest on a partitioned dataset which lives in hdfs.\r\n\r\nI think using the summary file will be another improvement to make on top of this, but this is extremely useful in the meantime."
        },
        {
            "created_at": "2018-07-24T09:05:05.084Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2656?focusedCommentId=16553982) by Uwe Korn (uwe):*\nIssue resolved by pull request 2185\n<https://github.com/apache/arrow/pull/2185>"
        }
    ]
}