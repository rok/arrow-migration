{
    "issue": {
        "title": "[Python] \"empty\" dtype metadata leads to wrong Parquet column type",
        "body": "***Note**: This issue was originally created as [ARROW-8378](https://issues.apache.org/jira/browse/ARROW-8378). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nRun the following code with Pandas 0.24.x-1.0.x, and PyArrow 0.16.0 on Python 3.7:\r\n```python\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf_1 = pd.DataFrame({'col': [None, None, None]})\r\ndf_1.col = df_1.col.astype(np.unicode_)\r\ndf_1.to_parquet('right.parq', engine='pyarrow')\r\n\r\nseries = pd.Series([None, None, None], dtype=np.unicode_)\r\ndf_2 = pd.DataFrame({'col': series})\r\ndf_2.to_parquet('wrong.parq', engine='pyarrow')\r\n```\r\nExamine the Parquet column type for each file (I use [parquet-tools](https://github.com/wesleypeck/parquet-tools)). `right.parq` has the expected UTF-8 string type. `wrong.parq` has an `INT32`.\r\n\r\nThe following metadata is stored in the Parquet files:\r\n\r\n`right.parq`\r\n```json\n\r\n{\r\n  \"column_indexes\": [],\r\n  \"columns\": [\r\n    {\r\n      \"field_name\": \"col\",\r\n      \"metadata\": null,\r\n      \"name\": \"col\",\r\n      \"numpy_type\": \"object\",\r\n      \"pandas_type\": \"unicode\"\r\n    }\r\n  ],\r\n  \"index_columns\": [],\r\n  \"pandas_version\": \"0.24.1\"\r\n}\r\n```\r\n`wrong.parq`\r\n```json\n\r\n{\r\n  \"column_indexes\": [],\r\n  \"columns\": [\r\n    {\r\n      \"field_name\": \"col\",\r\n      \"metadata\": null,\r\n      \"name\": \"col\",\r\n      \"numpy_type\": \"object\",\r\n      \"pandas_type\": \"empty\"\r\n    }\r\n  ],\r\n  \"index_columns\": [],\r\n  \"pandas_version\": \"0.24.1\"\r\n}\r\n```\r\nThe difference between the two is that the `pandas_type`\u00a0for the incorrect file is \"empty\" rather than the expected \"unicode\". PyArrow misinterprets this and defaults to a 32-bit integer column.\r\n\r\nThe incorrect datatype will cause Redshift to reject the file when we try to read it because the column type in the file doesn't match the column type in the database table.\r\n\r\nI originally filed this as a bug in Pandas (see [this ticket](https://github.com/pandas-dev/pandas/issues/25326)) but they punted me over here because the dtype conversion is handled in PyArrow. I'm not sure how you'd handle this here.",
        "created_at": "2020-04-08T23:24:24.000Z",
        "updated_at": "2020-04-09T14:38:11.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-04-09T14:38:11.000Z"
    },
    "comments": [
        {
            "created_at": "2020-04-08T23:36:59.540Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8378?focusedCommentId=17078813) by Wes McKinney (wesm):*\nI'm showing `\"pandas_type\": \"empty\"` for both `df_1` and `df_2`. How are we supposed to know that the column contains unicode values? pandas converts the unicode dtype to object dtype. \r\n\r\nIn these situations, the safest way to write Parquet files is for you to write down the exact Arrow schema you want\r\n\r\nNote:\r\n\r\n```Java\n\r\nIn [15]: pa.table(df_1, schema=pa.schema([pa.field(\"col\", \"string\")]))          \r\nOut[15]: \r\npyarrow.Table\r\ncol: string\r\n\r\nIn [16]: pa.table(df_1)                                                         \r\nOut[16]: \r\npyarrow.Table\r\ncol: null\r\n```\r\n\r\nWithout any objects in `df_1['col']`, Arrow infers null type for \"col\"\r\n\r\nEDIT: Oops, seems I copy-pasted wrong in creating df_1, what `[~jorisvandenbossche]` is right"
        },
        {
            "created_at": "2020-04-09T12:51:57.464Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8378?focusedCommentId=17079297) by Joris Van den Bossche (jorisvandenbossche):*\n`[~yiannisliodakis]` I think the \"bug\" lies in the expectation of what your pandas code is doing. The two dataframes that you create that way are actually not equal:\r\n```java\n\r\nIn [30]: df_1.col.values \r\nOut[30]: array(['None', 'None', 'None'], dtype=object)\r\n\r\nIn [31]: df_2.col.values   \r\nOut[31]: array([None, None, None], dtype=object)\r\n```\r\nWhere you can see that the first is actually a column with strings (`\"None\"`), while the second is an array with only \"nulls\" (`None`).\r\n\r\nSo from pyarrow's point of view, it is doing the expected thing: in the first case it creates a string column (because it gets passed strings), in the second case there are only nulls, and thus pyarrow decides to use a `null` type. \r\n As `[~wesm]` said, if you want to ensure correct types also for corner cases of all missing values, you need to pass an explicit expected schema (to do this, you will first need to create a pyarrow table and write that to parquet, as I think it is currently not possible through the pandas API).\r\n\r\nThe ultimate reason for this is the limitation of pandas' dtype system. Although you do `dtype=np.unicode_`, pandas does not support that data type, and will generally store those as \"object\" dtype. But an object dtype can contain anything, and thus the conversion of an object dtype column to pyarrow depends on the content of the column.\r\n\r\nNote that in the latest version of pandas, there is a \"string\" dtype, which will always be converted to string, even if it contains all missing values (see <https://pandas.pydata.org/docs/whatsnew/v1.0.0.html#dedicated-string-data-type>)"
        },
        {
            "created_at": "2020-04-09T14:38:11.870Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8378?focusedCommentId=17079427) by Wes McKinney (wesm):*\nWhat `[~jorisvandenbossche]` wrote is right \u2013 I missed that `astype(np.unicode_)` was coercing `None` to `'None'`, so this is not something that we're able to \"fix\" per se. "
        }
    ]
}