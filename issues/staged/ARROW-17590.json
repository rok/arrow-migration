{
    "issue": {
        "title": "Lower memory usage with filters",
        "body": "***Note**: This issue was originally created as [ARROW-17590](https://issues.apache.org/jira/browse/ARROW-17590). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\nWhen I read a parquet file (about 23MB with 250K rows and 600 object/string columns with lots of None) with filter on a not null column for a small number of rows (e.g. 1 to 500), the memory usage is pretty high (around 900MB to 1GB). The result table and dataframe have only a few rows (1 row 20kb, 500 rows 20MB). Looks like it scans/loads many rows from the parquet file. Not only the footprint or watermark of memory usage is high, but also it seems not releasing the memory in time (such as after GC in Python, but may get used for subsequent read).\r\n\r\nWhen reading the same parquet file for all columns without filtering, the memory usage is about the same at 900MB. It goes up to 2.3GB after to_pandas dataframe,. df.info(memory_usage='deep') shows 4.3GB maybe double counting something.\r\n\r\nIt helps to limit the number of columns read. Read 1 column with filter for 1 row or more or without filter, it takes about 10MB, which is quite smaller and better, but still bigger than the size of table or data frame with 1 or 500 rows of 1 columns (under 1MB)\r\n\r\nThe filtered column is not a partition key, which functionally works to get the correct rows. But the memory usage is quite high even when the parquet file is not really large, partitioned or not. There were some references similar to this issue, for example: <https://github.com/apache/arrow/issues/7338>\r\n\r\nRelated classes/methods in (pyarrow 9.0.0)\u00a0\r\n\r\n_ParquetDatasetV2.read\r\n\u00a0 \u00a0 self._dataset.to_table(columns=columns, filter=self._filter_expression, use_threads=use_threads)\r\n\r\npyarrow._dataset.FileSystemDatase.to_table\r\n\r\nI played with pyarrow._dataset.Scanner.to_table\r\n\u00a0 \u00a0 self._dataset.scanner(columns=columns, filter=self._filter_expression).to_table()\r\nThe memory usage is small to construct the scanner but then goes up after the to_table call materializes it.\r\n\r\nIs there some way or workaround to reduce the memory usage with read filtering?\u00a0\r\nIf not supported yet, can it be fixed/improved with priority?\u00a0\r\nThis is a blocking issue for us when we need to load all or many columns. \r\nI am not sure what improvement is possible with respect to how the parquet columnar format works, and if it can be patched somehow in the Pyarrow Python code, or need to change and build the arrow C++ code.\r\n\r\nThanks!",
        "created_at": "2022-09-01T02:28:59.000Z",
        "updated_at": "2022-09-02T13:20:09.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-09-02T13:20:09.000Z"
    },
    "comments": [
        {
            "created_at": "2022-09-01T15:07:57.745Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599019) by Will Jones (willjones127):*\nFirst, I don't believe the row-level filters avoid reading any data, unless they can be applied to the data set partition values. In order to evaluate the expression on the row, it needs to be parsed into Arrow data.\r\n\r\nIf you want to reduce memory usage, I have two suggestions:\r\n1. Turn off prebuffering, if you haven't already. In Python some interfaces it's on by default, some off. It gives better performance on some filesystems, but it uses more memory.\n1. Consider reading in batches, using the `iter_batches()` method on Parquet files for instance. Then you can filter as the data comes in and concatenate the results into a Table.\n   \n   Which interface are you using? `pyarrow.parquet.read_table` or datasets?"
        },
        {
            "created_at": "2022-09-01T17:14:24.044Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599083) by Weston Pace (westonpace):*\nCan we try an experiment?\r\n\r\nRead in all rows for just the columns that you want.  Leave it as a pyarrow table (e.g. do not convert to pandas dataframe).  Once you have the table print `pa.total_allocated_bytes()` and then also print `table.nbytes`.  These two numbers should be quite close and they will tell us how much actual data you have to load from disk (assuming there is no row-based pushdown available).\r\n\r\nIf this number is very small (e.g. much less than 900MB) then we can investigate the overhead.\r\n\r\nIf this number is large (e.g. closer to 900MB) then we can investigate why the data is large and potential alternate encodings (e.g. dictionary encoding).\r\n\r\n> I don't believe the row-level filters avoid reading any data, unless they can be applied to the data set partition values\r\n\r\nThis is not entirely true but probably pretty close to true in this case.  The parquet scanner will try and skip entire row groups based on row group statistics and the filter.  For example, if the filter is `x > 100` and the row group has `(x-min: 20, x-max: 73)` then that row group will not be read from disk.  However, in practice, a 23MB file is probably a single row group so this will not help.\r\n\r\n> Is there some way or workaround to reduce the memory usage with read filtering? \r\n\r\nPage indices (https://blog.cloudera.com/speeding-up-select-queries-with-parquet-page-indexes/) could help reduce the amount of data read from disk and I would like to implement this someday but this is sadly not a priority for me at the moment.  However, I don't think this (too much RAM used) is necessarily a \"reduce the amount of data read from disk\" type of problem."
        },
        {
            "created_at": "2022-09-01T18:13:57.759Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599102) by Yin (zyd02):*\nHi Will,\u00a0\r\nI am using pandas.read_parquet, which goes to pyarrow.parquet.read_table.\r\nTried pre_buffer=False, and saw no difference.\r\nI will try to use iter_batches and filtering.\r\n\r\nAttached the sample code [sample.py](sample.py), with memory stats on a Windows machine.\r\nUsed Pyarrow 9.0.0 (and 7.0.0) and the latest pandas 1.4.4 and numpy 1.23.2.\u00a0\r\nSaw that Pyarrow 9.0.0 improved read_table and saved some memory, but to_pandas is still about the same.\r\nThe main question here is why read_table uses the same amount memory when loads all columns with filtering similar as without filtering.\r\n\r\n\r\nBy the way, wonder if memory allocation for repeating strings (e.g. empty) and None can be more efficient in to_pandas as well, without requiring use Categorical columns explicitly.\r\n\r\nThanks,\r\n-Yin"
        },
        {
            "created_at": "2022-09-01T18:33:50.346Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599104) by Yin (zyd02):*\nHi Weston,\u00a0 Just saw your comment. Will try it in the sample code. Thanks\u00a0\r\n\r\nUpdate:\r\n\r\nPrinted out pyarrow.total_allocated_bytes and table.nbytes.\r\nBelow is in the updated sample code.\r\n\r\nIn the case A: reading all columns with the filter,\u00a0\r\ntotal_allocated_bytes is 289 MB and dt.nbytes is very small.\r\n\r\ncase B reads one column with the filter.\r\ncase C reads all columns without filter.\r\n\r\n1. total_allocated_bytes\u00a0 and table.nbytes\n1. pyarrow 7.0.0, pandas 1.4.4 numpy 1.23.2\n1. A: 289 MB 0.00115 MB B: 3.5 MB 9.53-e06 MB C: 289.72 MB 288.38 MB\n1. pyarrow 9.0.0, pandas 1.4.4 numpy 1.23.2\n1. A: 289 MB 0.0014 MB B: 3.5 MB 1.049-e05 MB C: 289.72 MB 288.38 MB\n   \n1. rss memory after read_table\n1. pyarrow 7.0.0, pandas 1.4.4 numpy 1.23.2\n1. A: 1008 MB B: 88 MB C: 1008 MB\n1. pyarrow 9.0.0 pandas 1.4.4 numpy 1.23.2\n1. A: 394 MB B: 85 MB C: 393 MB"
        },
        {
            "created_at": "2022-09-02T02:32:14.348Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599225) by Weston Pace (westonpace):*\nSo I debugged this a little today and I think one problem is that the scanner continues to run in the background for just a little bit after the read_table operation finishes.  This is something I am currently working on and will be addressed by ARROW-15732.  As a result, even though the table itself is not using much memory, there is still the scanner which is holding onto a bunch of memory in the background.\r\n\r\nIf you put a sleep (e.g. 5 seconds) after the call to read_table you should notice the total_allocated_bytes shrink down so that it matches the value in nbytes (or is close).\r\n\r\nThere is also a question as to why the scanner is holding onto so much memory.  I think this is a result of the way we are handling pre-buffering and I have filed ARROW-17599 to hopefully address that.\r\n"
        },
        {
            "created_at": "2022-09-02T13:18:18.505Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17590?focusedCommentId=17599489) by Yin (zyd02):*\nYep,\r\npa.total_allocated_bytes 289.74639892578125 MB dt.nbytes 0.0011539459228515625 MB\r\nsleep\u00a0 5 seconds\r\npa.total_allocated_bytes 0.0184326171875 MB dt.nbytes 0.0011539459228515625 MB\r\n\r\nThanks Weston. Let me close this jira."
        }
    ]
}