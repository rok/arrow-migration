{
    "issue": {
        "title": "[Python] After upgrade pyarrow from 0.15 to 0.17.1 connect to hdfs don`t work with libdfs jni",
        "body": "***Note**: This issue was originally created as [ARROW-8988](https://issues.apache.org/jira/browse/ARROW-8988). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n## Problem\r\n\r\nAfter upgrade pyarrow from 0.15 to 0.17, I have a some troubles. I understand, that libhdfs3 no support now. However, in my case, libhdfs not work too. See below.\r\n\r\nMy experience in the Hadoop ecosystem is not big. Maybe, I took a some wrongs. I installed Hortonworks HDP \u00a0from Ambari service on the virtual machine, installed on my PC.\r\n\r\nI try that..\r\n\r\n1.\u00a0 just connect..\r\n\r\n%xmode Verbose\r\nimport pyarrow as pa\r\n\r\nhdfs = pa.hdfs.connect(host='hdp.test.com', port=8020, user='hdfs')\r\n\r\n\u2014\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'hadoop': 'hadoop' ([#1.txt])\r\n\r\n2.\u00a0to bypass if driver == 'libhdfs'..\r\n\r\n%xmode Verbose\r\n\r\nimport pyarrow as pa\r\n\r\nhdfs = pa.HadoopFileSystem(host='hdp.test.com', port=8020, user='hdfs', driver=None')\r\n\r\n\u2014\r\n\r\nOSError: Unable to load libjvm: /usr/java/latest//lib/server/libjvm.so: cannot open shared object file: No such file or directory ([#2.txt])\r\n\r\n3. With libhdfs3 it working:\r\n\r\nimport hdfs3\u00a0\r\n\r\nhdfs = hdfs3.HDFileSystem(host='hdp.test.com', port=8020, user='hdfs')\r\n\r\n#ls remote folder\r\nhdfs.ls('/data/', detail=False)\r\n\r\n['/data/TimeSheet.2020-04-11', '/data/test', '/data/test.json']\r\n## Environment.\r\n#### Client PC:\r\n\r\nOS: Debian 10. Dev.: Anaconda3 (python 3.7.6), Jupyter Lab 2, pyarrow 0.17.1 (from conda-forge)\r\n\r\nHadoop (on VM \u2013 Oracle VirtualBox):\r\n\r\nOS: Oracle Linux 7.6.\u00a0 Distr.: Hortonworks HDP 3.1.4\r\n\r\nlibhdfs.so:\r\n\r\n[root@hdp /]# find / -name libhdfs.so\r\n /usr/lib/ams-hbase/lib/hadoop-native/libhdfs.so\r\n /usr/hdp/3.1.4.0-315/usr/lib/libhdfs.so\r\n\r\n\u00a0\r\n\r\n\u00a0Java path:\r\n\r\n[root@hdp /]# sudo alternatives -~~config java\r\n \r\n-~~---------------------------------------------\r\n \\*+ 1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/jre/bin/java)\r\n\r\n\u00a0\r\n\r\nlibjvm:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\r\n\r\n[root@hdp /]# find / -name libjvm.\\*\r\n /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64/jre/lib/amd64/server/libjvm.so\r\n /usr/jdk64/jdk1.8.0_112/jre/lib/amd64/server/libjvm.so\r\n\r\n\u00a0\r\n\r\nI tried many settings (. Below last :\r\n\r\n1. etc/profile.\n    ...\n   export JAVA_HOME=$(dirname $(dirname $(readlink $(readlink $(which javac)))))\n   export JRE_HOME=$JAVA_HOME/jre\n   export JAVA_CLASSPATH=$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar\n   export HADOOP_HOME=/usr/hdp/3.1.4.0-315/hadoop\n   export HADOOP_CLASSPATH=$(find $HADOOP_HOME -name '\\*.jar' | xargs echo | tr ' ' ':')\n   export ARROW_LIBHDFS_DIR=/usr/lib/ams-hbase/lib/hadoop-native\n   \n   export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n   export CLASSPATH==.:$CLASSPATH:$JAVA_CLASSPATH:$HADOOP_CLASSPATH\n   \n   export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JRE_HOME/lib/amd64/server\n   \n   \u00a0\n   \u00a0",
        "created_at": "2020-05-31T12:41:53.000Z",
        "updated_at": "2022-07-12T14:04:32.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-07-12T14:04:32.394Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8988?focusedCommentId=17565690) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}