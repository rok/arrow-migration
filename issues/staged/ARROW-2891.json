{
    "issue": {
        "title": "[Python] Preserve schema in write_to_dataset",
        "body": "***Note**: This issue was originally created as [ARROW-2891](https://issues.apache.org/jira/browse/ARROW-2891). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen using `pyarrow.parquet.write_to_dataset` with `partition_cols` set, the schema of the `table` passed into the function is not enforced when iterating over the `subgroup` to create the `subtable`. See\u00a0[here\\|[https://github.com/apache/arrow/blob/master/python/pyarrow/parquet.py#L1146]].\r\n\r\nSince pandas is used to generate the subtables, there is a risk that some specificity is lost from the original `table.schema` due to the data types supported by pandas and some of the internal type conversions pandas performs. It would be ideal if a `subschema` was generated from `table.schema` and passed to `Table` when instantiating the `subtable` to allow the user to enforce the original schema.\r\n\r\nHere is a simple example of where we are running into issues while trying to preserve a valid schema. This use case is more likely to occur when working with sparse data sets.\r\n\r\n\u00a0\r\n```java\n\r\n>>> from io import StringIO\r\n>>> import pandas as pd\r\n>>> import numpy as np\r\n>>> import pyarrow as pa\r\n>>> import parquet as pq\r\n>>> import pyarrow.parquet as pq\r\n\r\n# in csv col2 has no NaNs and in csv_nan col2 only has NaNs\r\n>>> csv = StringIO('\"1\",\"10\",\"100\"')\r\n>>> csv_nan = StringIO('\"2\",\"\",\"200\"')\r\n\r\n# read in col2 as a float since pandas does not support NaNs in ints\r\n>>> pd_dtype = {'col1': np.int32, 'col2': np.float32, 'col3': np.int32}\r\n>>> df = pd.read_csv(csv, header=None, names=['col1', 'col2', 'col3'], dtype=pd_dtype)\r\n>>> df_nan = pd.read_csv(csv_nan, header=None, names=['col1', 'col2', 'col3'], dtype=pd_dtype)\r\n\r\n# verify both dfs and their dtypes\r\n>>> df\r\ncol1 col2 col3\r\n0 1 10.0 100\r\n\r\n>>> df.dtypes\r\ncol1 int32\r\ncol2 float32\r\ncol3 int32\r\ndtype: object\r\n\r\n>>> df_nan\r\ncol1 col2 col3\r\n0 2 NaN 200\r\n\r\n>>> df_nan.dtypes\r\ncol1 int32\r\ncol2 float32\r\ncol3 int32\r\ndtype: object\r\n\r\n# define col2 as an int32 since pyarrow does support NaNs in ints\r\n# we want to preserve the original schema we started with and not\r\n# upcast just because we're using pandas to go from csv to pyarrow\r\n>>> schema = pa.schema([pa.field('col1', type=pa.int32()),\r\npa.field('col2', type=pa.int32()),\r\npa.field('col3', type=pa.int32())])\r\n\r\n# verify schema\r\n>>> schema\r\ncol1: int32\r\ncol2: int32\r\ncol3: int32\r\n\r\n# create tables\r\n>>> table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)\r\n>>> table_nan = pa.Table.from_pandas(df_nan, schema=schema, preserve_index=False)\r\n\r\n# verify table schemas and metadata\r\n# col2 has pandas_type int32 and numpy_type float32 in both tables\r\n>>> table\r\npyarrow.Table\r\ncol1: int32\r\ncol2: int32\r\ncol3: int32\r\nmetadata\r\n--------\r\n{b'pandas': b'{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":'\r\nb' \"col1\", \"field_name\": \"col1\", \"pandas_type\": \"int32\", \"numpy_ty'\r\nb'pe\": \"int32\", \"metadata\": null}, {\"name\": \"col2\", \"field_name\": '\r\nb'\"col2\", \"pandas_type\": \"int32\", \"numpy_type\": \"float32\", \"metada'\r\nb'ta\": null}, {\"name\": \"col3\", \"field_name\": \"col3\", \"pandas_type\"'\r\nb': \"int32\", \"numpy_type\": \"int32\", \"metadata\": null}], \"pandas_ve'\r\nb'rsion\": \"0.22.0\"}'}\r\n\r\n>>> table_nan\r\npyarrow.Table\r\ncol1: int32\r\ncol2: int32\r\ncol3: int32\r\nmetadata\r\n--------\r\n{b'pandas': b'{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":'\r\nb' \"col1\", \"field_name\": \"col1\", \"pandas_type\": \"int32\", \"numpy_ty'\r\nb'pe\": \"int32\", \"metadata\": null}, {\"name\": \"col2\", \"field_name\": '\r\nb'\"col2\", \"pandas_type\": \"int32\", \"numpy_type\": \"float32\", \"metada'\r\nb'ta\": null}, {\"name\": \"col3\", \"field_name\": \"col3\", \"pandas_type\"'\r\nb': \"int32\", \"numpy_type\": \"int32\", \"metadata\": null}], \"pandas_ve'\r\nb'rsion\": \"0.22.0\"}'}\r\n\r\n# write both tables to local filesystem\r\n>>> pq.write_to_dataset(table, '/Users/jkulzick/pyarrow_example',\r\npartition_cols=['col1'],\r\npreserve_index=False)\r\n>>> pq.write_to_dataset(table_nan, '/Users/jkulzick/pyarrow_example',\r\npartition_cols=['col1'],\r\npreserve_index=False)\r\n\r\n# read parquet files into a ParquetDataset to validate the schemas\r\n# the metadata and schemas for both files is different from their original tables\r\n# table now has pandas_type int32 and numpy_type int32 (was float32) for col2\r\n# table_nan now has pandas_type float64 (was int32) and numpy_type int64 (was float32) for col2\r\n>>> ds = pq.ParquetDataset('/Users/jkulzick/pyarrow_example')\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/Users/jkulzick/miniconda3/envs/bowerbird/lib/python3.6/site-packages/pyarrow/parquet.py\", line 745, in __init__\r\nself.validate_schemas()\r\nFile \"/Users/jkulzick/miniconda3/envs/bowerbird/lib/python3.6/site-packages/pyarrow/parquet.py\", line 775, in validate_schemas\r\ndataset_schema))\r\nValueError: Schema in partition[col1=1] /Users/jkulzick/pyarrow_example/col1=2/b7b42ce9de6a46a786a5361c42d28731.parquet was different. \r\ncol2: double\r\ncol3: int32\r\nmetadata\r\n--------\r\n{b'pandas': b'{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":'\r\nb' \"col2\", \"field_name\": \"col2\", \"pandas_type\": \"float64\", \"numpy_'\r\nb'type\": \"float64\", \"metadata\": null}, {\"name\": \"col3\", \"field_nam'\r\nb'e\": \"col3\", \"pandas_type\": \"int32\", \"numpy_type\": \"int32\", \"meta'\r\nb'data\": null}], \"pandas_version\": \"0.22.0\"}'}\r\n\r\nvs\r\n\r\ncol2: int32\r\ncol3: int32\r\nmetadata\r\n--------\r\n{b'pandas': b'{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":'\r\nb' \"col2\", \"field_name\": \"col2\", \"pandas_type\": \"int32\", \"numpy_ty'\r\nb'pe\": \"int32\", \"metadata\": null}, {\"name\": \"col3\", \"field_name\": '\r\nb'\"col3\", \"pandas_type\": \"int32\", \"numpy_type\": \"int32\", \"metadata'\r\nb'\": null}], \"pandas_version\": \"0.22.0\"}'}\n```",
        "created_at": "2018-07-21T04:40:15.000Z",
        "updated_at": "2019-05-08T11:35:54.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2018-07-23T16:32:27.000Z"
    },
    "comments": [
        {
            "created_at": "2018-07-23T16:32:27.984Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2891?focusedCommentId=16553079) by Uwe Korn (uwe):*\nIssue resolved by pull request 2302\n<https://github.com/apache/arrow/pull/2302>"
        }
    ]
}