{
    "issue": {
        "title": "[Python] pyarrow<3 incompatible with numpy>=1.20.0",
        "body": "***Note**: This issue was originally created as [ARROW-11450](https://issues.apache.org/jira/browse/ARROW-11450). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\npyarrow 1.0 and 2.0 is not compatible with numpy 1.20.0\r\n\r\nRunning the following command would fail:\r\n\r\n`pa.array(np.arange(10))`\r\n\r\nwith error\r\n\r\n`pyarrow.lib.ArrowTypeError: Did not pass numpy.dtype object`\r\n\r\nNumpy release note [[link\\|#compatibility-notes]]]\u00a0mentions about the np.dtype related compatibility breaks. Also there is a C API change, which implies numpy dependency constraints should be tighter (whether <=1.20 or >1.20) depending on the compiled numpy version (if pyarrow is depending on it; I'm not aware of the pyarrow internal).",
        "created_at": "2021-02-01T02:01:58.000Z",
        "updated_at": "2021-02-04T09:54:58.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-02-01T08:28:14.000Z"
    },
    "comments": [
        {
            "created_at": "2021-02-01T08:27:50.829Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17276135) by Joris Van den Bossche (jorisvandenbossche):*\nThis is known issue with wheels of older versions, see the linked issue and https://github.com/numpy/numpy/issues/17913. This is fixed in the latest `pyarrow` 3.0.0 release, so please update your `pyarrow` version if you want to use the latest NumPy release."
        },
        {
            "created_at": "2021-02-01T17:25:22.918Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17276510) by Zhuo Peng (brillsp):*\nThere are libraries whose latest release (and even the dev branch) caps the pyarrow at <3, for example, apache-beam:\r\n\r\n<https://github.com/apache/beam/blob/3d6cb85e4961585ae9c4fa3c7226916fef68998f/sdks/python/setup.py#L150>\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nI wonder if patch releases can be made to pyarrow 1.x and 2.x to simply cap numpy < 1.20?"
        },
        {
            "created_at": "2021-02-01T17:35:53.666Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17276519) by Antoine Pitrou (apitrou):*\nUnfortunately the release process is quite involved currently and making a patch release is heavier than we'd like it to. I would say we're unlikely to make a 2.x patch release at this point."
        },
        {
            "created_at": "2021-02-01T18:50:33.732Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17276582) by Joris Van den Bossche (jorisvandenbossche):*\n> There are libraries whose latest release (and even the dev branch) caps the pyarrow at <3, for example, apache-beam:\r\n\r\nIt's probably best to then pin numpy to < 1.20 as well"
        },
        {
            "created_at": "2021-02-03T19:58:36.816Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17278331) by Brian Hulette (bhulette):*\nYou're right, in our older releases we should have anticipated an issue like this and made our numpy bound <1.20.0, and if it's a problem we could **also** make a bugfix release to lower the bound (although we have the same problem with our release process).\r\n\r\nI'm actually more concerned about what we're going to do for future releases. We're trying to keep our version spec as wide as possible, and testing against pyarrow 1.x, 2.x, (and soon 3.x) so our users and downstream libraries don't need to upgrade in lockstep with us. I think we'll just have to keep a numpy <1.20.0 bound until we can drop 1.x, 2.x support. Experienced users could always override it."
        },
        {
            "created_at": "2021-02-03T19:59:19.403Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17278332) by Brian Hulette (bhulette):*\n(speaking as a Beam committer here when I say \"our\")"
        },
        {
            "created_at": "2021-02-03T20:11:12.367Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17278338) by Joris Van den Bossche (jorisvandenbossche):*\n>  We're trying to keep our version spec as wide as possible, and testing against pyarrow 1.x, 2.x, (and soon 3.x) so our users and downstream libraries don't need to upgrade in lockstep with us. I think we'll just have to keep a numpy <1.20.0 bound until we can drop 1.x, 2.x support.\r\n\r\nGenerally speaking (not knowing beam specifically here), I think it should still be possible to support this wide range, and for testing you can pin numpy < 1.20 for the builds testing pyarrow 1.x and 2.x, and not pin numpy on the build testing pyarrow 3.x (without needing to require numpy < 1.20 generally)? \r\n(at least that's what we do for pandas) "
        },
        {
            "created_at": "2021-02-03T20:19:39.978Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17278343) by Brian Hulette (bhulette):*\nThanks for the response. That's a good point, we could do that in Beam as well.\r\nI'm just trying to protect our users from unintentionally using numpy 1.20.0 and pyarrow <3. In practice it's probably hard for them to get into that situation, but it would be nice if we had some actual boundaries restricting it."
        },
        {
            "created_at": "2021-02-03T21:06:40.359Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11450?focusedCommentId=17278357) by Joris Van den Bossche (jorisvandenbossche):*\nIndeed, doing such pinning in your own CI of course doesn't prevent users bumping into this incompatible version combo. Unfortunately, I am not sure it easy to fix with version constraints (retro-actively). Even if you would do a bugfix release now to lower the numpy bound, a solver would probably find the older version of beam that doesn't have this bound when it wants to install beam together with numpy 1.20, in which case you can still have the same issue. \r\n(a new beam (bugfix) release to remove the pyarrow < 3.0 bound might be more effective, though) "
        }
    ]
}