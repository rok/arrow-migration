{
    "issue": {
        "title": "[JS] Unable to open reader on .arrow file after fetch: Uncaught (in promise) Error: Expected to read 1329865020 metadata bytes, but only read 1123.",
        "body": "***Note**: This issue was originally created as [ARROW-17123](https://issues.apache.org/jira/browse/ARROW-17123). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI created a file in raw arrow format with the script given in the Py arrow cookbook here: <https://arrow.apache.org/cookbook/py/io.html#saving-arrow-arrays-to-disk>\r\n\r\n\u00a0\r\n\r\nIn a Node.js application, this file can be read doing:\r\n\r\n\u00a0\r\n```java\n\r\nconst r = await RecordBatchReader.from(fs.createReadStream(filePath));                \r\nawait r.open();\r\n\r\nfor (let i = 0; i < r.numRecordBatches; i++) {\r\n    const rb = await r.readRecordBatch(i); \r\n    if (rb !== null) {\r\n        console.log(rb.numRows);\r\n    }\r\n} \n```\r\nHowever this method loads the whole file in memory (is that a bug?), which is not scalable.\r\n\r\n\u00a0\r\n\r\nTo solve this scalability issue, I try to load the data with fetch as described in the the [README.md\\|#load-data-with-fetch]. Both:\r\n\r\n\u00a0\r\n```java\n\r\nimport { tableFromIPC } from \"apache-arrow\";\r\n\r\nconst table = await tableFromIPC(fetch(filePath));\r\nconsole.table([...table]);\n```\r\n\u00a0and\r\n```java\n\r\nconst r = await RecordBatchReader.from(await fetch(filePath));                \r\nawait r.open(); \n```\r\nfail with error:\r\n\r\nUncaught (in promise) Error: Expected to read 1329865020 metadata bytes, but only read 1123.\r\n\r\n\u00a0",
        "created_at": "2022-07-19T09:57:15.000Z",
        "updated_at": "2022-07-26T23:43:58.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: JavaScript",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}