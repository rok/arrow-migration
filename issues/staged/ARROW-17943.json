{
    "issue": {
        "title": "[Python] Coredump when joining big large_strings",
        "body": "***Note**: This issue was originally created as [ARROW-17943](https://issues.apache.org/jira/browse/ARROW-17943). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\njoining large strings in pyarrow results in this error:\r\n\r\n\r\n```java\n\r\nterminate called after throwing an instance of 'std::length_error'\r\n\u00a0 what(): \u00a0vector::_M_default_append\r\nAborted (core dumped) \n```\r\n\r\nexample code:\r\nnote that this needs quite some ram (run on 128GB)\r\n```java\n\r\nimport pyarrow as pa \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0\r\nids = [x for x in range(2**24)] \u00a0 \u00a0\r\ntext = ['a'*2**10]*2**24 \u00a0 \u00a0\r\nschema = pa.schema([ \u00a0 \u00a0\r\n\u00a0 \u00a0 ('Id', pa.int32()), \u00a0 \u00a0\r\n\u00a0 \u00a0 ('Text', pa.large_string()), \u00a0 \u00a0\r\n\u00a0 \u00a0 ]) \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0\r\ntab1 = pa.Table.from_arrays([ids, text],schema=schema) \u00a0 \u00a0\r\ntab2 = pa.Table.from_arrays([ids, text],schema=schema) \u00a0 \u00a0\r\n\u00a0 \u00a0 \u00a0\r\njoined = tab1.join(tab2, keys='Id', right_keys='Id', left_suffix='tab1')\u00a0 \n```\r\n\r\nthe same results in a segfault, if i use this schema\r\n```java\n\r\nschema = pa.schema([\r\n\u00a0 \u00a0 ('Id', pa.int32()),\r\n\u00a0 \u00a0 ('Text', pa.string()),\r\n\u00a0 \u00a0 ])\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2022-10-05T17:28:04.000Z",
        "updated_at": "2022-10-11T02:10:07.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-10-08T05:14:32.455Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17943?focusedCommentId=17614361) by Yibo Cai (yibocai):*\nThe code below triggers same error log. Try it online: https://onlinegdb.com/UpqUsk4Zv\r\nLooks this might be caused by integer overflow which leads to a huge buffer size greater than `std::vector::max_size()`.\r\n\r\n```cpp\n\r\n#include <vector>\r\n\r\nint main() {\r\n    std::vector<int> v;\r\n    v.resize(-1ULL);\r\n    return 0;\r\n}\r\n```\r\n"
        },
        {
            "created_at": "2022-10-08T10:38:41.494Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17943?focusedCommentId=17614462) by Yibo Cai (yibocai):*\nError comes from below line, `total_length = -2128609280`\r\n<https://github.com/apache/arrow/blob/master/cpp/src/arrow/compute/kernels/row_encoder.cc#L344>\r\n\r\n"
        },
        {
            "created_at": "2022-10-10T01:52:34.064Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17943?focusedCommentId=17614836) by Yibo Cai (yibocai):*\n`[~michalno]`, do you have comments? change `offsets_` from `int32` to `int64`?"
        },
        {
            "created_at": "2022-10-10T17:11:31.426Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17943?focusedCommentId=17615221) by flowpoint (flowpoint):*\ncool, with this tip i think i got it to work for now for my usecase\r\nthis is ofc. not a true solution\r\n<https://gist.github.com/flowpoint/08e76e9a90544009b298e5bea9219236>\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-10-11T02:10:07.775Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17943?focusedCommentId=17615415) by Yibo Cai (yibocai):*\nGlad to hear that `[~flowpoint]`.\r\nSince you already have a patch (thought not a true solution), I think it might be worth to submit a PR for review."
        }
    ]
}