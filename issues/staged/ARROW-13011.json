{
    "issue": {
        "title": "[Python] HadoopFileSystem crash when called twice and Java was misconfigured",
        "body": "***Note**: This issue was originally created as [ARROW-13011](https://issues.apache.org/jira/browse/ARROW-13011). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nSee <https://github.com/dask/dask/pull/7752#issuecomment-856231163> and discussion below.\r\n\r\n\u00a0\r\n\r\nDidn't investigate yet (and I also think dask cannot yet use the new filesystem, since it has a different API, but it should nonetheless not crash ..), there is a docker workflow to reproduce the tests I can try.",
        "created_at": "2021-06-08T15:42:38.000Z",
        "updated_at": "2021-08-04T09:21:08.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-06-16T14:14:48.210Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13011?focusedCommentId=17364322) by Joris Van den Bossche (jorisvandenbossche):*\nI could reproduce it with the dask docker image, and could also further slim down to get a small reproducer. The segfault seems to occur when trying to create the HadoopFileSystem multiple times without having the proper env variable set up:\r\n\r\n```python\n\r\n>>> from pyarrow.fs import HadoopFileSystem\r\n>>> hdfs = HadoopFileSystem(host=\"localhost\", port=8020)\r\nEnvironment variable CLASSPATH not set!\r\ngetJNIEnv: getGlobalJNIEnv failed\r\n../src/arrow/filesystem/hdfs.cc:51: Failed to disconnect hdfs client: IOError: HDFS hdfsFS::Disconnect failed, errno: 9 (Bad file descriptor)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"pyarrow/_hdfs.pyx\", line 83, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow/error.pxi\", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 99, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n>>> hdfs = HadoopFileSystem(host=\"localhost\", port=8020)\r\nSegmentation fault (core dumped)\r\n```"
        },
        {
            "created_at": "2021-06-21T16:43:12.944Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13011?focusedCommentId=17366721) by Antoine Pitrou (apitrou):*\nOk, I investigated this and it looks very much like a bug in Hadoop's libhdfs native client. I will file a bug there."
        },
        {
            "created_at": "2021-06-21T17:01:44.542Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13011?focusedCommentId=17366736) by Antoine Pitrou (apitrou):*\nReported at HDFS-16084. I don't think there's any reasonable way to workaround this in Arrow."
        },
        {
            "created_at": "2021-06-22T09:13:47.345Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13011?focusedCommentId=17367162) by Joris Van den Bossche (jorisvandenbossche):*\nThanks for the investigation!"
        }
    ]
}