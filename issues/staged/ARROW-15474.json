{
    "issue": {
        "title": "[Python] Possibility of a table.drop_duplicates() function?",
        "body": "***Note**: This issue was originally created as [ARROW-15474](https://issues.apache.org/jira/browse/ARROW-15474). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI noticed that there is a group_by() and sort_by() function in the 7.0.0 branch. Is it possible to include a drop_duplicates() function as well? \r\n\r\n\n|id|updated_at|\r|\n|-|-|-|\n|1|2022-01-01 04:23:57|\r|\n|2|2022-01-01 07:19:21|\r|\n|2|2022-01-10 22:14:01|\r<br>\r<br>Something like this which would return a table without the second row in the example above would be great. \r<br>\r<br>I usually am reading an append-only dataset and then I need to report on latest version of each row. To drop duplicates, I am temporarily converting the append-only table to a pandas DataFrame, and then I convert it back to a table and save a separate \"latest-version\" dataset.\r<br>\r<br>```python<br>\r<br>table.sort_by(sorting=[(\"id\", \"ascending\"), (\"updated_at\", \"ascending\")]).drop_duplicates(subset=[\"id\"] keep=\"last\")\r<br>```\r<br>\r<br>\r<br>\r<br>\r|\n",
        "created_at": "2022-01-27T02:14:36.000Z",
        "updated_at": "2022-11-14T18:21:58.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-01-27T02:59:32.128Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17482846) by Weston Pace (westonpace):*\nWe're pretty close in 7.0.0 but \"first\" and \"last\" are tricky concepts for the execution engine at the moment (it generally processes items on a first-come first-serve basis and so order isn't really maintained).\r\n\r\nAre you ok with \"min\" or \"max\"?\r\n\r\n```Java\n\r\nimport pyarrow as pa\r\ntab = pa.Table.from_pydict({'x': [1, 2, 2, 3], 'y': ['a', 'b', 'c', 'd']})\r\n>>> tab.group_by(\"x\").aggregate([(\"y\", \"max\")])\r\npyarrow.Table\r\ny_max: string\r\nx: int64\r\n----\r\ny_max: [[\"a\",\"c\",\"d\"]]\r\nx: [[1,2,3]]\r\n>>> tab.group_by(\"x\").aggregate([(\"y\", \"min\")])\r\npyarrow.Table\r\ny_min: string\r\nx: int64\r\n----\r\ny_min: [[\"a\",\"b\",\"d\"]]\r\nx: [[1,2,3]]\r\n```"
        },
        {
            "created_at": "2022-01-27T03:02:43.385Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17482849) by Weston Pace (westonpace):*\nActually, I guess this would be tricky if you have multiple columns because it would return the min/max of each particular column independently.  So you might get a row that doesn't even really exist in the source data."
        },
        {
            "created_at": "2022-01-27T13:08:42.614Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17483114) by Lance Dacey (ldacey):*\nI would personally be okay with only having the first row retained since I could just sort the table before dropping duplicates to get the desired results.\r\n\r\nIs it possible to get the first or nth values from a table groupby? In pandas, we can do this which I think has the desired behavior even with multiple columns (as long as we sort the data first). If we can get the indices of which rows to keep, then we could use table.take() to return a new table with the latest values.\r\n```python\n\r\ndf = pd.DataFrame(\r\n    {\r\n        \"id\": [1, 1, 1, 2, 2, 2],\r\n        \"name\": [\"a\", \"a\", \"a\", \"b\", \"c\", \"c\"],\r\n        \"updated_at\": [\r\n            \"2021-01-01 00:02:19\",\r\n            \"2022-01-04 12:13:10\",\r\n            \"2022-01-06 04:10:52\",\r\n            \"2022-01-02 17:32:21\",\r\n            \"2022-01-06 01:27:14\",\r\n            \"2022-01-06 23:09:56\",\r\n        ],\r\n    }\r\n)\r\n\r\ndf.sort_values([\"id\", \"name\", \"updated_at\"], ascending=[1, 1, 0]).groupby([\"id\", \"name\"]).nth(0).reset_index()\r\n```"
        },
        {
            "created_at": "2022-01-27T23:21:21.701Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17483469) by Weston Pace (westonpace):*\nI think we are close (still probably need to add a dedicated exec node for this) to being able to do:\r\n\r\n```Java\n\r\ndrop_duplicates(subset=[\"id\"] keep=\"random\")\r\n```\r\n\r\nI think we are further away from either:\r\n```Java\n\r\ndrop_duplicates(subset=[\"id\"] keep=\"first\")\r\ndrop_duplicates(subset=[\"id\"] keep=\"last\")\r\n```\r\n\r\nThe problem is that the execution engine processes data in batches and those batches are processed in parallel.  We can resequence the batches, but only at the very end of a plan.  Otherwise they just get out of order again immediately after we resequence them.\r\n\r\nThis ability to run parts of a plan in sequence is something we need for a number of reasons (e.g. window functions) and so it is something we will likely have at some point.\r\n"
        },
        {
            "created_at": "2022-01-28T00:12:18.734Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17483481) by Lance Dacey (ldacey):*\nAhh, that would be great. Random is a bit risky for my use case since I generally care about the latest version.\r\n\r\nI found [this repository](https://github.com/TomScheffers/pyarrow_ops/tree/main/pyarrow_ops) which has a method to drop duplicates that I might be able to adopt in the meantime. I would need to digest exactly what is happening down below a bit more, but I think there are some compute functions like `pc.sort_indices`,  `pc.unique`, etc that could probably be used to replace some of the numpy code. \r\n\r\n```python\n\r\ndef drop_duplicates(table, on=[], keep='first'):\r\n# Gather columns to arr\r\n    arr = columns_to_array(table, (on if on else table.column_names))\r\n\r\n# Groupify\r\n    dic, counts, sort_idxs, bgn_idxs = groupify_array(arr)\r\n\r\n# Gather idxs\r\n    if keep == 'last':\r\n        idxs = (np.array(bgn_idxs) - 1)[1:].tolist() + [len(sort_idxs) - 1]\r\n    elif keep == 'first':\r\n        idxs = bgn_idxs\r\n    elif keep == 'drop':\r\n        idxs = [i for i, c in zip(bgn_idxs, counts) if c == 1]\r\n    return table.take(sort_idxs[idxs])\r\n\r\ndef groupify_array(arr):\r\n# Input: Pyarrow/Numpy array\r\n# Output:\r\n#   - 1. Unique values\r\n#   - 2. Sort index\r\n#   - 3. Count per unique\r\n#   - 4. Begin index per unique\r\n    dic, counts = np.unique(arr, return_counts=True)\r\n    sort_idx = np.argsort(arr)\r\n    return dic, counts, sort_idx, [0] + np.cumsum(counts)[:-1].tolist()\r\n\r\ndef combine_column(table, name):\r\n    return table.column(name).combine_chunks()\r\n\r\nf = np.vectorize(hash)\r\ndef columns_to_array(table, columns):\r\n    columns = ([columns] if isinstance(columns, str) else list(set(columns)))\r\n    if len(columns) == 1:\r\n        return f(combine_column(table, columns[0]).to_numpy(zero_copy_only=False))\r\n    else:\r\n        values = [c.to_numpy() for c in table.select(columns).itercolumns()]\r\n        return np.array(list(map(hash, zip(*values))))\r\n```\r\n\u00a0"
        },
        {
            "created_at": "2022-04-20T13:22:47.216Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17524991) by Lance Dacey (ldacey):*\nI'll keep this open since this is a major wish list item for me. If anyone has some sample functions they have implemented outside of core pyarrow to achieve this then I would be interested in seeing that as well."
        },
        {
            "created_at": "2022-10-24T19:19:56.518Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17623355) by Jacek Pliszka (jacek.pliszka):*\nLance - the code you have posted might not be very efficient - something like below should be faster:\r\n\r\n1. add column with sequential number - index\r\n\r\n```python\n\r\nimport numpy as np\r\nimport pyarrow.compute as pc\r\nt1=t.append_column('i', pa.array(np.arange(len(t))))\r\n```\r\n\r\n2. Find first row indices\r\n```python\n\r\nt2 = t1.group_by(['keys', 'values']).aggregate([('i', 'min')]).column('i_min')\r\n```\r\n\r\n3. select rows with first row indices:\r\n```python\n\r\npc.take(t, t2)\r\n```\r\n\r\n\r\nOn my PC your code is 1.19s while code above is 0.15s. to_pandas.drop_duplicates was around 0.36s"
        },
        {
            "created_at": "2022-10-24T19:26:08.400Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17623362) by Lance Dacey (ldacey):*\nNice - I will give that a shot, thanks. I have been using a library called `polars` to drop duplicates from a pyarrow table lately, but it would be nice to have a native-pyarrow way to do it.\r\n\r\nCan we sort the data before adding the `cumulative_sum`? My concern is that the order of the raw data might be messed up and we might select the wrong row to keep."
        },
        {
            "created_at": "2022-10-24T19:34:01.647Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17623366) by Jacek Pliszka (jacek.pliszka):*\nI added one more speedup - definitely you can sort it at the beginning."
        },
        {
            "created_at": "2022-11-09T20:40:20.266Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17631279) by Lance Dacey (ldacey):*\nNice, I was able to test it out and seemed to get the correct results. I have been using polars and duckdb to handle de-duplication for a while now so I used that as a comparison.\r\n\r\n```java\n\r\n%%time\r\n\r\ntable = con.execute(\"select distinct on (forecast_group) * from scanner order by session_id, date\").arrow()\r\n\r\nCPU times: user 735 ms, sys: 45.7 ms, total: 780 ms\r\nWall time: 1.92 s\r\n```\r\n\r\n\r\nYour suggestion:\r\n\r\n```java\n\r\n%%time \r\n\r\ntable = scanner.to_table()\r\n\r\nt1 = table.append_column('i', pa.array(np.arange(len(table))))\r\nt2 = t1.group_by(['forecast_group']).aggregate([('i', 'min')]).column('i_min')\r\ntable = pc.take(table, t2)\r\n\r\nCPU times: user 872 ms, sys: 60.9 ms, total: 933 ms\r\nWall time: 4.6 s\r\n```\r\n\r\n\r\nA bit slower than duckdb somehow, but for me it is acceptable and gives me an option to drop duplicates without requiring additional libraries, including pandas. Thanks!\r\n"
        },
        {
            "created_at": "2022-11-10T18:42:52.188Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17631856) by Jacek Pliszka (jacek.pliszka):*\nIf you really want performance and your algo is as above then it should be relatively easy to write Cython function to handle this case.\r\nEspecially if forecast_group can be 0 to n.\r\n"
        },
        {
            "created_at": "2022-11-11T11:01:12.493Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17632256) by Jacek Pliszka (jacek.pliszka):*\n`[~westonpace]` maybe approach similar to what I proposed, but in better version whould work?\r\n\r\nWe need compute function that for given array of values returns the index of the first/last appearance.\r\nThen all batches can be processed in parallel and at the end merged exactly as you described.\r\n\r\nOnce we have index of the first/last appearance - we can use compute.take to have the output table.\r\n\r\nMaybe even ordering function can be specified so there would be no need to sort the array a priori.\r\n\r\n"
        },
        {
            "created_at": "2022-11-14T17:05:29.372Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17633943) by Weston Pace (westonpace):*\n> Maybe even ordering function can be specified so there would be no need to sort the array a priori.\r\n\r\nYou can run your ordering function first so that your ordering is a column in the data.  Then, for the first/last kernel the ordering would just be a field ref.  The rest would be a straightforward aggregate kernel I think.  The \"state\" would be the current first/last and the value of the ordering field at that point.\r\n\r\nSo yes, I think that approach should work to avoid sorting beforehand."
        },
        {
            "created_at": "2022-11-14T18:21:58.538Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15474?focusedCommentId=17633988) by Weston Pace (westonpace):*\nI believe https://issues.apache.org/jira/browse/ARROW-15735 is related"
        }
    ]
}