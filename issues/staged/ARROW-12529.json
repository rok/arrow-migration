{
    "issue": {
        "title": "[R] Writing to Parquet from tibble Consumes Large Amount of Memory",
        "body": "***Note**: This issue was originally created as [ARROW-12529](https://issues.apache.org/jira/browse/ARROW-12529). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen writing a large `tibble` to a parquet file, a large amount of memory is consumed. I first discovered this when using `targets::tar_read(obj)` to load in an object that had been saved in the parquet format. That particular object was an `sf` object with about 20 million rows and 26 columns. For a 5-6 GB object, memory ballooned by 22 GB.\r\n\r\nI wrote the following code to test this using a regular `tibble`, not `sf`. In this test memory increases dramatically when writing, but not when reading, which I'm still trying to figure out.\r\n```java\n\r\nlibrary(arrow)\r\nlibrary(dplyr)\r\nlibrary(lobstr)\r\nlibrary(tictoc)n <- 10000000system('free -m')\r\ntic()\r\nfake <- tibble(\r\n    ID=seq(n),\r\n    x=runif(n=n, min=-170, max=170),\r\n    y=runif(n=n, min=-60, max=70),\r\n    text1=sample(x=state.name, size=n, replace=TRUE),\r\n    text2=sample(x=state.name, size=n, replace=TRUE),\r\n    text3=sample(x=state.division, size=n, replace=TRUE),\r\n    text4=sample(x=state.region, size=n, replace=TRUE),\r\n    text5=sample(x=state.abb, size=n, replace=TRUE),\r\n    num1=sample(x=state.center$x, size=n, replace=TRUE),\r\n    num2=sample(x=state.center$y, size=n, replace=TRUE),\r\n    num3=sample(x=state.area, size=n, replace=TRUE),\r\n    Rand1=rnorm(n=n),\r\n    Rand2=rnorm(n=n, mean=100, sd=3),\r\n    Rand3=rbinom(n=n, size=10, prob=0.4)\r\n)\r\ntoc()\r\nsystem('free -m')obj_size(fake)/1024/1024/1024system('free -m')\r\ntic()\r\nwrite_parquet(fake, 'data/write_fake.parquet')\r\ntoc()\r\nsystem('free -m')system('free -m')\r\ngc()\r\nsystem('free -m')system('free -m')\r\ntic()\r\nfake_parquet <- read_parquet('data/write_test.parquet')\r\ntoc()\r\nsystem('free -m')\r\nobj_size(spat_parquet)/1024/1024/1024\r\n\r\n```",
        "created_at": "2021-04-24T22:00:12.000Z",
        "updated_at": "2021-12-16T16:47:46.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-04-25T18:01:18.049Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12529?focusedCommentId=17331599) by Jared Lander (jaredlander):*\nIt appears to get non-linearly worse with larger data. Try setting n to 20,000,000 and try try converting x and y into a geometry column. Memory usage gets really out of hand."
        },
        {
            "created_at": "2021-04-26T13:25:26.274Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12529?focusedCommentId=17332331) by Jonathan Keane (jonkeane):*\nThanks for the report! What operating system are you running this on?"
        },
        {
            "created_at": "2021-04-26T16:18:13.697Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12529?focusedCommentId=17332559) by Jared Lander (jaredlander):*\nUbuntu 18.04 with R 4.0.5 and arrow 3.0.0."
        }
    ]
}