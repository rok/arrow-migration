{
    "issue": {
        "title": "[Python] Data incomplete when using pyarrow in pyspark in python 3.x",
        "body": "***Note**: This issue was originally created as [ARROW-6011](https://issues.apache.org/jira/browse/ARROW-6011). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n \u00a0\r\n Since Spark 2.3.x, pandas udf has been introduced as default ser/des method. However, an issue raises with python >= 3.5.x version.\r\n We use pandas udf to process batches of data, but we find the data is incomplete in python 3.x. At first , i think the process logical maybe wrong, so i change the code to very simple one and it has the same problem.After investigate for a week, i find it is related to pyarrow. \u00a0\u00a0\r\n \u00a0\r\n **Reproduce procedure:**\r\n\r\n1. prepare data\r\n The data have seven column, a\u3001b\u3001c\u3001d\u3001e\u3001f and g, data type is Integer\r\n a,b,c,d,e,f,g\r\n 1,2,3,4,5,6,7\r\n 1,2,3,4,5,6,7\r\n 1,2,3,4,5,6,7\r\n 1,2,3,4,5,6,7\r\n \u00a0produce 100,000 rows and name the file test.csv ,upload to hdfs, then load it , and repartition it to 1 partition.\r\n \u00a0\r\n```java\n\r\ndf=spark.read.format('csv').option(\"header\",\"true\").load('/test.csv')\r\ndf=df.select(*(col(c).cast(\"int\").alias(c) for c in df.columns))\r\ndf=df.repartition(1)\r\nspark_context = SparkContext.getOrCreate()\u00a0\n```\r\n\u00a0\r\n 2.register pandas udf\r\n \u00a0\r\n```java\n\r\ndef add_func(a,b,c,d,e,f,g):\r\n    print('iterator one time')\r\n    return a\r\nadd = pandas_udf(add_func, returnType=IntegerType())\r\ndf_result=df.select(add(col(\"a\"),col(\"b\"),col(\"c\"),col(\"d\"),col(\"e\"),col(\"f\"),col(\"g\")))\n```\r\n\u00a0\r\n 3.apply pandas udf\r\n \u00a0\r\n```java\n\r\ndef trigger_func(iterator):\r\n \u00a0 \u00a0 \u00a0yield iterator\r\ndf_result.rdd.foreachPartition(trigger_func)\n```\r\n\u00a0\r\n 4.execute it in pyspark (local or yarn)\r\n run it with conf spark.sql.execution.arrow.maxRecordsPerBatch=100000. As mentioned before the total row number is 1000000, it should print \"iterator one time \" 10 times.\r\n (1)Python 2.7 envs:\r\n \u00a0\r\n```java\n\r\nPYSPARK_PYTHON=/usr/lib/conda/envs/py2.7/bin/python pyspark --conf spark.sql.execution.arrow.maxRecordsPerBatch=100000 --conf spark.executor.pyspark.memory=2g --conf spark.sql.execution.arrow.enabled=true --executor-cores 1\n```\r\n\u00a0\r\n ![image-2019-07-23-16-06-49-889.png](https://issues.apache.org/jira/secure/attachment/12975476/image-2019-07-23-16-06-49-889.png) \u00a0\r\n The result is right, 10 times of print.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n(2)Python 3.5 or 3.6 envs:\r\n```java\n\r\nPYSPARK_PYTHON=/usr/lib/conda/envs/python3.6/bin/python pyspark --conf spark.sql.execution.arrow.maxRecordsPerBatch=100000 --conf spark.executor.pyspark.memory=2g --conf spark.sql.execution.arrow.enabled=true --executor-cores\n```\r\n\u00a0\r\n\r\n![py3.6.png](https://issues.apache.org/jira/secure/attachment/12975477/py3.6.png)\r\n\r\nThe data is incomplete. Exception is print by spark which have been added by us ,\u00a0I will explain it later.\r\n \u00a0\r\n \u00a0\r\n### **Investigation**\r\n\r\nThe \u201cprocess done\u201d is added in the worker.py.\r\n ![worker.png](https://issues.apache.org/jira/secure/attachment/12975478/worker.png) \u00a0\u00a0\r\n In order to get the exception,\u00a0 change the spark code, the code is under core/src/main/scala/org/apache/spark/util/Utils.scala , and add this code to print the exception.\r\n \u00a0\r\n\r\n\u00a0\r\n```java\n\r\n@@ -1362,6 +1362,8 @@ private[spark] object Utils extends Logging {\r\n case t: Throwable =>\r\n // Purposefully not using NonFatal, because even fatal exceptions\r\n // we don't want to have our finallyBlock suppress\r\n+ logInfo(t.getLocalizedMessage)\r\n+ t.printStackTrace()\r\n originalThrowable = t\r\n throw originalThrowable\r\n } finally {\n```\r\n\u00a0\r\n\r\n\u00a0\r\n It seems the pyspark get the data from jvm , but pyarrow get the data incomplete. Pyarrow side think the data is finished, then shutdown the socket. At the same time, the jvm side still writes to the same socket , but get socket close exception.\r\n The pyarrow part is in ipc.pxi:\r\n \u00a0\r\n```java\n\r\ncdef class _RecordBatchReader:\r\n cdef:\r\n shared_ptr[CRecordBatchReader] reader\r\n shared_ptr[InputStream] in_stream\r\ncdef readonly:\r\n Schema schema\r\ndef _cinit_(self):\r\n pass\r\ndef _open(self, source):\r\n get_input_stream(source, &self.in_stream)\r\n with nogil:\r\n check_status(CRecordBatchStreamReader.Open(\r\n self.in_stream.get(), &self.reader))\r\nself.schema = pyarrow_wrap_schema(self.reader.get().schema())\r\ndef _iter_(self):\r\n while True:\r\n yield self.read_next_batch()\r\ndef get_next_batch(self):\r\n import warnings\r\n warnings.warn('Please use read_next_batch instead of '\r\n 'get_next_batch', FutureWarning)\r\n return self.read_next_batch()\r\ndef read_next_batch(self):\r\n \"\"\"\r\n Read next RecordBatch from the stream. Raises StopIteration at end of\r\n stream\r\n \"\"\"\r\n cdef shared_ptr[CRecordBatch] batch\r\nwith nogil:\r\n check_status(self.reader.get().ReadNext(&batch))\r\nif batch.get() == NULL:\r\n raise StopIteration\r\n return pyarrow_wrap_batch(batch)\n```\r\n\u00a0\r\n\r\nread_next_batch function get NULL, think the iterator is over.\r\n \u00a0\r\n### **RESULT**\r\n\r\nOur environment is spark 2.4.3, we have tried pyarrow version 0.10.0 and 0.14.0 , python version is python 2.7, python 3.5, python 3.6.\r\n When using python 2.7, everything is fine. But when change to python 3.5,3,6, the data is wrong.\r\n The column number is critical to trigger this bug, if column number is less than 5 , this bug probably will not happen. But\u00a0If the column number is big , for example 7 or above, it happens every time.\r\n So we wonder if there is some conflict between python 3.x and pyarrow version?\u00a0\r\n I have put the code and data as attachment.",
        "created_at": "2019-07-23T08:10:16.000Z",
        "updated_at": "2022-08-27T14:41:40.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Java",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-08-21T22:57:29.000Z"
    },
    "comments": [
        {
            "created_at": "2019-07-24T12:47:43.297Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6011?focusedCommentId=16891827) by Wes McKinney (wesm):*\nThis is almost certainly a bug in Apache Spark (if at all), so I don't think there's anything to do here"
        },
        {
            "created_at": "2019-07-24T14:14:58.026Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6011?focusedCommentId=16891888) by jiangyu (jiangyu1211):*\nHi `[~wesmckinn]`, Spark just use the Arrow API, and python 2.x is ok for sure. `[~bryanc]` do you have any opinion on this issue?"
        },
        {
            "created_at": "2019-08-21T22:57:29.933Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6011?focusedCommentId=16912755) by Bryan Cutler (bryanc):*\nI could not reproduce. We can continue the discussion in SPARK-28482 and reopen if we find an issue in Arrow"
        },
        {
            "created_at": "2022-08-27T14:41:40.307Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6011?focusedCommentId=17585776) by @toddfarmer:*\nTransitioning issue from Resolved to Closed to based on resolution field value."
        }
    ]
}