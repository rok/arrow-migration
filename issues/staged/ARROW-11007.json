{
    "issue": {
        "title": "[Python] Memory leak in pq.read_table and table.to_pandas",
        "body": "***Note**: This issue was originally created as [ARROW-11007](https://issues.apache.org/jira/browse/ARROW-11007). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhile upgrading our application to use pyarrow 2.0.0 instead of 0.12.1, we observed a memory leak in the read_table and to_pandas methods. See below for sample code to reproduce it. Memory does not seem to be returned after deleting the table and df as it was in pyarrow 0.12.1.\r\n\r\n**Sample Code**\r\n```python\n\r\nimport io\r\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nfrom memory_profiler import profile\r\n\r\n\r\n@profile\r\ndef read_file(f):\r\n    table = pq.read_table(f)\r\n    df = table.to_pandas(strings_to_categorical=True)\r\n    del table\r\n    del df\r\n\r\n\r\ndef main():\r\n    rows = 2000000\r\n    df = pd.DataFrame({\r\n        \"string\": [\"test\"] * rows,\r\n        \"int\": [5] * rows,\r\n        \"float\": [2.0] * rows,\r\n    })\r\n    table = pa.Table.from_pandas(df, preserve_index=False)\r\n    parquet_stream = io.BytesIO()\r\n    pq.write_table(table, parquet_stream)\r\n\r\n    for i in range(3):\r\n        parquet_stream.seek(0)\r\n        read_file(parquet_stream)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n**Python 3.8.5 (conda), pyarrow 2.0.0 (pip), pandas 1.1.2 (pip) Logs**\r\n```java\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    161.7 MiB    161.7 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    212.1 MiB     50.4 MiB           1       table = pq.read_table(f)\r\n    12    258.2 MiB     46.1 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    258.2 MiB      0.0 MiB           1       del table\r\n    14    256.3 MiB     -1.9 MiB           1       del df\r\n\r\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    256.3 MiB    256.3 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    279.2 MiB     23.0 MiB           1       table = pq.read_table(f)\r\n    12    322.2 MiB     43.0 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    322.2 MiB      0.0 MiB           1       del table\r\n    14    320.3 MiB     -1.9 MiB           1       del df\r\n\r\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    320.3 MiB    320.3 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    326.9 MiB      6.5 MiB           1       table = pq.read_table(f)\r\n    12    361.7 MiB     34.8 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    361.7 MiB      0.0 MiB           1       del table\r\n    14    359.8 MiB     -1.9 MiB           1       del df\r\n```\r\n**Python 3.5.6 (conda), pyarrow 0.12.1 (pip), pandas 0.24.1 (pip) Logs**\r\n```java\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    138.4 MiB    138.4 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    186.2 MiB     47.8 MiB           1       table = pq.read_table(f)\r\n    12    219.2 MiB     33.0 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    171.7 MiB    -47.5 MiB           1       del table\r\n    14    139.3 MiB    -32.4 MiB           1       del df\r\n\r\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    139.3 MiB    139.3 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    186.8 MiB     47.5 MiB           1       table = pq.read_table(f)\r\n    12    219.2 MiB     32.4 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    171.5 MiB    -47.7 MiB           1       del table\r\n    14    139.1 MiB    -32.4 MiB           1       del df\r\n\r\n\r\nFilename: C:/run_pyarrow_memoy_leak_sample.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n     9    139.1 MiB    139.1 MiB           1   @profile\r\n    10                                         def read_file(f):\r\n    11    186.8 MiB     47.7 MiB           1       table = pq.read_table(f)\r\n    12    219.2 MiB     32.4 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    13    171.8 MiB    -47.5 MiB           1       del table\r\n    14    139.3 MiB    -32.4 MiB           1       del df\r\n```",
        "created_at": "2020-12-22T16:44:30.000Z",
        "updated_at": "2022-11-09T14:58:09.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-12-22T18:33:50.163Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17253676) by Weston Pace (westonpace):*\nHello, thank you for writing up this analysis.\u00a0 Pyarrow uses jemalloc, a custom memory allocator which does its best to hold onto memory allocated from the OS (since this can be an expensive operation).\u00a0 Unfortunately, this makes it difficult to track line by line memory usage with tools like memory_profiler.\u00a0 There are a couple of options:\r\n\r\n- You could use <https://arrow.apache.org/docs/python/generated/pyarrow.total_allocated_bytes.html#pyarrow.total_allocated_bytes> to track allocation instead of using memory_profiler (it might be interesting to see if there is a way to get memory_profile to use this function instead of kernel statistics).\n  \n- You can also put the following line at the top of your script, this will configure jemalloc to release memory immediately instead of holding on to it (this will likely have some performance implications):\n  \n  pa.jemalloc_set_decay_ms(0)\n  \n  \u00a0\n  \n  The behavior you are seeing is pretty typical for jemalloc.\u00a0 For further reading, in addition to reading up on jemalloc itself, I encourage you to take a look at these other issues for more discussions and examples of jemalloc behaviors:\n  \n  \u00a0\n  \n  https://issues.apache.org/jira/browse/ARROW-6910\n  \n  https://issues.apache.org/jira/browse/ARROW-7305\n  \n  \u00a0\n  \n  I have run your test read 10,000 times and it seems that memory usage does predictably stabilize.\u00a0 In addition, total_allocated_bytes is behaving exactly as expected.\u00a0 So I do not believe there is any evidence of a memory leak in this script."
        },
        {
            "created_at": "2020-12-22T18:47:15.925Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17253686) by Michael Peleshenko (mpeleshenko):*\n`[~westonpace]` \u00a0Thanks for the detailed comment. I tried adding pa.jemallic_set_decay_ms(0), but I ran into the below error which seems to indicate jemalloc is not being used. I suspect this is because I am running on Windows 10.\r\n\r\n```Java\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/mipelesh/Workspace/Git/Lynx/src-pyLynx/pyLynx/run_pyarrow_memoy_leak_sample.py\", line 35, in <module>\r\n    main()\r\n  File \"C:/Users/mipelesh/Workspace/Git/Lynx/src-pyLynx/pyLynx/run_pyarrow_memoy_leak_sample.py\", line 18, in main\r\n    pa.jemalloc_set_decay_ms(0)\r\n  File \"pyarrow\\memory.pxi\", line 171, in pyarrow.lib.jemalloc_set_decay_ms\r\n  File \"pyarrow\\error.pxi\", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: jemalloc support is not built\r\n```"
        },
        {
            "created_at": "2020-12-22T19:36:34.545Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17253714) by Wes McKinney (wesm):*\nDepending on how you obtained pyarrow, it may be using mimalloc rather than the system allocator. We've done comparatively much less analysis of memory usage behaviors when using mimalloc, but we do know empirically that mimalloc improves application performance on Windows. "
        },
        {
            "created_at": "2020-12-28T16:50:12.166Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17255644) by Antoine Pitrou (apitrou):*\nAlso note that \"does not return memory immediately\" is different from \"memory leak\". The allocator (mimalloc and/or the system allocator, here) may opt to cache deallocated blocks instead of returning them to the system, because returning them is costly (it's a system call), and allocations often happen in a repeated fashion.\r\n\r\nIf you're merely worried about a potential memory leak, the way to check for it is to run your function in a loop and check whether memory occupation is constantly increasing, or if it quickly reaches a stable plateau."
        },
        {
            "created_at": "2021-01-04T12:05:36.501Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17258165) by Joris Van den Bossche (jorisvandenbossche):*\nGiven that this comes up from time to time, it might be useful to document this to some extent: expectations around watching memory usage (explaining that the deallocated memory might be cached by the memory allocator etc), how you can actually see how much memory is used (total_allocated_bytes), how you can check if high \"apparent\" memory usage is indeed related to this and not caused by a memory leak (use pa.jemalloc_set_decay_ms(0); run your function many times in a loop and see it stabilizes or keeps constantly growing), .."
        },
        {
            "created_at": "2021-02-05T11:33:14.562Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279642) by Dmitry Kashtanov (mr2dark):*\nI have a somewhat similar issue observed both on `pyarrow` v1.0.0 and v3.0.0 in Linux (within Docker containers, ``python:3.8-slim``-based image, local and AWS Fargate). The issue is with reading from BigQuery with BigQuery Storage API using ARROW data format. Under the hood it downloads a set of RecordBatches and combines them into a Table. After this in my code the Table is converted to a pandas DataFrame and then deleted, but the Table's memory is not released to OS.\r\n\r\nThis behavior remains also if I use ``mimalloc`` or ``system``-based pools set either in code or via ``ARROW_DEFAULT_MEMORY_POOL`` environment variable.\r\n\r\nAlso after that I drop a referenced column (not copied) from that pandas DataFrame, this results in DataFrame data copy and the memory from the original DataFrame is also not released to OS. The subsequent transformations of the DataFrame release memory as expected.\r\n\r\nThe exactly same code with exactly same Python (3.8.7) and packages versions on MacOS releases memory to OS as expected (also will all kinds of the memory pool).\r\n\r\n\u00a0\r\n\r\nThe very first lines of the script are:\r\n```java\n\r\nimport pyarrow\r\npyarrow.jemalloc_set_decay_ms(0)\r\n```\r\n\u00a0\r\n\r\nMac OS:\r\n\r\n\u00a0\r\n```java\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n   460    141.5 MiB    141.5 MiB           1   @profile\r\n   461                                         def bqs_stream_to_pandas(session, stream_name):\r\n   463    142.2 MiB      0.7 MiB           1       client = bqs.BigQueryReadClient()\r\n   464    158.7 MiB     16.5 MiB           1       reader = client.read_rows(name=stream_name, offset=0)\r\n   465   1092.2 MiB    933.5 MiB           1       table = reader.to_arrow(session)\r\n   470   2725.1 MiB   1632.5 MiB           2       dataset = table.to_pandas(deduplicate_objects=False, split_blocks=False, self_destruct=False,\r\n   471   1092.6 MiB      0.0 MiB           1                                 strings_to_categorical=True,)\r\n   472   1405.0 MiB  -1320.1 MiB           1       del table\r\n   473   1405.0 MiB      0.0 MiB           1       del reader\r\n   474   1396.1 MiB     -8.9 MiB           1       del client\r\n   475   1396.1 MiB      0.0 MiB           1       time.sleep(1)\r\n   476   1396.1 MiB      0.0 MiB           1       if MEM_PROFILING:\r\n   477   1396.1 MiB      0.0 MiB           1           mem_pool = pyarrow.default_memory_pool()\r\n   478   1396.1 MiB      0.0 MiB           1           print(f\"PyArrow mem pool info: {mem_pool.backend_name} backend, {mem_pool.bytes_allocated()} allocated, \"\r\n   479                                                       f\"{mem_pool.max_memory()} max allocated, \")\r\n   480   1396.1 MiB      0.0 MiB           1           print(f\"PyArrow total allocated bytes: {pyarrow.total_allocated_bytes()}\")\r\n   481   1402.4 MiB      6.3 MiB           1       mem_usage = dataset.memory_usage(index=True, deep=True)\r\n   485   1404.2 MiB      0.0 MiB           1       return dataset\r\n\r\n# Output\r\nPyArrow mem pool info: jemalloc backend, 1313930816 allocated, 1340417472 max allocated,\r\nPyArrow total allocated bytes: 1313930816\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n...\r\n   139   1477.7 MiB      0.4 MiB           1           dataset_label = dataset[label_column].astype(np.int8)\r\n   140\r\n   141   1474.2 MiB     -3.5 MiB           1           dataset.drop(columns=label_column, inplace=True)\r\n   142   1474.2 MiB      0.0 MiB           1           gc.collect()\r\n   143\r\n   144   1474.2 MiB      0.0 MiB           1           if MEM_PROFILING:\r\n   145   1474.2 MiB      0.0 MiB           1               mem_pool = pyarrow.default_memory_pool()\r\n   146   1474.2 MiB      0.0 MiB           1               print(f\"PyArrow mem pool info: {mem_pool.backend_name} backend, {mem_pool.bytes_allocated()} allocated, \"\r\n   147                                                           f\"{mem_pool.max_memory()} max allocated, \")\r\n   148   1474.2 MiB      0.0 MiB           1               print(f\"PyArrow total allocated bytes: {pyarrow.total_allocated_bytes()}\")\r\n\r\n# Output\r\nPyArrow mem pool info: jemalloc backend, 0 allocated, 1340417472 max allocated,\r\nPyArrow total allocated bytes: 0\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n`Linux (`python:3.8-slim``-based image`):`\r\n\r\n\u00a0\r\n```java\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n   460    153.0 MiB    153.0 MiB           1   @profile\r\n   461                                         def bqs_stream_to_pandas(session, stream_name):\r\n   463    153.5 MiB      0.6 MiB           1       client = bqs.BigQueryReadClient()\r\n   464    166.9 MiB     13.4 MiB           1       reader = client.read_rows(name=stream_name, offset=0)\r\n   465   1567.5 MiB   1400.6 MiB           1       table = reader.to_arrow(session)\r\n   469   1567.5 MiB      0.0 MiB           1       report_metric('piano.ml.preproc.pyarrow.table.bytes', table.nbytes)\r\n   470   2843.7 MiB   1276.2 MiB           2       dataset = table.to_pandas(deduplicate_objects=False, split_blocks=False, self_destruct=False,\r\n   471   1567.5 MiB      0.0 MiB           1                                 strings_to_categorical=True,)\r\n   472   2843.7 MiB      0.0 MiB           1       del table\r\n   473   2843.7 MiB      0.0 MiB           1       del reader\r\n   474   2843.9 MiB      0.2 MiB           1       del client\r\n   475   2842.2 MiB     -1.8 MiB           1       time.sleep(1)\r\n   476   2842.2 MiB      0.0 MiB           1       if MEM_PROFILING:\r\n   477   2842.2 MiB      0.0 MiB           1           mem_pool = pyarrow.default_memory_pool()\r\n   478   2842.2 MiB      0.0 MiB           1           print(f\"PyArrow mem pool info: {mem_pool.backend_name} backend, {mem_pool.bytes_allocated()} allocated, \"\r\n   479                                                       f\"{mem_pool.max_memory()} max allocated, \")\r\n   480   2842.2 MiB      0.0 MiB           1           print(f\"PyArrow total allocated bytes: {pyarrow.total_allocated_bytes()}\")\r\n   481   2838.9 MiB     -3.3 MiB           1           mem_usage = dataset.memory_usage(index=True, deep=True)\r\n   485   2839.1 MiB      0.0 MiB           1       return dataset\r\n\r\n# Output\r\nPyArrow mem pool info: jemalloc backend, 1313930816 allocated, 1338112064 max allocated,\r\nPyArrow total allocated bytes: 1313930816\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n...\r\n   139   2839.1 MiB      0.0 MiB           1           dataset_label = dataset[label_column].astype(np.int8)\r\n   140\r\n   141   2836.6 MiB     -2.6 MiB           1           dataset.drop(columns=label_column, inplace=True)\r\n   142   2836.6 MiB      0.0 MiB           1           gc.collect()\r\n   143\r\n   144   2836.6 MiB      0.0 MiB           1           if MEM_PROFILING:\r\n   145   2836.6 MiB      0.0 MiB           1               mem_pool = pyarrow.default_memory_pool()\r\n   146   2836.6 MiB      0.0 MiB           1               print(f\"PyArrow mem pool info: {mem_pool.backend_name} backend, {mem_pool.bytes_allocated()} allocated, \"\r\n   147                                                           f\"{mem_pool.max_memory()} max allocated, \")\r\n   148   2836.6 MiB      0.0 MiB           1               print(f\"PyArrow total allocated bytes: {pyarrow.total_allocated_bytes()}\")\r\n\r\n# Output\r\nPyArrow mem pool info: jemalloc backend, 0 allocated, 1338112064 max allocated,\r\nPyArrow total allocated bytes: 0\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nA case with\u00a0dropping a referenced (not copied) column:\r\n\r\n\u00a0\r\n```java\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n...\r\n   134   2872.0 MiB      0.0 MiB           1           dataset_label = dataset[label_column]\r\n   135\r\n   136   4039.4 MiB   1167.4 MiB           1           dataset.drop(columns=label_column, inplace=True)\r\n   137   4035.9 MiB     -3.6 MiB           1               gc.collect()\r\n   138\r\n   139   4035.9 MiB      0.0 MiB           1           if MEM_PROFILING:\r\n   140   4035.9 MiB      0.0 MiB           1               mem_pool = pyarrow.default_memory_pool()\r\n   141   4035.9 MiB      0.0 MiB           1               print(f\"PyArrow mem pool info: {mem_pool.backend_name} backend, {mem_pool.bytes_allocated()} allocated, \"\r\n   142                                                           f\"{mem_pool.max_memory()} max allocated, \")\r\n\r\n# Output\r\nPyArrow mem pool info: jemalloc backend, 90227904 allocated, 1340299200 max allocated,\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nPackage versions:\r\n\r\n\u00a0\r\n```java\n\r\nboto3==1.17.1\r\nbotocore==1.20.1\r\ncachetools==4.2.1\r\ncertifi==2020.12.5\r\ncffi==1.14.4\r\nchardet==4.0.0\r\ngoogle-api-core[grpc]==1.25.1\r\ngoogle-auth==1.25.0\r\ngoogle-cloud-bigquery-storage==2.2.1\r\ngoogle-cloud-bigquery==2.7.0\r\ngoogle-cloud-core==1.5.0\r\ngoogle-crc32c==1.1.2\r\ngoogle-resumable-media==1.2.0\r\ngoogleapis-common-protos==1.52.0\r\ngrpcio==1.35.0\r\nidna==2.10\r\njmespath==0.10.0\r\njoblib==1.0.0\r\nlibcst==0.3.16\r\nmemory-profiler==0.58.0\r\nmypy-extensions==0.4.3\r\nnumpy==1.20.0\r\npandas==1.2.1\r\nproto-plus==1.13.0\r\nprotobuf==3.14.0\r\npsutil==5.8.0\r\npyarrow==3.0.0\r\npyasn1-modules==0.2.8\r\npyasn1==0.4.8\r\npycparser==2.20\r\npython-dateutil==2.8.1\r\npytz==2021.1\r\npyyaml==5.4.1\r\nrequests==2.25.1\r\nrsa==4.7\r\ns3transfer==0.3.4\r\nscikit-learn==0.24.1\r\nscipy==1.6.0\r\nsetuptools-scm==5.0.1\r\nsix==1.15.0\r\nsmart-open==4.1.2\r\nthreadpoolctl==2.1.0\r\ntyping-extensions==3.7.4.3\r\ntyping-inspect==0.6.0\r\nunidecode==1.1.2\r\nurllib3==1.26.3\r\n```\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-02-05T12:29:31.782Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279664) by Antoine Pitrou (apitrou):*\nAs you can see, the memory was returned to the allocator (\"0 allocated\"). The allocator is then free to return those pages to the OS or not.\r\n\r\nAlso, how is \"Mem usage\" measured in your script?"
        },
        {
            "created_at": "2021-02-05T13:01:03.398Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279681) by Dmitry Kashtanov (mr2dark):*\n\"Mem usage\" is by `memory_profiler`.\r\n\r\nAnd as we may see, the following line doesn't help.\r\npyarrow.jemalloc_set_decay_ms(0)"
        },
        {
            "created_at": "2021-02-05T13:11:45.089Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279687) by Antoine Pitrou (apitrou):*\n> \"Mem usage\" is by `memory_profiler`.\r\n\r\nThat doesn't really answer the question: what does it measure? RSS? Virtual memory size?\r\n\r\n> And as we may see, the following line doesn't help.\r\n\r\nPerhaps, but I still don't see what Arrow could do, or even if there is an actual problem.\r\n\r\nCan you run \"bqs_stream_to_pandas\" in a loop and see whether memory usage increases? Or does it stay stable as its initial peak value?"
        },
        {
            "created_at": "2021-02-05T13:13:41.405Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279688) by Dmitry Kashtanov (mr2dark):*\n```java\n\r\nMALLOC_CONF=\"background_thread:true,narenas:1,tcache:false,dirty_decay_ms:0,muzzy_decay_ms:0\"\r\n```\r\nSpecifying the above environment variable also doesn't help for jemalloc.\r\n\r\nThe suspicious things are that everything works in MacOS and that also that all allocators behave similarly.\u00a0"
        },
        {
            "created_at": "2021-02-05T14:53:06.748Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279733) by Dmitry Kashtanov (mr2dark):*\n\u00a0\r\n\r\n>\u00a0That doesn't really answer the question: what does it measure? RSS? Virtual memory size?\r\n\r\nIt looks like `memory_profiler` uses the first item from the tuple returned by `psutil.Process().memory_info()` which is `rss`.\r\n\r\n\u00a0\r\n\r\n>\u00a0Can you run \"bqs_stream_to_pandas\" in a loop and see whether memory usage increases? Or does it stay stable as its initial peak value?\r\n\r\nPSB. It doesn't increase (almost).\r\n```java\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n...\r\n   117   2866.0 MiB   2713.1 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   118   2865.6 MiB     -0.4 MiB           1       del dataset\r\n   119   2874.6 MiB      9.0 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   120   2874.6 MiB      0.0 MiB           1       del dataset\r\n   121   2887.0 MiB     12.4 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   122   2878.2 MiB     -8.8 MiB           1       del dataset\r\n   123   2903.2 MiB     25.1 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   124   2903.2 MiB      0.0 MiB           1       del dataset\r\n   125   2899.2 MiB     -4.1 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   126   2899.2 MiB      0.0 MiB           1       del dataset\r\n   127   2887.9 MiB    -11.3 MiB           1       dataset = bqs_stream_to_pandas(session, stream_name)\r\n   128   2887.9 MiB      0.0 MiB           1       del dataset\r\n```\r\n\u00a0\r\n\r\nInterestingly, the first chunk of memory is freed when gRPC connection/session (may call it incorrecty) is reset:\u00a0\r\n```java\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n   471   2898.9 MiB   2898.9 MiB           1   @profile\r\n   472                                         def bqs_stream_to_pandas(session, stream_name, row_limit=3660000):\r\n   474   2898.9 MiB      0.0 MiB           1       client = bqs.BigQueryReadClient()\r\n   475   1628.4 MiB  -1270.5 MiB           1       reader = client.read_rows(name=stream_name, offset=0)\r\n   476   1628.4 MiB      0.0 MiB           1       rows = reader.rows(session)\r\n...\r\n```\r\nIf a `message` is `google.protobuf` message and a batch is created like below, will it be a zero-copy operation?\r\n```java\n\r\npyarrow.ipc.read_record_batch(\r\n    pyarrow.py_buffer(message.arrow_record_batch.serialized_record_batch),\r\n    self._schema,\r\n)\r\n```\r\n\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-02-05T15:45:06.422Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17279763) by Antoine Pitrou (apitrou):*\nAh, I didn't know that gRPC was involved. Since Arrow returned all the memory it had allocated, it's quite possible that the memory is held at the gRPC level.\r\n\r\n> If a `message` is `google.protobuf` message and a batch is created like below, will it be a zero-copy operation?\r\n\r\nHmm... I guess it probably should? But I think you may find more expertise about this by asking the BigQuery developers / community."
        },
        {
            "created_at": "2021-02-06T11:43:59.680Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17280160) by Dmitry Kashtanov (mr2dark):*\nIt looks like it's a zero-copy operation since after `pyarrow.Table` creation and before `pandas.DataFrame` creation, pyarrow reports zero prior memory allocation (both in Linux and MacOS):\r\n\r\n\u00a0\r\n```java\n\r\nBefore pandas dataframe creation\r\nPyArrow mem pool info: jemalloc backend, 0 allocated, 0 max allocated, \r\nPyArrow total allocated bytes: 0\r\n```\r\nSo with this, it looks like we have the following container sequence:\r\n\r\n\u00a0\r\n1. a list of `pyarrow.RecordBatch`es backed by memory allocated by `google.protobuf`\n1. `pyarrow.Table` backed by (most likely, exactly the same) memory allocated by `google.protobuf`\n1. then, `pandas.DataFrame` backed by memory allocated by `pyarrow`\n1. then, after a column drop,\u00a0`pandas.DataFrame` backed by memory allocated by `pandas`/`numpy`\n   \n   So my current assumption is that\u00a0`google.protobuf` uses a memory allocator for Linux, different from the one used for MacOS. The former one can be Google's TCMalloc (which [is Linux only](https://github.com/google/tcmalloc/blob/master/docs/platforms.md))."
        },
        {
            "created_at": "2021-03-03T09:08:39.314Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17294410) by shadowdsp (shadowdsp):*\nI have the similar issue in `nested data` on Ubuntu16.04 pyarrow v3.0, even if I set `pa.jemalloc_set_decay_ms(0)`. But `non-nested data` can work well.\r\n\u00a0\r\nHere is my script:\r\n```java\n\r\nimport io\r\nimport pandas as pd\r\nimport pyarrow as pa\r\npa.jemalloc_set_decay_ms(0)\r\nimport pyarrow.parquet as pq\r\nfrom memory_profiler import profile\r\n\r\n@profile\r\ndef read_file(f):\r\n    table = pq.read_table(f)\r\n    df = table.to_pandas(strings_to_categorical=True)\r\n    del table\r\n    del df\r\n\r\ndef main():\r\n    rows = 2000000\r\n    df = pd.DataFrame({\r\n        \"string\": [{\"test\": [1, 2], \"test1\": [3, 4]}] * rows,\r\n        \"int\": [5] * rows,\r\n        \"float\": [2.0] * rows,\r\n    })\r\n    table = pa.Table.from_pandas(df, preserve_index=False)\r\n    parquet_stream = io.BytesIO()\r\n    pq.write_table(table, parquet_stream)\r\n    for i in range(3):\r\n        parquet_stream.seek(0)\r\n        read_file(parquet_stream)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nOutput:\r\n```java\n\r\nFilename: memory_leak.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n    14    329.5 MiB    329.5 MiB           1   @profile\r\n    15                                         def read_file(f):\r\n    16    424.4 MiB     94.9 MiB           1       table = pq.read_table(f)\r\n    17   1356.6 MiB    932.2 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    18   1310.5 MiB    -46.1 MiB           1       del table\r\n    19    606.7 MiB   -703.8 MiB           1       del df\r\n\r\n\r\nFilename: memory_leak.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n    14    606.7 MiB    606.7 MiB           1   @profile\r\n    15                                         def read_file(f):\r\n    16    714.9 MiB    108.3 MiB           1       table = pq.read_table(f)\r\n    17   1720.8 MiB   1005.9 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    18   1674.5 MiB    -46.3 MiB           1       del table\r\n    19    970.6 MiB   -703.8 MiB           1       del df\r\n\r\n\r\nFilename: memory_leak.py\r\n\r\nLine #    Mem usage    Increment  Occurences   Line Contents\r\n============================================================\r\n    14    970.6 MiB    970.6 MiB           1   @profile\r\n    15                                         def read_file(f):\r\n    16   1079.6 MiB    109.0 MiB           1       table = pq.read_table(f)\r\n    17   2085.5 MiB   1005.9 MiB           1       df = table.to_pandas(strings_to_categorical=True)\r\n    18   2039.2 MiB    -46.3 MiB           1       del table\r\n    19   1335.3 MiB   -703.8 MiB           1       del df\r\n```\r\n`df` and `table` cannot fully release in this case.\r\n\u00a0\r\npkg info\r\n```java\n\r\n                                                                                                      \r\n\u25b6 pip show pyarrow     \r\nName: pyarrow\r\nVersion: 3.0.0\r\nSummary: Python library for Apache Arrow\r\nHome-page: https://arrow.apache.org/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: Apache License, Version 2.0\r\nLocation: \r\nRequires: numpy\r\nRequired-by: utify\r\n                                                                                                    \r\n\u25b6 pip show pandas \r\nName: pandas\r\nVersion: 1.2.1\r\nSummary: Powerful data structures for data analysis, time series, and statistics\r\nHome-page: https://pandas.pydata.org\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: BSD\r\nLocation: \r\nRequires: python-dateutil, pytz, numpy\r\nRequired-by: utify, seaborn, fastparquet\r\n```"
        },
        {
            "created_at": "2021-03-04T02:06:38.417Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17294934) by Weston Pace (westonpace):*\n`[~shadowdsp]` Thanks for the great reproducible test case.\u00a0 I worked on this today and believe it is different than the problem described earlier in this issue.\u00a0 I have created ARROW-11855 to track your bug."
        },
        {
            "created_at": "2021-03-04T02:15:38.926Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17294938) by shadowdsp (shadowdsp):*\n`[~westonpace]` \u00a0thank you very much!"
        },
        {
            "created_at": "2022-02-03T17:00:57.834Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17486601) by Peter Gaultney (petergaultney):*\nHi,\r\n\r\nI think this bug still exists in 6.0.0 of pyarrow.\r\n\r\nI'm attaching a script that requires fastparquet, pyarrow, and psutil to be installed.\u00a0[benchmark-pandas-parquet.py](benchmark-pandas-parquet.py)\r\n\r\n\r\nIt allows switching between fastparquet and pyarrow to see the difference between memory usage between each iteration, where the number of calls to read_table is also parameterizable, but defaults to 5.\r\n\r\nThere seems to be a large memory leak, followed by smaller ones on every iteration. Even with `pyarrow.jemalloc_set_decay_ms(0)`, I cannot get pyarrow to ever give up the memory it allocates.\r\n\r\nI've been able to reproduce with many different kinds of parquet files, but I don't know about nested vs non-nested data.\u00a0\r\n\r\n\u00a0"
        },
        {
            "created_at": "2022-04-13T16:07:04.947Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17521806) by Cory Nezin (cnezin):*\nI am also seeing similar behavior with pd.read_parquet and the latest version, 7.0.0.\u00a0 Interestingly, this seems to only happen in the particular case of running it on a gunicorn server."
        },
        {
            "created_at": "2022-08-17T09:24:01.700Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17580695) by Jan Skorepa (skorepaj):*\n\u00a0\r\n\r\nI have been struggling with memory leak in `to_table` method and also in other use cases. Here is simple example to reproduce it.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```java\n\r\nimport pandas as pd\r\nfrom pyarrow import dataset as ds\r\nimport pyarrow as pa\r\n\r\n\r\ndef create_parquet(path: str):\r\n    pd.DataFrame({'range': [x for x in range(1000000)]}).to_parquet(path)\r\n\r\n\r\ndef load_parquet_to_table(path: str):\r\n    dataset = ds.dataset(path, format='parquet')\r\n    dataset.to_table()\r\n\r\n\r\nif __name__ == '__main__':\r\n    PATH = 'test.parquet'\r\n    pa.jemalloc_set_decay_ms(0)\r\n    create_parquet(PATH)\r\n    for x in range(100):\r\n        load_parquet_to_table(PATH) \n```\r\n\u00a0\r\n\r\nI tested on version 9.0.0 with python 3.8 on macOS.\r\n\r\nAnd `pa.jemalloc_set_decay_ms(0)` also didn't help with this.\r\n\r\nMemory Usage:\r\n\r\n![Screenshot 2022-08-17 at 11.10.05.png](https://issues.apache.org/jira/secure/attachment/13048229/Screenshot+2022-08-17+at+11.10.05.png)\r\n\r\n\u00a0\r\n\r\nEven though the memory usage doesn't grow linearly here, when I used this in more complex example in long running process it ended up increasing linearly until exceeding the memory limit."
        },
        {
            "created_at": "2022-08-17T09:29:10.543Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17580698) by Antoine Pitrou (apitrou):*\n`[~skorepaj]` Can you try calling `pa.jemalloc_memory_pool().release_unused()` after each call to `load_parquet_to_table` ?"
        },
        {
            "created_at": "2022-08-17T10:50:47.371Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17580727) by Jan Skorepa (skorepaj):*\n`[~apitrou]` Thanks for your fast reply. Unfortunately it has no effect."
        },
        {
            "created_at": "2022-08-17T19:12:15.519Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17580959) by Ninh Chu (chulucninh09):*\nHi, I also encounter memory problem in v9.0.0. But in my case, the memory pool is scaled with dataset size, even I tried to limit batch size. Based on the document, RecordBatchReader is the safe way to read dataset big dataset. But in my case, if the memory scales with dataset size, it counters the purpose of Dataset and RecordBatchReader.\r\n\r\nI'm running on Ubuntu20.04 / WSL2\r\n\r\n```python\n\r\nimport pyarrow.dataset as ds\r\nimport pyarrow as pa\r\npa.jemalloc_set_decay_ms(0)\r\n\r\ndelta_ds = ds.dataset(\"delta\")\r\n\r\nrow_count = delta_ds.count_rows()\r\nprint(\"row_count = \", row_count)\r\n\r\nreader = delta_ds.scanner(batch_size=10000).to_reader()\r\nbatch = reader.read_next_batch()\r\nprint(\"first batch row count = \", batch.num_rows)\r\nprint(\"Total allocated mem for pyarrow = \", pa.total_allocated_bytes() // 1024**2)\r\n```\r\n\u00a0\r\nThe results are interesting:\r\n\r\nSmall dataset\r\n```Java\n\r\ndataset row_count =  66651\r\nfirst batch row count =  10000\r\nTotal allocated mem for pyarrow =  103\r\n```\r\n\r\nBig dataset created by duplicating the same file 4 times\r\n```Java\n\r\ndataset row_count =  333255\r\nfirst batch row count =  10000\r\nTotal allocated mem for pyarrow =  412\r\n```\r\n\r\nIf load all the data in dataset into Table:\r\n```python\n\r\nimport pyarrow.dataset as ds\r\nimport pyarrow as pa\r\npa.jemalloc_set_decay_ms(0)\r\n\r\ndelta_ds = ds.dataset(\"delta\")\r\n\r\nrow_count = delta_ds.count_rows()\r\nprint(\"dataset row_count = \", row_count)\r\n\r\npa_table = delta_ds.to_table()\r\nprint(\"Total allocated mem for pyarrow = \", pa.total_allocated_bytes() // 1024**2)\r\n```\r\n\r\n```Java\n\r\ndataset row_count =  333255\r\nTotal allocated mem for pyarrow =  512\r\n```"
        },
        {
            "created_at": "2022-11-09T07:48:55.605Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17630821) by Julius Uotila (isoanselmi):*\nHi,\r\n\r\nI am having the exact same issue as Jan Skorepa, but with Windows/Windows Server.\r\n\r\nI have a process building a dataset overnight from SQL database to .parquet with a predefined save interval (does 30+ saves a night) and limited memory. Each save is slowly creeping up memory until process crashes.\r\n\r\npython 3.9.12\r\npyarrow 9.0.0\r\n\r\nWindows Server 2019\r\n\r\nWindows 10\r\n\r\nMany thanks,\r\n\r\nJulius"
        },
        {
            "created_at": "2022-11-09T14:57:34.629Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11007?focusedCommentId=17631119) by wondertx (wondertx):*\nAlso encountered memory leak when using `pyarrow.fs.HadoopFileSystem.open_input_stream`"
        }
    ]
}