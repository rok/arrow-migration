{
    "issue": {
        "title": "[Python] pq.read_pandas with use_legacy_dataset=False does not accept columns as a set (kartothek integration failure)",
        "body": "***Note**: This issue was originally created as [ARROW-12314](https://issues.apache.org/jira/browse/ARROW-12314). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe kartothek nightly integration builds started to fail(https://github.com/ursacomputing/crossbow/runs/2303373464), I assume because of ARROW-11464 (https://github.com/apache/arrow/pull/9910).\r\n\r\nIt seems that in the new ParquetDatasetV2 (what you get with `use_legacy_dataset=False`), the handling of the columns argument is slightly different.\r\n\r\n\r\nExample failure:\r\n\r\n```Java\n\r\n_____________________ test_add_column_to_existing_index[4] _____________________\r\n\r\nstore_factory = functools.partial(<function get_store_from_url at 0x7faf12e9d0e0>, 'hfs:///tmp/pytest-of-root/pytest-0/test_add_column_to_existing_in1/store')\r\nmetadata_version = 4\r\nbound_build_dataset_indices = <function build_dataset_indices at 0x7faf0c509830>\r\n\r\n    def test_add_column_to_existing_index(\r\n        store_factory, metadata_version, bound_build_dataset_indices\r\n    ):\r\n        dataset_uuid = \"dataset_uuid\"\r\n        partitions = [\r\n            pd.DataFrame({\"p\": [1, 2], \"x\": [100, 4500]}),\r\n            pd.DataFrame({\"p\": [4, 3], \"x\": [500, 10]}),\r\n        ]\r\n    \r\n        dataset = store_dataframes_as_dataset(\r\n            dfs=partitions,\r\n            store=store_factory,\r\n            dataset_uuid=dataset_uuid,\r\n            metadata_version=metadata_version,\r\n            secondary_indices=\"p\",\r\n        )\r\n        assert dataset.load_all_indices(store=store_factory()).indices.keys() == {\"p\"}\r\n    \r\n# Create indices\r\n>       bound_build_dataset_indices(store_factory, dataset_uuid, columns=[\"x\"])\r\n\r\nkartothek/io/testing/index.py:88: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/opt/conda/envs/arrow/lib/python3.7/site-packages/decorator.py:231: in fun\r\n    return caller(func, *(extras + args), **kw)\r\nkartothek/io_components/utils.py:277: in normalize_args\r\n    return _wrapper(*args, **kwargs)\r\nkartothek/io_components/utils.py:275: in _wrapper\r\n    return function(*args, **kwargs)\r\nkartothek/io/eager.py:706: in build_dataset_indices\r\n    mp = mp.load_dataframes(store=ds_factory.store, columns=cols_to_load,)\r\nkartothek/io_components/metapartition.py:150: in _impl\r\n    method_return = method(mp, *method_args, **method_kwargs)\r\nkartothek/io_components/metapartition.py:696: in load_dataframes\r\n    date_as_object=dates_as_object,\r\nkartothek/serialization/_generic.py:122: in restore_dataframe\r\n    date_as_object=date_as_object,\r\nkartothek/serialization/_parquet.py:302: in restore_dataframe\r\n    date_as_object=date_as_object,\r\nkartothek/serialization/_parquet.py:249: in _restore_dataframe\r\n    table = pq.read_pandas(reader, columns=columns)\r\n/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1768: in read_pandas\r\n    source, columns=columns, use_pandas_metadata=True, **kwargs\r\n/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1730: in read_table\r\n    use_pandas_metadata=use_pandas_metadata)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <pyarrow.parquet._ParquetDatasetV2 object at 0x7faee1ed9550>\r\ncolumns = {'x'}, use_threads = True, use_pandas_metadata = True\r\n\r\n    def read(self, columns=None, use_threads=True, use_pandas_metadata=False):\r\n        \"\"\"\r\n        Read (multiple) Parquet files as a single pyarrow.Table.\r\n    \r\n        Parameters\r\n        ----------\r\n        columns : List[str]\r\n            Names of columns to read from the dataset. The partition fields\r\n            are not automatically included (in contrast to when setting\r\n            ``use_legacy_dataset=True``).\r\n        use_threads : bool, default True\r\n            Perform multi-threaded column reads.\r\n        use_pandas_metadata : bool, default False\r\n            If True and file has custom pandas schema metadata, ensure that\r\n            index columns are also loaded.\r\n    \r\n        Returns\r\n        -------\r\n        pyarrow.Table\r\n            Content of the file as a table (of columns).\r\n        \"\"\"\r\n# if use_pandas_metadata, we need to include index columns in the\r\n# column selection, to be able to restore those in the pandas DataFrame\r\n        metadata = self.schema.metadata\r\n        if columns is not None and use_pandas_metadata:\r\n            if metadata and b'pandas' in metadata:\r\n# RangeIndex can be represented as dict instead of column name\r\n                index_columns = [\r\n                    col for col in _get_pandas_index_columns(metadata)\r\n                    if not isinstance(col, dict)\r\n                ]\r\n>               columns = columns + list(set(index_columns) - set(columns))\r\nE               TypeError: unsupported operand type(s) for +: 'set' and 'list'\r\n\r\n/opt/conda/envs/arrow/lib/python3.7/site-packages/pyarrow/parquet.py:1598: TypeError\r\n```",
        "created_at": "2021-04-09T14:26:22.000Z",
        "updated_at": "2021-04-12T18:33:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-04-12T18:21:36.000Z"
    },
    "comments": [
        {
            "created_at": "2021-04-12T18:21:36.404Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-12314?focusedCommentId=17319638) by David Li (lidavidm):*\nIssue resolved by pull request 9966\n<https://github.com/apache/arrow/pull/9966>"
        }
    ]
}