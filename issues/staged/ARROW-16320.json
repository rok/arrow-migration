{
    "issue": {
        "title": "[R] Dataset re-partitioning consumes considerable amount of memory",
        "body": "***Note**: This issue was originally created as [ARROW-16320](https://issues.apache.org/jira/browse/ARROW-16320). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nA short background: I was trying to create a dataset from a big pile of csv files (couple of hundreds). In first step the csv were parsed and saved to parquet files because there were many inconsistencies between csv files. In a consequent step the dataset was re-partitioned using one column (code_key).\r\n\r\n\u00a0\r\n```java\n\r\nnew_dataset <- open_dataset(\r\n\u00a0 temp_parquet_folder,\u00a0\r\n\u00a0 format = \"parquet\",\r\n\u00a0 unify_schemas = TRUE\r\n\u00a0 )\r\nnew_dataset |>\u00a0\r\n\u00a0 group_by(code_key) |>\u00a0\r\n  write_dataset(\r\n\u00a0 \u00a0 folder_repartitioned_dataset,\u00a0\r\n\u00a0 \u00a0 format = \"parquet\"\r\n\u00a0 )\r\n```\r\n\u00a0\r\n\r\nThis re-partitioning consumed a considerable amount of memory (5 GB).\u00a0\r\n \\* Is this a normal behavior? \u00a0Or a bug?\r\n \\* Is there any rule of thumb to estimate the memory requirement for a dataset re-partitioning? (it\u2019s important when scaling up this approach)\r\n\r\nThe drawback is that this memory space is not freed up after the re-partitioning\u00a0 (I am using RStudio).\u00a0\r\nThe `gc()` useless in this situation. And there is no any associated object (to the repartitioning) in the `R` environment which can be removed from memory (using the `rm()` function).\r\n \\* How one can regain this memory space used by re-partitioning?\r\n\r\nThe rationale behind choosing the dataset re-partitioning: if my understanding is correct, \u00a0in the current arrow version the append is not working when writing parquet files/datasets. (the original csv files were partly partitioned according to a different variable)\r\n\r\nCan you recommend any better approach?",
        "created_at": "2022-04-25T15:45:20.000Z",
        "updated_at": "2022-05-03T12:37:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-04-25T17:13:03.149Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17527653) by Weston Pace (westonpace):*\nHow are you measuring memory used?\r\n\r\nThere is a known issue when scanning parquet that it uses more RAM than expected.    8.0.0 should behave a little more reliably.  Exactly how RAM is expected depends on the structure of your input files.  I'm working on documenting that this week.  However, in general, I would estimate a few GB of process RAM to be needed for this operation.\r\n\r\nI would not expect any process memory (e.g. RSS assigned to the process) to remain after the operation.  If you are on Linux, we use jemalloc by default, and it is configured so you might need to wait up to 1 second for all the memory to be returned to the OS.\r\n\r\nIf you are measuring RAM with a tool like Linux's `free` then I would also expect you would see a large (potentially all) chunk of RAM move from the `free` column and into the `buf/cache` column.  That would persist even after the repartitioning is done.  However, that RAM should be \"available\" RAM and this is just kind of how the Linux disk cache works.  I'd like to add a solution to do writes with direct I/O at some point which would avoid this."
        },
        {
            "created_at": "2022-04-25T18:54:31.429Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17527717) by Zsolt Kegyes-Brassai (kbzsl):*\nHi `[~westonpace]`. Thank you for your prompt answer.\r\n\r\n\r\nSorry, I forget to describe the environment: I am using a laptop with 64-bit win10, R 4.1.2 and quite up to date R packages (arrow 7.0.0). I am running my scripts from RStudio IDE.\u00a0\r\n\r\n\r\nI was checking the memory utilization both in the RStudio environment pane and the windows task manager.\u00a0\r\nBoth are showing around 5.6 GB memory utilization increase: in RStudio from 300 MB to 5.9 GB (the task manager is showing about 250 MB higher \u2013 most probably the memory occupied by the IDE). \u00a0\r\nThere is no (new) visible object in the RStudio Environment which can be associated with this re-partitioning activity.\r\n\r\n\r\nAnd this memory remained occupied until the RStudio session (or the R project) is closed. \u00a0I waited for 15 minutes before closing the IDE."
        },
        {
            "created_at": "2022-04-26T06:41:56.665Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17527930) by Zsolt Kegyes-Brassai (kbzsl):*\nI tried to find what is causing this issue.\r\n\r\nFirst, I saved my data as a non-partitioned dataset to simplify a bit. It resulted one 617 MB parquet file (having strings, timestamp and integers).\r\n\r\nThen I tried to read and check the memory utilization. I run the code both in RStudio IDE and R Console (RGui).\r\n\r\nThe result were the same: a large amount of memory was occupied and not freed up until the IDE or Console was shut down (having 15+ minutes waiting time).\r\n\r\nThe occupied memory was much larger (11GB)\u00a0 than R object size (700 MB) shown by `{}obj_size(){`}.\r\n\r\n\u00a0\r\n```java\n\r\nfs::file_size(here::here(\"db\", \"large_parquet\", \"part-0.parquet\"))\r\n#> 617M\r\na = arrow::read_parquet(here::here(\"db\", \"large_parquet\", \"part-0.parquet\"))\r\nlobstr::obj_size(a)\r\n#> 721,474,112 B\u00a0\r\n```\r\nHere are some screenshots using the IDE\r\n\r\n![Rstudio_env.jpg](https://issues.apache.org/jira/secure/attachment/13042926/Rstudio_env.jpg)\r\n\r\n![Rstudio_mem.jpg](https://issues.apache.org/jira/secure/attachment/13042927/Rstudio_mem.jpg)\r\n\r\nand the RGui\r\n\r\n![Rgui_mem.jpg](https://issues.apache.org/jira/secure/attachment/13042928/Rgui_mem.jpg)"
        },
        {
            "created_at": "2022-04-27T03:31:27.117Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17528511) by Weston Pace (westonpace):*\nWhat do you get from the following?\r\n\r\n```\n\r\na = arrow::read_parquet(here::here(\"db\", \"large_parquet\", \"part-0.parquet\"))\r\na$nbytes()\r\n```\r\n\r\nThe `nbytes` function should print a pretty decent approximation of the C memory referenced by `a`.\r\n\r\n`lobstr::obj_size` prints only the R memory used (I think).\r\n\r\n`fs::file_size` is going to give you the size of the file, which is possibly encoded and compressed.  Some parquet files can be much larger in memory than they are on disk.  So it is not unheard of for a 620MB parquet file to end up occupying gigabytes in memory (11 GB seems a little extreme but within the realm of possibility)"
        },
        {
            "created_at": "2022-04-27T08:01:45.293Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17528616) by Zsolt Kegyes-Brassai (kbzsl):*\nIt's an error message:\r\n```java\n\r\na$nbytes()\r\n#> Error: attempt to apply non-function\r\n#> In addition: Warning message:\r\n#> Unknown or uninitialised column: `nbytes`\r\n```\r\n\u00a0"
        },
        {
            "created_at": "2022-04-27T08:50:42.936Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17528661) by Zsolt Kegyes-Brassai (kbzsl):*\nHi `[~westonpace]` \u00a0\r\n\r\nI tried to create a reproducible example.\r\n\r\nIn the first step I created a dummy dataset wit nearly 100 M rows, having different column types and missing data.\r\nWhen writing this dataset to a parquet file I realized, that even the `write_parquet()` consumes a large amount of memory which is not returned back.\r\n\r\nHere is the data generation part:\r\n\r\n\u00a0\r\n```java\n\r\nlibrary(tidyverse)\r\nn = 99e6 + as.integer(1e6 * runif(n = 1))\r\n# n = 1000\r\na =\u00a0\r\n\u00a0 tibble(\r\n\u00a0 \u00a0 key1 = sample(datasets::state.abb, size = n, replace = TRUE),\r\n\u00a0 \u00a0 key2 = sample(datasets::state.name, size = n, replace = TRUE),\r\n\u00a0 \u00a0 subkey1 = sample(LETTERS, size = n, replace = TRUE),\r\n\u00a0 \u00a0 subkey2 = sample(letters, size = n, replace = TRUE),\r\n\u00a0 \u00a0 value1 = runif(n = n),\r\n\u00a0 \u00a0 value2 = as.integer(1000 * runif(n = n)),\r\n\u00a0 \u00a0 time = as.POSIXct(1e8 * runif(n = n), tz = \"UTC\", origin = \"2020-01-01\")\r\n\u00a0 ) |>\u00a0\r\n\u00a0 mutate(\r\n\u00a0 \u00a0 subkey1 = if_else(key1 %in% c(\"WA\", \"WV\", \"WI\", \"WY\"),\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subkey1, NA_character_),\r\n\u00a0 \u00a0 subkey2 = if_else(key2 %in% c(\"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"),\u00a0\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 subkey2, NA_character_),\r\n\u00a0 )\r\nlobstr::obj_size(a)\r\n#> 5,177,583,640 B\r\n```\r\nand the memory utilization after the dataset creation\r\n\r\n![100m_1_create.jpg](https://issues.apache.org/jira/secure/attachment/13042998/100m_1_create.jpg)\r\n\r\nand writing to **`rds`** file\r\n```java\n\r\nreadr::write_rds(a, here::here(\"db\", \"test100m.rds\"))\n```\r\nno visible memory utilization increase\r\n\r\n![100m_2_rds.jpg](https://issues.apache.org/jira/secure/attachment/13042999/100m_2_rds.jpg)\r\n\r\nand writing to **parquet** file\u00a0\r\n```java\n\r\narrow::write_parquet(a, here::here(\"db\", \"test100m.parquet\"))\n```\r\nthere is a drastic increase in memory utilization 10.6 GB -> 15 GB - just for writing the file\r\n\r\n![100m_3_parquet.jpg](https://issues.apache.org/jira/secure/attachment/13043000/100m_3_parquet.jpg)\r\nIt looks that this memory amount consumed during writing the parquet file was not returned back even after 15 minutes.\r\n\r\nMy biggest concern is that the ability to handle datasets larger than the available memory seems increasingly remote.\r\nI consider that this is a critical bug, but it might happen that is affecting only me\u2026 as I don\u2019t have possibility to test elsewhere."
        },
        {
            "created_at": "2022-04-27T10:08:26.161Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17528712) by Zsolt Kegyes-Brassai (kbzsl):*\nAnd here is the result of the reading back these files.\r\n\r\n\u00a0\r\n```java\n\r\na = readr::read_rds(here::here(\"db\", \"test100m.rds\"))\r\nlobstr::obj_size(a)\r\n#> 5,177,583,640 B\n```\r\n\u00a0\r\n\r\n![100m_4_read_rds.jpg](https://issues.apache.org/jira/secure/attachment/13043008/100m_4_read_rds.jpg)\r\n\r\n\u00a0\r\n```java\n\r\na = arrow::read_parquet(here::here(\"db\", \"test100m.parquet\"))\r\nlobstr::obj_size(a)\r\n#> 796,553,696 B\n```\r\n\u00a0\r\n\r\n![100m_5_read-parquet.jpg](https://issues.apache.org/jira/secure/attachment/13043009/100m_5_read-parquet.jpg)\r\n\r\nThis time there is no considerable difference in the memory utilization.\r\n\r\nIt\u2019s a bit hard for me to understand when additional memory is used for parquet activities, but more important when this memory amount is returned and when is not (and what can trigger it). \u00a0 \r\n\r\nSorry, I am a bit puzzled. It might happen that this is not an bug, just lack in my understanding."
        },
        {
            "created_at": "2022-04-28T04:04:18.284Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16320?focusedCommentId=17529181) by Weston Pace (westonpace):*\nThe writing behavior you described seemed odd so I modified your script a little (and added a memory print which, sadly, will only work on Linux):\r\n\r\n```\n\r\n> \r\n> print_rss <- function() {\r\n+   print(grep(\"vmrss\", readLines(\"/proc/self/status\"), ignore.case=TRUE, value=TRUE))\r\n+ }\r\n> \r\n> n = 99e6 + as.integer(1e6 * runif(n = 1))\r\n> a = \r\n+   tibble(\r\n+     key1 = sample(datasets::state.abb, size = n, replace = TRUE),\r\n+     key2 = sample(datasets::state.name, size = n, replace = TRUE),\r\n+     subkey1 = sample(LETTERS, size = n, replace = TRUE),\r\n+     subkey2 = sample(letters, size = n, replace = TRUE),\r\n+     value1 = runif(n = n),\r\n+     value2 = as.integer(1000 * runif(n = n)),\r\n+     time = as.POSIXct(1e8 * runif(n = n), tz = \"UTC\", origin = \"2020-01-01\")\r\n+   ) |> \r\n+   mutate(\r\n+     subkey1 = if_else(key1 %in% c(\"WA\", \"WV\", \"WI\", \"WY\"), \r\n+                       subkey1, NA_character_),\r\n+     subkey2 = if_else(key2 %in% c(\"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"), \r\n+                       subkey2, NA_character_),\r\n+   )\r\n> lobstr::obj_size(a)\r\n5,171,792,240 B\r\n> print(\"Memory usage after creating the tibble\")\r\n[1] \"Memory usage after creating the tibble\"\r\n> print_rss()\r\n[1] \"VmRSS:\\t 5159276 kB\"\r\n> \r\n> \r\n> readr::write_rds(a, here::here(\"db\", \"test100m.rds\"))\r\n> print(\"Memory usage after writing rds\")\r\n[1] \"Memory usage after writing rds\"\r\n> print_rss()\r\n[1] \"VmRSS:\\t 5161776 kB\"\r\n> \r\n> \r\n> arrow::write_parquet(a, here::here(\"db\", \"test100m.parquet\"))\r\n> print(\"Memory usage after writing parquet\")\r\n[1] \"Memory usage after writing parquet\"\r\n> print_rss()\r\n[1] \"VmRSS:\\t 8990620 kB\"\r\n> Sys.sleep(5)\r\n> print(\"And after sleeping 5 seconds\")\r\n[1] \"And after sleeping 5 seconds\"\r\n> print_rss()\r\n[1] \"VmRSS:\\t 8990620 kB\"\r\n> print(gc())\r\n            used   (Mb) gc trigger    (Mb)   max used   (Mb)\r\nNcells    892040   47.7    1749524    93.5    1265150   67.6\r\nVcells 647980229 4943.7 1392905158 10627.1 1240800333 9466.6\r\n> Sys.sleep(5)\r\n> print(\"And again after a garbage collection and 5 more seconds\")\r\n[1] \"And again after a garbage collection and 5 more seconds\"\r\n> print_rss()\r\n[1] \"VmRSS:\\t 5377900 kB\"\r\n```\r\n\r\nSummarizing...\r\n```\n\r\nCreate table\r\n~5.15GB RAM used\r\nWrite RDS\r\n~5.16GB RAM used\r\nWrite Parquet\r\n~9GB RAM used\r\nWait 5 seconds\r\n~9GB RAM used\r\nRun garbage collection\r\nWait 5 seconds\r\n~5.38GB RAM used\r\n```\r\n\r\nThis doesn't seem terribly ideal.  I think, after writing, some R objects are holding references (possibly transitively) to some shared pointers to record batches in C++.  When the garbage collection runs those R objects are destroyed and the shared pointers (and buffers) can be freed.\r\n\r\n"
        }
    ]
}