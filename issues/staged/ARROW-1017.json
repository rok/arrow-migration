{
    "issue": {
        "title": "Python: Table.to_pandas leaks memory",
        "body": "***Note**: This issue was originally created as [ARROW-1017](https://issues.apache.org/jira/browse/ARROW-1017). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nRunning the following code results in ever increasing memory usage, even though I would expect the dataframe to be garbage collected when it goes out of scope. For the size of my parquet file, I see the usage increasing about 3GB per loop:\n\n```Java\nfrom pyarrow import HdfsClient\n\ndef read_parquet_file(client, parquet_file):\n    parquet = client.read_parquet(parquet_file)\n    df = parquet.to_pandas()\n\nclient = HdfsClient(\"hdfshost\", 8020, \"myuser\", driver='libhdfs3')\nparquet_file = '/my/parquet/file\nwhile True:\n    read_parquet_file(client, parquet_file)\n```\n\nIs there a reference count issue similar to ARROW-362?",
        "created_at": "2017-05-12T17:12:33.000Z",
        "updated_at": "2017-05-15T12:59:22.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2017-05-14T18:35:37.000Z"
    },
    "comments": [
        {
            "created_at": "2017-05-12T17:56:21.571Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16008481) by Wes McKinney (wesm):*\nThanks `[~jporritt]`, I will take a look and see if I can reproduce the issue. "
        },
        {
            "created_at": "2017-05-12T18:18:31.057Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16008513) by Wes McKinney (wesm):*\n`[~xhochy]` we should try to investigate this before approving a parquet-cpp 1.1.0 release in case there is a memory leak in libparquet"
        },
        {
            "created_at": "2017-05-14T14:22:10.173Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16009755) by Wes McKinney (wesm):*\nI'm able to reproduce this issue without involving Parquet or HDFS. I will dig into it"
        },
        {
            "created_at": "2017-05-14T14:30:24.181Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16009757) by Wes McKinney (wesm):*\nUpdate. The following leaks memory:\n\n```Java\nimport pyarrow as pa\nimport numpy as np\n\ndef leak():\n    data = [pa.array(np.concatenate([np.random.randn(1000000)] * 100))]\n    table = pa.Table.from_arrays(data, ['foo'])\n    while True:\n        table.to_pandas()\n```\n\nthis does not:\n\n```Java\ndef leak():\n    data = [pa.array(np.concatenate([np.random.randn(1000000)] * 100))]\n    table = pa.Table.from_arrays(data, ['foo'])\n    while True:\n        table[0].to_pandas()\n```"
        },
        {
            "created_at": "2017-05-14T15:56:14.332Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16009781) by Wes McKinney (wesm):*\nPR: https://github.com/apache/arrow/pull/685"
        },
        {
            "created_at": "2017-05-14T18:35:37.259Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16009812) by Wes McKinney (wesm):*\nIssue resolved by pull request 685\n<https://github.com/apache/arrow/pull/685>"
        },
        {
            "created_at": "2017-05-15T12:59:22.440Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1017?focusedCommentId=16010445) by Wes McKinney (wesm):*\n`[~jporritt]` the binaries on conda-forge have been updated to include this bug fix. We are working to get the 0.4.0 release ready this week, so that should be out officially in the next week or so"
        }
    ]
}