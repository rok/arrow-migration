{
    "issue": {
        "title": "[Release][Rust] Format error in verification script",
        "body": "***Note**: This issue was originally created as [ARROW-5044](https://issues.apache.org/jira/browse/ARROW-5044). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n```\n\r\n+ cargo fmt --all -- --check\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/arrow/src/csv/writer.rs at line 53:\r\n //! let batch = RecordBatch::try_new(\r\n //!     Arc::new(schema),\r\n //!     vec![Arc::new(c1), Arc::new(c2), Arc::new(c3), Arc::new(c4)],\r\n-//! ).unwrap();\r\n+//! )\r\n+//! .unwrap();\r\n //!\r\n //! let file = get_temp_file(\"out.csv\", &[]);\r\n //!\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/datasource.rs at line 24:\r\n \r\n use crate::error::Result;\r\n \r\n-/// Returned by implementors of `Table#scan`, this `RecordBatchIterator` is wrapped with an `Arc`\r\n-/// and `Mutex` so that it can be shared across threads as it is used.\r\n+/// Returned by implementors of `Table#scan`, this `RecordBatchIterator` is wrapped with\r\n+/// an `Arc` and `Mutex` so that it can be shared across threads as it is used.\r\n pub type ScanResult = Arc<Mutex<RecordBatchIterator>>;\r\n \r\n /// Source table\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/datasource.rs at line 33:\r\n     /// Get a reference to the schema for this table\r\n     fn schema(&self) -> &Arc<Schema>;\r\n \r\n-    /// Perform a scan of a table and return a sequence of iterators over the data (one iterator per partition)\r\n+    /// Perform a scan of a table and return a sequence of iterators over the data (one\r\n+    /// iterator per partition)\r\n     fn scan(\r\n         &self,\r\n         projection: &Option<Vec<usize>>,\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/datasource/memory.rs at line 16:\r\n // under the License.\r\n \r\n //! In-memory data source for presenting a Vec<RecordBatch> as a data source that can be\r\n-//! queried by DataFusion. This allows data to be pre-loaded into memory and then repeatedly\r\n-//! queried without incurring additional file I/O overhead.\r\n+//! queried by DataFusion. This allows data to be pre-loaded into memory and then\r\n+//! repeatedly queried without incurring additional file I/O overhead.\r\n \r\n use std::sync::{Arc, Mutex};\r\n \r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/context.rs at line 15:\r\n // specific language governing permissions and limitations\r\n // under the License.\r\n \r\n-//! ExecutionContext contains methods for registering data sources and executing SQL queries\r\n+//! ExecutionContext contains methods for registering data sources and executing SQL\r\n+//! queries\r\n \r\n use std::cell::RefCell;\r\n use std::collections::HashMap;\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/context.rs at line 139:\r\n         Ok(plan)\r\n     }\r\n \r\n-    /// Execute a logical plan and produce a Relation (a schema-aware iterator over a series\r\n-    /// of RecordBatch instances)\r\n+    /// Execute a logical plan and produce a Relation (a schema-aware iterator over a\r\n+    /// series of RecordBatch instances)\r\n     pub fn execute(\r\n         &mut self,\r\n         plan: &LogicalPlan,\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/filter.rs at line 15:\r\n // specific language governing permissions and limitations\r\n // under the License.\r\n \r\n-//! Execution of a filter (predicate) relation. The SQL clause `WHERE expr` represents a filter.\r\n+//! Execution of a filter (predicate) relation. The SQL clause `WHERE expr` represents a\r\n+//! filter.\r\n \r\n use std::cell::RefCell;\r\n use std::rc::Rc;\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/filter.rs at line 32:\r\n \r\n /// Implementation of a filter relation\r\n pub(super) struct FilterRelation {\r\n-    /// The schema for the filter relation. This is always the same as the schema of the input relation.\r\n+    /// The schema for the filter relation. This is always the same as the schema of the\r\n+    /// input relation.\r\n     schema: Arc<Schema>,\r\n     /// Relation that is  being filtered\r\n     input: Rc<RefCell<Relation>>,\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/limit.rs at line 33:\r\n pub(super) struct LimitRelation {\r\n     /// The relation which the limit is being applied to\r\n     input: Rc<RefCell<Relation>>,\r\n-    /// The schema for the limit relation, which is always the same as the schema of the input relation\r\n+    /// The schema for the limit relation, which is always the same as the schema of the\r\n+    /// input relation\r\n     schema: Arc<Schema>,\r\n     /// The number of rows returned by this relation\r\n     limit: usize,\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/projection.rs at line 15:\r\n // specific language governing permissions and limitations\r\n // under the License.\r\n \r\n-//! Defines the projection relation. A projection determines which columns or expressions are\r\n-//! returned from a query. The SQL statement `SELECT a, b, a+b FROM t1` is an example of a\r\n-//! projection on table `t1` where the expressions `a`, `b`, and `a+b` are the projection\r\n-//! expressions.\r\n+//! Defines the projection relation. A projection determines which columns or expressions\r\n+//! are returned from a query. The SQL statement `SELECT a, b, a+b FROM t1` is an example\r\n+//! of a projection on table `t1` where the expressions `a`, `b`, and `a+b` are the\r\n+//! projection expressions.\r\n \r\n use std::cell::RefCell;\r\n use std::rc::Rc;\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/execution/relation.rs at line 16:\r\n // under the License.\r\n \r\n //! A relation is a representation of a set of tuples. A database table is a\r\n-//! type of relation. During query execution, each operation on a relation (such as projection,\r\n-//! selection, aggregation) results in a new relation.\r\n+//! type of relation. During query execution, each operation on a relation (such as\r\n+//! projection, selection, aggregation) results in a new relation.\r\n \r\n use std::sync::{Arc, Mutex};\r\n \r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/optimizer.rs at line 21:\r\n use crate::logicalplan::LogicalPlan;\r\n use std::sync::Arc;\r\n \r\n-/// An optimizer rules performs a transformation on a logical plan to produce an optimized logical plan.\r\n+/// An optimizer rules performs a transformation on a logical plan to produce an optimized\r\n+/// logical plan.\r\n pub trait OptimizerRule {\r\n     /// Perform optimizations on the plan\r\n     fn optimize(&mut self, plan: &LogicalPlan) -> Result<Arc<LogicalPlan>>;\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/projection_push_down.rs at line 142:\r\n                 schema,\r\n                 ..\r\n             } => {\r\n-                // once we reach the table scan, we can use the accumulated set of column indexes as\r\n-                // the projection in the table scan\r\n+                // once we reach the table scan, we can use the accumulated set of column\r\n+                // indexes as the projection in the table scan\r\n                 let mut projection: Vec<usize> = Vec::with_capacity(accum.len());\r\n                 accum.iter().for_each(|i| projection.push(*i));\r\n \r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/projection_push_down.rs at line 158:\r\n                 }\r\n                 let projected_schema = Schema::new(projected_fields);\r\n \r\n-                // now that the table scan is returning a different schema we need to create a\r\n-                // mapping from the original column index to the new column index so that we\r\n-                // can rewrite expressions as we walk back up the tree\r\n+                // now that the table scan is returning a different schema we need to\r\n+                // create a mapping from the original column index to the\r\n+                // new column index so that we can rewrite expressions as\r\n+                // we walk back up the tree\r\n \r\n                 if mapping.len() != 0 {\r\n                     return Err(ExecutionError::InternalError(\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/type_coercion.rs at line 17:\r\n \r\n //! The type_coercion optimizer rule ensures that all binary operators are operating on\r\n //! compatible types by adding explicit cast operations to expressions. For example,\r\n-//! the operation `c_float + c_int` would be rewritten as `c_float + CAST(c_int AS float)`.\r\n-//! This keeps the runtime query execution code much simpler.\r\n+//! the operation `c_float + c_int` would be rewritten as `c_float + CAST(c_int AS\r\n+//! float)`. This keeps the runtime query execution code much simpler.\r\n \r\n use std::sync::Arc;\r\n \r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/optimizer/utils.rs at line 24:\r\n use crate::error::{ExecutionError, Result};\r\n use crate::logicalplan::Expr;\r\n \r\n-/// Recursively walk a list of expression trees, collecting the unique set of column indexes\r\n-/// referenced in the expression\r\n+/// Recursively walk a list of expression trees, collecting the unique set of column\r\n+/// indexes referenced in the expression\r\n pub fn exprlist_to_column_indices(expr: &Vec<Expr>, accum: &mut HashSet<usize>) {\r\n     expr.iter().for_each(|e| expr_to_column_indices(e, accum));\r\n }\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/src/table.rs at line 15:\r\n // specific language governing permissions and limitations\r\n // under the License.\r\n \r\n-//! Table API for building a logical query plan. This is similar to the Table API in Ibis and\r\n-//! the DataFrame API in Apache Spark\r\n+//! Table API for building a logical query plan. This is similar to the Table API in Ibis\r\n+//! and the DataFrame API in Apache Spark\r\n \r\n use crate::error::Result;\r\n use crate::logicalplan::LogicalPlan;\r\nDiff in /tmp/arrow-0.13.0.tW4Dz/apache-arrow-0.13.0/rust/datafusion/tests/sql.rs at line 129:\r\n     assert_eq!(expected, actual);\r\n }\r\n \r\n-//TODO Uncomment the following test when ORDER BY is implemented to be able to test ORDER BY + LIMIT\r\n+//TODO Uncomment the following test when ORDER BY is implemented to be able to test ORDER\r\n+// BY + LIMIT\r\n /*\r\n #[test]\r\n fn csv_query_limit_with_order_by() {\r\n\r\n```",
        "created_at": "2019-03-27T22:01:27.000Z",
        "updated_at": "2019-03-28T01:28:49.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Packaging",
            "Component: Rust",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-03-28T01:28:17.000Z"
    },
    "comments": [
        {
            "created_at": "2019-03-28T01:28:17.380Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-5044?focusedCommentId=16803511) by Kouhei Sutou (kou):*\nIssue resolved by pull request 4059\n<https://github.com/apache/arrow/pull/4059>"
        }
    ]
}