{
    "issue": {
        "title": "[Python][Parquet] Performance regression in pyarrow-0.15.1 vs pyarrow-0.12.1 when reading a \"partitioned parquet table\" ?",
        "body": "***Note**: This issue was originally created as [ARROW-7556](https://issues.apache.org/jira/browse/ARROW-7556). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nI am currently running a small test with pyarrow to load 2 \"partitioned\" parquet tables.\r\n\r\nThe performance seems to be 2 times less with pyarrow-0.15.1 than with pyarrow-0.12.1.\r\n\r\nIn my test:\r\n \\* the 'parquet' tables I am loading are called 'reports' & 'memory'\r\n \\* they have been generated through pandas.to_parquet by specifying the partitions columns\r\n \\* they are both partitioned by 2 columns 'p_type' and 'p_start'\r\n \\* it is small tables:\r\n \\*\\* reports\r\n \\*\\*\\* 90 partitions (1 parquet file / partition)\r\n \\*\\*\\* total size: 6.2MB\r\n \\*\\* memory\r\n \\*\\*\\* 105 partitions (1 parquet file / partition)\r\n \\*\\*\\* total size: 9.1MB\r\n\r\n\u00a0\r\n\r\nHere is the code of my simple test that tries to read them (I'm using a filter on the p_start partition):\r\n\r\n\u00a0\r\n```java\n\r\n// code placeholder\r\n\r\nimport os\r\nimport sys\r\nimport time\r\nimport pyarrow\r\nfrom pyarrow.parquet import ParquetDataset\r\ndef load_dataframe(data_dir, table, start_date, end_date):\r\n    return ParquetDataset(os.path.join(data_dir, table),\r\n                          filters=[('p_start', '>=', start_date),\r\n                                   ('p_start', '<=', end_date)\r\n                                   ]).read().to_pandas()\r\n\r\nprint(f'pyarrow version;{pyarrow.__version__}')\r\n\r\ndata_dir = sys.argv[1]\r\n\r\nfor i in range(1, 10):\r\n    start = time.time()\r\n    start_date = '201912230000'\r\n    end_date = '202001080000'\r\n    load_dataframe(sys.argv[1], 'reports', start_date, end_date)\r\n    load_dataframe(sys.argv[1], 'memory', start_date, end_date)\r\n    print(f'loaded;in;{time.time()-start}')\r\n\r\n```\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nHere are the results:\r\n \\* with pyarrow-0.12.1\r\n\r\n$ python -m cProfile -o load_pyarrow_0.12.1.cprof load_df_from_pyarrow.py parquet/\r\n\r\npyarrow version;0.12.1\r\n\r\nloaded;in;0.5566098690032959\r\nloaded;in;0.32605648040771484\r\nloaded;in;0.28951501846313477\r\nloaded;in;0.29279112815856934\r\nloaded;in;0.3474299907684326\r\nloaded;in;0.4075736999511719\r\nloaded;in;0.425199031829834\r\nloaded;in;0.34653329849243164\r\nloaded;in;0.300839900970459\r\n\r\n(~350ms to load the 2 tables)\r\n\r\n\u00a0\r\n\r\n![load_df_pyarrow-0.12.1.png](https://issues.apache.org/jira/secure/attachment/12990670/load_df_pyarrow-0.12.1.png)\r\n\r\n\u00a0\r\n \\* with pyarrow-0.15.1\r\n\r\n\u00a0\r\n\r\n$ python -m cProfile -o load_pyarrow_0.15.1.cprof load_df_from_pyarrow.py parquet/\r\n\r\npyarrow version;0.15.1\r\nloaded;in;1.1126022338867188\r\nloaded;in;0.8931224346160889\r\nloaded;in;1.3298325538635254\r\nloaded;in;0.8584625720977783\r\nloaded;in;0.9232609272003174\r\nloaded;in;1.0619215965270996\r\nloaded;in;0.8619768619537354\r\nloaded;in;0.8686420917510986\r\nloaded;in;1.1183602809906006\r\n\r\n(>800ms to load the 2 tables)\r\n\r\n\u00a0\r\n\r\n![load_df_pyarrow-0-15.1.png](https://issues.apache.org/jira/secure/attachment/12990669/load_df_pyarrow-0-15.1.png)\r\n\r\n\u00a0\r\n\r\nIs there a performance regression here ?\r\n\r\nAm I missing something ?\r\n\r\n\u00a0\r\n\r\nIn attachment, you can find the 2 .cprof files.\r\n\r\n[load_pyarrow_0.12.1.cprof](load_pyarrow_0.12.1.cprof)\r\n\r\n[load_pyarrow_0.15.1.cprof](load_pyarrow_0.15.1.cprof)",
        "created_at": "2020-01-12T22:44:48.000Z",
        "updated_at": "2020-06-12T20:19:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Benchmarking",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-01-12T23:55:11.475Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7556?focusedCommentId=17013925) by Wes McKinney (wesm):*\nCan you give more details about what is in the Parquet files themselves?\r\n\r\n- Number of rows and columns\n- Any other relevant information\n  \n  If you can provide a minimally reproducible example that would be the best way to help others determine what is wrong"
        },
        {
            "created_at": "2020-01-13T01:03:08.536Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7556?focusedCommentId=17013947) by Julien MASSIOT (jmassiot):*\nHere are the shape and the dtypes of the pandas dataframes obtained from the 2 parquet tables:\r\n\r\n**reports**\r\n\r\nIn [4]: reports = ParquetDataset('parquet/reports/').read().to_pandas()\r\n\r\nIn [5]: reports.shape\r\nOut[5]: (92468, 29)\r\n\r\nIn [6]: reports.dtypes\r\nOut[6]: \r\nts datetime64[ns]\r\nreport object\r\ntask int64\r\norigin object\r\nnegran int64\r\ntimegran int64\r\nnbnes int64\r\nnbtslots int64\r\ncells float64\r\nformulas float64\r\nmappings float64\r\ncum_items float64\r\nmax_items float64\r\ncum_depth float64\r\nmax_depth float64\r\nextr_attr float64\r\nextr_counters float64\r\ntime int64\r\npod object\r\np_end int64\r\nin_progress bool\r\nstart_ts datetime64[ns]\r\ntime-minutes float64\r\nts_rounded datetime64[ns]\r\ntime_per_extr_counters float64\r\nextr_counters_M float64\r\ntimegran_uf object\r\np_type category\r\np_start category\r\ndtype: object\r\n\r\n\u00a0\r\n\r\n**memory**\r\n\r\n\u00a0\r\n\r\nIn [7]: memory = ParquetDataset('parquet/memory').read().to_pandas()\r\n\r\nIn [8]: memory.shape\r\nOut[8]: (278332, 8)\r\n\r\nIn [9]: memory.dtypes\r\nOut[9]: \r\nts datetime64[ns]\r\ntotal float64\r\nfree float64\r\nused float64\r\npod object\r\np_end int64\r\np_type category\r\np_start category\r\ndtype: object\r\n\r\n\u00a0\r\n\r\nYou can find in attachment a tar with the 2 tables + the python script I am using to load them.\r\n\r\nJust untar, and then 'python load_df_from_pyarrow.py arrow-7756'.\r\n\r\nWith pyarrow 0.12.1, it is faster than with pyarrow 0.15.1.\r\n\r\n[arrow-7756.tar.gz](arrow-7756.tar.gz)\r\n\r\n<sup>[load_df_from_pyarrow.py](load_df_from_pyarrow.py)</sup>"
        },
        {
            "created_at": "2020-01-13T02:28:49.073Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7556?focusedCommentId=17013971) by Wes McKinney (wesm):*\nThanks. Hopefully a volunteer will have time to investigate"
        },
        {
            "created_at": "2020-01-16T10:39:51.045Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7556?focusedCommentId=17016793) by Joris Van den Bossche (jorisvandenbossche):*\nJust a note: I quickly checked your example files. And reading a single file out of the partitioned dataset _also_ shows a slowdown, more or less with the same factor as reading the full table. So it doesn't seem related to the fact that it is a \"partitioned dataset\" (which might simplify investigating this). \r\n\r\nAnd for just reading parquet, we have some other issues that reported slowdowns as well, eg ARROW-7059 (although that was mainly about tables with many columns, which doesn't seem to be the case here)\r\n\r\nTested on:\r\n\r\n```Java\n\r\n%timeit pq.read_table(\"arrow-7756/memory/p_type=area-1/p_start=201912300000/29fd41a24eb54840b0a3e448508f7c68.parquet\")\r\n```"
        },
        {
            "created_at": "2020-04-27T02:46:37.712Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7556?focusedCommentId=17092951) by Wes McKinney (wesm):*\nIt would be nice to assess this using the new datasets API"
        }
    ]
}