{
    "issue": {
        "title": "[CI][Python] Dask docker integration test broken perhaps by statistics-related change",
        "body": "***Note**: This issue was originally created as [ARROW-6623](https://issues.apache.org/jira/browse/ARROW-6623). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nsee new failure \r\n\r\nhttps://circleci.com/gh/ursa-labs/crossbow/3027?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link\r\n\r\n```Java\n\r\n=================================== FAILURES ===================================\r\n___________________ test_timeseries_nulls_in_schema[pyarrow] ___________________\r\n\r\ntmpdir = local('/tmp/pytest-of-root/pytest-0/test_timeseries_nulls_in_schem0')\r\nengine = 'pyarrow'\r\n\r\n    def test_timeseries_nulls_in_schema(tmpdir, engine):\r\n        tmp_path = str(tmpdir)\r\n        ddf2 = (\r\n            dask.datasets.timeseries(start=\"2000-01-01\", end=\"2000-01-03\", freq=\"1h\")\r\n            .reset_index()\r\n            .map_partitions(lambda x: x.loc[:5])\r\n        )\r\n        ddf2 = ddf2.set_index(\"x\").reset_index().persist()\r\n        ddf2.name = ddf2.name.where(ddf2.timestamp == \"2000-01-01\", None)\r\n    \r\n        ddf2.to_parquet(tmp_path, engine=engine)\r\n        ddf_read = dd.read_parquet(tmp_path, engine=engine)\r\n    \r\n        assert_eq(ddf_read, ddf2, check_divisions=False, check_index=False)\r\n    \r\n# Can force schema validation on each partition in pyarrow\r\n        if engine == \"pyarrow\":\r\n# The schema mismatch should raise an error\r\n            with pytest.raises(ValueError):\r\n                ddf_read = dd.read_parquet(\r\n                    tmp_path, dataset={\"validate_schema\": True}, engine=engine\r\n                )\r\n# There should be no error if you specify a schema on write\r\n            schema = pa.schema(\r\n                [\r\n                    (\"x\", pa.float64()),\r\n                    (\"timestamp\", pa.timestamp(\"ns\")),\r\n                    (\"id\", pa.int64()),\r\n                    (\"name\", pa.string()),\r\n                    (\"y\", pa.float64()),\r\n                ]\r\n            )\r\n            ddf2.to_parquet(tmp_path, schema=schema, engine=engine)\r\n            assert_eq(\r\n>               dd.read_parquet(tmp_path, dataset={\"validate_schema\": True}, engine=engine),\r\n                ddf2,\r\n                check_divisions=False,\r\n                check_index=False,\r\n            )\r\n\r\nopt/conda/lib/python3.6/site-packages/dask/dataframe/io/tests/test_parquet.py:1964: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\nopt/conda/lib/python3.6/site-packages/dask/dataframe/io/parquet/core.py:190: in read_parquet\r\n    out = sorted_columns(statistics)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nstatistics = ({'columns': [{'max': -0.25838390663957256, 'min': -0.979681447427093, 'name': 'x', 'null_count': 0}, {'max': Timestam...ull_count': 0}, {'max': 0.8978352477516438, 'min': -0.7218571212693894, 'name': 'y', 'null_count': 0}], 'num-rows': 7})\r\n\r\n    def sorted_columns(statistics):\r\n        \"\"\" Find sorted columns given row-group statistics\r\n    \r\n        This finds all columns that are sorted, along with appropriate divisions\r\n        values for those columns\r\n    \r\n        Returns\r\n        -------\r\n        out: List of {'name': str, 'divisions': List[str]} dictionaries\r\n        \"\"\"\r\n        if not statistics:\r\n            return []\r\n    \r\n        out = []\r\n        for i, c in enumerate(statistics[0][\"columns\"]):\r\n            if not all(\r\n                \"min\" in s[\"columns\"][i] and \"max\" in s[\"columns\"][i] for s in statistics\r\n            ):\r\n                continue\r\n            divisions = [c[\"min\"]]\r\n            max = c[\"max\"]\r\n            success = True\r\n            for stats in statistics[1:]:\r\n                c = stats[\"columns\"][i]\r\n>               if c[\"min\"] >= max:\r\nE               TypeError: '>=' not supported between instances of 'numpy.ndarray' and 'str'\r\n\r\nopt/conda/lib/python3.6/site-packages/dask/dataframe/io/parquet/core.py:570: TypeError\r\n```",
        "created_at": "2019-09-19T19:11:34.000Z",
        "updated_at": "2019-09-30T13:57:30.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2019-09-30T13:57:30.000Z"
    },
    "comments": [
        {
            "created_at": "2019-09-20T09:33:30.177Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6623?focusedCommentId=16934241) by Joris Van den Bossche (jorisvandenbossche):*\nI opened an issue on the dask tracker: https://github.com/dask/dask/issues/5418\r\n\r\nThere are actually two errors that happen (when rerunning the test multiple times). At least one of them is due to the recent `schema` changes (index now needs to be included) that I did.  \r\nThe other might be related to change in statistics for null columns: https://github.com/apache/arrow/pull/5403 (ARROW-6623)\r\n\r\nI am investigating this a bit further."
        },
        {
            "created_at": "2019-09-20T10:25:21.112Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6623?focusedCommentId=16934278) by Joris Van den Bossche (jorisvandenbossche):*\nI elaborated a bit more on the dask issue, but indeed, there were two failures, one caused by the schema change, one caused by the statistics for null columns change."
        },
        {
            "created_at": "2019-09-30T13:57:30.505Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-6623?focusedCommentId=16940970) by Krisztian Szucs (kszucs):*\nIt had been fixed on desk's side. "
        }
    ]
}