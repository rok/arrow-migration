{
    "issue": {
        "title": "[Integration] Enable Arrow to read Parquet files from Spark 2.x with illegal nulls",
        "body": "***Note**: This issue was originally created as [ARROW-11409](https://issues.apache.org/jira/browse/ARROW-11409). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhile running integration tests with Arrow and Spark, I observed that Spark 2.x can in some circumstances\u00a0write Parquet files with illegal nulls in non-nullable columns. (This appears to have been fixed in Spark 3.0.)\u00a0Arrow throws an `Unexpected end of stream` error when attempting to read illegal Parquet files like this.\r\n\r\nThe attached Parquet file written by Spark 2.0.0 can be used to repro this behavior. It contains only one column, a non-nullable integer named `x`, with three records:\r\n```java\n\r\n+-----+\r\n|    x|\r\n+-----+\r\n|    1|\r\n| null|\r\n|    3|\r\n+-----+\u00a0\r\n```\r\nThis issue is for awareness only. I expect this should be closed as \"won't fix\".",
        "created_at": "2021-01-27T22:12:26.000Z",
        "updated_at": "2021-01-27T22:26:14.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Integration",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-01-27T22:14:19.632Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-11409?focusedCommentId=17273173) by Ian Cook (icook):*\nFYI\u00a0 `[~bryanc]`"
        }
    ]
}