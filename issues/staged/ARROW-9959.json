{
    "issue": {
        "title": "[Python][C++][Parquet] Ability to delete row groups from metadata",
        "body": "***Note**: This issue was originally created as [ARROW-9959](https://issues.apache.org/jira/browse/ARROW-9959). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nWe currently use PyArrow to maintain a partitioned dataset of Parquet files on disk. We also manage our own `_metadata` file - when new rows are written to the dataset, we use the `metadata_collector` argument of `write_to_dataset` to collect all metadata that was written inside individual files. We then load the existing `_metadata` file and merge it with all the newly-written metadatas using `metadata.append_row_groups` (as in the docs) and then write the result to `_metadata` on disk.\r\n\r\nHowever, we would also like to occasionally amend this dataset by deleting individual files. In order to keep the `_metadata` file in sync, we would need to load the metadata of all the files we're willing to delete, then find their row groups inside `_metadata` and remove them. Therefore we require a method such as `delete_row_groups` to exist on the `FileMetaData` object. Would it be possible for PyArrow to support this? Another way of accomplishing the same thing would be to initialise an empty `FileMetaData` object and simply use `append_row_groups` to add back all the row groups that are required. However, I've been unable to accomplish this programmaticaly as the constructor for `FileMetaData` seems to ask for a C structure which I'm not sure how to construct.\r\n\r\nMany thanks.",
        "created_at": "2020-09-10T10:17:27.000Z",
        "updated_at": "2021-12-16T17:12:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}