{
    "issue": {
        "title": "[JS] Values returned as undefined when arrow file bigger than 2gb",
        "body": "***Note**: This issue was originally created as [ARROW-18007](https://issues.apache.org/jira/browse/ARROW-18007). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nSteps:\r\n\r\n\u00a0\r\n\r\n1. Generate arrow file bigger than 2gb\r\n```java\n\r\nimport pyarrow as pa\r\n\r\nnums1 = [42]\r\nnums2 = [42.42]\r\nmil = 1000000\r\n\r\nfor n in range(1, 140 * mil):\r\n  nums1.append(n)\r\n  nums2.append(1 / n)\r\n\r\narr1 = pa.array(nums1)\r\narr2 = pa.array(nums2)\r\n\r\nschema = pa.schema([\r\n  pa.field('nums1', arr1.type),\r\n  pa.field('nums2', arr2.type),\r\n])\r\n\r\nwith pa.OSFile('arraydata.arrow', 'wb') as sink:\r\n  with pa.ipc.new_file(sink, schema=schema) as writer:\r\n    batch = pa.record_batch([arr1, arr2], schema=schema)\r\n    writer.write(batch) \n```\r\n2. Try to read it via the JS SDK\r\n```java\n\r\nconst fs = require(\"fs\");\r\nconst { tableFromIPC, RecordBatchReader } = require(\"apache-arrow\");\r\n\r\nconst filePath = \"./arraydata.arrow\";\r\n\r\nconst stream = fs.createReadStream(filePath);\r\nconst reader = RecordBatchReader.from(stream);\r\n\r\n(async function () {\r\n  const table = await tableFromIPC(reader);\r\n\r\n  console.log(\"numRows\", table.numRows);\r\n  console.log(\"first row\", table.get(0).toArray());\r\n})(); \n```\r\nThe code above prints:\r\n```java\n\r\nnumRows 140000000\r\nfirst row [ undefined, undefined ] \n```\r\n`numRows`\u00a0is correct, but the values are coming out as\u00a0`{}undefined{`}.",
        "created_at": "2022-10-12T16:05:59.000Z",
        "updated_at": "2022-10-12T16:06:33.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: JavaScript",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}