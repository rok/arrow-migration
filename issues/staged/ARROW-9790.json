{
    "issue": {
        "title": "[Rust] [Parquet] ParquetFileArrowReader fails to decode all pages if batches fall exactly on row group boundaries",
        "body": "***Note**: This issue was originally created as [ARROW-9790](https://issues.apache.org/jira/browse/ARROW-9790). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen I was reading a parquet file into RecordBatches using `ParquetFileArrowReader` that had row groups that were 100,000 rows in length with a batch size of 60,000, after reading 300,000 rows successfully, I started seeing this error\r\n\r\n```Java\n\r\n ParquetError(\"Parquet error: Not all children array length are the same!\")\r\n```\r\n\r\nUpon investigation, I found that when reading with `ParquetFileArrowReader`, if the parquet input file has multiple row groups, and if a batch happens to end at the end of a row group for Int or Float, no subsequent row groups are read\r\n\r\nVisually:\r\n```Java\n\r\n+-----+\r\n| RG1 |\r\n|     |\r\n+-----+  <-- If a batch ends exactly at the end of this row group (page), RG2 is never read\r\n+-----+\r\n| RG2 |\r\n|     |\r\n+-----+\r\n```\r\n\r\nA reproducer is attached. 20 values should be read by the `ParquetFileArrowReader` regardless of the batch size. However, when using batch sizes such as `5` or `3` (which fall on a boundary between row groups) not all the rows are read. \r\n\r\nTo run the reproducer, decompress the attachment  [parquet_file_arrow_reader.zip](parquet_file_arrow_reader.zip) and do `cargo run`\r\n\r\nThe output is as follows:\r\n\r\n```Java\n\r\nwrote 20 rows in 4 row groups to /tmp/repro.parquet\r\nSize when reading with batch_size 100 : 20\r\nSize when reading with batch_size 7 : 20\r\nSize when reading with batch_size 5 : 5\r\n```\r\n\r\nThe expected output is as follows (should always read 20 rows, regardless of the batch size):\r\n```Java\n\r\nwrote 20 rows in 4 row groups to /tmp/repro.parquet\r\nSize when reading with batch_size 100 : 20\r\nSize when reading with batch_size 7 : 20\r\nSize when reading with batch_size 5 : 20\r\n```\r\n\r\n## Workaround\r\nUse a different batch size that will not fall on record batch boundaries",
        "created_at": "2020-08-18T22:24:04.000Z",
        "updated_at": "2022-07-08T18:38:49.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Rust",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-08-20T01:02:47.000Z"
    },
    "comments": [
        {
            "created_at": "2020-08-18T22:24:30.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9790?focusedCommentId=17180135) by Andrew Lamb (alamb):*\nI am pretty sure I know what the problem is, and I will have a PR up to fix this tomorrow morning"
        },
        {
            "created_at": "2020-08-19T12:44:59.768Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9790?focusedCommentId=17180510) by Andrew Lamb (alamb):*\nHere is the reproducer code, to save you from having to download it:\r\n\r\n```Java\n\r\n\r\nuse std::{fs, path::Path, rc::Rc};\r\n\r\nuse parquet::{\r\n    file::{\r\n        properties::WriterProperties,\r\n        writer::{FileWriter, SerializedFileWriter},\r\n    },\r\n    schema::parser::parse_message_type, column::writer::ColumnWriter,\r\n};\r\n\r\nuse parquet::arrow::arrow_reader::ArrowReader;\r\nuse arrow::record_batch::RecordBatchReader;\r\n\r\nfn main() {\r\n    make_repro_file();\r\n\r\n    println!(\"Size when reading with batch_size 100 : {}\", read_record_batches(100));\r\n    println!(\"Size when reading with batch_size 7 : {}\", read_record_batches(7));\r\n    println!(\"Size when reading with batch_size 5 : {}\", read_record_batches(5));\r\n\r\n}\r\n\r\n/// create repro.parquet file\r\nfn make_repro_file() {\r\n    let path = Path::new(\"/tmp/repro.parquet\");\r\n\r\n    let message_type = \"\r\n  message schema {\r\n    REQUIRED INT32 b;\r\n  }\r\n\";\r\n    let schema = Rc::new(parse_message_type(message_type).unwrap());\r\n    let props = Rc::new(WriterProperties::builder().build());\r\n    let file = fs::File::create(&path).unwrap();\r\n    let mut writer = SerializedFileWriter::new(file, schema, props).unwrap();\r\n\r\n    let data = [1,2,3,4,5];\r\n\r\n    for _ in 0..4 {\r\n        let mut row_group_writer = writer.next_row_group().unwrap();\r\n        if let Some(mut col_writer) = row_group_writer.next_column().unwrap() {\r\n            match &mut col_writer {\r\n                ColumnWriter::Int32ColumnWriter(w) => {\r\n                    w.write_batch(&data, None, None).expect(\"Writing data\");\r\n                },\r\n                _=> unimplemented!(\"unexpected type\")\r\n            }\r\n            row_group_writer.close_column(col_writer).unwrap();\r\n        }\r\n        writer.close_row_group(row_group_writer).unwrap();\r\n    }\r\n    writer.close().unwrap();\r\n    println!(\"wrote 20 rows in 4 row groups to /tmp/repro.parquet\");\r\n}\r\n\r\n\r\n\r\n\r\n\r\n// returns the number of total rows, using batch_size\r\nfn read_record_batches(batch_size: usize) -> usize {\r\n    //println!(\"Opening the file....\");\r\n    let r = fs::File::open(\"/tmp/repro.parquet\").unwrap();\r\n\r\n    let parquet_reader = parquet::file::reader::SerializedFileReader::new(r).unwrap();\r\n    let mut reader = parquet::arrow::arrow_reader::ParquetFileArrowReader::new(Rc::new(parquet_reader));\r\n    let mut record_batch_reader = reader.get_record_reader(batch_size).unwrap();\r\n\r\n    let mut total_rows = 0;\r\n    loop {\r\n        let rb = record_batch_reader.next_batch();\r\n        match rb {\r\n            Err(e) => println!(\"WARNING: error reading batch: {:?}, SKIPPING\", e),\r\n            Ok(Some(rb)) => {\r\n                //println!(\"Successfully got a new record batch of {} rows\", rb.num_rows());\r\n                total_rows += rb.num_rows();\r\n            },\r\n            Ok(None) => {\r\n                //println!(\"No more batches\");\r\n                break;\r\n            }\r\n        }\r\n    }\r\n\r\n    total_rows\r\n}\r\n```"
        },
        {
            "created_at": "2020-08-20T01:02:47.479Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9790?focusedCommentId=17180898) by Chao Sun (csun):*\nIssue resolved by pull request 8007\n<https://github.com/apache/arrow/pull/8007>"
        },
        {
            "created_at": "2022-07-08T18:38:49.055Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9790?focusedCommentId=17564399) by Andrew Lamb (alamb):*\nSee also https://github.com/apache/arrow-rs/issues/2025"
        }
    ]
}