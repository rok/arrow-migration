{
    "issue": {
        "title": "[Python] Unable to read/write Parquet datasets with fsspec on Azure Blob",
        "body": "***Note**: This issue was originally created as [ARROW-10517](https://issues.apache.org/jira/browse/ARROW-10517). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n\u00a0\r\n```python\n\r\n# adal==1.2.5\r\n# adlfs==0.2.5\r\n# fsspec==0.7.4\r\n# pandas==1.1.3\r\n# pyarrow==2.0.0\r\n# azure-storage-blob==2.1.0\r\n# azure-storage-common==2.1.0\r\n\r\nimport pyarrow.dataset as ds\r\nimport fsspec\r\nfrom pyarrow.dataset import DirectoryPartitioning\r\n\r\nfs = fsspec.filesystem(protocol='abfs', \r\n                       account_name=base.login, \r\n                       account_key=base.password)\r\n\r\n\r\nds.write_dataset(data=table, \r\n                 base_dir=\"dev/test7\", \r\n                 basename_template=None, \r\n                 format=\"parquet\",\r\n                 partitioning=DirectoryPartitioning(pa.schema([(\"year\", pa.string()), (\"month\", pa.string()), (\"day\", pa.string())])), \r\n                 schema=table.schema,\r\n                 filesystem=fs, \r\n                )\r\n```\r\n\u00a0I think this is due to early versions of adlfs having mkdir(). Although I use write_to_dataset and write_table all of the time, so I am not sure why this would be an issue.\r\n```python\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-40-bb38d83f896e> in <module>\r\n     13 \r\n     14 \r\n---> 15 ds.write_dataset(data=table, \r\n     16                  base_dir=\"dev/test7\",\r\n     17                  basename_template=None,\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in write_dataset(data, base_dir, basename_template, format, partitioning, schema, filesystem, file_options, use_threads)\r\n    771     filesystem, _ = _ensure_fs(filesystem)\r\n    772 \r\n--> 773     _filesystemdataset_write(\r\n    774         data, base_dir, basename_template, schema,\r\n    775         filesystem, partitioning, file_options, use_threads,\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset._filesystemdataset_write()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_fs.pyx in pyarrow._fs._cb_create_dir()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/fs.py in create_dir(self, path, recursive)\r\n    226     def create_dir(self, path, recursive):\r\n    227         # mkdir also raises FileNotFoundError when base directory is not found\r\n--> 228         self.fs.mkdir(path, create_parents=recursive)\r\n    229 \r\n    230     def delete_dir(self, path):\r\n\r\n/opt/conda/lib/python3.8/site-packages/adlfs/core.py in mkdir(self, path, delimiter, exists_ok, **kwargs)\r\n    561             else:\r\n    562                 ## everything else\r\n--> 563                 raise RuntimeError(f\"Cannot create {container_name}{delimiter}{path}.\")\r\n    564         else:\r\n    565             if container_name in self.ls(\"\") and path:\r\n\r\nRuntimeError: Cannot create dev/test7/2020/01/28.\r\n```\r\n\u00a0\r\nNext, if I try to read a dataset (keep in mind that this works with read_table and ParquetDataset):\r\n\r\n```python\n\r\nds.dataset(source=\"dev/staging/evaluations\", \r\n           format=\"parquet\", \r\n           partitioning=\"hive\",\r\n           exclude_invalid_files=False,\r\n           filesystem=fs\r\n          )\r\n```\r\n\u00a0\r\nThis doesn't seem to respect the filesystem connected to Azure Blob.\r\n```python\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-41-4de65fe95db7> in <module>\r\n----> 1 ds.dataset(source=\"dev/staging/evaluations\", \r\n      2            format=\"parquet\",\r\n      3            partitioning=\"hive\",\r\n      4            exclude_invalid_files=False,\r\n      5            filesystem=fs\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    669     # TODO(kszucs): support InMemoryDataset for a table input\r\n    670     if _is_path_like(source):\r\n--> 671         return _filesystem_dataset(source, **kwargs)\r\n    672     elif isinstance(source, (tuple, list)):\r\n    673         if all(_is_path_like(elem) for elem in source):\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    426         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\r\n    427     else:\r\n--> 428         fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n    429 \r\n    430     options = FileSystemFactoryOptions(\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in _ensure_single_source(path, filesystem)\r\n    402         paths_or_selector = [path]\r\n    403     else:\r\n--> 404         raise FileNotFoundError(path)\r\n    405 \r\n    406     return filesystem, paths_or_selector\r\n\r\nFileNotFoundError: dev/staging/evaluations\r\n```\r\n\r\nThis **does** work though when I list the blobs before passing them to ds.dataset:\r\n\r\n```python\n\r\nblobs = wasb.list_blobs(container_name=\"dev\", prefix=\"staging/evaluations\")\r\n\r\ndataset = ds.dataset(source=[\"dev/\" + blob.name for blob in blobs], \r\n                     format=\"parquet\", \r\n                     partitioning=\"hive\",\r\n                     exclude_invalid_files=False,\r\n                     filesystem=fs)\r\n```\r\n\r\nNext, if I downgrade to pyarrow 1.0.1, I am able to read datasets (but there is no write_datasets):\r\n\r\n```python\n\r\n# adal==1.2.5\r\n# adlfs==0.2.5\r\n# azure-storage-blob==2.1.0\r\n# azure-storage-common==2.1.0\r\n# fsspec==0.7.4\r\n# pandas==1.1.3\r\n# pyarrow==1.0.1\r\n\r\ndataset = ds.dataset(\"dev/staging/evaluations\", format=\"parquet\", filesystem=fs)\r\ndataset.to_table().to_pandas()\r\n```\r\n\r\n\r\n\r\nedit: \r\n\r\npyarrow 2.0.0\r\nfsspec 0.8.4\r\nadlfs v0.5.5\r\npandas 1.1.4\r\nnumpy 1.19.4\r\nazure.storage.blob 12.6.0\r\n\r\n\r\n```python\n\r\nx = adlfs.AzureBlobFileSystem(account_name=name, account_key=key)\r\ntype(x.find(\"dev/test\", detail=True))\r\nlist\r\n\r\nfs = fsspec.filesystem(protocol=\"abfs\", account_name=name, account_key=key)\r\ntype(fs.find(\"dev/test\", detail=True))\r\nlist\r\n```\r\n",
        "created_at": "2020-11-08T17:13:08.000Z",
        "updated_at": "2020-12-04T12:03:42.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-11-20T18:26:19.000Z"
    },
    "comments": [
        {
            "created_at": "2020-11-08T17:23:36.740Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17228257) by Lance Dacey (ldacey):*\n+ `[~mdurant]` \u00a0and\u00a0 `[~jorisvandenbossche]`\r\n\r\nYou guys helped me with a similar issue before. There seems to be some incompatibility with fsspec and the new pyarrow.dataset feature. If I upgrade adlfs and the azure blob SDK, then it it looks like fs.find() is returning a list instead of a dictionary like pyarrow expects. If I downgrade adlfs to use SDK v2.1, then I get the correct dictionary that pyarrow expects, but there does not seem to be a method for mkdir (which is required). Is there a way for me to get this to work? I tried tweaking the installed versions of fsspec, adlfs, and azure-storage-blob but I could not find a combination that worked."
        },
        {
            "created_at": "2020-11-09T12:40:54.961Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17228551) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` can you paste the full error tracebacks you see?  Right now it's quite hard to follow what error you exactly get and from where it is coming."
        },
        {
            "created_at": "2020-11-09T12:43:20.738Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17228552) by Joris Van den Bossche (jorisvandenbossche):*\n> it looks like fs.find() is returning a list instead of a dictionary\r\n\r\nThat would be a bug in `adlfs`, I would say, if that's the case. But based on https://github.com/dask/adlfs/blob/c95c0512fda523bf79e04bf2c667e6df40f0351d/adlfs/spec.py#L810 it seems to return a dict when `detail=True` (which is what pyarrow.dataset uses).\r\n\r\n"
        },
        {
            "created_at": "2020-11-13T10:21:40.598Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17231353) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` are you able to provide some more information?"
        },
        {
            "created_at": "2020-11-14T01:24:00.567Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17231903) by Lance Dacey (ldacey):*\nHello - let me know if my edit covers it.\r\n\r\nPreviously I did have some tests for azure-blob v12 SDK, but I cannot use that in production anyways right now (apache-airflow requirements), so I am stuck with adlfs 0.2.5 I think."
        },
        {
            "created_at": "2020-11-19T00:59:10.087Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235084) by Lance Dacey (ldacey):*\nAdded an edit with the results of pure fsspec and adlfs find() commands against a dataset I created with pyarrow. For some reason, a list is being output although I am using the latest version of each library. \r\n\r\nI checked the versions by doing a conda list, and then inside of the notebook I ran:\r\n\r\n```java\n\r\nprint('\\n'.join(f'{m.__name__} {m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\r\n```\r\n\r\n\r\nA separate attempt on my laptop locally using a fresh env file:\r\n\r\n```java\n\r\nname: airflow\r\nchannels:\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - python=3.8\r\n  - azure-storage-blob=12\r\n  - pandas=1.1\r\n  - pyarrow=2\r\n  - adlfs=0.5\r\n\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    434         selector_ignore_prefixes=selector_ignore_prefixes\r\n    435     )\r\n--> 436     factory = FileSystemDatasetFactory(fs, paths_or_selector, format, options)\r\n    437 \r\n    438     return factory.finish(schema)\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset.FileSystemDatasetFactory.__init__()\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/_fs.pyx in pyarrow._fs._cb_get_file_info_selector()\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/fs.py in get_file_info_selector(self, selector)\r\n    219             selector.base_dir, maxdepth=maxdepth, withdirs=True, detail=True\r\n    220         )\r\n--> 221         for path, info in selected_files.items():\r\n    222             infos.append(self._create_file_info(path, info))\r\n    223 \r\n\r\nAttributeError: 'list' object has no attribute 'items'\r\n```\r\n\r\n"
        },
        {
            "created_at": "2020-11-19T07:49:36.689Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235231) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` thanks for the updates!\r\n\r\nFor the issue about `find` returning a list and not a dictionary, could you open an issue on the adlfs repo? (or `[~mdurant]` could you react here if that is expected or not?)"
        },
        {
            "created_at": "2020-11-19T07:55:41.380Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235244) by Joris Van den Bossche (jorisvandenbossche):*\nFor the first error in the top post (when doing a `write_dataset`)), what happens if you run `fs.mkdir(\"dev/test7/2020/01/28\", create_parents=True)` yourself? Does this also error? (it is something I would expect to work)\r\n"
        },
        {
            "created_at": "2020-11-19T08:54:48.968Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235277) by Lance Dacey (ldacey):*\nThis works on my local conda environment (dependencies posted on my last edit, using the latest version of fsspec and adlfs). The \"28\" partition was a file instead of a folder in this case.\r\n\r\n```python\n\r\nfs.mkdir(\"dev/test7/2020/01/28\", create_parents=True)\r\n```\r\n\r\nIf I run the same code on my production environment it fails. I am using this environment with read_table and write_to_dataset often though.\r\n\r\n\r\n```python\n\r\nname: old\r\nchannels:\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - python=3.8\r\n  - azure-storage-blob=2\r\n  - pandas=1.1\r\n  - pyarrow=2\r\n  - pip=20.2\r\n  - pip:\r\n      - adlfs==0.2.5\r\n      - fsspec==0.7.4\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-b95d56a11f83> in <module>\r\n----> 1 fs.mkdir(\"dev/test8/2020/01/28\", create_parents=True)\r\n\r\n/opt/conda/lib/python3.8/site-packages/adlfs/core.py in mkdir(self, path, delimiter, exists_ok, **kwargs)\r\n    561             else:\r\n    562                 ## everything else\r\n--> 563                 raise RuntimeError(f\"Cannot create {container_name}{delimiter}{path}.\")\r\n    564         else:\r\n    565             if container_name in self.ls(\"\") and path:\r\n\r\nRuntimeError: Cannot create dev/test8/2020/01/28.\r\n\r\n```\r\n\r\nHowever, the dataset read function now works and it supports the row level filtering which is great (the dataset below is over 65 million rows and I am able to filter quickly for specific IDs across multiple files in under 2 seconds):\r\n\r\n\r\n```java\n\r\ndataset = ds.dataset(source=ds_path, \r\n                     format=\"parquet\", \r\n                     partitioning=\"hive\",\r\n                     exclude_invalid_files=False,\r\n                     filesystem=fs)\r\n\r\nlen(dataset.files)\r\n1050\r\n\r\ntable = dataset.to_table(columns=None, filter=\r\n     (ds.field(\"year\") == \"2020\") & \r\n     (ds.field(\"month\") == \"11\") & \r\n     (ds.field(\"day\") > \"10\") &\r\n     (ds.field(\"id\") == \"102648\"))\r\n```\r\n\r\nBut I cannot use write_dataset (along with the new partitioning features), unfortunately.\r\n"
        },
        {
            "created_at": "2020-11-19T09:13:26.957Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235297) by Joris Van den Bossche (jorisvandenbossche):*\n> This works on my local conda environment (dependencies posted on my last edit, using the latest version of fsspec and adlfs). The \"28\" partition was a file instead of a folder in this case.\r\n\r\nWhat do you mean exactly with \"28\" being a file? Because the command is \"mkdir\", so it should create directories, not files .. (unless this is related to Azure Blob details of its filesystem / directories that I am not familiar with)\r\n\r\n> If I run the same code on my production environment it fails. \r\n\r\nAny idea why it would fail creating directories there? The correct rights to create directories? Does the directory already exist? (just some suggestions to start looking, no experience with Azure myself)\r\n\r\nIf the `mkdir` directly is failing, this seems an issue with `adlfs`, so I would report an issue there."
        },
        {
            "created_at": "2020-11-19T09:26:46.673Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235306) by Lance Dacey (ldacey):*\n ![ss.PNG](https://issues.apache.org/jira/secure/attachment/13015606/ss.PNG) \r\n\r\n\r\nAdded a screenshot of the results of the mkdir command. I am not sure why it created a file for the 28 partition, but it looks like that is what happened.\r\n\r\nmkdir is failing on my production environment because I am stuck using old versions of adlfs and fsspec (bound to azure-blob-storage v2 SDK, unable to use v12 due to Airflow dependencies which is what runs all of my tasks using pyarrow in the first place).\r\n\r\nWhat I don't understand is why I can use write_to_dataset (legacy version) without any issues, but the write_dataset method will fail? Is the filesystem implementation different? I suppose both would be using adlfs and fsspec in my case on Azure Blob - it seems weird that one method successfully creates the directories and partitions, but the other method will fail (which is why I raised this as a pyarrow issue)."
        },
        {
            "created_at": "2020-11-19T09:52:55.152Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235318) by Joris Van den Bossche (jorisvandenbossche):*\nI suppose the writing/mkdir issue is related to the container/directory already existing. The error message you run into:\r\n\r\n```Java\n\r\n/opt/conda/lib/python3.8/site-packages/adlfs/core.py in mkdir(self, path, delimiter, exists_ok, **kwargs)\r\n    561             else:\r\n    562                 ## everything else\r\n--> 563                 raise RuntimeError(f\"Cannot create {container_name}{delimiter}{path}.\")\r\n    564         else:\r\n    565             if container_name in self.ls(\"\") and path:\r\n\r\nRuntimeError: Cannot create dev/test7/2020/01/28.\r\n```\r\n\r\nhas been changed on latest master to be \"FileExistsError(f\"Cannot overwrite existing Azure container \u2013 \\{container_name\\} already exists.\")\" (see https://github.com/dask/adlfs/commit/3266bda73e60befa848b97075701a4fe58ee34ca)\r\n\r\nI opened https://github.com/dask/adlfs/issues/134 related to this\r\n\r\n"
        },
        {
            "created_at": "2020-11-19T10:23:27.668Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235337) by Joris Van den Bossche (jorisvandenbossche):*\n> What I don't understand is why I can use write_to_dataset (legacy version) without any issues, but the write_dataset method will fail? Is the filesystem implementation different? I suppose both would be using adlfs and fsspec in my case on Azure Blob - it seems weird that one method successfully creates the directories and partitions, but the other method will fail (which is why I raised this as a pyarrow issue).\r\n\r\nBoth are indeed using the `adlfs` filesystem implementation, but the code is a bit different. Looking at the implementation of `write_to_dataset` (https://github.com/apache/arrow/blob/60ea0dcac5a8258f0e22335deb981971a92cf137/python/pyarrow/parquet.py#L1875-L1894), it seems we are explicitly checking if the directory already exists (https://github.com/apache/arrow/blob/60ea0dcac5a8258f0e22335deb981971a92cf137/python/pyarrow/parquet.py#L1760-L1765).   \r\nHowever, in the new implementation (using an fsspec wrapper, https://github.com/apache/arrow/blob/60ea0dcac5a8258f0e22335deb981971a92cf137/python/pyarrow/fs.py#L226-L228), we don't check if the directory already exists, because I was assuming that the fsspec `mkdir` works for directories that already exists. But it seems this is a part of the fsspec specification that is not clearly defined and inconsistently implemented across fsspec-based implementations. I opened https://github.com/intake/filesystem_spec/issues/479 about this.\r\n\r\nI assume on the short term, the safe thing to do in pyarrow is to also do a check if the directory already exists in the new implementation as well.\r\n"
        },
        {
            "created_at": "2020-11-19T12:20:55.788Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235388) by Lance Dacey (ldacey):*\nLatest adlfs (0.5.5):\r\n\r\n\u00a0\r\n\r\nThis really creates the test.parquet file as well, not just the directory:\r\n```java\n\r\nfs.mkdir(\"dev/test99999999999/2020/01/28/test.parquet\", create_parents=True)\r\n```\r\nAnd if I try to run the same line again it it fails because the partition exists:\r\n```python\n\r\n---------------------------------------------------------------------------\r\nStorageErrorException: Operation returned an invalid status 'The specified blob already exists.'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nResourceExistsError                       Traceback (most recent call last)\r\n/c/airflow/test.py in <module>\r\n----> 6 fs.mkdir(\"dev/test99999999999/2020/01/28/test.parquet\", create_parents=True)\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/adlfs/spec.py in mkdir(self, path, delimiter, exist_ok, **kwargs)\r\n    880 \r\n    881     def mkdir(self, path, delimiter=\"/\", exist_ok=False, **kwargs):\r\n--> 882         maybe_sync(self._mkdir, self, path, delimiter, exist_ok)\r\n    883 \r\n    884     async def _mkdir(self, path, delimiter=\"/\", exist_ok=False, **kwargs):\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/fsspec/asyn.py in maybe_sync(func, self, *args, **kwargs)\r\n     98         if inspect.iscoroutinefunction(func):\r\n     99             # run the awaitable on the loop\r\n--> 100             return sync(loop, func, *args, **kwargs)\r\n    101         else:\r\n    102             # just call the blocking function\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/fsspec/asyn.py in sync(loop, func, callback_timeout, *args, **kwargs)\r\n     69     if error[0]:\r\n     70         typ, exc, tb = error[0]\r\n---> 71         raise exc.with_traceback(tb)\r\n     72     else:\r\n     73         return result[0]\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/fsspec/asyn.py in f()\r\n     53             if callback_timeout is not None:\r\n     54                 future = asyncio.wait_for(future, callback_timeout)\r\n---> 55             result[0] = await future\r\n     56         except Exception:\r\n     57             error[0] = sys.exc_info()\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/adlfs/spec.py in _mkdir(self, path, delimiter, exist_ok, **kwargs)\r\n    918                     container=container_name\r\n    919                 )\r\n--> 920                 await container_client.upload_blob(name=path, data=\"\")\r\n    921             else:\r\n    922                 ## everything else\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/core/tracing/decorator_async.py in wrapper_use_tracer(*args, **kwargs)\r\n     72             span_impl_type = settings.tracing_implementation()\r\n     73             if span_impl_type is None:\r\n---> 74                 return await func(*args, **kwargs)\r\n     75 \r\n     76             # Merge span is parameter is set, but only if no explicit parent are passed\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/storage/blob/aio/_container_client_async.py in upload_blob(self, name, data, blob_type, length, metadata, **kwargs)\r\n    715         timeout = kwargs.pop('timeout', None)\r\n    716         encoding = kwargs.pop('encoding', 'UTF-8')\r\n--> 717         await blob.upload_blob(\r\n    718             data,\r\n    719             blob_type=blob_type,\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/core/tracing/decorator_async.py in wrapper_use_tracer(*args, **kwargs)\r\n     72             span_impl_type = settings.tracing_implementation()\r\n     73             if span_impl_type is None:\r\n---> 74                 return await func(*args, **kwargs)\r\n     75 \r\n     76             # Merge span is parameter is set, but only if no explicit parent are passed\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/storage/blob/aio/_blob_client_async.py in upload_blob(self, data, blob_type, length, metadata, **kwargs)\r\n    267             **kwargs)\r\n    268         if blob_type == BlobType.BlockBlob:\r\n--> 269             return await upload_block_blob(**options)\r\n    270         if blob_type == BlobType.PageBlob:\r\n    271             return await upload_page_blob(**options)\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/storage/blob/aio/_upload_helpers.py in upload_block_blob(client, data, stream, length, overwrite, headers, validate_content, max_concurrency, blob_settings, encryption_options, **kwargs)\r\n    131     except StorageErrorException as error:\r\n    132         try:\r\n--> 133             process_storage_error(error)\r\n    134         except ResourceModifiedError as mod_error:\r\n    135             if not overwrite:\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/azure/storage/blob/_shared/response_handlers.py in process_storage_error(storage_error)\r\n    145     error.error_code = error_code\r\n    146     error.additional_info = additional_data\r\n--> 147     raise error\r\n    148 \r\n    149 \r\n\r\nResourceExistsError: The specified blob already exists.\r\nRequestId:85acda2e-401e-0080-5166-be4d32000000\r\nTime:2020-11-19T11:23:28.7193393Z\r\nErrorCode:BlobAlreadyExists\r\nError:None\r\n```\r\nIf I switch to adlfs 0.2.5 (old version which works for ds.dataset()), there error is different when I try to create a directory which already exists but I also cannot create any directory at all for some reason. I also tried to create an entirely new directory which definitely does not exist and ran into an error:\r\n```python\n\r\nfs.mkdir(\"dev/testab1234123/2020/01/28/new.parquet\", create_parents=True)\r\nRuntimeError: Cannot create dev/testab1234123/2020/01/28/new.parquet.\r\n\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n----> 6 fs.mkdir(\"dev/test99999999999/2020/01/28/test.parquet\", create_parents=True)\r\n\r\n~/miniconda3/envs/old/lib/python3.8/site-packages/adlfs/core.py in mkdir(self, path, delimiter, exists_ok, **kwargs)\r\n    561             else:\r\n    562                 ## everything else\r\n--> 563                 raise RuntimeError(f\"Cannot create {container_name}{delimiter}{path}.\")\r\n    564         else:\r\n    565             if container_name in self.ls(\"\") and path:\r\n\r\nRuntimeError: Cannot create dev/test99999999999/2020/01/28/test.parquet.\r\n```\r\nBut I **am** able to read a dataset which I could **not** do with adlfs 0.5.5 (I get that error about a list of files instead of a dictionary using fs.find() with the latest version).\r\n\r\nSo this is bizarre. I can only read data (with ds.dataset()) with an old version of adlfs, and I can only write data with the newest version.\r\n\r\nEven pq.read_table() will not work for me using the latest version of adlfs (0.5.5):\r\n```python\n\r\n----> 7 table = pq.read_table(source=\"dev/testing10/evaluations\", columns=None, filters=[('year', '==', '2020')], filesystem=fs)\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/parquet.py in read_table(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes)\r\n   1605             )\r\n   1606         try:\r\n-> 1607             dataset = _ParquetDatasetV2(\r\n   1608                 source,\r\n   1609                 filesystem=filesystem,\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/parquet.py in __init__(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, **kwargs)\r\n   1465                 infer_dictionary=True)\r\n   1466 \r\n-> 1467         self._dataset = ds.dataset(path_or_paths, filesystem=filesystem,\r\n   1468                                    format=parquet_format,\r\n   1469                                    partitioning=partitioning,\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    669     # TODO(kszucs): support InMemoryDataset for a table input\r\n    670     if _is_path_like(source):\r\n--> 671         return _filesystem_dataset(source, **kwargs)\r\n    672     elif isinstance(source, (tuple, list)):\r\n    673         if all(_is_path_like(elem) for elem in source):\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    426         fs, paths_or_selector = _ensure_multiple_sources(source, filesystem)\r\n    427     else:\r\n--> 428         fs, paths_or_selector = _ensure_single_source(source, filesystem)\r\n    429 \r\n    430     options = FileSystemFactoryOptions(\r\n\r\n~/miniconda3/envs/airflow/lib/python3.8/site-packages/pyarrow/dataset.py in _ensure_single_source(path, filesystem)\r\n    402         paths_or_selector = [path]\r\n    403     else:\r\n--> 404         raise FileNotFoundError(path)\r\n    405 \r\n    406     return filesystem, paths_or_selector\r\n\r\nFileNotFoundError: dev/testing10/evaluations\r\n```\r\nIf I turn on use_legacy_dataset=True, then it works though and I am able to use the write_dataset feature. So this is definitely some interaction between the new dataset module and adlfs.\r\n```python\n\r\ntable = pq.read_table(source=\"dev/testing10/evaluations\", columns=None, filters=[('year', '==', '2020')], filesystem=fs, use_legacy_dataset=True)\r\nds.write_dataset(table, \r\n                 base_dir=\"dev/adlfs-0.5.5\", \r\n                 format=\"parquet\", \r\n                 partitioning=ds.DirectoryPartitioning(pa.schema([(\"year\", pa.int64()), (\"month\", pa.string()), (\"day\", pa.string())])),\r\n                 schema=table.schema, \r\n                 filesystem=fs)\r\n```\r\nThis does seem to create empty files for each partition as well, which is strange, and the files are named with a part- prefix now instead of a UUID.\r\n\r\ndev/adlfs-0.5.5/2020/11/15/part-2.parquet\r\n dev/adlfs-0.5.5/2020/11/16/part-3.parquet\r\n\r\n![ss2.PNG](https://issues.apache.org/jira/secure/attachment/13015624/ss2.PNG)"
        },
        {
            "created_at": "2020-11-19T12:55:35.542Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235434) by Joris Van den Bossche (jorisvandenbossche):*\n> But I am able to read a dataset which I could not do with adlfs 0.5.5 (I get that error about a list of files instead of a dictionary using fs.find() with the latest version).\r\n\r\n`[~ldacey]` can you open an issue about that at `adlfs`? `find` is supposed to return a dictionary with `detail=True`\r\n\r\n(although I am wondering if you are not accidentally using an older version, as from the source code it seems to be returning a dictionary ..)"
        },
        {
            "created_at": "2020-11-19T13:10:22.867Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235450) by Joris Van den Bossche (jorisvandenbossche):*\n> This really creates the test.parquet file as well, not just the directory:\r\n\r\nMaybe the reason is because Azure Blob storage file system doesn't have the concept of (empty) directories, so `adlfs` creates an empty file to keep track of it ( `[~mdurant]` is that correct?)\r\n\r\n> Even pq.read_table() will not work for me using the latest version of adlfs (0.5.5)\r\n\r\nBased on the error, it doesn't seem to be finding that path. \r\nCan you try to run `fs.info(\"dev/testing10/evaluations\")` and show the output?\r\n"
        },
        {
            "created_at": "2020-11-19T13:47:23.668Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235477) by Lance Dacey (ldacey):*\nYeah, I can open an issue there. https://github.com/dask/adlfs/issues/135\r\n\r\nI think that this might be the major issue I am facing with v12 Azure Blob SDK. I cannot read a dataset because I get a list of files returned instead of a dictionary (but I am able to write a dataset).\r\n\r\nI think I might have to open some fsspec issues as well because mkdir is creating those empty files instead of a directory which doesn't seem right. Also ran into an issue with read_table(use_legacy_dataset=True) where data was trying to be read from the wrong partition with a similar name \"domain=tnt\" and \"domain=tntplus\". So it looks like perhaps only the prefix was being used to list the files.\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nedit:\r\n \u00a0\r\n```java\n\r\nfs.info(\"dev/testing10/evaluations\")\u00a0  \u00a0 \u00a0  \u00a0 \u00a0  \u00a0 \u00a0 \u00a0 \u00a0           \r\n{'name': 'dev/testing10/evaluations/', 'size': 0, 'type': 'directory'}   \n```\r\n\u00a0"
        },
        {
            "created_at": "2020-11-19T13:48:38.946Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235478) by Martin Durant (mdurant):*\nI don't actually know. That kind of thing happens in s3/gcs for some frameworks, but not by the fsspec-libraries, where mkdir is a no-op. adlfs may do differently."
        },
        {
            "created_at": "2020-11-19T14:22:33.626Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17235506) by Joris Van den Bossche (jorisvandenbossche):*\n> ```Java\n> >>>  fs.info(\"dev/testing10/evaluations\")                               \n> {'name': 'dev/testing10/evaluations/', 'size': 0, 'type': 'directory'}   \n> ```\r\n\r\n`[~ldacey]` that looks correct. But so in the same environment as where you get the above result, `pq.read_table(\"dev/testing10/evaluations\")` is failing with \"FileNotFoundError: dev/testing10/evaluations\"?"
        },
        {
            "created_at": "2020-11-20T18:23:53.274Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17236357) by Lance Dacey (ldacey):*\nThanks for your help. By adding \\*\\*kwargs to the adlfs find() return, I was able to get ds.dataset features to work (read and write) with the latest version of adlfs. I am sure the library will be updated soon:\u00a0https://github.com/dask/adlfs/issues/135.\r\n\r\n\u00a0\r\n\r\nSince I am stuck with azure-storage-blob SDK v2 in production, I have been using an old version of adlfs (0.2.5). I am unable to use write_dataset, but I am able to use write_to_dataset() with the legacy system. This error leads back to adlfs core.py in the mkdir function.\r\n\r\nI think I will close this issue now since write_to_dataset() works for my needs right now and it supports the _partition_filename_cb_ which I find useful. I will just wait until I can safely upgrade to the latest version of adlfs where I know it will work fine.\r\n\r\n\u00a0\r\n```java\n\r\nds.write_dataset(data=table, \r\n                 base_dir=\"dev/test-write\", \r\n                 format=\"parquet\",\r\n                 partitioning=ds.DirectoryPartitioning(pyarrow.schema([(\"report_date\", pyarrow.date32())])),\r\n                 filesystem=fs)\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-15-260d7b0098e1> in <module>\r\n----> 1 ds.write_dataset(data=table, \r\n      2                  base_dir=\"dev/test-write\",\r\n      3                  format=\"parquet\",\r\n      4                  partitioning=ds.DirectoryPartitioning(pyarrow.schema([(\"report_date\", pyarrow.date32())])),\r\n      5                  filesystem=fs)\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in write_dataset(data, base_dir, basename_template, format, partitioning, schema, filesystem, file_options, use_threads)\r\n    771     filesystem, _ = _ensure_fs(filesystem)\r\n    772 \r\n--> 773     _filesystemdataset_write(\r\n    774         data, base_dir, basename_template, schema,\r\n    775         filesystem, partitioning, file_options, use_threads,\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset._filesystemdataset_write()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_fs.pyx in pyarrow._fs._cb_create_dir()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/fs.py in create_dir(self, path, recursive)\r\n    226     def create_dir(self, path, recursive):\r\n    227         # mkdir also raises FileNotFoundError when base directory is not found\r\n--> 228         self.fs.mkdir(path, create_parents=recursive)\r\n    229 \r\n    230     def delete_dir(self, path):\r\n\r\n/opt/conda/lib/python3.8/site-packages/adlfs/core.py in mkdir(self, path, delimiter, exists_ok, **kwargs)\r\n    561             else:\r\n    562                 ## everything else\r\n--> 563                 raise RuntimeError(f\"Cannot create {container_name}{delimiter}{path}.\")\r\n    564         else:\r\n    565             if container_name in self.ls(\"\") and path:\r\n\r\nRuntimeError: Cannot create dev/test-write/2018-03-01.\r\n```"
        },
        {
            "created_at": "2020-11-20T18:26:19.069Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17236361) by Lance Dacey (ldacey):*\nMy issue is caused by another library (adlfs). Once this is fixed, this issue will not be relevant.\r\n\r\nhttps://github.com/dask/adlfs/issues/135"
        },
        {
            "created_at": "2020-11-20T18:31:32.607Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17236364) by Joris Van den Bossche (jorisvandenbossche):*\n> write_to_dataset() works for my needs right now and it supports the partition_filename_cb which I find useful.\r\n\r\nSome feedback on how you are using this feature, and if the `ds.write_dataset` `basename_template` keyword is a sufficient replacement, would be welcome. As it is one of the things for which we currently don't have a 1:1 replacement."
        },
        {
            "created_at": "2020-11-21T13:29:58.821Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17236667) by Gregory Hayes (hayesgb):*\nThe update to adlfs for public find has been merged into master as #[136](https://github.com/dask/adlfs/pull/136)."
        },
        {
            "created_at": "2020-11-21T16:13:39.534Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17236702) by Lance Dacey (ldacey):*\nRegarding partition_filename_cb, some common ones I am using are to create a full date name from the partition folders.\r\n```java\n\r\nyear=2020/month=8/day=4\r\npartition_filename_cb=lambda x: \"-\".join(str(y).zfill(2) for y in x) + \".parquet\"\r\n2020-08-04.parquet\n```\r\nI am doing this to address a \"many small files\" situation in a few scenarios. Perhaps there is a better way to go about it though where this would not be necessary.\r\n\r\n\u00a0\r\n\r\nScenario 1):\r\n \\* I use turbodbc to query 6 different SQL servers every 30 minutes (48 schedules per date \\* 6) directly into pyarrow tables which I then write to a partitioned dataset.\r\n \\* This creates a lot of small files which I then filter for and write to a separate dataset with the partition_filename_cb to consolidate the data into a single daily file\r\n\r\nScenario 2):\r\n \\* I query for data every hour from some REST APIs (Zendesk and ServiceNow) for any tickets which have changed since my last query (based on the latest updated_at timestamp)\r\n \\* I partition this data based on the created_at date. So we have a lot of small files due to the frequency of downloads, and a single download might have tickets which were created_at in the past. At least 24 files \\* the amount of unique dates which were updated.\r\n \\* So again, I filter for any created_at partition which was changed in the last hour and then rewrite a \"final\" consolidated version of the data in a separate dataset using the partition_filename_cb which is then used for downstream tasks and transformation.\r\n \\* Ultimately, I need to ensure that our visualizations/reports only display the latest version of each ticket even if it was updated a dozen times, so this step generally includes me sorting the data and dropping duplicates on some unique constraints\r\n\r\n\u00a0\r\n\r\nBoth scenarios have tiny files each download interval or based on how I partition the data, but are pretty large overall (scenario 1 is over 500 million rows, and scenario 2 is over 70 million rows from March of this year). Maybe it is not required to use the partition_filename_cb though, it just seemed faster and more organized (under 300ms to read a single file compared to over 1 minute to filter for a day with 96 UUID filenames)\r\n\r\n\u00a0\r\n\r\nAny best practices here to avoid the need to use the partition_filename_cb function?"
        },
        {
            "created_at": "2020-11-23T09:58:58.679Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17237262) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` thanks for the feedback. I think it is certainly reasonable to gather the many small files into a single larger file at certain timestamps. \r\nNow, to be clear, there is still an alternative to `partition_filename_cb` in the new `write_dataset` function: the `basename_template` keyword (see https://github.com/apache/arrow/blob/6cea669a0a7fb836a555f3d87177b2517543ddb5/python/pyarrow/dataset.py#L713-L717 for the docstring). \r\n\r\nSo the new keyword is no longer a callback _function_, but should still allow to specify the date as the base name for the written file, I think (so it's more the question for you if this new keyword also allows you to do what you want to achieve). "
        },
        {
            "created_at": "2020-12-01T20:07:00.115Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17241833) by Lance Dacey (ldacey):*\nThanks - since the \\{i} increments each time a new file is written, I am not sure if this can work for my use case unless I am designing this incorrectly.\r\n\r\nI am using the partition_filename_cb similar to how I would create a materialized view in a database to ensure that there is only one row per unique ID based on the latest update timestamp. I can then connect this parquet dataset to our visualization tool, or I can export it to CSV format and email it to another team, etc.\r\n\r\n\u00a0\r\n```java\n\r\n#the historical dataset includes all rows, the number of files will depend on the frequency of scheduled downloads. it is possible to have multiple rows per unique ID\r\nhistorical_dataset = [\r\n 'dev/test/report_date=2018-01-01/part-0.parquet',\r\n 'dev/test/report_date=2018-01-01/part-1.parquet',\r\n 'dev/test/report_date=2018-01-01/part-2.parquet',\r\n 'dev/test/report_date=2018-01-01/part-3.parquet',\r\n 'dev/test/report_date=2018-01-01/part-4.parquet',\r\n 'dev/test/report_date=2018-01-01/part-5.parquet',\r\n]\r\n#read the historical dataset and filter for the partition. in this case, report_date = 2018-01-01, so all data from that date is read into a table\r\n#convert to pandas dataframe, sort based on \"id\" and \"updated_at\" fields\r\n#drop duplicates based on \"id\" field, retaining the latest version\r\n#write to a new dataset which is just the latest version of each \"id\". The 6 parts are now in a single file which will be continuously overwritten if any new data is added to the historical_dataset. Our visualization tool connects to these finalized files, and sometimes I send the data through email for reporting purposes\r\nlatest_dataset = [\r\n 'dev/test/report_date=2018-01-01/2018-01-01.parquet',\r\n]\r\n```\r\n\u00a0\r\n\r\nPerhaps there is a better way to go about this? With a database, I would just create view which selects distinct on the ID column based on the latest update timestamp. This seems to be a common use case, so I am not sure how people would go about it with Parquet.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-12-02T15:12:02.942Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17242425) by Lance Dacey (ldacey):*\nFYI, it seems like the \"part-\\{i}\" basename_template does not work well if schedules run in parallel. For example, I ran 30 schedules (in parallel) which read separate JSON files and output the data into the same partitioned parquet dataset. Only part-0.parquet was being overwritten each time. For now, I imported the guid() function from pyarrow.utils to ensure that all files are written."
        },
        {
            "created_at": "2020-12-04T10:16:58.735Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17243917) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` thanks for the follow-up. \r\nWhat I don't fully understand in your described workflow: when combining all small files into a single file eg for a single day (eg \"2018-01-01.parquet\"), then you are writing a _single_ parquet file, right? So why do you need `pq.write_to_dateset` or `ds.write_dataset` with their filename callback / template functionality? You could also use `pq.write_table` and pass it an exact file name as you want it (without any callback, as I would think you can create the resulting filename in the code before calling write_table?).  \r\n(I am probably missing something, though)\r\n\r\n>  FYI, it seems like the \"part-{i}\" basename_template does not work well if schedules run in parallel. ... For now, I imported the guid() function from pyarrow.utils to ensure that all files are written.\r\n\r\nI recently opened ARROW-10695 for the idea to allow a `\"\\{uuid\\}\"` specifier in the template. That should help for this?"
        },
        {
            "created_at": "2020-12-04T12:03:42.782Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10517?focusedCommentId=17243963) by Lance Dacey (ldacey):*\nYes, I think the uuid specifier would work fine for my purposes. Generally, I have had pyarrow create the resulting filenames with the partition_filename_cb function, but you are right - I could probably generate the filenames directly since I am dictating which filters to use in the first place (and each filter becomes a file).\r\n\u00a0\r\n```python\n\r\nd1 = {\r\n    \"id\": [1, 2, 3, 4, 5],\r\n    \"created_at\": [\r\n        datetime.date(2020, 5, 7),\r\n        datetime.date(2020, 6, 19),\r\n        datetime.date(2020, 9, 14),\r\n        datetime.date(2020, 11, 22),\r\n        datetime.date(2020, 12, 2),\r\n    ],\r\n    \"updated_at\": [\r\n        datetime.date(2020, 12, 2),\r\n        datetime.date(2020, 12, 2),\r\n        datetime.date(2020, 12, 2),\r\n        datetime.date(2020, 12, 2),\r\n        datetime.date(2020, 12, 2),\r\n    ],\r\n}\r\ndf = pd.DataFrame(data=d1)\r\ntable = pa.Table.from_pandas(df)\r\n\r\n#historical dataset which has all history of each ID each time it gets updated\r\n#each created_at partition would have a sub-partition for updated_at since historical data can change - this can generate many small files depending on how often my schedule runs to download data\r\n#I use pa.string() as the partition data type here because I have had issues using pa.date32(), sometimes I will get an error that we cannot convert a string to date32() but using a date works perfectly fine\r\nds.write_dataset(\r\n    data=table,\r\n    base_dir=output_path,\r\n    format=\"parquet\",\r\n    partitioning=ds.partitioning(pa.schema([(\"created_at\", pa.string()), (\"updated_at\", pa.string())]), flavor=\"hive\"),\r\n    schema=table.schema,\r\n    filesystem=fs,\r\n)\r\n\r\n#the next task would read the dataset and filter for the created_at partition (ignoring the updated_at partition)\r\ndataset = ds.dataset(\r\n    source=output_path, \r\n    format=\"parquet\",\r\n    partitioning=\"hive\",\r\n    filesystem=fs,\r\n)\r\n\r\n#I save the unique filters (each created_at value) externally and build the dataset filter expression\r\nfilter_expression = pq._filters_to_expression(filters=[[('created_at', '==', '2020-05-07')], \r\n[('created_at', '==', '2020-06-19')], [('created_at', '==', '2020-09-14')], [('created_at', '==', '2020-11-22')], [('created_at', '==', '2020-12-02')]])\r\n\r\ntable = dataset.to_table(filter=filter_expression)\r\n\r\n#Turn the table into a pandas dataframe to remove duplicates and retain the latest row for each ID\r\ndf = table.to_pandas(self_destruct=True).sort_values([\"id\", \"updated_at\"], ascending=True).drop_duplicates([\"id\"], keep=\"last\")\r\ntable = pa.Table.from_pandas(df)\r\n\r\n#this writes the final dataset. \r\n#There would be one file per created_at partition. \"container/created_at=2020-05-07/2020-05-07.parquet\"\r\n#our visualization tool connects directly to these parquet files so we can report on the latest status of each ticket (not much attention is paid to the historical changes)\r\npq.write_to_dataset(\r\n    table=table,\r\n    root_path=output_path,\r\n    partition_cols=[\"created_at\"],\r\n    partition_filename_cb=lambda x: str(x[-1]) + '.parquet',,\r\n    filesystem=fs,\r\n)\r\n```\r\n\r\n\\*\\*\\*Note regarding the filters I use. I am using code similar to something I found in the pyarrow.write_to_dataset function (pasted below) to generate these filters. I could probably generate filenames instead though and use write_table like you mentioned.\r\n\r\n```python\n\r\n        for keys, subgroup in data_df.groupby(partition_keys):\r\n            if not isinstance(keys, tuple):\r\n                keys = (keys,)\r\n            subdir = '/'.join(\r\n                ['{colname}={value}'.format(colname=name, value=val)\r\n                 for name, val in zip(partition_cols, keys)])\r\n```\r\n"
        }
    ]
}