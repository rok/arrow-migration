{
    "issue": {
        "title": "[Python] Incorrect Timestamp Unit in Embedded Arrow Schema Within Parquet",
        "body": "***Note**: This issue was originally created as [ARROW-16184](https://issues.apache.org/jira/browse/ARROW-16184). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nAs pointed out in https://issues.apache.org/jira/browse/ARROW-2429 the following code results in the schema changing when reading/writing a parquet file.\r\n```python\n\r\n#!/usr/bin/env python\r\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pandas as pd\r\n\r\n# create DataFrame with a datetime column\r\ndf = pd.DataFrame({'created': ['2018-04-04T10:14:14Z']})\r\ndf['created'] = pd.to_datetime(df['created'])\r\n\r\n# create Arrow table from DataFrame\r\ntable = pa.Table.from_pandas(df, preserve_index=False)\r\n\r\n# write the table as a parquet file, then read it back again\r\npq.write_table(table, 'foo.parquet')\r\ntable2 = pq.read_table('foo.parquet')\r\n\r\nprint(table.schema[0])  # pyarrow.Field<created: timestamp[ns]> (nanosecond units)\r\nprint(table2.schema[0]) # pyarrow.Field<created: timestamp[us]> (microsecond units)\r\n```\r\nThis was closed as a limitation of the parquet 1.x format for representing nanosecond timestamps. This is fine, however, the arrow schema embedded within the parquet metadata still lists the data as being a nanosecond array. This causes issues depending on which schema the reader opts to \"trust\".\r\n\r\nThis was discovered as part of the investigation into a bug report on the arrow-rs parquet implementation - <https://github.com/apache/arrow-rs/issues/1459>\r\n\r\nSpecifically the metadata written is\r\n```java\n\r\nSchema {\r\n\u00a0 \u00a0 endianness: Little,\r\n\u00a0 \u00a0 fields: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Field {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"created\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nullable: true,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type_type: Timestamp,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type_: Timestamp {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unit: NANOSECOND,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timezone: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"UTC\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dictionary: None,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 children: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 custom_metadata: None,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\u00a0 \u00a0 ),\r\n\u00a0 \u00a0 custom_metadata: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 [\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 KeyValue {\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"pandas\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: Some(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"{\\\"index_columns\\\": [], \\\"column_indexes\\\": [], \\\"columns\\\": [{\\\"name\\\": \\\"created\\\", \\\"field_name\\\": \\\"created\\\", \\\"pandas_type\\\": \\\"datetimetz\\\", \\\"numpy_type\\\": \\\"datetime64[ns]\\\", \\\"metadata\\\": {\\\"timezone\\\": \\\"UTC\\\"}}], \\\"creator\\\": {\\\"library\\\": \\\"pyarrow\\\", \\\"version\\\": \\\"6.0.1\\\"}, \\\"pandas_version\\\": \\\"1.4.0\\\"}\",\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\r\n\u00a0 \u00a0 \u00a0 \u00a0 ],\r\n\u00a0 \u00a0 ),\r\n\u00a0 \u00a0 features: None,\r\n} \n```",
        "created_at": "2022-04-13T10:20:44.000Z",
        "updated_at": "2022-08-27T14:41:40.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-06-01T10:16:36.000Z"
    },
    "comments": [
        {
            "created_at": "2022-04-15T13:33:24.266Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17522831) by Joris Van den Bossche (jorisvandenbossche):*\n> however, the arrow schema embedded within the parquet metadata still lists the data as being a nanosecond array. This causes issues depending on which schema the reader opts to \"trust\".\r\n\r\n`[~tustvold]` I think you need to see the stored Arrow schema as \"the schema of the original Arrow data\" that was used to write the Parquet file. In that sense, the schema _is_ correct (the original Arrow data _did_ have nanosecond resolution). \r\nSo that also means that this stored Arrow schema doesn't necessarily say anything about the data that is actually stored in the Parquet file. This is one example where there is a difference, but there are also other examples (eg extension types, duration, fixed sized list, ... are all types that are not directly supported in parquet, and thus would give a difference between the Parquet schema and the stored Arrow schema)\r\n\r\n\r\nArrow uses this stored Arrow schema to restore some types (that Parquet doesn't natively support) to make the roundtrip more faithful. But in case of timestamp data, we only restore the timezone, and not the actual unit at the moment (so that might be the reason it seems we \"ignore\" the embedded Arrow schema, but so that is not the case)."
        },
        {
            "created_at": "2022-04-15T13:37:45.395Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17522833) by Joris Van den Bossche (jorisvandenbossche):*\nSide note: if you want to store the data actually as nanosecond in Parquet, that is possible by specifying `version=\"2.6\"` in `pq.write_table`"
        },
        {
            "created_at": "2022-04-15T13:40:43.199Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17522834) by Raphael Taylor-Davies (tustvold):*\nDo you know if this convention is documented anywhere, this would be a breaking change to the arrow-rs implementation and so it would be good to have something authoritative to reference as justification. That being said it seems odd to me that the less expressive schema would be treated as the authoritative one - if you can't trust the arrow schema, what is the point in embedding it?"
        },
        {
            "created_at": "2022-04-15T13:48:30.687Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17522835) by Joris Van den Bossche (jorisvandenbossche):*\nThere is a \"Roundtripping Arrow types\" section in the Arrow parquet docs: https://arrow.apache.org/docs/dev/cpp/parquet.html#roundtripping-arrow-types  \r\n(we should probably update that with an example for timestamp as well, instead of only the LargeList example, to make this clearer)\r\n\r\n> That being said it seems odd to me that the less expressive schema would be treated as the authoritative one - if you can't trust the arrow schema, what is the point in embedding it?\r\n\r\nNote that it _can_ be trusted, but for what it is meant for: to be a description of the original Arrow schema, and _not_ for a description of what is in the Parquet file / for the Parquet schema. \r\nWhen reading the actual parquet data and restoring information from the Arrow schema, you still need to do a proper conversion of the Parquet data to a potentially different Arrow type. It is up to the reader implementation to what extent you want to restore information of the stored Arrow schema (and to do this correctly).\r\n\r\n"
        },
        {
            "created_at": "2022-06-01T10:14:01.467Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17544792) by Joris Van den Bossche (jorisvandenbossche):*\n`[~tustvold]` given that this has been resolved in arrow-rs, this can be closed as well? (unless we want to improve some docs based on this discussion)"
        },
        {
            "created_at": "2022-06-01T10:16:36.159Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17544793) by Raphael Taylor-Davies (tustvold):*\nThis was a misunderstanding of the relationship between the embedded arrow schema and the parquet schema"
        },
        {
            "created_at": "2022-08-27T14:41:40.544Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16184?focusedCommentId=17585778) by @toddfarmer:*\nTransitioning issue from Resolved to Closed to based on resolution field value."
        }
    ]
}