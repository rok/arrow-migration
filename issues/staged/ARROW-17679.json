{
    "issue": {
        "title": "slow performance when reading data from GCP",
        "body": "***Note**: This issue was originally created as [ARROW-17679](https://issues.apache.org/jira/browse/ARROW-17679). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI am using pyarrow and duckdb to query some parquet files in GCP, thanks for making\u00a0 the experience so smooth, but I have an issue with the performance, see code used.\r\nimport\u00a0pyarrow.dataset\u00a0as\u00a0ds\r\nimport\u00a0duckdb\r\nimport\u00a0json\r\nlineitem = ds.dataset(\"gs://xxxxx/lineitem\")\r\nlineitem_partition = ds.dataset(\"gs://xxxx/yyy\",format=\"parquet\", partitioning=\"hive\")\r\nlineitem_180 = ds.dataset(\"gs://xxxxx/lineitem_180\",format=\"parquet\", partitioning=\"hive\")\r\ncon\u00a0=\u00a0duckdb.connect()\r\ncon.register(\"lineitem\",\u00a0lineitem)\r\ncon.register(\"lineitem_partition\",\u00a0lineitem_partition)\r\ncon.register(\"lineitem_180\",\u00a0lineitem_180)\r\ndef\u00a0Query(request):\r\n\u00a0\u00a0\u00a0\u00a0SQL\u00a0=\u00a0request.get_json().get('name')\r\n\u00a0\u00a0\u00a0\u00a0df\u00a0=\u00a0con.execute(SQL).df()\r\n\u00a0\u00a0\u00a0\u00a0return\u00a0json.dumps(df.to_json(orient=\"records\")),\u00a0200,\u00a0\\{'Content-Type':\u00a0'application/json'}\r\n\u00a0\r\nthe issue is I am getting slow some extremely slow throughput performance, around 30 MBper second, the same files using local ssd laptop is extremely fast.\r\nI am not sure what's the issue, I tried using pyarrow compute Query and it is the same performance\u00a0",
        "created_at": "2022-09-12T05:13:36.000Z",
        "updated_at": "2022-09-12T05:14:25.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Parquet",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}