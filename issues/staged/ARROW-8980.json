{
    "issue": {
        "title": "[Python] Metadata grows exponentially when using schema from disk",
        "body": "***Note**: This issue was originally created as [ARROW-8980](https://issues.apache.org/jira/browse/ARROW-8980). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen overwriting parquet files we first read the schema that is already on disk this is mainly to deal with some type harmonizing between pyarrow and pandas (that I wont go into).\r\n\r\nRegardless here is a simple example (below) with no weirdness. If I continously re-write the same file by first fetching the schema from disk, creating a writer with that schema and then writing same dataframe the file size keeps growing even though the amount of rows has not changed.\r\n\r\nNote: My solution was to remove `b'ARROW:schema'` data from the `schema.metadata.` this seems to stop the file size growing. So I wonder if the writer keeps appending to it or something? TBH I'm not entirely sure but I have a hunch that the ARROW:schema is just the metadata serialised or something.\r\n\r\nI should also note that once the metadata gets to big this leads to a buffer overflow in another part of the code 'thrift' which was referenced here:\u00a0https://issues.apache.org/jira/browse/PARQUET-1345\r\n```java\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow as pa\r\nimport pandas as pd\r\nimport pathlib\r\nimport sys\r\ndef main():\r\n    print(f\"python: {sys.version}\")\r\n    print(f\"pa version: {pa.__version__}\")\r\n    print(f\"pd version: {pd.__version__}\")    fname = \"test.pq\"\r\n    path = pathlib.Path(fname)    df = pd.DataFrame({\"A\": [0] * 100000})\r\n    df.to_parquet(fname)    print(f\"Wrote test frame to {fname}\")\r\n    print(f\"Size of {fname}: {path.stat().st_size}\")    for _ in range(5):\r\n        file = pq.ParquetFile(fname)\r\n        tmp_df = file.read().to_pandas()\r\n        print(f\"Number of rows on disk: {tmp_df.shape}\")\r\n        print(\"Reading schema from disk\")\r\n        schema = file.schema.to_arrow_schema()\r\n        print(\"Creating new writer\")\r\n        writer = pq.ParquetWriter(fname, schema=schema)\r\n        print(\"Re-writing the dataframe\")\r\n        writer.write_table(pa.Table.from_pandas(df))\r\n        writer.close()\r\n        print(f\"Size of {fname}: {path.stat().st_size}\")\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n```java\n\r\n(sdm) \u279c ~ python growing_metadata.py\r\npython: 3.7.3 | packaged by conda-forge | (default, Dec 6 2019, 08:36:57)\r\n[Clang 9.0.0 (tags/RELEASE_900/final)]\r\npa version: 0.16.0\r\npd version: 0.25.2\r\nWrote test frame to test.pq\r\nSize of test.pq: 1643\r\nNumber of rows on disk: (100000, 1)\r\nReading schema from disk\r\nCreating new writer\r\nRe-writing the dataframe\r\nSize of test.pq: 3637\r\nNumber of rows on disk: (100000, 1)\r\nReading schema from disk\r\nCreating new writer\r\nRe-writing the dataframe\r\nSize of test.pq: 8327\r\nNumber of rows on disk: (100000, 1)\r\nReading schema from disk\r\nCreating new writer\r\nRe-writing the dataframe\r\nSize of test.pq: 19301\r\nNumber of rows on disk: (100000, 1)\r\nReading schema from disk\r\nCreating new writer\r\nRe-writing the dataframe\r\nSize of test.pq: 44944\r\nNumber of rows on disk: (100000, 1)\r\nReading schema from disk\r\nCreating new writer\r\nRe-writing the dataframe\r\nSize of test.pq: 104815\n```",
        "created_at": "2020-05-29T11:19:55.000Z",
        "updated_at": "2020-06-29T04:09:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-06-29T04:08:44.000Z"
    },
    "comments": [
        {
            "created_at": "2020-06-02T09:17:43.564Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8980?focusedCommentId=17123549) by Joris Van den Bossche (jorisvandenbossche):*\n`[~kevinglasson]` thanks for the report!\r\n\r\nA bit modified example to visualize the issue:\r\n\r\n```python\n\r\nimport pyarrow.parquet as pq\r\n\r\nfname = \"test_metadata_size.parquet\" \r\ndf = pd.DataFrame({\"A\": [0] * 100000})\r\ndf.to_parquet(fname)\r\n\r\n# first read\r\nfile1 = pq.ParquetFile(\"test_metadata_size.parquet\")                                                                                                                                  \r\ntable1 = file1.read()                                                                                                                                                                  \r\nschema1 = file1.schema.to_arrow_schema()                                                                                                                                               \r\n\r\n# writing\r\nwriter = pq.ParquetWriter(fname, schema=schema1)                                                                                                                                      \r\nwriter.write_table(pa.Table.from_pandas(df))                                                                                                                                         \r\nwriter.close()                                                                                                                                                                       \r\n\r\n# second read\r\nfile2 = pq.ParquetFile(fname)                                                                                                                                                        \r\ntable2 = file2.read()                                                                                                                                                                \r\nschema2 = file2.schema.to_arrow_schema() \r\n```\r\n\r\nand then looking at the different schemas:\r\n\r\n```Java\n\r\n>>> schema1                                                                                                                                                                               \r\nA: int64\r\n  -- field metadata --\r\n  PARQUET:field_id: '1'\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 408\r\nARROW:schema: '/////4gCAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAA' + 818\r\n\r\n>>> table1.schema                                                                                                                                                                         \r\nA: int64\r\n  -- field metadata --\r\n  PARQUET:field_id: '1'\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 408\r\n\r\n>>> schema2  \r\nA: int64\r\n  -- field metadata --\r\n  PARQUET:field_id: '1'\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 408\r\nARROW:schema: '/////4gCAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAA' + 818\r\nARROW:schema: '/////2AGAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAA' + 2130\r\n\r\n>>> table2.schema\r\nA: int64\r\n  -- field metadata --\r\n  PARQUET:field_id: '1'\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 408\r\nARROW:schema: '/////2AGAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABAwAQAAAAAAAKAAwAAA' + 2130\r\n```\r\n\r\nSo indeed, as you said, it's the ARROW:schema size that is accumulating.\r\n\r\nSome observations:\r\n\r\n- In actual `Table.schema`, the ARROW:schema field is removed from the metadata (after reading). Sidenote: so if you would use this instead of `file.schema.to_arrow_schema()` that could be a temporary workaround for you.\n- When converting the ParquetSchema to a pyarrow Schema, we don't remove the \"ARROW:schema\" key, which we probably should do? (since that information is only used to propertly reconstruct the arrow schema, so once you have this arrow schema, we can drop the metadata for this. Similarly as we do when reading the actual file)\n- When writing with a schema that already has a \"ARROW:schema\"  metadata field, another field (with a duplicated key) gets added. I suppose this might be expected since the metadata doesn't check for duplicate keys right now. But it would also help in this case if the field would be over-written."
        },
        {
            "created_at": "2020-06-29T04:08:44.697Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8980?focusedCommentId=17147529) by Wes McKinney (wesm):*\nIssue resolved by pull request 7577\n<https://github.com/apache/arrow/pull/7577>"
        }
    ]
}