{
    "issue": {
        "title": "[Python] Support row_group_size/chunk_size keyword in pq.write_to_dataset with use_legacy_dataset=False",
        "body": "***Note**: This issue was originally created as [ARROW-16240](https://issues.apache.org/jira/browse/ARROW-16240). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe `pq.write_to_dataset` (legacy implementation) supports the `row_group_size`/`chunk_size` keyword to specify the row group size of the written parquet files.\r\n\r\nNow that we made `use_legacy_dataset=False` the default, this keyword doesn't work anymore.\r\n\r\nThis is because `dataset.write_dataset(..)` doesn't support the parquet `row_group_size` keyword. The `ParquetFileWriteOptions` class doesn't support this keyword. \r\n\r\nOn the parquet side, this is also the only keyword that is not passed to the `ParquetWriter` init (and thus to parquet's `WriterProperties` or `ArrowWriterProperties`), but to the actual `write_table` call. In C++ this can be seen at https://github.com/apache/arrow/blob/76d064c729f5e2287bf2a2d5e02d1fb192ae5738/cpp/src/parquet/arrow/writer.h#L62-L71\r\n\r\n\r\nSee discussion: <https://github.com/apache/arrow/pull/12811#discussion_r845304218>",
        "created_at": "2022-04-19T18:03:20.000Z",
        "updated_at": "2022-04-27T14:21:20.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: task"
        ],
        "closed": true,
        "closed_at": "2022-04-22T21:43:39.000Z"
    },
    "comments": [
        {
            "created_at": "2022-04-21T18:38:11.192Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16240?focusedCommentId=17525966) by Joris Van den Bossche (jorisvandenbossche):*\ncc `[~westonpace]` do you remember if this has been discussed before how the row_group_size/chunk_size setting from Parquet fits into the dataset API?\r\n\r\nThe dataset API now has a `max_rows_per_group`, I see, but that doesn't necessarily directly relate to Parquet row groups? \r\nIt's more generic about how many rows are written in one go, but so effectively is therefore also a max parquet row group size? (since those need to be written in one go) In that sense the parquet `row_group_size` keyword could be translated into that keyword to preserve the intended usecase?\r\n"
        },
        {
            "created_at": "2022-04-21T19:42:06.417Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16240?focusedCommentId=17526047) by Weston Pace (westonpace):*\nYour understanding is correct.  I think `max_rows_per_group` is the correct choice here.  Each call to `Write` (e.g. one go) results in \r\n```\n\r\nparquet_writer_->WriteTable(*table, batch->num_rows())\r\n```\r\nso it will create a new parquet row group.\r\n\r\nIt might also be useful to also set `min_rows_per_group` to `row_group_size` but that would be a change in behavior so maybe we shouldn't do this too (the legacy behavior would just write tiny groups in this case)."
        },
        {
            "created_at": "2022-04-22T21:43:39.806Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-16240?focusedCommentId=17526692) by Krisztian Szucs (kszucs):*\nIssue resolved by pull request 12955\n<https://github.com/apache/arrow/pull/12955>"
        }
    ]
}