{
    "issue": {
        "title": "[Python] Specifying columns in a dataset drops the index (pandas) metadata.",
        "body": "***Note**: This issue was originally created as [ARROW-9302](https://issues.apache.org/jira/browse/ARROW-9302). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm not sure if this is a missing feature, or just undocumented, or perhaps not even something I should expect to work.\r\n\r\nLet's start with a multi-index dataframe.\r\n\r\n```Java\n\r\n>>> import pyarrow as pa\r\n>>> import pyarrow.dataset as ds\r\n>>> import pyarrow.parquet as pq\r\n>>>\r\n>>> df\r\n               data  id                      when\r\nletter number\r\na      1        0.0  a1 2020-05-05 08:30:01+00:00\r\nb      2        1.1  b2 2020-05-05 08:30:01+00:00\r\n       3        1.2  b3 2020-05-05 08:30:01+00:00\r\nc      4        2.1  c4 2020-05-05 08:30:01+00:00\r\n       5        2.2  c5 2020-05-05 08:30:01+00:00\r\n       6        2.3  c6 2020-05-05 08:30:01+00:00\r\n\r\n>>> tbl = pa.Table.from_pandas(df)\r\n>>> tbl\r\npyarrow.Table\r\ndata: double\r\nid: string\r\nwhen: timestamp[ns, tz=+00:00]\r\nletter: string\r\nnumber: int64\r\n>>> tbl.schema\r\ndata: double\r\nid: string\r\nwhen: timestamp[ns, tz=+00:00]\r\nletter: string\r\nnumber: int64\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [\"letter\", \"number\"], \"column_indexes\": [{\"nam' + 783\r\n```\r\n\r\nThis of course works as expected, so let's write the table to disk, and read it with a `dataset`.\r\n\r\n```Java\n\r\n>>> pq.write_table(tbl, \"/tmp/df.parquet\")\r\n>>> data = ds.dataset(\"/tmp/df.parquet\")\r\n>>> data.to_table(filter=ds.field(\"letter\") == \"c\").to_pandas()\r\n               data  id                      when\r\nletter number\r\nc      4        2.1  c4 2020-05-05 08:30:01+00:00\r\n       5        2.2  c5 2020-05-05 08:30:01+00:00\r\n       6        2.3  c6 2020-05-05 08:30:01+00:00\r\n```\r\n\r\nThe filter also works as expected, and the dataframe is reconstructed properly. Let's do it again, but this time with a column selection.\r\n\r\n```Java\n\r\n>>> data.to_table(filter=ds.field(\"letter\") == \"c\", columns=[\"data\", \"id\"]).to_pandas()\r\n   data  id\r\n0   2.1  c4\r\n1   2.2  c5\r\n2   2.3  c6\r\n```\r\n\r\nHmm, not quite what I was thinking, but excluding the indices from the columns seems like a dumb move on my part, so let's try again, and this time include all columns to be safe.\r\n\r\n```Java\n\r\n>>> tbl = data.to_table(filter=ds.field(\"letter\") == \"c\", columns=[\"letter\", \"number\", \"data\", \"id\", \"when\"])\r\n>>> tbl.to_pandas()\r\n  letter  number  data  id                      when\r\n0      c       4   2.1  c4 2020-05-05 08:30:01+00:00\r\n1      c       5   2.2  c5 2020-05-05 08:30:01+00:00\r\n2      c       6   2.3  c6 2020-05-05 08:30:01+00:00\r\n>>> tbl\r\npyarrow.Table\r\nletter: string\r\nnumber: int64\r\ndata: double\r\nid: string\r\nwhen: timestamp[us, tz=UTC]\r\n>>> tbl.schema\r\nletter: string\r\n  -- field metadata --\r\n  PARQUET:field_id: '4'\r\nnumber: int64\r\n  -- field metadata --\r\n  PARQUET:field_id: '5'\r\ndata: double\r\n  -- field metadata --\r\n  PARQUET:field_id: '1'\r\nid: string\r\n  -- field metadata --\r\n  PARQUET:field_id: '2'\r\nwhen: timestamp[us, tz=UTC]\r\n  -- field metadata --\r\n  PARQUET:field_id: '3'\r\n```\r\n\r\nIt seems that when I specify any or all columns, the schema metadata is lost along the way, so `to_pandas` doesn't reconstruct the dataframe to match the original.\r\n\r\nHere's my relevant versions:\r\n\r\n- arrow-cpp: 0.17.1\n- pyarrow: 0.17.1\n- parquet-cpp: 1.5.1\n- python: 3.7.6\n- thrift-cpp: 0.13.0",
        "created_at": "2020-07-02T20:38:21.000Z",
        "updated_at": "2020-07-03T13:27:35.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-07-03T07:13:22.549Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9302?focusedCommentId=17150786) by Joris Van den Bossche (jorisvandenbossche):*\n`[~tazimmerman]` thanks for the report! \r\n\r\nThe actual cause of the metadata being lost when doing a column selection has been solved recently (-> ARROW-8802). \r\n\r\nSo at least, when including the index columns in your column selection, those will now be properly set as the index in the conversion to pandas (on master, to be released in 1.0):\r\n\r\n```Java\n\r\nIn [55]: df = pd.DataFrame({\"col1\": range(9), \"col2\": np.arange(0, 0.9, 0.1)}, index=pd.MultiIndex.from_product([['A', 'B', 'C'], [1, 2, 3]], names=[\"level1\", \"level2\"]))                                         \r\n\r\nIn [56]: df                                                                                                                                                                                                        \r\nOut[56]: \r\n               col1  col2\r\nlevel1 level2            \r\nA      1          0   0.0\r\n       2          1   0.1\r\n       3          2   0.2\r\nB      1          3   0.3\r\n       2          4   0.4\r\n       3          5   0.5\r\nC      1          6   0.6\r\n       2          7   0.7\r\n       3          8   0.8\r\n\r\nIn [58]: pq.write_table(tbl, \"/tmp/df.parquet\") \r\n    ...: data = ds.dataset(\"/tmp/df.parquet\")   \r\n\r\nIn [63]: data.to_table(filter=ds.field(\"level1\") == \"C\", columns=[\"level1\", \"level2\", \"col1\"]).to_pandas()                                                                                                         \r\nOut[63]: \r\n               col1\r\nlevel1 level2      \r\nC      1          6\r\n       2          7\r\n       3          8\r\n```\r\n\r\nHowever, when not including those columns in your selection, you still don't get the index:\r\n\r\n```Java\n\r\nIn [64]: data.to_table(filter=ds.field(\"level1\") == \"C\", columns=[\"col1\"]).to_pandas()                                                                                                                             \r\nOut[64]: \r\n   col1\r\n0     6\r\n1     7\r\n2     8\r\n```\r\n\r\nBut since the user explicitly lists the columns to select in this case, I am not fully sure whether we should automatically include the pandas columns or not.  \r\nIn the `parquet.read_table` functionality, we actually do this (triggered with a `use_pandas_metadata` keyword):\r\n\r\n```Java\n\r\nIn [72]: pq.read_table(\"/tmp/df.parquet\", columns=[\"col1\"]).to_pandas()                                                                                                                                            \r\nOut[72]: \r\n   col1\r\n0     0\r\n1     1\r\n2     2\r\n3     3\r\n4     4\r\n5     5\r\n6     6\r\n7     7\r\n8     8\r\n\r\nIn [73]: pq.read_table(\"/tmp/df.parquet\", columns=[\"col1\"], use_pandas_metadata=True).to_pandas()                                                                                                                  \r\nOut[73]: \r\n               col1\r\nlevel1 level2      \r\nA      1          0\r\n       2          1\r\n       3          2\r\nB      1          3\r\n       2          4\r\n       3          5\r\nC      1          6\r\n       2          7\r\n       3          8\r\n```\r\n\r\nSo where pyarrow will automatically add all index columns to the list of columns to select, if pandas metadata are available.\r\n\r\nIn principle this is a feature that we could also add to the `Dataset.to_table` API (it shouldn't be too hard, since the actual implementation already exists in the python parquet.py code, so that can be reused. We only might need to do it in multiple places). But I am not fully sure if we should add that."
        },
        {
            "created_at": "2020-07-03T13:27:24.559Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9302?focusedCommentId=17150991) by Troy Zimmerman (tazimmerman):*\n`[~jorisvandenbossche]` thank you for the quick response!\r\n\r\nPresumably the dataset API uses the same parquet code under the hood as in your example, so I would vote for consistency, and add the\u00a0`use_pandas_metadata` argument to `Dataset.to_table()` and pass it through to the parquet reader (if that makes sense). While doing it \"auto-magically\" is convenient, personally I'm a fan of the explicit over implicit rule."
        }
    ]
}