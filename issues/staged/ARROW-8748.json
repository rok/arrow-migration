{
    "issue": {
        "title": "[R] Add bindings to ConcatenateTables",
        "body": "***Note**: This issue was originally created as [ARROW-8748](https://issues.apache.org/jira/browse/ARROW-8748). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nFirst at all, many thanks for your hard work! I was quite exited, when you guys implemented some basic function of the the `dplyr` package. Is there a why to combine tow or more arrow tables into one by rows or columns? At the moment my workaround looks like this:\r\n```r\n\r\ndplyr::bind_rows(\r\n   \"a\" = arrow.table.1 %>% dplyr::collect(),\r\n   \"b\" = arrow.table.2 %>% dplyr::collect(),\r\n   \"c\" = arrow.table.3 %>% dplyr::collect(),\r\n   \"d\" = arrow.table.4 %>% dplyr::collect(),\r\n   .id = \"ID\"\r\n ) %>% \r\n arrow::write_ipc_stream(sink = \"file_name_combined_tables.arrow\")\r\n```\r\nBut this is actually not really a meaningful measure because of putting the data back as dataframes/tibbles into the r environment, which might lead to an exhaust of RAM space. Perhaps you might have a better workaround on hand. It would be great if you guys could implement the `bind_rows` and `bind_cols` methods provided by `dplyr`.\r\n```java\n\r\ndplyr::bind_rows(\r\n   \"a\" = arrow.table.1,\r\n   \"b\" = arrow.table.2,\r\n   \"c\" = arrow.table.3,\r\n   \"d\" = arrow.table.4, \r\n   .id = \"ID\"\r\n) %>% \r\n arrow::write_ipc_stream(sink = \"file_name_combined_tables.arrow\")\n```\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2020-05-09T18:42:14.000Z",
        "updated_at": "2020-12-30T18:22:10.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-05-11T18:05:16.326Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8748?focusedCommentId=17104730) by Neal Richardson (npr):*\nWe could add methods to concatenate Tables in Arrow memory (the function probably exists in the C++ library). But I'm not sure that's the best solution to your problem. If you have several Tables and you dump them to a file, you don't need to concatenate them in memory first. You can use the lower-level `RecordBatchStreamWriter` that `write_ipc_stream` wraps. Something like:\r\n\r\n```r\n\r\nfile_obj <- FileOutputStream$create(file_name)\r\nwriter <- RecordBatchFileWriter$create(file_obj, batch$schema)\r\nfor (batch in batches) {\r\n  writer$write(batch)\r\n}\r\nwriter$close()\r\nfile_obj$close()\r\n```\r\n\r\nSee `?RecordBatchWriter`."
        },
        {
            "created_at": "2020-05-12T09:40:01.185Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8748?focusedCommentId=17105284) by Dominic Dennenmoser (domiden):*\nYour solution seems a nicer workaround than mine. I will definitively look into it. (y)\r\n\r\nHowever, concatenate tables into Arrow memory would have the advantages of writing a new file without loading all table into memory, and linking (already processed) tables into one unit for further processing. I think, a `dplyr::bind_cols()` and `dplyr::bind_rows()` method would be a flexible extension to `arrow::open_dataset()`, which need predefined folder structure. (Please correct me if I haven't understand it correctly.)"
        }
    ]
}