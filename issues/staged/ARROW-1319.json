{
    "issue": {
        "title": "[Python] Add additional HDFS filesystem methods",
        "body": "***Note**: This issue was originally created as [ARROW-1319](https://issues.apache.org/jira/browse/ARROW-1319). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe python library hdfs3 http://hdfs3.readthedocs.io/en/latest/api.html contains a wider set of file-system methods than arrow's python bindings. These are probably simple to implement for arrow-hdfs.",
        "created_at": "2017-08-02T15:03:46.000Z",
        "updated_at": "2021-08-04T16:09:38.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-08-04T08:35:10.000Z"
    },
    "comments": [
        {
            "created_at": "2017-08-02T15:12:45.648Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16111084) by Wes McKinney (wesm):*\nQuite a few of them were added in ARROW-1301. Can you make a list of which additional ones are needed (that are not accounted for by other JIRAs already)?"
        },
        {
            "created_at": "2017-08-02T16:24:53.298Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16111211) by Martin Durant (mdurant):*\nMethods that I don't think exist, and some that have different names (and maybe already aliased). Not all are \"basic filesystem\" operations.\n 'delegate_token'\n 'disconnect' (maybe not required)\n 'get' = download\n 'get_block_locations',\n 'getmerge' (many remote files to one local file)\n 'glob',\n 'head',\n 'makedirs',\n 'mv' = rename\n 'put' = upload\n 'read_block' (delimited read)\n 'renew_token',\n 'rm' = delete\n 'set_replication',\n 'tail',\n 'touch'\n\nOn files: readlines/iteration (maybe better with io.TextIOWrapper); flush?; not sure if all standard file methods are there (readable, read1...)\n\nMethods implemented in unreleased hdfs3:\n 'cancel_token',\n 'concat' (limited to whole blocks for hadoop 1.6)\n 'create_encryption_zone',\n 'list_encryption_zones',\n"
        },
        {
            "created_at": "2017-09-03T20:41:28.921Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16151939) by Wes McKinney (wesm):*\nHelp on these would be welcome. Can create off-shoot JIRAs to implement a handful of new functions at a time as needed"
        },
        {
            "created_at": "2017-12-01T02:17:52.450Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16273831) by Robert Dailey (rdailey):*\nI am a newb when it comes to contributing to any project, but would love to help in anyway I can with Python implementation.  I'm not sure where to begin (other than raising my hand here)."
        },
        {
            "created_at": "2017-12-04T14:51:40.201Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16276889) by Wes McKinney (wesm):*\nThanks `[~rdailey]` \u2013 have a look at \r\n\r\n- https://github.com/apache/arrow/blob/master/python/pyarrow/filesystem.py\n- https://github.com/apache/arrow/blob/master/python/pyarrow/hdfs.py\n  \n  You'll need to set yourself up to build the project (http://arrow.apache.org/docs/python/development.html) and set up an HDFS to develop against (there is https://github.com/dask/hdfs3/tree/master/continuous_integration which may help)"
        },
        {
            "created_at": "2017-12-05T15:48:50.991Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16278759) by Robert Dailey (rdailey):*\nThanks `[~wesmckinn]`.  \r\n\r\nI've gone through the build process.  Sorry for the newb question, but I received 2 xfailed and 9 warnings when running py.test after the build process (on a mac).  Is that expected?  For the HDFS, I have a one containing a few billion rows that I have been working using pyarrow for the past few weeks.  I was planning on testing the new methods using that HDFS. "
        },
        {
            "created_at": "2017-12-05T18:57:40.440Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=16279041) by Wes McKinney (wesm):*\nYes, as long as there are no failures or errors that sounds fine"
        },
        {
            "created_at": "2021-08-04T08:35:10.118Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=17392861) by Antoine Pitrou (apitrou):*\nWe now have a [filesystem API](https://arrow.apache.org/docs/cpp/api/filesystem.html) that the HDFS implementation follows."
        },
        {
            "created_at": "2021-08-04T12:54:33.730Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=17393120) by Martin Durant (mdurant):*\nHDFS has additional functionality, such as delegation tokens. Are you saying that, anything that doesn't fit into the generic filesystem API, can't be supported?"
        },
        {
            "created_at": "2021-08-04T13:35:43.801Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=17393161) by Antoine Pitrou (apitrou):*\nAt the minimum you should explain how it can fit into the current API, or what a generic API would look like.\r\n\r\nAlso, the filesystem API is mostly meant for Arrow purposes of reading and writing datasets, it doesn't aim to support system administration workflows. Since I don't know what \"HDFS delegation tokens\" are, I'm not able to say whether those would fit into the intended use cases.\r\n"
        },
        {
            "created_at": "2021-08-04T15:30:52.097Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=17393269) by Martin Durant (mdurant):*\n>\u00a0how it can fit into the current API\r\n\r\n\u00a0\r\n\r\nI mean, it's not really up me. It's something that HDFS allows you to do that some people may find useful for file-system operations on HDFS. Yes, hdfs3 had this functionality.\u00a0\r\n\r\n\u00a0\r\n\r\n>\u00a0\u00a0the filesystem API is mostly meant for Arrow purposes of reading and writing datasets\r\n\r\n\u00a0\r\n\r\n!! I thought this was meant to be a general-purpose, cross-platform file-system interface for the supported backends. pyarrow is the **only** way for python users to interact with HDFS. If they can't make delegation tokens with this interface, they won't be able to anywhere else. Other functionality falls into this bucket too, such as setting the number of replications of some file.\r\n\r\n\u00a0"
        },
        {
            "created_at": "2021-08-04T16:09:38.728Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1319?focusedCommentId=17393296) by Antoine Pitrou (apitrou):*\nAgain, feel free to suggest an API for this and if possible to submit an implementation."
        }
    ]
}