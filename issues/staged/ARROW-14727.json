{
    "issue": {
        "title": "[R] Excessive memory usage on Windows",
        "body": "***Note**: This issue was originally created as [ARROW-14727](https://issues.apache.org/jira/browse/ARROW-14727). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI have the following workflow which worked on Arrow 5.0 on Windows 10 and R 4.1.2:\r\n```r\n\r\nopen_dataset(path) %>%\r\n  select(i, j) %>%\r\n  collect()\r\n```\r\nThe dataset in `path` is partitioned by `i` and `{}j{`}, with 16 partitions in total, 5 million rows in each partition and each partition has several other regular columns (i.e.\u00a0present in every partition). The entire dataset can be read into memory on my 16GB machine, which results in an R data.frame of around 3GB. However, on Arrow 6.0 the same operation fails, and R runs out of memory. Interestingly, this still works:\r\n```r\n\r\nopen_dataset(path) %>%\r\n  select(i, j, x) %>%\r\n  collect() %>%\r\n```\r\nwhere `x` is a regular column.\r\n\r\nI cannot reproduce the same issue on Linux. Measuring the actual memory consumption with GNU time (`{}--format=%Mmax{`}) I get very similar figures for the first pipeline both on 5.0 and 6.0. The same is true for the second pipeline, which of course consumes slightly more memory, as expected. On Windows I don\u2019t know of a simple method to measure maximum memory consumption but eyeballing it from Process Explorer, Arrow 5.0 needs around 0.5GB for the first example, while with Arrow 6.0 my 16GB machine becomes unresponsive, starts swapping, and depending on the circumstances, other apps might crash before R crashes with this error:\r\n```\n\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc \n```\r\nWith the second example, both versions consume roughly the same amount of memory.\r\n\r\nWith the new features in Arrow 6.0, this doesn\u2019t work in Windows either, memory consumption shoots up into the 10s of GBs:\r\n```r\n\r\nopen_dataset(path) %>%\r\n  distinct(i, j) %>%\r\n  collect()\r\n```\r\nMeanwhile this works, with under 1GB memory needed:\r\n```r\n\r\nopen_dataset(path) %>%\r\n  distinct(i, j, x) %>%\r\n  collect()\r\n```\r\nThese last two examples work without any issue on Linux, and as expected, they consume significantly less memory, as the select-then-collect examples.",
        "created_at": "2021-11-16T17:18:21.000Z",
        "updated_at": "2021-12-16T00:57:37.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-11-19T23:06:47.326Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-14727?focusedCommentId=17446716) by Will Jones (willjones127):*\nHi Andr\u00e1s! I've started to work to reproduce this, though haven't yet had success. You might try using the profmem package like below, or even adapt the below script to be closer to your data such that it starts reproducing the behavior.\r\n\r\nI tested this on Windows 10 with Arrow 6.0.0 and R 4.1.2. If you run both open_dataset() in same R session, you'll notice the one you run first having a larger number of allocations. But I consistently saw the version just selecting i and j to allocate less memory in total.\r\n\r\nLet me know if there is some tweak to the script to make it more like your situation. Or what results you are seeing using profmem.\r\n```r\n\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(arrow)\r\nlibrary(purrr)\r\nlibrary(profmem)\r\n\r\npath <- \"test_data\"\r\n\r\n# Create big dataset\r\nrows_per_partition <- 5e6\r\ni_values <- letters[1:4]\r\nj_values <- letters[1:4]\r\n\r\nrpartition <- function(n) {\r\n  tibble(x=rnorm(n), y=rnorm(n), z=sample(letters, size=n, replace=TRUE))\r\n}\r\n\r\nds <- expand_grid(i=i_values, j=j_values) %>%\r\n  mutate(data = rerun(n(), rpartition(n=rows_per_partition))) %>%\r\n  unnest(c(\"data\"))\r\n\r\nds %>%\r\n  group_by(i, j) %>%\r\n  arrow::write_dataset(path, format=\"parquet\")\r\n\r\n\r\n# Try 1 : partition cols only\r\nremove(ds)\r\ngc()\r\n\r\np1 <- profmem({\r\n  ds <- open_dataset(path) %>%\r\n    select(i, j) %>%\r\n    collect()\r\n})\r\nprint(p1, expr = FALSE)\r\n\r\n\r\n# Try 2 : add another column\r\nremove(ds)\r\ngc()\r\n\r\n\r\np2 <- profmem({\r\n  ds <- open_dataset(path) %>%\r\n    select(i, j, x) %>%\r\n    collect()\r\n})\r\nprint(p2, expr = FALSE)\r\n\r\n\r\nsum(p1$bytes, na.rm=TRUE)\r\n# 1280025656\r\nsum(p2$bytes, na.rm=TRUE)\r\n# 1934404384\r\n\r\n \n```"
        }
    ]
}