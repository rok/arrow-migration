{
    "issue": {
        "title": "[R] Expand RecordBatchReader use-ability ",
        "body": "***Note**: This issue was originally created as [ARROW-15489](https://issues.apache.org/jira/browse/ARROW-15489). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIn ARROW-14745 we thought about having `to_arrow()` returning a RecordBatchReader only. Though this would work, it's not quite as friendly as wrapping the RecordBatchReader since `arrow_dplyr_query`s have a (slightly) nicer print method.\r\n\r\nWe should add more methods and a print method that makes it clearer what a RecordBatchReader is and what it might be useful to do (e.g. continue a dplyr query)\r\n\r\nIs it possible that we could make up a name/class that encompasses all of the Arrow tabular like things that we could wrap all of these up in (for UX purposes only, really). We have ArrowTabular now, maybe we lean into that more (along side an LazyArrowTabular like dbplyr has?).",
        "created_at": "2022-01-27T19:27:17.000Z",
        "updated_at": "2022-03-22T08:22:01.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-03-21T21:01:03.000Z"
    },
    "comments": [
        {
            "created_at": "2022-01-31T13:15:23.262Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15489?focusedCommentId=17484667) by Dewey Dunnington (paleolimbot):*\nAdding a few notes here:\r\n\r\n- RecordBatchReaders area really useful because they can be exported via the C API and can stream bigger-than-memory data sets. One vision for a future DBI abstraction is that database results could be streamed as RecordBatchReaders (which would mean better support for big databse results and column types that are lossy when they go through R). That would all have to go through the C API, in which case the RecordBatchReader will see a lot more use than it currently does.\n- The `ArrowTabular` abstract class ( https://github.com/apache/arrow/blob/7eba11595c9753f18ac901eb3187f414a19a871c/r/R/arrow-tabular.R ) is mostly about selecting columns and filtering rows. Neither of those are good fits for a RecordBatchReader, whose contents are stateful (i.e., touching anything except `$schema` changes the contents of the object). Probably the only overlap is printing the schema.\n- We don't currently export the `RecordBatchReader` class and probably should as part of this ticket.\n- The way DuckDB does its registration is via a function that, when called, produces a `RecordBatchReader`, which becomes the basis for the registered View. In narrow I prototyped this as a `as_narrow_array_stream()` S3 generic that can take a function as input. Whether this is officially called a `LazyArrowTabular` or implemented as something simpler, it's an important concept.\n- Python has a `RecordBatchReader.from_batches(schema, iterator_that_produces_record_batches)` method that we should implement too (useful for testing)."
        },
        {
            "created_at": "2022-03-21T21:01:03.738Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15489?focusedCommentId=17510107) by Jonathan Keane (jonkeane):*\nIssue resolved by pull request 12567\n<https://github.com/apache/arrow/pull/12567>"
        }
    ]
}