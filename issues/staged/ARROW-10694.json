{
    "issue": {
        "title": "[Python] ds.write_dataset() generates empty files for each final partition",
        "body": "***Note**: This issue was originally created as [ARROW-10694](https://issues.apache.org/jira/browse/ARROW-10694). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nds.write_dataset() is generating empty files for the final partition folder which causes errors when reading the dataset or converting a dataset to a table.\r\n\r\nI believe this may be caused by fs.mkdir(). Without the final slash in the path, an empty file is created in the \"dev\" container:\r\n\r\n\u00a0\r\n```java\n\r\nfs = fsspec.filesystem(protocol='abfs', account_name=base.login, account_key=base.password)\r\nfs.mkdir(\"dev/test2\")\r\n```\r\n\u00a0\r\n\r\nIf the final slash is added, a proper folder is created:\r\n```java\n\r\nfs.mkdir(\"dev/test2/\")\n```\r\n\u00a0\r\n\r\nHere is a full example of what happens with ds.write_dataset:\r\n```java\n\r\nschema = pa.schema(\r\n    [\r\n        (\"year\", pa.int16()),\r\n        (\"month\", pa.int8()),\r\n        (\"day\", pa.int8()),\r\n        (\"report_date\", pa.date32()),\r\n        (\"employee_id\", pa.string()),\r\n        (\"designation\", pa.dictionary(index_type=pa.int16(), value_type=pa.string())),\r\n    ]\r\n)\r\n\r\npart = DirectoryPartitioning(pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int8())]))\r\n\r\nds.write_dataset(data=table, \r\n                 base_dir=\"dev/test-dataset\", \r\n                 basename_template=\"test-{i}.parquet\", \r\n                 format=\"parquet\",\r\n                 partitioning=part, \r\n                 schema=schema,\r\n                 filesystem=fs)\r\n\r\ndataset.files\r\n\r\n#sample printed below, note the empty files\r\n[\r\n 'dev/test-dataset/2018/1/1/test-0.parquet',\r\n 'dev/test-dataset/2018/10/1',\r\n 'dev/test-dataset/2018/10/1/test-27.parquet',\r\n 'dev/test-dataset/2018/3/1',\r\n 'dev/test-dataset/2018/3/1/test-6.parquet',\r\n 'dev/test-dataset/2020/1/1',\r\n 'dev/test-dataset/2020/1/1/test-2.parquet',\r\n 'dev/test-dataset/2020/10/1',\r\n 'dev/test-dataset/2020/10/1/test-29.parquet',\r\n 'dev/test-dataset/2020/11/1',\r\n 'dev/test-dataset/2020/11/1/test-32.parquet',\r\n 'dev/test-dataset/2020/2/1',\r\n 'dev/test-dataset/2020/2/1/test-5.parquet',\r\n 'dev/test-dataset/2020/7/1',\r\n 'dev/test-dataset/2020/7/1/test-20.parquet',\r\n 'dev/test-dataset/2020/8/1',\r\n 'dev/test-dataset/2020/8/1/test-23.parquet',\r\n 'dev/test-dataset/2020/9/1',\r\n 'dev/test-dataset/2020/9/1/test-26.parquet'\r\n]\n```\r\nAs you can see, there is an empty file for each \"day\" partition. I was not even able to read the dataset at all until I manually deleted the first empty file in the dataset (2018/1/1).\r\n\r\nI then get an error when I try to use the to_table() method:\r\n```java\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-127-6fb0d79c4511> in <module>\r\n----> 1 dataset.to_table()/opt/conda/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()/opt/conda/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()/opt/conda/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()/opt/conda/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()OSError: Could not open parquet input source 'dev/test-dataset/2018/10/1': Invalid: Parquet file size is 0 bytes\r\n```\r\nIf I manually delete the empty file, I can then use the to_table() function:\r\n```java\n\r\ndataset.to_table(filter=(ds.field(\"year\") == 2020) & (ds.field(\"month\") == 10)).to_pandas()\r\n```\r\nIs this a bug with pyarrow, adlfs, or fsspec?\r\n\r\n\u00a0",
        "created_at": "2020-11-23T10:33:03.000Z",
        "updated_at": "2021-03-16T15:53:04.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2021-03-10T13:23:54.000Z"
    },
    "comments": [
        {
            "created_at": "2020-11-23T11:17:30.623Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17237284) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` specifically for the reading part, there is an option to exclude invalid files in the `ds.dataset(..)` function, by specifying `exclude_invalid_files=True` (the docs seem incorrect to indicate the default is True, I think it is actually False). \r\n\r\nNow, of course, that's only a workaround, as I fully agree those empty files shouldn't be created in the first place. \r\nAs you mention, this seems to be the behaviour of `fs.mkdir()`, so I think we should rather discuss this in the `adlfs` project."
        },
        {
            "created_at": "2020-11-23T11:50:34.416Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17237299) by Lance Dacey (ldacey):*\nSure.\u00a0https://github.com/dask/adlfs/issues/137\r\n\r\nI tried the exclude_invalid_files argument but ran into an error:\r\n\r\n\u00a0\r\n```java\n\r\ndataset = ds.dataset(source=\"dev/test-dataset\", \r\n                     format=\"parquet\", \r\n                     partitioning=partition,\r\n                     exclude_invalid_files=True,\r\n                     filesystem=fs)\r\n\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-12-69571963d552> in <module>\r\n----> 1 dataset = ds.dataset(source=\"dev/test-dataset\", \r\n      2                      format=\"parquet\",\r\n      3                      partitioning=partition,\r\n      4                      exclude_invalid_files=True,\r\n      5                      filesystem=fs)\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in dataset(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\r\n    669     # TODO(kszucs): support InMemoryDataset for a table input\r\n    670     if _is_path_like(source):\r\n--> 671         return _filesystem_dataset(source, **kwargs)\r\n    672     elif isinstance(source, (tuple, list)):\r\n    673         if all(_is_path_like(elem) for elem in source):\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/dataset.py in _filesystem_dataset(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\r\n    434         selector_ignore_prefixes=selector_ignore_prefixes\r\n    435     )\r\n--> 436     factory = FileSystemDatasetFactory(fs, paths_or_selector, format, options)\r\n    437 \r\n    438     return factory.finish(schema)\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_dataset.pyx in pyarrow._dataset.FileSystemDatasetFactory.__init__()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/_fs.pyx in pyarrow._fs._cb_open_input_file()\r\n\r\n/opt/conda/lib/python3.8/site-packages/pyarrow/fs.py in open_input_file(self, path)\r\n    274 \r\n    275         if not self.fs.isfile(path):\r\n--> 276             raise FileNotFoundError(path)\r\n    277 \r\n    278         return PythonFile(self.fs.open(path, mode=\"rb\"), mode=\"r\")\r\n\r\nFileNotFoundError: dev/test-dataset/2018/1/1\r\n```\r\nThat folder and the empty file exists though:\r\n```java\n\r\nfor file in fs.find(\"dev/test-dataset\"):\r\n    print(file)\r\n\r\ndev/test-dataset/2018/1/1\r\ndev/test-dataset/2018/1/1/test-0.parquet\r\ndev/test-dataset/2018/10/1\r\ndev/test-dataset/2018/10/1/test-27.parquet\r\ndev/test-dataset/2018/11/1\r\ndev/test-dataset/2018/11/1/test-30.parquet\r\ndev/test-dataset/2018/12/1\r\ndev/test-dataset/2018/12/1/test-33.parquet\r\ndev/test-dataset/2018/2/1\r\ndev/test-dataset/2018/2/1/test-3.parquet\r\n\r\n```\r\n\u00a0"
        },
        {
            "created_at": "2020-11-23T11:57:29.777Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17237304) by Joris Van den Bossche (jorisvandenbossche):*\nCan you check what `fs.isfile(\"dev/test-dataset/2018/1/1\")` gives? And `fs.info(\"dev/test-dataset/2018/1/1\", detailTrue)` ?"
        },
        {
            "created_at": "2020-11-23T12:19:06.721Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17237316) by Lance Dacey (ldacey):*\n```java\n\r\nprint(fs.isfile(\"dev/test-dataset/2018/1/1\"))\r\nprint(fs.info(\"dev/test-dataset/2018/1/1\", detail=True))\n```\r\nFalse\r\n{'name': 'dev/test-dataset/2018/1/1/', 'size': 0, 'type': 'directory'}\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-11-23T12:51:56.952Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17237341) by Lance Dacey (ldacey):*\nFYI, I tested HivePartitioning as well, but faced the same issue.\u00a0\r\n\r\n\u00a0\r\n```java\n\r\nfrom pyarrow.dataset import HivePartitioning \r\n\r\npartition = HivePartitioning(pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int8())]))\r\n\r\nFileNotFoundError: dev/test-dataset2/year=2018/month=1/day=1\n```"
        },
        {
            "created_at": "2020-12-02T09:57:17.933Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17242213) by Lance Dacey (ldacey):*\nI am simply listing and deleting blobs without \".parquet\" as a workaround for now. I think this is still an issue that should be resolved since this can delete _common_metadata and _metadata files unless I specifically ignore them"
        },
        {
            "created_at": "2021-03-10T13:22:02.074Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17298828) by Lance Dacey (ldacey):*\nThis is being worked on in the adlfs library so I will close this. There are working aldfs branches that I have tested, but they have unfortunately also included new problems. Hopefully there will be a final solution soon."
        },
        {
            "created_at": "2021-03-10T13:23:55.002Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17298830) by Lance Dacey (ldacey):*\nhttps://github.com/dask/adlfs/pull/193"
        },
        {
            "created_at": "2021-03-16T15:53:04.037Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10694?focusedCommentId=17302643) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` thanks for the follow-up!"
        }
    ]
}