{
    "issue": {
        "title": "[Python] IPC roundtrip fails in to_pandas with empty table and extension type ",
        "body": "***Note**: This issue was originally created as [ARROW-13413](https://issues.apache.org/jira/browse/ARROW-13413). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWith pyarrow=4.0.1 and pandas=1.2.3, when writing then reading an empty DataFrame with an extension dtype, `to_pandas` subsequently fails to convert the arrow table:\r\n```python\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\ndf1 = pd.DataFrame({\"x\": pd.Series([], dtype=\"Int8\")})\r\ntbl1 = pa.Table.from_pandas(df1)\r\n\r\n# In memory roundtrip seems to work fine\r\npa.Table.from_pandas(tbl1.to_pandas()).to_pandas()\r\n\r\npath = \"/tmp/tmp.arr\"\r\nwriter = pa.RecordBatchStreamWriter(path, tbl1.schema)\r\nwriter.write_table(tbl1)\r\nwriter.close()\r\nreader = pa.RecordBatchStreamReader(path)\r\ntbl2 = reader.read_all()\r\n\r\nassert tbl1.schema.equals(tbl2.schema)\r\nassert tbl2.schema.metadata == tbl2.schema.metadata\r\n\r\ndf2 = tbl1.to_pandas()\r\ntry:\r\n    df2 = tbl2.to_pandas()\r\nexcept Exception as e:\r\n    print(f\"Error: {e}\")\r\n    df2 = tbl2.replace_schema_metadata(None).to_pandas()\r\n```\r\nIn the above example (with `Int8` as the pandas dtype), the table read from disk cannot be converted to a DataFrame, even though its schema and metadata are supposedly equal\u00a0 to the original table. Removing its metadata \"fixes\" the issue.\r\n\r\nThe problem doesn't occur with \"normal\" dtypes. This may well be a bug in Pandas, but it seems to depend on some change in Arrow's metadata.\r\n\r\nThe full stacktrace:\r\n```java\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-08855adb276d> in <module>\r\n----> 1 df2 = tbl2.to_pandas()\r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()\r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()\r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata, types_mapper)\r\n    787     _check_data_column_metadata_consistency(all_columns)\r\n    788     columns = _deserialize_column_index(table, all_columns, column_indexes)\r\n--> 789     blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n    790 \r\n    791     axes = [columns, index]\r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/pandas_compat.py in _table_to_blocks(options, block_table, categories, extension_columns)\r\n   1128     result = pa.lib.table_to_blocks(options, block_table, categories,\r\n   1129                                     list(extension_columns.keys()))\r\n-> 1130     return [_reconstruct_block(item, columns, extension_columns)\r\n   1131             for item in result]\r\n   1132 \r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/pandas_compat.py in <listcomp>(.0)\r\n   1128     result = pa.lib.table_to_blocks(options, block_table, categories,\r\n   1129                                     list(extension_columns.keys()))\r\n-> 1130     return [_reconstruct_block(item, columns, extension_columns)\r\n   1131             for item in result]\r\n   1132 \r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pyarrow/pandas_compat.py in _reconstruct_block(item, columns, extension_columns)\r\n    747             raise ValueError(\"This column does not support to be converted \"\r\n    748                              \"to a pandas ExtensionArray\")\r\n--> 749         pd_ext_arr = pandas_dtype.__from_arrow__(arr)\r\n    750         block = _int.make_block(pd_ext_arr, placement=placement)\r\n    751     else:\r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pandas/core/arrays/integer.py in __from_arrow__(self, array)\r\n    119             results.append(int_arr)\r\n    120 \r\n--> 121         return IntegerArray._concat_same_type(results)\r\n    122 \r\n    123 \r\n\r\n~/miniforge3/envs/grapy/lib/python3.8/site-packages/pandas/core/arrays/masked.py in _concat_same_type(cls, to_concat)\r\n    269         cls: Type[BaseMaskedArrayT], to_concat: Sequence[BaseMaskedArrayT]\r\n    270     ) -> BaseMaskedArrayT:\r\n--> 271         data = np.concatenate([x._data for x in to_concat])\r\n    272         mask = np.concatenate([x._mask for x in to_concat])\r\n    273         return cls(data, mask)\r\n\r\n<__array_function__ internals> in concatenate(*args, **kwargs)\r\n\r\nValueError: need at least one array to concatenate\r\n```",
        "created_at": "2021-07-20T16:34:32.000Z",
        "updated_at": "2022-02-03T09:48:08.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-02-03T09:48:08.000Z"
    },
    "comments": [
        {
            "created_at": "2021-07-22T18:38:13.756Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13413?focusedCommentId=17385708) by Weston Pace (westonpace):*\nThe key difference appears to be that tbl1 (the table created when converting from pandas) is a table with 1 chunk with 0 values.\u00a0 Meanwhile, tbl2 (the table created when reading an Arrow file) is a table with 0 chunks.\r\n\r\n\u00a0\r\n```java\n\r\n>>> len(tbl1.column('x').chunks)\r\n1\r\n>>> len(tbl2.column('x').chunks)\r\n0\r\n>>> len(tbl1.column('x').chunks[0])\r\n0\r\n```\r\nI think that both are valid representations of an empty table.\u00a0 I'm not sure it makes sense for Arrow to define one of these as more correct than the other.\u00a0 I believe the correct fix would be for Pandas to be able to process either form.\r\n\r\n\u00a0\r\n\r\nDeleting the metadata presumably helps because then pandas no longer knows what the data type is and the bug appears to be specific to the Int8 array parsing:\r\n```java\n\r\n116  \t        for arr in chunks:\r\n117  \t            data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=self.type)\r\n118  \t            int_arr = IntegerArray(data.copy(), ~mask, copy=False)\r\n119  \t            results.append(int_arr)\r\n120  \t\r\n121  ->\t        return IntegerArray._concat_same_type(results)\r\n\r\n```\r\nHere you can see that `results` will be `[[]]` for `tbl1` and `[]` for `tbl2`."
        },
        {
            "created_at": "2021-08-03T13:22:03.674Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13413?focusedCommentId=17392294) by Joris Van den Bossche (jorisvandenbossche):*\nThis is indeed something to be fixed in pandas. \r\n\r\n`[~buhrmann]` can you check the latest pandas version? I think we recently already fixed this (https://github.com/pandas-dev/pandas/pull/41052)"
        },
        {
            "created_at": "2022-02-03T09:47:51.039Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13413?focusedCommentId=17486335) by Joris Van den Bossche (jorisvandenbossche):*\nClosing this as fixed on the pandas side."
        }
    ]
}