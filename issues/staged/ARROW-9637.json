{
    "issue": {
        "title": "[Python] Speed degradation with categoricals",
        "body": "***Note**: This issue was originally created as [ARROW-9637](https://issues.apache.org/jira/browse/ARROW-9637). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI have noticed some major speed degradation when using categorical data types.\u00a0 For example, a Parquet file with 1 million rows that sums 10 float columns and groups by two columns (one a date column and one a category column).\u00a0 The cardinality of the category seems to have a major effect.\u00a0 When grouping on category column of cardinality 10, performance is decent (query runs in 150 ms).\u00a0 But with cardinality of 100, the query runs in 10 seconds.\u00a0 If I switch over to my Parquet file that does **not** have categorical columns, the same query that took 10 seconds with categoricals now runs in 350 ms.\r\n\r\nI would be happy to post the Pandas code that I'm using (including how I'm creating the Parquet file), but I first wanted to report this and see if it's a known issue.\r\n\r\nThanks.",
        "created_at": "2020-08-03T21:13:39.000Z",
        "updated_at": "2020-08-04T15:30:00.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-08-04T15:30:00.000Z"
    },
    "comments": [
        {
            "created_at": "2020-08-04T06:48:25.586Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170593) by Joris Van den Bossche (jorisvandenbossche):*\nBased on your description, I might be an issue with the groupby calculation in pandas, rather than the reading of parquet files with categoricals (which is what pyarrow is used for in your case). In which case it's something to report to pandas (https://github.com/pandas-dev/pandas/issues).\r\n\r\nBut to be sure, you will need to post a reproducible example."
        },
        {
            "created_at": "2020-08-04T13:20:48.030Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170790) by Larry Parker (lparker):*\n[parquet_files.zip](parquet_files.zip) [fact1__c.parquet.zip](fact1__c.parquet.zip)"
        },
        {
            "created_at": "2020-08-04T13:22:59.763Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170791) by Larry Parker (lparker):*\nThe file _fact1____c.parquet.zip_ contains the categorical Parquet file.\u00a0 I could not upload _fact1____nc.parquet.zip_ as it exceeded the 60 MB upload limit (by 2 MB).\u00a0 That file contains the non-categorical Parquet file, but you should be able to convert the uploaded file to not use category columns, and save it out to _fact1____nc.parquet_."
        },
        {
            "created_at": "2020-08-04T13:24:53.097Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170793) by Larry Parker (lparker):*\n```java\n\r\nimport datetime as dt\r\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\npd.options.display.float_format = '{:,.4f}'.format\r\n\r\nprint(\"Pandas v\" + pd.__version__)\r\nprint(\"PyArrow v\" + pa.__version__)\r\n\r\nfolder = \"/users/lparker/data/pandas_perf/\"\r\n\r\ndef load(table_name):\r\n    file_name = folder + table_name + \".parquet\"\r\n\r\n    df = pd.read_parquet(file_name)\r\n\r\n    return df\r\n\r\ndf__fact1 = load(\"fact1__nc\")\r\n# df__fact1 = load(\"fact1__c\")\r\n\r\nprint(\"\\nRow count = {}\".format(len(df__fact1)))\r\nprint(df__fact1.dtypes)\r\n\r\ndim_col = \"a2\"\r\nprint(\"\\nCardinality = {}\".format(df__fact1[dim_col].nunique()))\r\n\r\nts0 = dt.datetime.now()\r\n\r\ndf = df__fact1.groupby([\"date\", dim_col]).agg(m1=(\"m1\", \"sum\"), m2=(\"m2\", \"sum\"), m3=(\"m3\", \"sum\"), m4=(\"m4\", \"sum\"), m5=(\"m5\", \"sum\"), m6=(\"m6\", \"sum\"), m7=(\"m7\", \"sum\"), m8=(\"m8\", \"sum\"), m9=(\"m9\", \"sum\"), m10=(\"m10\", \"sum\"))\r\n\r\nts1 = dt.datetime.now()\r\nprint(\"Query time (ms) = \" + str(int((ts1 - ts0).total_seconds() * 1000)))\r\n\r\nprint(\"\\nRow count = {}\".format(len(df)))\r\nprint(df.head(10))\r\n```"
        },
        {
            "created_at": "2020-08-04T13:38:02.144Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170801) by Larry Parker (lparker):*\nHere are some benchmarks on my Mac:\r\n\r\nPandas v0.25.0\r\n\r\nPyArrow v1.0.0\r\n\n|Parquet File|Dim Column|Cardinality|Query Time (ms)|\r|\n|-|-|-|-|-|\n|fact1__nc.parquet|a2|100|321|\r|\n|fact1__c.parquet|a2|100|9,772|\r|\n|fact1__nc.parquet|a4|1000|1,266|\r|\n|fact1__c.parquet|a4|1000|102,904|\n"
        },
        {
            "created_at": "2020-08-04T15:08:07.743Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170862) by Larry Parker (lparker):*\nFor reference:\u00a0\u00a0<https://github.com/pandas-dev/pandas/issues/35546>\r\n\r\n\u00a0"
        },
        {
            "created_at": "2020-08-04T15:29:50.119Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9637?focusedCommentId=17170879) by Joris Van den Bossche (jorisvandenbossche):*\nThanks for opening an issue on the pandas side! Since the timing you do is only for the groupby operation (which is fully done in pandas), and does not include the reading of the parquet file, I am going to close the issue here, but will further follow-up on the pandas issue."
        }
    ]
}