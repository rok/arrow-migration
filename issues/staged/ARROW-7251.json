{
    "issue": {
        "title": "[Python] Open CSVs with different encodings",
        "body": "***Note**: This issue was originally created as [ARROW-7251](https://issues.apache.org/jira/browse/ARROW-7251). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI would like to open an UTF-16 encoded CSVs (among others) without preprocessing in let's say Pandas. Is there maybe a way to do this already ?",
        "created_at": "2019-11-25T12:28:24.000Z",
        "updated_at": "2020-06-16T17:34:33.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2020-06-16T17:34:33.000Z"
    },
    "comments": [
        {
            "created_at": "2019-11-25T12:46:42.316Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981523) by Antoine Pitrou (apitrou):*\nThe only way to do this is to convert the CSV upfront to UTF-8."
        },
        {
            "created_at": "2019-11-25T13:40:06.282Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981556) by Sascha Hofmann (saschahofmann):*\nDo you know whether there are any plans to maybe implement an encoding option on the ParseOptions? Like e.g. pandas, does it?"
        },
        {
            "created_at": "2019-11-25T14:08:40.682Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981588) by Antoine Pitrou (apitrou):*\nWe don't have any plans currently. We'd rather avoid adding a character set conversion machinery to Arrow. Do you know why you have UTF-16 encoded CSVs?"
        },
        {
            "created_at": "2019-11-25T17:07:48.805Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981709) by Sascha Hofmann (saschahofmann):*\nI think it's a Kaggle dataset. In general, we'd like to ingest all kind of legacy CSVs. Do you by any chance know how the pyarrow read_csv compares to Pandas'?\r\n\r\n\u00a0\r\n\r\nIn the meantime, we might simply check for a BOM and fall back to the pandas csv reader if we find a None UTF-8 one."
        },
        {
            "created_at": "2019-11-25T17:19:01.724Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981718) by Antoine Pitrou (apitrou):*\n> Do you by any chance know how the pyarrow read_csv compares to Pandas'?\r\n\r\nThat's a vague question. Which aspect are you interested in?\r\n\r\n\u00a0"
        },
        {
            "created_at": "2019-11-25T17:50:33.166Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981746) by Sascha Hofmann (saschahofmann):*\nBasically, I could imagine simply switching everything over to use pandas.read_csv and then get the pyarrow table from there (pa.Table.from_pandas).\u00a0\r\n\r\nQuestion would be something like: how much time and memory overhead would that give compared to a pure pa.csv.read_csv approach (when possible). I thought maybe someone has benchmarked that but couldn't find anything.\u00a0"
        },
        {
            "created_at": "2019-11-25T17:52:47.646Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981748) by Antoine Pitrou (apitrou):*\nWell, using Arrow directly should generally be much faster than going through Pandas. I'd be curious about your experience.\r\n\r\nIf disk space is not an issue, you might convert your CSV files to UTF-8 upfront (for example using iconv or even Python)."
        },
        {
            "created_at": "2019-11-25T18:39:02.690Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981785) by Sascha Hofmann (saschahofmann):*\nI just ran a simple timeit notebook for 30MB csv with 7 string columns one bool and one int column. Differences are quite massive.\r\n\r\n**Pandas to pyarrow:**\r\n\r\n314 ms \u00b1 20 ms\u00a0\r\n\r\n**Only pyarrow:**\r\n\r\n38.7 ms \u00b1 3.15 ms\r\n\r\n\u00a0\r\n\r\nWe are trying to hide as much complexity from the user, who might throw in arbitrarily old CSVs, as possible. That's why converting upfront is not really an option but I guess having the pandas reader as a fallback should be sufficient."
        },
        {
            "created_at": "2019-11-25T18:50:24.107Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981790) by Antoine Pitrou (apitrou):*\nThank you for the numbers. Which encodings do you usually encounter?"
        },
        {
            "created_at": "2019-11-25T19:14:03.165Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981815) by Sascha Hofmann (saschahofmann):*\nAs mentioned above, today I had a UTF-16 file. Previously, I encountered a latin-1 encoded CSV but that was actually fine because only one of the columns had the weird byte so only that column was read as binary instead of string (you actually answered me on that one here:\u00a0https://issues.apache.org/jira/browse/ARROW-6934)"
        },
        {
            "created_at": "2019-11-25T19:19:15.423Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16981820) by Sascha Hofmann (saschahofmann):*\nback then I thought. It might be useful to have different string encoded column types."
        },
        {
            "created_at": "2019-11-27T13:59:56.725Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16983529) by Sascha Hofmann (saschahofmann):*\nI have another CSV related question. How does the csv reader decide how many chunks the created arrow table will have? I know I can fix it by giving a block_size on the read options but the default is None. I observed that sometimes the reader is creating a lot of chunks but it doesn't seem to necessarily scale with file size?"
        },
        {
            "created_at": "2019-11-27T14:04:21.614Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16983531) by Antoine Pitrou (apitrou):*\nThe default is not None:\r\n```python\n\r\n>>> from pyarrow import csv                                                                                                                                                     \r\n>>> opts = csv.ReadOptions()                                                                                                                                                    \r\n>>> opts.block_size                                                                                                                                                             \r\n1048576\r\n```\r\n\r\nBy changing this value you will control the size of chunks. But do note this has an impact in performance, especially in parallel mode."
        },
        {
            "created_at": "2019-11-27T14:25:22.340Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=16983561) by Sascha Hofmann (saschahofmann):*\nAh, good to know.\u00a0\r\n\r\nMaybe it might be worth noting on the docs\r\n <https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions>\r\n\r\nthat providing None uses the default of 1MB"
        },
        {
            "created_at": "2020-04-27T02:11:17.425Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=17092934) by Wes McKinney (wesm):*\nAny further thoughts about this? Perhaps at minimum we should document the expectation about non-UTF-8 CSV files. FTR I'm not sure many SQL databases will ingest non-UTF-8 CSV files"
        },
        {
            "created_at": "2020-04-27T12:25:40.749Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=17093435) by Sascha Hofmann (saschahofmann):*\nFor us having different string encoding support would be amazing. That being said, I admit other encodings are rare/dying out but we stumble upon them once in a while. From those, I don't know how many are using a BOM to identify their encoding. We haven't actually tried it but we might use pandas as mentioned above in cases where a file has a BOM different than the utf-8 (see comment above).\u00a0 I am not sure how you did the csv reading in pandas but I assume it might not be worth going through it again. In the end, it might be best to force people using UTF-8.\u00a0"
        },
        {
            "created_at": "2020-04-27T12:45:01.527Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=17093465) by Antoine Pitrou (apitrou):*\ncc `[~saschahofmann]` Is there anything that prevents you from recoding the CSV file before opening it with Arrow?\r\n(what are your constraints? performance? file size?)\r\n\r\nWith some care, you could even implement a file-like object in Python that recodes data to UTF-8 on the fly. It should be accepted by `csv.read_csv`."
        },
        {
            "created_at": "2020-04-27T14:08:57.003Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-7251?focusedCommentId=17093550) by Sascha Hofmann (saschahofmann):*\nOur current setup allows users to upload CSV files which we are parsing to arrow. Right now, we are not doing any the preprocessing of the csv file so we can receive arbitrary weird files. I will propose your \"recode on the fly\" suggestion.\r\n\r\n\u00a0"
        }
    ]
}