{
    "issue": {
        "title": "[R] open_dataset very slow on heavily partitioned parquet dataset",
        "body": "***Note**: This issue was originally created as [ARROW-15069](https://issues.apache.org/jira/browse/ARROW-15069). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nOpening a (particular) partitioned hive-style parquet dataset is very slow (45s to 1 minute).\u00a0 I have a reproducible example below that takes 780 csv files and writes them to parquet using the `open_dataset(\"csv files\") |> group_by(vars) |> write_dataset(\"parquet\")`\u00a0suggested\u00a0[here](https://arrow.apache.org/docs/r/articles/dataset.html#writing-datasets). Opening and querying the subsequent parquet dataset is much slower than doing it on the original csv files, which is not what I expected.\r\n```java\n\r\nlibrary(arrow)\r\nlibrary(dplyr)\r\nlibrary(tictoc)\r\n\r\nzipfile <- \"ahccd.zip\"\r\ncsv_dir <- \"data/csv\"\r\nparquet_dir <- \"data/parquet\"\r\n\r\ndir.create(csv_dir, recursive = TRUE)\r\ndir.create(parquet_dir, recursive = TRUE)\r\n\r\n# A zip of 780 csvs of daily temperature data at Canadian climate stations (one file per station)\r\ndownload.file(\"https://www.dropbox.com/s/f0a18jp0lvbp1hp/ahccd.zip?dl=1\", destfile = zipfile)\r\n\r\nunzip(zipfile, exdir = csv_dir)\r\n\r\ncsv_schema <- schema(\r\n  field(\"stn_id\", string()),\r\n  field(\"stn_name\", string()),\r\n  field(\"measure\", string()),\r\n  field(\"date\", date32()),\r\n  field(\"year\", int64()),\r\n  field(\"month\", int64()),\r\n  field(\"temp\", double()),\r\n  field(\"province\", string()),\r\n  field(\"stn_joined\", string()),\r\n  field(\"element\", string()),\r\n  field(\"unit\", string()),\r\n  field(\"stn_last_updated\", string()),\r\n  field(\"flag\", string())\r\n)\r\n\r\ncsv_format <- FileFormat$create(format = \"csv\", quoting = FALSE)\r\n\r\n# Write to parquet, partitioning on stn_id, year, measure\r\ntic(\"write csv to parquet\")\r\narrow::open_dataset(\"data/csv\", schema = csv_schema,\r\n                    format = csv_format) |>\r\n  group_by(stn_id, year, measure) |>\r\n  write_dataset(parquet_dir, format = \"parquet\")\r\ntoc()\r\n#> write csv to parquet: 2067.093 sec elapsed\r\n\r\nstations <- c(\"1100031\", \"1100120\", \"1100119\", \"1036B06\")\r\n\r\n## Query directory of original csv files\r\ntic(\"query csv\")\r\nfoo <- arrow::open_dataset(csv_dir, schema = csv_schema,\r\n                           format = csv_format) |>\r\n  filter(year >= 1990,\r\n         year <= 2020,\r\n         stn_id %in% stations,\r\n         measure == \"daily_max\") |>\r\n  collect()\r\ntoc()\r\n#> query csv: 12.571 sec elapsed\r\n\r\n## Query the hive-style parquet directory\r\ntic(\"query parquet\")\r\nbar <- arrow::open_dataset(\"data/parquet\") |>\r\n  filter(year >= 1990,\r\n         year <= 2020,\r\n         stn_id %in% stations,\r\n         measure == \"daily_max\") |>\r\n  collect()\r\ntoc()\r\n#> query parquet: 41.79 sec elapsed\r\n\r\n## It turns out that it is just the opening of the dataset \r\n## that takes so long\r\ntic(\"open parquet dataset\")\r\nds <- arrow::open_dataset(\"~/Desktop/arrow-report/data/parquet\")\r\ntoc()\r\n#> open parquet dataset: 45.581 sec elapsed\r\n\r\nds\r\n#> FileSystemDataset with 191171 Parquet files\r\n#> stn_name: string\r\n#> date: date32[day]\r\n#> month: int64\r\n#> temp: double\r\n#> province: string\r\n#> stn_joined: string\r\n#> element: string\r\n#> unit: string\r\n#> stn_last_updated: string\r\n#> flag: string\r\n#> stn_id: string\r\n#> year: int32\r\n#> measure: string\r\n\r\ntic(\"query already openend dataset\")\r\nds |> \r\n  filter(year >= 1990,\r\n         year <= 2020,\r\n         stn_id %in% stations,\r\n         measure == \"daily_max\") |>\r\n  collect()\r\n#> # A tibble: 44,469 \u00d7 13\r\n#>    stn_name date       month  temp province stn_joined     element        unit  \r\n#>    <chr>    <date>     <int> <dbl> <chr>    <chr>          <chr>          <chr> \r\n#>  1 ALBERNI  1992-01-01     1   6.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  2 ALBERNI  1992-01-02     1   5.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  3 ALBERNI  1992-01-03     1   3.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  4 ALBERNI  1992-01-04     1   6   BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  5 ALBERNI  1992-01-05     1   0.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  6 ALBERNI  1992-01-06     1   0   BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  7 ALBERNI  1992-01-07     1   0   BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  8 ALBERNI  1992-01-08     1   1.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#>  9 ALBERNI  1992-01-09     1   4   BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#> 10 ALBERNI  1992-01-10     1   5.5 BC       station joined Homogenized d\u2026 Deg C\u2026\r\n#> # \u2026 with 44,459 more rows, and 5 more variables: stn_last_updated <chr>,\r\n#> #   flag <chr>, stn_id <chr>, year <int>, measure <chr>\r\n\r\ntoc()\r\n#> query already openend dataset: 0.356 sec elapsed\r\n```\r\nThe above reprex is self-contained, but will take a while to run, specifically the writing of the parquet dataset can take up to 30 min. It also downloads a 380MB zip file of csvs from my Dropbox.",
        "created_at": "2021-12-10T20:50:08.000Z",
        "updated_at": "2022-07-12T14:04:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: R",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-12-10T21:26:16.691Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17457367) by Jonathan Keane (jonkeane):*\nThanks for the detailed report! \r\n\r\nI know that `[~westonpace]` and some folks on his team are looking into dataset performance in this realm. They might have an idea of what's going on here."
        },
        {
            "created_at": "2021-12-10T22:22:51.975Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17457394) by Andy Teucher (ateucher):*\nFantastic, thank you! I hesitated to call it a bug, since the code does work, just not as fast as I expected."
        },
        {
            "created_at": "2021-12-13T17:59:27.722Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17458583) by Weston Pace (westonpace):*\nI'm going to try and take a look at this soon.  I have a hunch the problem is that your 780 CSV files are turning into 191,171 parquet files.  Luckily, we have done some work recently that should help with this by batching files up in memory to ensure minimum row group sizes when doing a dataset write."
        },
        {
            "created_at": "2021-12-13T21:00:43.688Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17458712) by Andy Teucher (ateucher):*\nThanks very much `[~westonpace]` - that definitely rings true. Looking forward to what you're working on - and I wonder if there could be any documentation on trade-offs/strategies for optimal partitioning schemes."
        },
        {
            "created_at": "2021-12-14T02:59:54.866Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17458871) by Weston Pace (westonpace):*\nThanks for the great reproducer.  This is an interesting case to look at.  So you have 68M rows spread out more or less evenly across 187,896 partitions.  This should result in about 300 rows per file which is pretty low.  The recent work I was thinking of wouldn't really help here as the problem wasn't what I had originally thought.\r\n\r\nThat being said, I don't quite get the performance that you do, it takes only around 2.6seconds to open+query the parquet dataset and about 4.5 seconds to open+query the CSV dataset (and this was in debug mode).  Running it through a quick run of perf it looks like most of the time is spent listing directories (system calls) and parsing and making sense of filenames into partition objects.\r\n\r\nSo I'm not sure where your bottleneck is coming from.  It's possible that MacOS' filesystem operations are simply slower.  I don't have a system available to try it out right now.\r\n\r\n> I wonder if there could be any documentation on trade-offs/strategies for optimal partitioning schemes.\r\n\r\nWe could probably create some documentation around this.  One thing that you've already noted is that you can reuse the dataset.  So if you plan to make a lot of queries you can open the dataset once and reuse it.\r\n\r\nOn the other hand, if you're trying to minimize latency around a single query then that might be a bit of a challenge.\r\n\r\n"
        },
        {
            "created_at": "2021-12-14T03:03:44.173Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17458872) by Weston Pace (westonpace):*\nIf I only group by year then it takes ~20 seconds to create the dataset and 0.02s to open the dataset.  Though of course it takes slightly longer to query the dataset but that might be a reasonable tradeoff."
        },
        {
            "created_at": "2021-12-14T19:58:26.188Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17459442) by Andy Teucher (ateucher):*\nThanks so much! Only ~300 rows per file does seem excessively partitioned. I had partitioned on the fields I would query most on, but it makes sense that the directory listing is the bottleneck with so many nested directories. Interesting about the OS differences; it's pretty drastic (You are on Linux I'm guessing). `[~stephhazlitt]` also reproduced this on her new M1 mac and opening the heavily partitioned dataset was much faster than mine (~11s vs ~45s) but still slower than yours (2.6s). A colleague on Windows had ~60s to open the dataset, and similar performance patterns overal..\r\n\r\nPartitioning only on year gave me similar timings to you, so that's great. Definitely a reasonable tradeoff."
        },
        {
            "created_at": "2022-07-12T14:04:26.907Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-15069?focusedCommentId=17565616) by @toddfarmer:*\nThis issue was last updated over 90 days ago, which may be an indication it is no longer being actively worked. To better reflect the current state, the issue is being unassigned. Please feel free to re-take assignment of the issue if it is being actively worked, or if you plan to start that work soon."
        }
    ]
}