{
    "issue": {
        "title": "Error when using toPandas with PyArrow",
        "body": "***Note**: This issue was originally created as [ARROW-8731](https://issues.apache.org/jira/browse/ARROW-8731). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'm getting the following error when calling toPandas on a spark dataframe. I imagine my pyspark and pyarrow versions are clashing somehow but I haven't found this same issue by anyone else online\r\n \\* This is a blocker to our use of pyarrow on a project\r\n\r\n\u00a0\r\n```java\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-e2ed63d96b43> in <module>\r\n----> 1 df.limit(100).toPandas()\r\n\r\n/venv/lib/python3.6/site-packages/pyspark/sql/dataframe.py in toPandas(self)\r\n   2119                         _check_dataframe_localize_timestamps\r\n   2120                     import pyarrow\r\n-> 2121                     batches = self._collectAsArrow()\r\n   2122                     if len(batches) > 0:\r\n   2123                         table = pyarrow.Table.from_batches(batches)\r\n\r\n/venv/lib/python3.6/site-packages/pyspark/sql/dataframe.py in _collectAsArrow(self)\r\n   2177         with SCCallSiteSync(self._sc) as css:\r\n   2178             sock_info = self._jdf.collectAsArrowToPython()\r\n-> 2179         return list(_load_from_socket(sock_info, ArrowStreamSerializer()))\r\n   2180 \r\n   2181     ##########################################################################################\r\n\r\n/venv/lib/python3.6/site-packages/pyspark/rdd.py in _load_from_socket(sock_info, serializer)\r\n    142 \r\n    143 def _load_from_socket(sock_info, serializer):\r\n--> 144     (sockfile, sock) = local_connect_and_auth(*sock_info)\r\n    145     # The RDD materialization time is unpredicable, if we set a timeout for socket reading\r\n    146     # operation, it will very possibly fail. See SPARK-18281.\r\n\r\nTypeError: local_connect_and_auth() takes 2 positional arguments but 3 were given\r\n```",
        "created_at": "2020-05-07T14:12:25.000Z",
        "updated_at": "2020-05-07T20:23:26.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-05-07T15:46:36.000Z"
    },
    "comments": [
        {
            "created_at": "2020-05-07T15:46:29.652Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8731?focusedCommentId=17101807) by Andrew Redd (aredd1@wayfair.com):*\nThis was resolved when I moved to Pyspark 2.4.4 and pyarrow 0.8.0.\u00a0\r\n\r\n\u00a0\r\n\r\nI needed to make sure I had pandas < 1.0"
        },
        {
            "created_at": "2020-05-07T15:57:16.293Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8731?focusedCommentId=17101823) by Wes McKinney (wesm):*\ncc `[~bryanc]` \u2013 I think you need to set an environment variable"
        },
        {
            "created_at": "2020-05-07T16:17:24.474Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8731?focusedCommentId=17101844) by Bryan Cutler (bryanc):*\n[~aredd1@wayfair.com] you should be able to use a newer version of pyarrow with pyspark 2.4.4 by following the instructions here https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x"
        },
        {
            "created_at": "2020-05-07T20:23:26.218Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8731?focusedCommentId=17102012) by Andrew Redd (aredd1@wayfair.com):*\nPerfect thank you for the help!"
        }
    ]
}