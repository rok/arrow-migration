{
    "issue": {
        "title": "[C++][Python] Behavior of parquet.read_table with filter and parquets containing null",
        "body": "***Note**: This issue was originally created as [ARROW-10501](https://issues.apache.org/jira/browse/ARROW-10501). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi,\r\n\r\nI investigated what parquet.read_table with filter returns and\r\n found some strange behaviors.\r\n\r\nPlease see the following source code to reproduce.\r\n Details are written as comments.\r\n```java\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport os\r\n\r\n\r\ndef test_filter_with_null_contained_parquets():\r\n    def check(path, filter, expected_df):\r\n        params = {\r\n            'columns': ['field'],\r\n            'filters': filter,\r\n        }        tb = pq.read_table(path, **params)\r\n        df = tb.to_pandas()\r\n        ret = df.equals(expected_df)\r\n        return ret\r\n\r\n# see below how to make these parquets\r\n    dir_name = './read_table_regression/'\r\n    pq_an = dir_name + 'all_null.snappy.parquet'\r\n    pq_sn = dir_name + 'some_null.snappy.parquet'\r\n    pq_hn = dir_name + 'half_null.snappy.parquet'\r\n    pq_es = dir_name + 'empty_string.snappy.parquet' \r\n\r\n# actual DataFrames from read_table\r\n    empty_df = pd.DataFrame(columns=['field'])\r\n    one_null_df = pd.DataFrame({'field': [None]})\r\n    non_null_df = pd.DataFrame({'field': ['123']})\r\n    es_contained_df = pd.DataFrame({'field': ['123', '']})\r\n    es_removed_df = pd.DataFrame({'field': ['123']})\r\n\r\n\r\n    #\r\n# case 1: 'not equals' and empty string\r\n    #\r\n    f0 = [('field', '!=', '')]\r\n# why nulls are removed?\r\n    assert check(pq_an, f0, empty_df)       # [null]              -> []\r\n    assert check(pq_sn, f0, non_null_df)    # [null, null, '123'] -> ['123']\r\n    assert check(pq_es, f0, es_removed_df)  # [null, '123', '']   -> ['123']\r\n\r\n    #\r\n# case 2: 'not equals' and null\r\n    #\r\n    f1 = [('field', '!=', None)]\r\n# ok.\r\n    assert check(pq_an, f1, empty_df)     # [null]              -> []\r\n# why empty?\r\n    assert check(pq_sn, f1, empty_df)     # [null, null, '123'] -> []\r\n    assert check(pq_es, f1, empty_df)     # [null, '123', '']   -> []\r\n\r\n    #\r\n# case 3: 'not in' and empty string\r\n    #\r\n    f2 = [('field', 'not in', [''])]\r\n    f3 = [('field', 'not in', ['abc'])]\r\n# seems inconsistent results\r\n# null remains.\r\n    assert check(pq_an, f2, one_null_df)      # [null]              -> [null]\r\n    assert check(pq_an, f3, one_null_df)      # [null]              -> [null]\r\n# null removed.\r\n    assert check(pq_sn, f2, non_null_df)      # [null, null, '123'] -> ['123']\r\n    assert check(pq_es, f2, es_removed_df)    # [null, '123', '']   -> ['123']\r\n    assert check(pq_sn, f3, non_null_df)      # [null, null, '123'] -> ['123']\r\n    assert check(pq_es, f3, es_contained_df)  # [null, '123', '']   -> ['123', '']\r\n\r\n    #\r\n# case 4: 'not in' and null\r\n    #\r\n    f4 = [('field', 'not in', [None])]\r\n# seems no problem\r\n    assert check(pq_an, f4, empty_df)         # [null]              -> []\r\n    assert check(pq_sn, f4, non_null_df)      # [null, null, '123'] -> ['123']\r\n    assert check(pq_es, f4, es_contained_df)  # [null, '123', '']   -> ['123', '']\r\n\r\n    #\r\n# case 5: half the data are null\r\n    #\r\n# Obviously, these are wrong results.\r\n# It seems this only happens with a parquet which have its statistics metadata and\r\n# just half the data are null.\r\n    #\r\n# Actually, I already have looked into the c++ layer by myself to find a root cause.\r\n    #\r\n#   https://github.com/apache/arrow/blob/d4121d8a17d9e53ad4421960e357dd2f89771603/cpp/src/arrow/dataset/file_parquet.cc#L150\r\n#   > // Optimize for corner case where all values are nulls\r\n#   > if (statistics->num_values() == statistics->null_count()) {\r\n    #\r\n# This compare looks wrong because num_values() returs non-null count.\r\n    assert check(pq_hn, f0, empty_df)     # [null, '123'] -> []\r\n    assert check(pq_hn, f1, empty_df)     # [null, '123'] -> []\r\n    assert check(pq_hn, f2, non_null_df)  # [null, '123'] -> ['123']\r\n    assert check(pq_hn, f3, non_null_df)  # [null, '123'] -> ['123']\r\n    assert check(pq_hn, f4, empty_df)     # [null, '123'] -> []\r\n\r\n```\r\nThe code which make the parquets above as follows. \r\nI also attached the parquets I made just in case.\r\n[read_table_regression.zip](read_table_regression.zip)\r\n```java\n\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport os\r\ndef write_table(table, path):\r\n    pq.write_table(table, path, compression='snappy', use_dictionary=True)\r\ndef main():\r\n    dir_name = 'read_table_regression/'\r\n    os.makedirs(dir_name, exist_ok=True)    schema = pa.schema([('field', pa.string())])    df = pd.DataFrame({'field': [None]})\r\n    table = pa.Table.from_pandas(df, schema=schema)\r\n    write_table(table, f'{dir_name}/all_null.snappy.parquet')    df = pd.DataFrame({'field': [None, None, '123']})\r\n    table = pa.Table.from_pandas(df, schema=schema)\r\n    write_table(table, f'{dir_name}/some_null.snappy.parquet')    df = pd.DataFrame({'field': [None, '123']})\r\n    table = pa.Table.from_pandas(df, schema=schema)\r\n    write_table(table, f'{dir_name}/half_null.snappy.parquet')    df = pd.DataFrame({'field': [None, '123', '']})\r\n    table = pa.Table.from_pandas(df, schema=schema)\r\n    write_table(table, f'{dir_name}/empty_string.snappy.parquet')\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```",
        "created_at": "2020-11-05T12:47:48.000Z",
        "updated_at": "2021-06-08T18:18:03.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-04-22T08:46:43.092Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10501?focusedCommentId=17327179) by Malthe Borch (malthe):*\nI have not tried to run this test case, but at least I can comment that applying a filter \"!=\" with value \"None\" (in Python), yields an empty result even though some rows have a non-null value.\r\n\r\nA common problem is to load in data where a particular column is non-trivial (i.e. not null)."
        },
        {
            "created_at": "2021-06-08T18:18:03.042Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10501?focusedCommentId=17359509) by David Li (lidavidm):*\nI think the unintuitive behavior of Parquet filters with nulls is due to a couple things:\r\n \\* Nulls don't compare equal to anything else. (This explains most of your cases.) Instead, the Dataset expressions syntax has separate checks for null/not null.\r\n \\* The filter tuple/string syntax doesn't support explicit checks for null/not null.\r\n\r\nAs an aside, I think your case 5 has been fixed in the meantime for 4.0 or 5.0. (Sorry for the delay!)\r\n\r\nWe could add support for nulls - either by changing a comparison like `x != null` to `is_valid(x)` or adding an explicit operator for it. `[~jorisvandenbossche]` as this support will get reflected into Pandas, what do you think? (Also, we probably wouldn't support this for the 'legacy' dataset.)\r\n\r\nAlso for case 2, see ARROW-12659 and the PR: <https://github.com/apache/arrow/pull/10253> where these sorts of behaviors are being discussed."
        }
    ]
}