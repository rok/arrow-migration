{
    "issue": {
        "title": "[Python]  Get all columns names (or schema) from Feather file, before loading whole Feather file",
        "body": "***Note**: This issue was originally created as [ARROW-10344](https://issues.apache.org/jira/browse/ARROW-10344). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIs there a way to get all column names (or schema) from a Feather file before loading the full Feather file?\r\n\r\nMy Feather files are big (like 100GB) and the names of the columns are different per analysis and can't be hard coded.\r\n\r\n```python\n\r\nimport pyarrow.feather as feather\r\n\r\n# Code here to check which columns are in the feather file.\r\n...\r\nmy_columns = ...\r\n\r\n# Result is pandas.DataFrame\r\nread_df = feather.read_feather('/path/to/file', columns=my_columns)\r\n\r\n# Result is pyarrow.Table\r\nread_arrow = feather.read_table('/path/to/file', columns=my_columns)\r\n\r\n\r\n```",
        "created_at": "2020-10-19T10:14:19.000Z",
        "updated_at": "2021-06-25T16:10:47.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2020-10-20T06:55:12.407Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17217350) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ghuls]` good question, this is not really well documented. \r\n\r\nThis is possible (at least starting with pyarrow 1.0), but not directly with the `pyarrow.feather` module. \r\n\r\nTwo options:\r\n\r\n1) Since a feather file is basically the IPC serialization format written to a file, you can use the `pyarrow.ipc` functionality to interact with it (see http://arrow.apache.org/docs/python/ipc.html#writing-and-reading-random-access-files).  \r\nSmall example:\r\n\r\n```python\n \r\n# writing a small file\r\nimport pyarrow as pa\r\nfrom pyarrow import feather\r\ntable = pa.table({'a': [1, 2, 3], 'b': [.1, .2, .3]})\r\nfeather.write_feather(table, \"data.feather\")\r\n\r\nIn [13]: import pyarrow.ipc\r\n\r\nIn [14]: reader = pa.ipc.open_file(\"data.feather\")\r\n\r\nIn [15]: reader\r\nOut[15]: <pyarrow.ipc.RecordBatchFileReader at 0x7fe6d3d51798>\r\n\r\nIn [16]: reader.schema\r\nOut[16]: \r\na: int64\r\nb: double\r\n```\r\n\r\n2) Use the new Datasets API (http://arrow.apache.org/docs/python/dataset.html):\r\n\r\n```python\n\r\nIn [17]: import pyarrow.dataset as ds\r\n\r\nIn [18]: dataset = ds.dataset(\"data.feather\", format=\"feather\")\r\n\r\nIn [19]: dataset.schema\r\nOut[19]: \r\na: int64\r\nb: double\r\n\r\nIn [20]: dataset.to_table().to_pandas()\r\nOut[20]: \r\n   a    b\r\n0  1  0.1\r\n1  2  0.2\r\n2  3  0.3\r\n```\r\n\r\nIn addition, this datasets API also allows do directly filter rows using an expression while reading (http://arrow.apache.org/docs/python/dataset.html#filtering-data), and also can read from a collection of (partitioned) files at once.\r\n\r\n\u2014\r\n\r\nFor both options you need Feather version 2 files (https://ursalabs.org/blog/2020-feather-v2/), so if you are already using Feather for a longer time (and have version 1 files), it might be worth to convert those."
        },
        {
            "created_at": "2020-10-20T08:24:01.476Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17217425) by Gert Hulselmans (ghuls):*\nThanks, this looks very helpful.\r\n\r\nI was using Feather v2, but had to switch back to v1, due to a metadata bug when writing Feather v2 files:\r\nhttps://issues.apache.org/jira/browse/ARROW-10056\r\n\r\nIs this pandas metadata very useful to have in my case? My feather files just contain one string column (row indices) and for the rest I have just columns of int16, int32, float32 (all other columns have the same type in one feather file).\r\n\r\nFor filtering the data, is there an easy way to create a filter for thousands of columns?\r\ne.g.: only return rows for which at least one column has a value < 5000?"
        },
        {
            "created_at": "2020-10-20T13:01:09.618Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17217594) by Joris Van den Bossche (jorisvandenbossche):*\n> For filtering the data, is there an easy way to create a filter for thousands of columns? e.g.: only return rows for which at least one column has a value < 5000?\r\n\r\nI don't think we provide a direct way, but with some python utilities, you could construct such a filter. Eg with:\r\n\r\n```Java\n\r\nIn [13]: import pyarrow.dataset as ds\r\n\r\nIn [14]: import operator\r\n\r\nIn [15]: import functools\r\n\r\nIn [16]: expr = functools.reduce(operator.or_, [ds.field(f\"col{i}\") < 5000 for i in range(5000)])\r\n```\r\n\r\nBut I have _no_ idea how that will perform if you use this as a filter (I don't think we really considered such usecase for filtering up to now, so not sure the expressions/filtering code are optimized for a filter with that many columns)"
        },
        {
            "created_at": "2020-10-20T14:21:04.177Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17217629) by Gert Hulselmans (ghuls):*\nThanks.\r\n I can still do the filtering afterwards, so that is not a big problem.\r\n\r\nIs there also something similar than pyarrow.dataset that allows combining multiple feather files with one common column (index, which is in the same order in all feather files) , while the other columns are different. It seems only appending rows is supported by pyarrow.dataset).\r\n\r\n\u00a0\r\n```\n\r\nfeather1:\r\nindex  col1  col2\r\n1\r\n...\r\nn\r\n\r\nfeather2:\r\nindex  col3  col4 col5\r\n1\r\n...\r\nn\r\n\r\nfeather3:\r\nindex  col6  col7 col 8\r\n1\r\n...\r\nn\r\n\r\n\r\nread feather1,2,3 as one combined table:\r\nindex  col1 col2 col3 col4 col5 col6  col7 col 8\r\n1\r\n...\r\nn\r\n```"
        },
        {
            "created_at": "2020-10-20T20:37:42.678Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17217917) by Joris Van den Bossche (jorisvandenbossche):*\nNo, there is no way to concat horizontally with the datasets API. \r\n\r\nFrom your descriptions and the type of data, I am wondering if Feather is actually a suited file format. It seems you have a big array like data (and numeric data, but with labeled dimensions), which is potentially very wide, and which you would ideally chunk both on the rows as the columns. For something like that, it might be interesting to look at the zarr format (https://zarr.readthedocs.io/en/stable/index.html, it was originally started by someone working on genome data, but is now also becoming popular in geoscience/climate data that historically uses a lot of NetCDF)"
        },
        {
            "created_at": "2020-10-21T08:34:59.435Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17218187) by Gert Hulselmans (ghuls):*\nWe need the final data to be readable from Python and R, so Feather looked like a good choice.\r\n\r\nTo create the dataset the data is generated:\r\n  - 30k columns with 1M entries (data is generated separately for each of the 30k columns):\r\n       - This first part I had split up before so each feather file (of 10 files)  had 3k columns (I can transpose those so I can used the dataset API).\r\n  - transpose data ==> 1M columns with 30k entries\r\n\r\nTransposed data needs to be usable from python and R: around 20k columns (all 30k values) from 1M are extracted in each analysis.\r\n"
        },
        {
            "created_at": "2020-11-04T12:00:31.670Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17226013) by Gert Hulselmans (ghuls):*\nCould reading the metadata schema for Feather v1 also be supported as for now I am required to have the files in v1 format due to https://issues.apache.org/jira/browse/ARROW-10056 or are there plans to fix that soon?"
        },
        {
            "created_at": "2020-11-11T16:51:24.689Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17230091) by Joris Van den Bossche (jorisvandenbossche):*\n> .. or are there plans to fix that soon?\r\n\r\nSee my new comment on that issue. TLDR I don't think it is solvable in general.\r\n\r\n> Could reading the metadata schema for Feather v1 also be supported\r\n\r\nI think that is technically certainly possible. The C++ Reader interface already exposes a `schema()` function, but this is not exposed in Python. I suppose also for V2 this would be nice to have in the `pyarrow.feather` module.\r\n\r\n> We need the final data to be readable from Python and R, so Feather looked like a good choice.\r\n\r\nThat's indeed one of the selling points of Feather, and I also didn't find any up to date R interface for zarr.   \r\nI think it might still be worth looking for other options (giving the inherent limitation for V2 mentioned above). I don't have any experience with it myself, but might also be worth taking a look at TileDB.\r\n\r\nIf you want to stay with arrow/feather files, one other alternative is to use a \"trick\" of putting all columns (of the same type) in a FixedSizeList column (the data under the hood is then stored in a contiguous array, which can be easily \"viewed\" as a 2D array). However, then you can no longer read only a subset of the columns, which might be an important use case.\r\n\r\n\r\n\r\n\r\n"
        },
        {
            "created_at": "2021-01-18T19:52:20.747Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17267494) by al-hadi boublenza (weldingwelding):*\nFacing the same issue and wondering how to know if you're dealing with a Feather V1 or Feather V2 file? (Using\u00a0pyarrow)"
        },
        {
            "created_at": "2021-02-01T13:33:22.414Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17276321) by Gert Hulselmans (ghuls):*\n`[~weldingwelding]` The first 4/6 bytes (and last 4/6 bytes) of the Feather file would tell you. For example, you can check it with `hexdump`.\r\n\r\n```bash\n\r\n \u276f hexdump -C -n 8 feather_version1.feather\r\n 00000000 46 45 41 31 00 00 00 00 |FEA1....|\r\n 00000008\r\n\r\n\u276f hexdump -C -n 8 feather_version2.feather\r\n 00000000 41 52 52 4f 57 31 00 00 |ARROW1..|\r\n 00000008\r\n```\r\n\r\n```python\n\r\n def feather_v1_or_v2(feather_file):\r\n with open(feather_file, 'rb') as fh_feather:\r\n fh_feather.seek(0, 0)\r\n feather_v1_magic_bytes_header = fh_feather.read(4)\r\n fh_feather.seek(-4, 2)\r\n feather_v1_magic_bytes_footer = fh_feather.read(4)\r\n\r\nif feather_v1_magic_bytes_header == feather_v1_magic_bytes_footer == b'FEA1':\r\n return 1\r\n\r\nfh_feather.seek(0, 0)\r\n feather_v2_magic_bytes_header = fh_feather.read(6)\r\n fh_feather.seek(-6, 2)\r\n feather_v2_magic_bytes_footer = fh_feather.read(6)\r\n\r\nif feather_v2_magic_bytes_header == feather_v2_magic_bytes_footer == b'ARROW1':\r\n return 2\r\n\r\nreturn None\r\n```\r\n\r\n`[~jorisvandenbossche]` Now that https://issues.apache.org/jira/browse/ARROW-10056 is resolved, Feather v1 support is less critical for me. so the IPC and dataset API workaround are now useful for me. It still would be good to have Feather v1 support and exposure of the columns in the feather submodule directly."
        },
        {
            "created_at": "2021-02-10T17:23:04.014Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17282589) by Gert Hulselmans (ghuls):*\nI figured out a way to get the column names for a feather v1 and v2 file.\r\n\r\nFor getting the info from a Feather v1 file, I used the flatbuffer file:\r\n```bash\n\r\n# Download FlatBuffer Feather v1 schema.\r\nwget https://github.com/apache/arrow/raw/master/cpp/src/arrow/ipc/feather.fbs\r\n\r\n# Create python code for FlatBuffer Feather v1 schema.\r\nflatc --python feather.fbs\r\n```\r\n```python\n\r\nimport numpy as np\r\nimport pyarrow.dataset as ds\r\n\r\n# wget https://github.com/apache/arrow/raw/master/cpp/src/arrow/ipc/feather.fbs\r\n# flatc --python feather.fbs\r\nimport feather.fbs.CTable as feather_v1_fbs\r\n\r\n\r\ndef is_feather_v1_or_v2(feather_file):\r\n    with open(feather_file, 'rb') as fh_feather:\r\n# Read first 6 and last 6 bytes to see if we have a Feather v2 file.\r\n        fh_feather.seek(0, 0)\r\n        feather_v2_magic_bytes_header = fh_feather.read(6)\r\n        fh_feather.seek(-6, 2)\r\n        feather_v2_magic_bytes_footer = fh_feather.read(6)\r\n\r\n        if feather_v2_magic_bytes_header == feather_v2_magic_bytes_footer == b'ARROW1':\r\n           return 2\r\n\r\n# Read first 4 and last 4 bytes to see if we have a Feather v1 file.\r\n        feather_v1_magic_bytes_header = feather_v2_magic_bytes_header[0:4]\r\n        feather_v1_magic_bytes_footer = feather_v2_magic_bytes_footer[2:]\r\n\r\n        if feather_v1_magic_bytes_header == feather_v1_magic_bytes_footer == b'FEA1':\r\n            return 1\r\n\r\n    return None\r\n\r\n\r\ndef get_column_names_from_feather(feather_file):\r\n    feather_v1_or_v2 = is_feather_v1_or_v2(feather_file)\r\n\r\n    if feather_v1_or_v2 == 1:\r\n        with open(feather_file, 'rb') as fh_feather:\r\n            fh_feather.seek(-8, 2)\r\n\r\n# Get Feather v1 metadata length.\r\n            metadata_length = np.frombuffer(fh_feather.read(4), dtype=np.int32)[0]\r\n\r\n# Read Feather v1 metadata.\r\n            fh_feather.seek(- (metadata_length +  8), 2)\r\n            feather_metadata = feather_v1_fbs.CTable.GetRootAsCTable(bytearray(fh_feather.read(metadata_length)), 0)\r\n\r\n            num_columns = feather_metadata.ColumnsLength()\r\n\r\n            column_names = [\r\n                feather_metadata.Columns(column_idx).Name().decode('utf-8')\r\n                for column_idx in range(0, num_columns)\r\n            ]\r\n    elif feather_v1_or_v2 == 2:\r\n        feather_v2_dataset = ds.dataset(feather_file, format=\"feather\")\r\n        column_names = feather_v2_dataset.schema.names\r\n    else:\r\n        return None\r\n\r\n    return column_names\r\n```\r\n```python\n\r\nIn [7]: is_feather_v1_or_v2('test/ct_rankings_db_genes_vs_tracks.feather_version1.genes_vs_tracks.rankings.feather')\r\n   ...:\r\nOut[7]: 1\r\n\r\nIn [8]: is_feather_v1_or_v2('test/ct_rankings_db_genes_vs_tracks.feather_version2.genes_vs_tracks.rankings.feather')\r\nOut[8]: 2\r\n\r\nIn [9]: get_column_names_from_feather('test/ct_rankings_db_genes_vs_tracks.feather_version1.genes_vs_tracks.rankings.feather')\r\nOut[9]: ['gene1', 'gene2', 'gene3', 'gene4', 'gene5', 'gene6', 'gene7', 'tracks']\r\n\r\nIn [10]: get_column_names_from_feather('test/ct_rankings_db_genes_vs_tracks.feather_version2.genes_vs_tracks.rankings.feather')\r\nOut[10]: ['gene1', 'gene2', 'gene3', 'gene4', 'gene5', 'gene6', 'gene7', 'tracks']\r\n```"
        },
        {
            "created_at": "2021-03-26T10:32:35.611Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17309339) by Gert Hulselmans (ghuls):*\n`[~jorisvandenbossche]` Are there plans to add reading column information from Feather v1 files before loading them from Python (and R (didn't look at it yet, but we need this support in R too))."
        },
        {
            "created_at": "2021-06-25T16:10:47.802Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10344?focusedCommentId=17369567) by Gert Hulselmans (ghuls):*\nCombined the above snippets in a cleaner way:\r\nhttps://github.com/aertslab/create_cisTarget_databases/commit/dcf70e60e915d2dc6850343960e7a7d3d3d56c41"
        }
    ]
}