{
    "issue": {
        "title": "[Rust][DataFusion] Refactor scan nodes to allow extensions",
        "body": "***Note**: This issue was originally created as [ARROW-10368](https://issues.apache.org/jira/browse/ARROW-10368). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe first intention of this issue was to refactor InMemoryScan to use an iterator to make it more flexible:\r\n\r\n> Currently, InMemoryScan takes a Vec<Vec<RecordBatch>> as data.\n> - the outer Vec separates the partitions\n> - the inner Vec contains all the RecordBatch for one partition\n>   The inner Vec is then converted into an iterator when the LogicalPlan is turned into a PhysicalPlan.\n> \n> I suggest that InMemoryScan should take Vec<Iter<RecordBatch>>.  This would make it possible to plug custom Scan implementations into datafusion without the need to read them entirely into memory. It would still work pretty seamlessly with Vec<Vec<RecordBatch>> that would just need a to be converted with data.map(|x| x.iter()) first.\r\n\r\nAfter further inspection (see discussion below), it seems more appropriate to completely refactor the way scan nodes are organized. The idea is to replace all specific XxxScan nodes with a generic SourceScan node:\r\n\r\n```java\n\r\n/// A node that generates source data\r\nLogicalPlan::SourceScan {\r\n    /// A shared reference to the source implementation\r\n    scanner: Arc<dyn SourceScanner>,\r\n},\r\n```\r\n\r\nwith:\r\n\r\n```java\n\r\n#[async_trait]\r\n/// A scanner implementation that can be used by datafusion\r\npub trait SourceScanner: Send + Sync + fmt::Debug {\r\n\r\n  /// reference to the schema of the data as it will be read by this scanner\r\n  fn projected_schema(&self) -> &SchemaRef;\r\n\r\n  /// string display of this scanner\r\n  fn format(&self) -> &str;\r\n\r\n  /// apply projection on this scanner\r\n  fn project(\r\n    &self,\r\n    required_columns: &HashSet<String>,\r\n    has_projection: bool,\r\n  ) -> Result<Arc<dyn SourceScanner>>;\r\n\r\n  /// get scanner partitioning\r\n  fn output_partitioning(&self) -> Partitioning;\r\n\r\n  /// get iterator for a given partition\r\n  async fn execute(&self, partition: usize) -> Result<Box<dyn RecordBatchReader + Send>>;\r\n\r\n}\r\n```\r\n\r\nThe current specific implementations of scanner will then be provided by implementations of SourceScanner.\r\n\r\n\r\n",
        "created_at": "2020-10-22T11:24:43.000Z",
        "updated_at": "2020-10-26T07:35:41.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Rust",
            "Component: Rust - DataFusion",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2020-10-26T07:35:41.000Z"
    },
    "comments": [
        {
            "created_at": "2020-10-22T11:29:55.124Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17218952) by R\u00e9mi Dettai (rdettai):*\n`[~andygrove]` If this change seems reasonable to you, I can give it a try! I wonder if we could not go one step further and try to add a new logical plan that makes it possible to add custom sources. This would make it possible to also have access to the projection info..."
        },
        {
            "created_at": "2020-10-22T16:20:42.379Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219142) by R\u00e9mi Dettai (rdettai):*\n>  I wonder if we could not go one step further and try to add a new logical plan that makes it possible to add custom sources\r\n\r\nThis is definitively outside the scope of the original ticket, but here is what I came up with for the custom source logical plan:\r\n\r\n```java\n\r\n/// Produces rows from a custom user implementation\r\nLogicalPlan::CustomScan {\r\n    /// A shared reference to the custom implementation\r\n    scanner: Arc<dyn CustomScanner>,\r\n},\r\n```\r\n\r\nwith:\r\n\r\n```java\n\r\n#[async_trait]\r\n/// A user implemented scanner that can be used by datafusion\r\npub trait CustomScanner: Send + Sync + fmt::Debug {\r\n  /// reference to the schema of the data as it will be read by this scanner\r\n  fn projected_schema(&self) -> &SchemaRef;\r\n  /// string display of this scanner\r\n  fn format(&self) -> &str;\r\n  /// apply projection on this scanner\r\n  fn project(\r\n    &self,\r\n    required_columns: &HashSet<String>,\r\n    has_projection: bool,\r\n  ) -> Result<Arc<dyn CustomScanner>>;\r\n  /// get scanner partitioning\r\n  fn output_partitioning(&self) -> Partitioning;\r\n  /// get iterator for a given partition\r\n  async fn execute(&self, partition: usize) -> Result<Box<dyn RecordBatchReader + Send>>;\r\n}\r\n```\r\n\r\nI am now wondering if we shouldn't make this the common interface for all LogicalPlan::XxxScan rather than having them in the enum.\r\n\r\nI need guidance on the best way to achieve some kind of flexibility in that API. In the end, I don't even want an exotic data source, just to read parquet from S3 and the current parquet LogicalPlan/ExecutionPlan is very fs oriented.\r\n\r\n"
        },
        {
            "created_at": "2020-10-22T17:08:58.202Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219181) by R\u00e9mi Dettai (rdettai):*\n_I know what this is: It's just myself talking to myself about myself_ :D\r\n\r\nI just came accross ExtensionPlanner and LogicalPlan::Extension. This is very nice, and is pretty similar to what I just proposed. But it seems a bit WIP as there is no \"nominal condition\" test path, only failure tests. On top of that, Extension is not capable of receiving projection push down. "
        },
        {
            "created_at": "2020-10-22T17:28:30.043Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219210) by R\u00e9mi Dettai (rdettai):*\nIf I summarize all of the above, these are the paths I could take to implement my S3 parquet datasource:\r\n- modify InMemoryScan to use iterators of RecordBatch. This iterator can be provided by a s3 reader or any other datasource. **No way to have projection pushdown here.**\n- use ExtensionPlanner and LogicalPlan::Extension to implement a new logical and execution plan. **No way to have projection pushdown here.**\n- extend the current ParquetExec with a specific code path if the filename starts with \"s3://\". I am not sure how this could be done without bringing in the dependency to s3 right in the middle of datafusion, which would definitively not scale.\n- replace all the LogicalPlan::XxxScan by a single LogicalPlan::SourceScan (equivalent to the LogicalPlan::CustomScan above) that dynamically dispatches to any source implementation.\n  \n  Last solution seems to be the best, but I'm curious to have your opinions !"
        },
        {
            "created_at": "2020-10-23T02:52:57.869Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219426) by Jorge Leit\u00e3o (jorgecarleitao):*\n`[~rdettai]`, great proposal and comments.\r\n\r\nIMO the last item, for the same reasons that you concluded :)\r\n\r\na scan is very fundamental and has a different semantics than the extension, which was designed to be a generic compute node.\r\n1. check what is the common pattern of each Scan node (both logical and physical)\n1. check what differences it has vs e.g. s3\n1. abstract the pattern out to a new generic logical node\n1. migrate all scanners to it\n1. implement s3 on top of the new pattern\n   \n   Note that the code now expects a stream, not an iterator, of RecordBatch, which may help at reading s3 sources.\n   \n   As it stands, this is a large task, so, let us know if you any help. One idea is to have a PR only with the new interface (and only\u00a0`todo!` on the implementation) for the new node, so that we can all go through, before committing the bulk of the work."
        },
        {
            "created_at": "2020-10-23T09:02:30.844Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219558) by R\u00e9mi Dettai (rdettai):*\n`[~jorgecarleitao]` thanks for looking into this. It's so nice not to be talking alone :D\r\n\r\nI have renamed the issue and changed the description to better relate the new direction this issue is taking.\r\n\r\nI am going to open a PR with the new abstraction, and we can move this very technical question over there. I think I can handle the change, but I will definitively need your opinions on various aspect.\r\n> Note that the code now expects a stream, not an iterator, of RecordBatch, which may help at reading s3 sources.\r\nNot sure that helps a lot, as the parquet reader still requires you to plunge back to sync world (ARROW-10307 is really really hard)"
        },
        {
            "created_at": "2020-10-23T14:34:01.705Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-10368?focusedCommentId=17219723) by R\u00e9mi Dettai (rdettai):*\nJust found out the addition of custom sources can be done with the `TableProvider` trait. See [explanations](https://github.com/apache/arrow/pull/8513)."
        }
    ]
}