{
    "issue": {
        "title": "Selective reading of rows for parquet file",
        "body": "***Note**: This issue was originally created as [ARROW-13517](https://issues.apache.org/jira/browse/ARROW-13517). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThe current interface for selective reading is to use\u00a0**filters**\u00a0<https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html>\r\n\r\nThe approach works well when the filters are simple (field in (v1, v2, v3, \u2026), and when the number of columns in small. It does not work well for the folllowing conditions, which currently requires reading the complete set into (python) memory.\r\n \\* when condition is complex (e.g. condition between attributes: field1 + field2 > filed3)\r\n \\* When file as many columns (making it costly to create python structures).\r\n\r\nI have a repository of large number of parquet files (thousands of files, 500 MB each, 200 \u00a0column), where specific records had to be selected quickly based on logical condition that does not fit the filter condition. Very small numbers of rows (<500) have to be returned.\r\n\r\nProposed feature is to aextend read_row_group to support passing an array of rows to read (list of integer in ascending order).\u00a0\r\n```java\n\r\npq =  pyarrow.parquet.ParquetFile(\u2026)\r\ndd = PY.read_row_group(\u2026, rows=[ 5, 35, \u2026. ]\n```\r\nUsing this method will enable complex filtering in two stages, eliminitating the need to read all rows into memory.\r\n1. First pass - read attributes for filtering, collect row numbers that match (complex) condition.\n1. second pass - create a python table with matching rows using the proposed rows= parameter to read row group.\n   \n   I believe possible to achieve something similar using the c++ stream_reader (<https://github.com/apache/arrow/blob/master/cpp/src/parquet/stream_reader.cc>), which is not exposed to python.",
        "created_at": "2021-08-01T10:40:05.000Z",
        "updated_at": "2021-08-05T10:22:29.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Parquet",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-08-02T19:06:33.678Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13517?focusedCommentId=17391771) by Weston Pace (westonpace):*\n> when condition is complex (e.g. condition between attributes: field1 + field2 > filed3)\r\n\r\nThis is currently possible using the new datasets API and expressions:\r\n\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as pads\r\n\r\ntable = pa.Table.from_pydict({'x': [1, 2, 3, 4, 5, 6], 'y': [-2, -2, -2, -2, -2, -2], 'z': [-10, 0, 0, -10, 0, 0]})\r\npq.write_table(table, '/tmp/foo.parquet')\r\n\r\nds = pads.dataset('/tmp/foo.parquet')\r\ntable2 = ds.to_table(filter=(pads.field('x') + pads.field('y')) > pads.field('z'))\r\nprint(table2.to_pydict())\r\n```\r\n\r\n> When file as many columns (making it costly to create python structures).\r\n\r\nIt's not clear to me what is expensive here.  You mention there are 200 columns.  Both the old and new approaches should be pretty workable manipulating metadata of 200 columns in python.\r\n\r\n> I believe possible to achieve something similar using the c++ stream_reader\r\n\r\nYou might be able to achieve some benefits in some situations by skipping data.  In general this is a difficult problem with Parquet.  For example, compressed pages typically need to be fully read into memory and can't support any kind of skipping.  Furthermore, many pages are going to use run length encoding, which makes it impossible to skip to a particular value.  As a general rule of thumb you cannot do a partial read of a parquet page.  So if a page has 100k rows in it and you only want 10 rows out of it then, even if you have the indices, you typically need to read the page into memory first.\r\n\r\n"
        },
        {
            "created_at": "2021-08-05T03:20:06.890Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13517?focusedCommentId=17393579) by Yair Lenga (Yair.lenga):*\nThanks for pointing to the new data set API. For my situation (reading small number of rows from a large data set) - I believe it be beneficial if the above will be implemented. In particular two benefits:\r\n \\* Queries result (e.g., list of rows matching a condition) - can be cached, and reused to re-load data without having to perform linear scans over complete data set.\r\n \\* The C++ Stream API seems to support both skipping over row groups, and skipping over column chunks. This can potentially reduce reading by significant factor when recalling data for queries that have been processed in the past.\r\n\r\nHow hard it will be to build this logic into Python to realize above saving ? While it might not be trivial to implement - for certain cases it will be extremely valuable.\r\n\r\nI believe that the AWS S3 select (<https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-select.html)>\u00a0\u00a0has similar capabilities - as it can deliver results (for similar situation like I've describe) - I get much faster performance than the performance I see on my desktop Python - leading me to believe that they figure out a way to selectivly skip of parquest\u00a0 data quickly."
        },
        {
            "created_at": "2021-08-05T04:01:53.491Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13517?focusedCommentId=17393593) by Weston Pace (westonpace):*\n> Queries result (e.g., list of rows matching a condition) - can be cached, and reused to re-load data without having to perform linear scans over complete data set.\r\n\r\nI don't think you'd be able to reduce the amount of I/O actually required since you'd need to load whole row-groups / column-chunks either way.  I do agree that you'd reduce the computational load somewhat as handling indices should be simpler than applying a query.  However, if you're doing actual I/O this is likely to be a fraction of the total cost and if you're working on in-memory data you'd be better off memory mapping an IPC file.\r\n\r\n> The C++ Stream API seems to support both skipping over row groups, and skipping over column chunks. This can potentially reduce reading by significant factor when recalling data for queries that have been processed in the past.\r\n\r\nThe datasets API supports both of these types of skips.\r\n\r\n> I believe that the AWS S3 select has similar capabilities\r\n\r\nFrom (https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html) I see \"For Parquet objects, all of the row groups that start within the scan range requested are processed.\" so I believe it is doing the same column-chunk and row-group skipping that the dataset API currently supports.  There will be an advantage with S3 select as the parquet metadata is read and processed in the data center although I am not sure how much of a difference that will make.\r\n\r\n> I get much faster performance than the performance I see on my desktop Python\r\n\r\nCan you describe what you are trying on your desktop python and what s3 select you are performing to get the similar results?\r\n\r\n> How hard it will be to build this logic into Python to realize above saving ? While it might not be trivial to implement - for certain cases it will be extremely valuable.\r\n\r\nGiven that you are going to have to load the entire column chunk into memory either way you could probably do this in pure python using the compute module with something like...\r\n\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.compute as pc\r\nimport pyarrow.parquet as pq\r\n\r\ntable = pa.Table.from_pydict({'x': [1, 2, 3], 'y': ['x', 'y', 'z']})\r\npq.write_table(table, '/tmp/foo.parquet')\r\n\r\npfile = pq.ParquetFile('/tmp/foo.parquet')\r\nrow_group = pfile.read_row_group(0)\r\n# Assuming you want indices 0,2.  This is just an example\r\nrow_group.take([0, 2])\r\n```"
        },
        {
            "created_at": "2021-08-05T10:22:29.210Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-13517?focusedCommentId=17393784) by Yair Lenga (Yair.lenga):*\nThanks for taking the time to provide good feedback.\n\nYou are correct that there is something \"wrong\" with my local box. I suspect that I am running out of actual memory (low End free AWS instance), resulting in actual IO/swap, whereas the S3 select is not short on resources.\n\nRunning on a \"fresh\" instance solve make the processing noticeably better.\n\nThanks again for your patience. Yair"
        }
    ]
}