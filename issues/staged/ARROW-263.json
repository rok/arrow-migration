{
    "issue": {
        "title": "Design an initial IPC mechanism for Arrow Vectors",
        "body": "***Note**: This issue was originally created as [ARROW-263](https://issues.apache.org/jira/browse/ARROW-263). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nPrior discussion on this topic [1].\n\nUse-cases:\n1.  User defined function (UDF) execution:  One process wants to execute a user defined function written in another language (e.g. Java executing a function defined in python, this involves creating Arrow Arrays in java, sending them to python and receiving a new set of Arrow Arrays produced in python back in the java process).\n2.  If a storage system and a query engine are running on the same host we might want use IPC instead of RPC (e.g. Apache Drill querying Apache Kudu)\n\nAssumptions:\n1.  IPC mechanism should be useable from the core set of supported languages (Java, Python, C) on POSIX and ideally windows systems.  Ideally, we would not need to add dependencies on additional libraries outside of each languages outside of this document.\nWe want leverage shared memory for Arrays to avoid doubling RAM requirements by duplicating the same Array in different memory locations.  \n2. Under some circumstances shared memory might be more efficient than FIFOs or sockets (in other scenarios they won\u2019t see thread below).\n3. Security is not a concern for V1, we assume all processes running are \u201ctrusted\u201d.\n\nRequirements:\n1.Resource management: \n    a.  Both processes need a way of allocating memory for Arrow Arrays so that data can be passed from one process to another.\n    b. There must be a mechanism to cleanup unused Arrow Arrays to limit resource usage but avoid race conditions when processing arrays\n2.  Schema negotiation - before sending data, both processes need to agree on schema each one will produce.\n\nOut of scope requirements:\n1.  IPC channel metadata discovery is out of scope of this document.  Discovery can be provided by passing appropriate command line arguments, configuration files or other mechanisms like RPC (in which case RPC channel discovery is still an issue).\n\n[1] http://mail-archives.apache.org/mod_mbox/arrow-dev/201603.mbox/%3C8D5F7E3237B3ED47B84CF187BB17B666148E7322@SHSMSX103.ccr.corp.intel.com%3E\n",
        "created_at": "2016-08-19T01:52:15.000Z",
        "updated_at": "2022-08-27T14:41:44.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2019-06-03T15:31:17.000Z"
    },
    "comments": [
        {
            "created_at": "2016-08-19T02:53:34.074Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=15427516) by Micah Kornfield (emkornfield@gmail.com):*\nTL;DR:  Current rough proposal/thoughts:\n1.  used memory mapped files (clients of the library should use a directory backed by an in memory file system, e.g. TMPFS, but for debugging it could be useful to use traditional file system).  \n2.  An initial version of IPC will focus on one way, producer->consumer channel.  UDF execution can be handled by creating two channels (one in each direction from the process).\n3.  It is still an open question, but trying to do IPC purely though shared memory seems like a more difficult approach compared to a traditional RPC (e.g. thrift/GRPC) but more complex.\n\n\nMore in depth analysis based on my research:\n\nOn POSIX systems there are a two core APIs to consider from the C++ implementation side:\n1.  Shared Memory APIs [1]:  These create shared memory objects.  Shared memory objects are named and persist after a process terminates (but not after a system restart).  The APIs for manipulating shared memory return a file descriptor that is MMAPPED.\n2.  MMAP APIs [2]: These take either a file descriptor (either shared memory descriptor or a traditional file descriptor) and map the contents of the object into the processes memory space (an option exists that is anonymous MMAP, but this is only useful for sharing memory between forked processes, that don't run execve, which doesn't conform to our use-case).\n\n \"Shared memory\" vs traditional file system:\n1.  From what I can tell (experiments needed), memory mapping a file created on tmpfs should have identical performance to mapping shared memory object.\n2.  Both approaches suffer from the fact that if all processes crash for some reason there will be garbage left over that consumes resources\\* (if a file is stored on a memory backed file system then it will get collected on a system reset).\n3.  Java doesn't support shared memory natively.  It does support memory mapping files.  The behavior for when other processes see changes to files is undefined, but based on various articles it seems that at least on linux, setting up the file as read/write should be suitable for RPC (without having to call sync repeatedly)\n\n\\*In C/C++ is theoretically possible open a file (get a file descriptor) then unlink the file, then send the file descriptor over a Unix domain socket to another process.  Once all processes using the file descriptor exist all resources should get cleaned up.  Java doesn't support unix domain sockets in its core library and this solution is less likely to be as easily portable to non-posix operating systems.\n\n[1] http://man7.org/linux/man-pages/man3/shm_open.3.html\n[2] http://man7.org/linux/man-pages/man2/mmap.2.html"
        },
        {
            "created_at": "2016-08-19T05:01:27.703Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=15427632) by Philipp Moritz (pcmoritz):*\nHey Micah,\n\nthanks for your insights, seems we have been thinking along very similar lines. I wrote one shared memory object store based on boost::IPC, which under the hood uses shm_open, one based on Chrome's Mojo IPC system (which is great but pulls in a lot of dependencies) and currently I am writing a new very lightweight one based on POSIX. I'm happy to share my findings:\n\nAnswering your in-depth analysis questions:\n1. This is essentially the design of boost::IPC and this design makes it hard to ensure that named shared objects are removed properly (they cannot be removed automatically by the OS). If the process that is supposed to clean them up crashes, you will need to delete them manually.\n\n2. Using mmaped based APIs is the way to go in my opionion. To create the file, you create a temporary file of size 0, keep the file descriptor around, unlink the file from the file system, and manipulate the file by mmaping it from the file descriptor. If you want to share the file with another process, you pass the file descriptor over a Unix Domain Socket. On Windows there also seem to be ways of doing this.\n\n3. I also found the guarantees given by the JVM too restrictive. One way to do it which is working for me is to use JNI and to wrap some native C code. The alternative is to use JVM memory mapped files anyways and to hope they do the right thing (which they seem to do on linux), and keep the file underlying the memory around (with similar problems as 1).\n\nYou find my implementation here: https://github.com/pcmoritz/plasma, it essentially implements what you describe in \\*. I'd love to get your feedback, and if we can share some code that would be even better. I also think that Mojo (Google Chrome's new IPC layer) got this \"open file of size 0, unlinke the file, keep the file descriptor around\" approach working on Windows, so this should be ok.\n\nA bit more about plasma: It allows one process to create a shared memory object, modify it, and then at some point seal it, which makes it immutable. Other processes can \"get\" the objects, which will block the getting thread until the object is sealed by the creator process, and then return address and size of the object.\n\nThe central question is then if people are ok with using JNI for java to implement passing filedescriptors around and mmapping the files. I have some code to do this which I'm happy to share with you in the next couple of days.\n\nAll the best,\nPhilipp."
        },
        {
            "created_at": "2016-08-19T18:50:40.080Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=15428637) by Micah Kornfield (emkornfield@gmail.com):*\n`[~pcmoritz]` this looks cool.  I think JNI might be a requirement here to guarantee correctness, but I will let people doing more of the JVM development chime in.  Hopefully, somebody who develops on windows can add the equivalent work.  \n\nUnix domain sockets also seem to suffer from the same resource cleanup issue if we want to be portable to non-linux POSIX systems.  The man-page [1] indicates normal unix domain sockets need a file path as well or you can use abstract sockets, but those are linux specific.  I see in your code you unlink the path as part of starting the server, so it seems maybe its ok for the path not to exist for the client programs to connect correctly?\n\nOne other thought.  It might nice to be able to re-use FD after the data is no longer used by either process.  Assuming roughly even size batches, I think think could avoid the page faults that would occur by creating a new shared memory/object file each time.\n\n[1] http://man7.org/linux/man-pages/man7/unix.7.html"
        },
        {
            "created_at": "2016-08-22T01:43:26.016Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=15429948) by Philipp Moritz (pcmoritz):*\nHey Micah,\n\nthanks for your answer!\n\nI got the trick of unlinking the domain socket from here: https://troydhanson.github.io/network/Unix_domain_sockets.html (\"Unlink before bind\"). On Linux and Mac OS it seems to work and prevents leaking of the file. Note that at some point we need to introduce a named object that can be seen by all processes to bootstrap the communication between processes and this has been the least problematic way of doing that I have seen.\n\nAt the moment I'm also working on a distributed version of the object store (with a separate process that can be used to ship objects between object stores on different nodes in a network) and investigating libuv to do it in a platform independent way. Libuv is a small dependency and my experience so far is pretty enjoyable. It also includes limited functionality to exchange file descriptors, but this might not work on windows (see also https://groups.google.com/forum/#!msg/libuv/0xxXBIGlzLc/H1HbL-igb84J, I haven't tried it yet).\n\nConcerning your last comment: The plasma store is a long running process that keeps its file descriptor and the data alive. Are page faults still a problem if data does not need to be reloaded from hard disk?\n\nIf somebody else has a platform independent way of achieving some of these goals, I'd be happy to learn about their ideas.\n"
        },
        {
            "created_at": "2016-08-22T04:31:30.950Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=15430029) by Micah Kornfield (emkornfield@gmail.com):*\nRegarding the unix domain sockets.  Yes I think you are right that there needs to be a centralized named file to so that communication can be bootstrapped. \n\nThere are two types of page faults.  Minor and Major.  Minor ones are when the pages are in memory but not mapped properly the address space of the new process yet and major ones are when the page isn't in memory at all and needs to be loaded mapped [1].  If I understood Todd Lipcon on the thread mentioned above when minor page faults are accounted for, the latency of shared memory might not be better then just transferring over unix domain sockets.\n\nI think I have a workable design for IPC now in my head, but not sure that I will get it documented tonight, and will likely  be Away from keyboard for at least a couple of days.  \n\n[1] https://en.wikipedia.org/wiki/Page_fault#Minor"
        },
        {
            "created_at": "2019-06-03T12:18:24.300Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=16854525) by Antoine Pitrou (apitrou):*\nShould this be kept open? It looks essentially like a brain dump and no discussion took place for the last 3 years."
        },
        {
            "created_at": "2019-06-03T15:31:03.345Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=16854722) by Micah Kornfield (emkornfield@gmail.com):*\nI thin it can be closed."
        },
        {
            "created_at": "2022-08-27T14:41:44.934Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-263?focusedCommentId=17585822) by @toddfarmer:*\nTransitioning issue from Resolved to Closed to based on resolution field value."
        }
    ]
}