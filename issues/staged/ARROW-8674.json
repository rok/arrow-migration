{
    "issue": {
        "title": "[JS] Implement IPC RecordBatch body buffer compression from ARROW-300",
        "body": "***Note**: This issue was originally created as [ARROW-8674](https://issues.apache.org/jira/browse/ARROW-8674). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nThis may not be a hard requirement for JS because this would require pulling in implementations of LZ4 and ZSTD which not all users may want",
        "created_at": "2020-05-02T23:04:14.000Z",
        "updated_at": "2022-05-09T22:57:45.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: JavaScript",
            "Type: task"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2021-01-17T18:56:35.268Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17266879) by eric mauviere (icem7):*\nI strongly suport this because if library size is a concern (but a small one), loading a 10 mb lz4 compressed arrow file, rather than a 100 mb one, is much more crucial!"
        },
        {
            "created_at": "2021-04-25T00:29:57.851Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17331343) by Dominik Moritz (domoritz):*\nYes, but at the same time someone might want to use Arrow with a small file. I don't think we want to increase the bundle size for everyone. I would prefer an optional (external) module instead if the bundle size increases significantly. I think I would like to see some numbers (file sizes) before making the call one way or another. "
        },
        {
            "created_at": "2022-04-15T19:39:02.300Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17522941) by Kyle Barron (kylebarron2):*\nHello! I'd like to revisit this issue and potentially submit a PR for this.\r\n\r\nI think there are various reasons why we might not want to pull in LZ4 and ZSTD implementations by default:\r\n \\* Bundle-size conscious users who don't want any codecs, or who might not use the arrow IPC features at all. The WASM codecs in [numcodecs.js](https://github.com/manzt/numcodecs.js) appear to be 17.1KB for LZ4 and 206KB for ZSTD (uncompressed).\r\n \\* Some users may prefer dynamically importing codecs as required but this requires a slightly more complex setup (at least it requires choosing a CDN from which to import the bundle, right?)\r\n \\* I came across at least 4 LZ4 implementations and at least 6 ZSTD implementations. It could be better to leave to the user the choice of which implementation to use. If the user is using one implementation in their app already, then allowing the user to choose the same implementation in Arrow JS would reduce their bundle size.\r\n \\* At least one LZ4 implementation is in [pure JS](https://github.com/Benzinga/lz4js), with no WASM components. Some users may prefer a pure JS library for simplicity.\r\n\r\nHow would others feel about a codec registry system? Something like what [Zarr.js allows](http://guido.io/zarr.js/#/installation?id=zarrjs-core-export), where you can [dynamically register codecs](https://github.com/gzuidhof/zarr.js/blob/29280463ff2f275c31c1fa0f002daa947b8f09b2/src/compression/registry.ts) on demand.\r\n\r\nThe `arrow.tableFromIPC` function is currently synchronous, so unless we changed that function to be async, we wouldn't be able to import the codec _after_ seeing that a data file has a given compression, because a dynamic import would have to be async.\r\n\r\nIn terms of implementation, I'd expect it to be relatively straightforward? Presumably look to update `decodeBuffers` here: <https://github.com/apache/arrow/blob/b67e3c8ef1e173e1840c4fa897b7c6c493932e10/js/src/ipc/metadata/message.ts#L303>.\r\n\r\n\u00a0\r\n\r\nReferences:\r\n\r\nLZ4 implementations:\r\n \\* <https://github.com/gorhill/lz4-wasm> Edit: Looks like this is LZ4 block format only, whereas we need the LZ4 frame format.\r\n \\* <https://github.com/manzt/numcodecs.js/tree/main/codecs/lz4>\u00a0\r\n \\* <https://www.npmjs.com/package/lz4-wasm>\r\n \\* <https://github.com/Benzinga/lz4js>\u00a0\r\n\r\nZSTD implementations:\r\n \\* <https://github.com/manzt/numcodecs.js/tree/main/codecs/zstd>\u00a0\r\n \\* <https://github.com/bokuweb/zstd-wasm>\r\n \\* <https://github.com/yoshihitoh/zstd-codec>\r\n \\* <https://github.com/donmccurdy/zstddec>\u00a0\r\n \\* <https://github.com/fabiospampinato/zstandard-wasm>\r\n \\* <https://github.com/OneIdentity/zstd-js>\u00a0"
        },
        {
            "created_at": "2022-04-17T19:30:57.736Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17523434) by Dominik Moritz (domoritz):*\nLooking at lz4js, it's so small (https://cdn.jsdelivr.net/npm/lz4js@0.2.0/lz4.min.js) that it's probably okay to pull in a dependency by default. I agree that having some system to register a different decompress function could be nice. lz4js is a bit old so we would want to carefully look at the available libraries. It would be nice to have some out of the box support. To avoid increasing bundle sizes, we can decide which functions actually use the decompression library.\r\n\r\nCould you look at the available js libraries and see what their sizes are? Also, is lz4 or zstd much more common than the other? \r\n\r\nWe also should look into how much benefit we actually get from compression since most servers already support transparent gzip compression and so compressing an already compressed file will just incur overhead. \r\n\r\nIf the libraries are too heavy, we can think about a plugin system. We could make our registry be synchronous. \r\n\r\nI definitely don't want to pull in wasm into the library as it will break people's workflows. "
        },
        {
            "created_at": "2022-04-18T16:42:10.242Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17523786) by Kyle Barron (kylebarron2):*\nThanks for the feedback!\r\n> We also should look into how much benefit we actually get from compression since most servers already support transparent gzip compression and so compressing an already compressed file will just incur overhead.\r\nI think there are several reasons why it's important to support compressed files:\r\n \\* Popular tools in the ecosystem write data with compression turned on by default. I'm specifically looking at Pyarrow/Pandas, which [writes LZ4-compressed files by default](https://arrow.apache.org/docs/python/generated/pyarrow.feather.write_feather.html#pyarrow.feather.write_feather). If a web app wants to display arrow data from unknown sources, having some way to load all files is ideal.\r\n \\* It's true that servers usually offer transparent gzip compression, but there are reasons why a user wouldn't want that. For one, gzip compression is much slower than LZ4 or ZSTD compression. An example below using [this file](https://ookla-open-data.s3.us-west-2.amazonaws.com/parquet/performance/type=mobile/year=2019/quarter=1/2019-01-01_performance_mobile_tiles.parquet)\u00a0of writing a 753MB Arrow table to a memory buffer uncompressed, then using the standard library's `gzip.compress` took {**}2m46s{**}. The Python interface is slower than the gzip command line, but `time gzip -c uncompressed_table.arrow > /dev/null` still took {**}36s{**}. Meanwhile, using LZ4 output took only **1.48s** and ZSTD output took only {**}1.63s{**}. In this example, the LZ4 file was 75% larger than the gzip file, but the ZSTD one was 6% smaller than the gzip one. Of course this is just one example, but it at least gives credence to times when a developer would prefer lz4 or zstd over gzip.\r\n \\* I think supporting compression in `tableToIPC` would be quite valuable for any use case where an app wants to push Arrow data to a server.\r\n\r\n> <font color=\"#172b4d\">Looking at lz4js, it's so small</font><font color=\"#172b4d\">\u00a0that it's probably okay to pull in a dependency by default. </font>\r\n<font color=\"#172b4d\">Wow that is impressively small. It might make sense to pull that in by default. The issue tracker is mostly empty, though there is one report of data compressed by lz4js not being able to be read by other tools.</font>\r\n> I definitely don't want to pull in wasm into the library as it will break people's workflows.\r\nI agree. I'm fine with not pulling in a wasm library by default.\r\n> Could you look at the available js libraries and see what their sizes are? Also, is lz4 or zstd much more common than the other?\r\nNone of the ZSTD libraries I came across were pure JS. The only LZ4 one that was pure JS was lz4js. Aside from considering something like trying to transpile wasm to JS, which I think would be too complex for arrow JS, the only default I see that's possible is using lz4js while also supporting a registry. I don't know if LZ4 or ZSTD are more common. LZ4 is the default for Pyarrow when writing a table.\r\n> If the libraries are too heavy, we can think about a plugin system. We could make our registry be synchronous.\r\nI think it would be possible to force the `compress` and `decompress` functions in the plugin system to be synchronous. That would just force the user to finish any async initialization before trying to read/write a file, since wasm bundles can't be instantiated synchronously I think.\r\n\r\n\u00a0\r\n\r\n------\r\n\r\nExample of writing table to buffer uncompressed, then using `gzip.compress` from the Python standard library\r\n\r\n```\r\n\r\nIn [37]: %%time\r\n\u00a0 \u00a0 ...: options = pa.ipc.IpcWriteOptions(compression=None)\r\n\u00a0 \u00a0 ...: with pa.BufferOutputStream() as buf:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 with pa.ipc.new_stream(buf, table.schema, options=options) as writer:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 \u00a0 \u00a0 writer.write_table(table)\r\n\u00a0 \u00a0 ...:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 reader = pa.BufferReader(buf.getvalue())\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 reader.seek(0)\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 out = gzip.compress(reader.read())\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 print(len(out))\r\n\u00a0 \u00a0 ...:\r\n175807183\r\nCPU times: user 2min 41s, sys: 1.74 s, total: 2min 43s\r\nWall time: 2min 46s\r\n\r\n```\r\n\r\nExample of writing table to buffer with lz4 compression:\r\n\r\n```\r\n\r\nIn [40]: %%time\r\n\u00a0 \u00a0 ...: options = pa.ipc.IpcWriteOptions(compression='lz4')\r\n\u00a0 \u00a0 ...: with pa.BufferOutputStream() as buf:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 with pa.ipc.new_stream(buf, table.schema, options=options) as writer:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 \u00a0 \u00a0 writer.write_table(table)\r\n\u00a0 \u00a0 ...:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 print(buf.tell())\r\n313078576\r\nCPU times: user 1.48 s, sys: 322 ms, total: 1.81 s\r\nWall time: 1.48 s\r\n\r\n```\r\n\r\nExample of writing table to buffer with zstd compression:\r\n\r\n```\r\n\r\nIn [41]: %%time\r\n\u00a0 \u00a0 ...: options = pa.ipc.IpcWriteOptions(compression='zstd')\r\n\u00a0 \u00a0 ...: with pa.BufferOutputStream() as buf:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 with pa.ipc.new_stream(buf, table.schema, options=options) as writer:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 \u00a0 \u00a0 writer.write_table(table)\r\n\u00a0 \u00a0 ...:\r\n\u00a0 \u00a0 ...: \u00a0 \u00a0 print(buf.tell())\r\n166563176\r\nCPU times: user 2.28 s, sys: 178 ms, total: 2.45 s\r\nWall time: 1.63 s\r\n\r\n```"
        },
        {
            "created_at": "2022-04-19T01:59:13.698Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17523985) by Dominik Moritz (domoritz):*\n> For one, gzip compression is much slower than LZ4 or ZSTD compression.\r\n\r\nMaybe. Let's make sure to compare native gzip compression that a web server uses with js lz4/zstd compression.\r\n\r\n> I think it would be possible to force the `compress` and `decompress` functions in the plugin system to be synchronous. That would just force the user to finish any async initialization before trying to read/write a file, since wasm bundles can't be instantiated synchronously I think.\r\n\r\nIt would unfortunately also preclude people from putting decompression into a worker. Maybe we can make the relevant IPC methods return return promises when the compression/decompression method is async (returns a promise).\r\n\r\n> None of the ZSTD libraries I came across were pure JS. The only LZ4 one that was pure JS was lz4js.\r\n\r\nWe could consider inlining the wasm code with base64 if it's tiny but I suspect it will not. Worth considering, though. \r\n\r\nAnyway, I think it makes sense to work on this and send a pull request. We should definitely have a way to pass in/register compression algorithms. Then let's look into whether we want to bundle any algorithms. Let's start with lz4 and try a few libraries (e.g. https://github.com/gorhill/lz4-wasm, https://github.com/Benzinga/lz4js, https://github.com/pierrec/node-lz4). If they are small enough, I would consider including a default lz4 implementation. Sounds good?"
        },
        {
            "created_at": "2022-04-19T02:02:53.991Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17523988) by Dominik Moritz (domoritz):*\nhttps://github.com/manzt/numcodecs.js looks interesting as well. It used wasm inlined lz4. "
        },
        {
            "created_at": "2022-04-19T14:59:44.114Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8674?focusedCommentId=17524365) by Kyle Barron (kylebarron2):*\n>\u00a0Maybe. Let's make sure to compare native gzip compression that a web server uses with js lz4/zstd compression.\r\n\r\nI'm most familiar with [fastapi](https://github.com/tiangolo/fastapi), which is probably the third most-popular Python web server framework after Django and Flask. Its suggested gzip middleware [uses the standard library's gzip implementation](https://github.com/encode/starlette/blob/d7cbe2a4887ad6b15fe7523ed62e28a426b7697d/starlette/middleware/gzip.py#L37-L39) so I don't think my example above was completely out of place. The [lzbench native benchmarks](https://github.com/inikep/lzbench#benchmarks) still have lz4 and zstd as 4-6x faster than zlib.\r\n\r\nBut I think these performance discussions are more of a side discussion; given that the Arrow IPC format allows for compression, I'd love to find a way for Arrow JS to support these files.\r\n\r\n> It would unfortunately also preclude people from putting decompression into a worker. Maybe we can make the relevant IPC methods return return promises when the compression/decompression method is async (returns a promise).\r\n\r\nThat's a very good point. If we implement a registry of some sort, we could consider allowing both sync and async of\u00a0 compression. Then the `RecordBatchReader` could use sync compression and `AsyncRecordBatchReader` could use the async compression. So if the user wants to use de/compression on a worker they would be able to use the AsyncRecordBatchReader. Not sure if that's a great idea; but having a synchronous `tableFromIPC` option is nice.\r\n\r\n> If they are small enough, I would consider including a default lz4 implementation. Sounds good?\r\n\r\nSounds good! I'll try to find time soon to put up a draft."
        }
    ]
}