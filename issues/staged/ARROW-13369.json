{
    "issue": {
        "title": "[C++][python] performance of read_table using filters on a partitioned parquet file",
        "body": "***Note**: This issue was originally created as [ARROW-13369](https://issues.apache.org/jira/browse/ARROW-13369). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nReading a single partition of a parquet file via filters is significantly slower than reading the partition directly.\r\n```java\n\r\nimport pandas as pd\r\nsize = 100_000\r\ndf = pd.DataFrame({'a': [1, 2, 3] * size, 'b': [4, 5, 6] * size})\r\ndf.to_parquet('test.parquet', partition_cols=['a'])\r\n%timeit pyarrow.parquet.read_table('test.parquet/a=1')\r\n%timeit pyarrow.parquet.read_table('test.parquet', filters=[('a', '=', 1)])\r\n```\r\ngives the timings\r\n```python\n\r\n2.57 ms \u00b1 41.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n5.18 ms \u00b1 148 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\u00a0\n```\r\nLikewise, changing size to 1_000_000 in the above code gives\r\n```python\n\r\n16.3 ms \u00b1 269 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n32.7 ms \u00b1 1.02 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n```\r\nPart of the docs for\u00a0[read_table](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)\u00a0states:\r\n\r\n>\u00a0Partition keys embedded in a nested directory structure will be exploited to avoid loading files at all if they contain no matching rows.\r\n\r\nFrom this, I expected the performance to be roughly the same.\u00a0",
        "created_at": "2021-07-17T19:18:38.000Z",
        "updated_at": "2021-07-19T18:09:44.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}