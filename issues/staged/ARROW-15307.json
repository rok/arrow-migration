{
    "issue": {
        "title": "[C++][Dataset] Provide more context in error message if cast fails during scanning",
        "body": "***Note**: This issue was originally created as [ARROW-15307](https://issues.apache.org/jira/browse/ARROW-15307). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nIf you have a partitioned dataset, and in one of the files there is a column with a mismatching type and that cannot be safely casted to the dataset schema's type for that column, you get (as expected) get an error about this cast. \r\n\r\nSmall illustrative example code:\r\n\r\n```python\n\r\nimport pyarrow as pa\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.dataset as ds\r\n\r\nimport pathlib\r\n\r\n## constructing a small dataset with two files\r\n\r\nbasedir = pathlib.Path(\".\") / \"dataset_test_mismatched_schema\"\r\nbasedir.mkdir(exist_ok=True)\r\n\r\ntable1 = pa.table({'a': [1, 2, 3], 'b': [1, 2, 3]})\r\npq.write_table(table1, basedir / \"data1.parquet\")\r\n\r\ntable2 = pa.table({'a': [1.5, 2.0, 3.0], 'b': [1, 2, 3]})\r\npq.write_table(table2, basedir / \"data2.parquet\")\r\n\r\n## reading the dataset\r\n\r\ndataset = ds.dataset(basedir)\r\n# by default infer dataset schema from first file\r\ndataset.schema\r\n# actually reading gives expected error\r\ndataset.to_table()\r\n```\r\n\r\ngives\r\n\r\n```python\n\r\n>>> dataset.schema\r\na: int64\r\nb: int64\r\n>>> dataset.to_table()\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-1-a2d19a590e3b> in <module>\r\n     22 dataset.schema\r\n     23 # actually reading gives expected error\r\n---> 24 dataset.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Dataset.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/_dataset.pyx in pyarrow._dataset.Scanner.to_table()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n~/scipy/repos/arrow/python/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Float value 1.5 was truncated converting to int64\r\n\r\n../src/arrow/compute/kernels/scalar_cast_numeric.cc:177  CheckFloatToIntTruncation(batch[0], *out)\r\n../src/arrow/compute/exec.cc:700  kernel_->exec(kernel_ctx_, batch, &out)\r\n../src/arrow/compute/exec.cc:641  ExecuteBatch(batch, listener)\r\n../src/arrow/compute/function.cc:248  executor->Execute(implicitly_cast_args, &listener)\r\n../src/arrow/compute/exec/expression.cc:444  compute::Cast(column, field->type(), compute::CastOptions::Safe())\r\n../src/arrow/dataset/scanner.cc:816  compute::MakeExecBatch(*scan_options->dataset_schema, partial.record_batch.value)\r\n```\r\n\r\nSo the actual error message (without the extra C++ context) is only **\"ArrowInvalid: Float value 1.5 was truncated converting to int64\"**.\r\n\r\nSo this error message only says something about the two types and the first value that cannot be cast, but if you have a large dataset with many fragments and/or many columns, it can be hard to know 1) for which column this is failing and 2) for which fragment it is failing.\r\n\r\nSo it would be nice to add some extra context to the error message.  \r\nThe cast itself of course doesn't know it, but I suppose when doing the cast in the scanner code, there at least we know eg the physical schema and dataset schema, so we could append or prepend the error message with something like \"Casting from schema1 to schema2 failed with ...\". \r\n\r\ncc `[~alenkaf]`",
        "created_at": "2022-01-12T10:50:29.000Z",
        "updated_at": "2022-01-12T10:50:29.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}