{
    "issue": {
        "title": "[Python] Unable to specify the partition style with pq.write_to_dataset",
        "body": "***Note**: This issue was originally created as [ARROW-9682](https://issues.apache.org/jira/browse/ARROW-9682). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI am able to import and test DirectoryPartitioning but I am not able to figure out a way to write a dataset using this feature. It seems like write_to_dataset defaults to the \"hive\" style. Is there a way to test this?\r\n\r\n\r\n```java\n\r\nfrom pyarrow.dataset import DirectoryPartitioning\r\n\r\npartitioning = DirectoryPartitioning(pa.schema([(\"year\", pa.int16()), (\"month\", pa.int8()), (\"day\", pa.int8())]))\r\n\r\nprint(partitioning.parse(\"/2009/11/3\"))\r\n\r\n```\r\n\u00a0",
        "created_at": "2020-08-10T15:42:56.000Z",
        "updated_at": "2021-04-16T10:13:55.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2021-04-16T10:13:55.000Z"
    },
    "comments": [
        {
            "created_at": "2020-08-10T20:03:08.310Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9682?focusedCommentId=17175034) by Joris Van den Bossche (jorisvandenbossche):*\n`[~ldacey]` the writing functionality in the Datasets API was not yet implemented for pyarrow 1.0. It has been recently added to C++, and I am actually right now creating the python bindings for it -> ARROW-9658 / https://github.com/apache/arrow/pull/7921\r\n\r\nThat PR enables you to write a partitioned dataset with the directory-style partitioning (right now this is through a new `pyarrow.dataset.write_dataset` but it will probably also be exposed in the `pyarrow.parquet.write_to_dataset`)"
        },
        {
            "created_at": "2020-09-16T13:20:34.957Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9682?focusedCommentId=17196967) by Lance Dacey (ldacey):*\nExcellent. `[~jorisvandenbossche]` will there be a way to potentially repartition datasets? My use case is this:\r\n\r\n1) I download data every 30 minutes from a source using UUID parquet filenames (each file just contains new or updated records since the last hour so I could not think of a good callback function name). This is 48 parquet files per day.\r\n2) The data is then partitioned based on the created_date which creates even more files (some can be quite small)\r\n3) When I query the dataset, I need to read in a lot of very small files.\r\n\r\nI would then want to read the data and repartition the files using a callback function so the dozens of files in partition (\"date\", \"==\", \"2020-09-15\") would become 2020-09-15.parquet, consolidated as a single file to keep things tidy. I know I can do this with Spark, but it would be nice to have a native pyarrow method."
        },
        {
            "created_at": "2021-04-16T10:13:55.725Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9682?focusedCommentId=17322757) by Lance Dacey (ldacey):*\nThis works using ds.write_dataset()"
        }
    ]
}