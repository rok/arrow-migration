{
    "issue": {
        "title": "[C++][Python] Support AWS Firehose partition_scheme implementation for Parquet datasets",
        "body": "***Note**: This issue was originally created as [ARROW-2882](https://issues.apache.org/jira/browse/ARROW-2882). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI'd like to be able to read a ParquetDataset generated by AWS Firehose.\r\n\r\nThe only implementation at the time of writting was the partition scheme created by hive (year=2018/month=01/day=11).\r\n\r\nAWS Firehose partition scheme is a little bit different (2018/01/11).\r\n\r\n\u00a0\r\n\r\nThanks",
        "created_at": "2018-07-19T13:48:14.000Z",
        "updated_at": "2020-06-16T06:10:34.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2020-06-16T06:10:34.000Z"
    },
    "comments": [
        {
            "created_at": "2018-07-19T13:51:52.089Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2882?focusedCommentId=16549292) by Wes McKinney (wesm):*\nThanks `[~IceS2]` I updated the title and added a component and label"
        },
        {
            "created_at": "2019-03-30T22:10:56.795Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2882?focusedCommentId=16806004) by Wes McKinney (wesm):*\nI added the C++ component since this will be handled as part of the Datasets project"
        },
        {
            "created_at": "2020-04-01T19:19:59.469Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2882?focusedCommentId=17073116) by Joris Van den Bossche (jorisvandenbossche):*\nThis kind of partitioning scheme is already implemented in the Datasets project, and exposed in the python bindings.\r\n\r\nYou can do:\r\n\r\n```Java\n\r\nimport pyarrow.dataset as ds\r\n\r\nds.dataset(\"root_directory/\", partitioning=ds.partitioning([\"year\", \"month\", \"day\"])\r\n```\r\n\r\nto give the names for the parts of the file path. Alternatively, you can also pass an actual schema, in which case you specify data types per field as well, instead of letting it be inferred from the file path.\r\n\r\n"
        },
        {
            "created_at": "2020-04-14T07:47:37.698Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2882?focusedCommentId=17082959) by Joris Van den Bossche (jorisvandenbossche):*\nWith ARROW-8039, this is now also exposed in the existing `pq.ParquetDataset`, when using `use_legacy_dataset=False`."
        },
        {
            "created_at": "2020-06-16T06:10:21.348Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-2882?focusedCommentId=17136341) by Joris Van den Bossche (jorisvandenbossche):*\nLet's declare this as resolved, now the new Datasets API supports this."
        }
    ]
}