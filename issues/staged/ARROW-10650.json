{
    "issue": {
        "title": "[C++][parquet][hadoop]memory leak when read parquet file from hadoop",
        "body": "***Note**: This issue was originally created as [ARROW-10650](https://issues.apache.org/jira/browse/ARROW-10650). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nwhen I use hdfs interface under arrow/io folder to access and read parquets files, it will lead memory leak. some of example codes blow:\r\n\r\n\u00a0\r\n\r\n_std::shared_ptr<arrow::io::HadoopFileSystem> hdfs = nullptr;_\r\n\r\n_...//set hdfs conf var;_\r\n\r\n_msg = arrow::io::HadoopFileSystem::Connect(&conf, &hdfs);_\r\n\r\n_...//check is connect hdfs or not;_\r\n\r\n_std::shared_ptr<arrow::io::HdfsReadableFile>_\u00a0_file_reader;_\r\n\r\n_...//set file_path var;_\r\n\r\n_arrow::Status ret = hdfs->OpenReadable(file_path, &file_reader);_\r\n\r\n_...//check open success or not;_\r\n\r\n_std::unique_ptr<parquet::ParquetFileReader>_\u00a0_parquet_reader;_\r\n\r\n_parquet_reader = parquet::ParquetFileReader::Open(file_reader);_\r\n\r\n_...//through_\u00a0_parquet_reader to get data;_\r\n\r\n\u00a0\r\n\r\nAnd I check when I read parquet file in local path,\r\n\r\n_static std::unique_ptr<ParquetFileReader> OpenFile(_\r\n _const std::string& path, bool memory_map = true,_\r\n _const ReaderProperties& props = default_reader_properties(),_\r\n _std::shared_ptr<FileMetaData> metadata = NULLPTR);_\r\n\r\nuse OpenFile API, it is OK, will not lead memory leak;\r\n\r\n\u00a0\r\n\r\nis there any wrong step when I use hdfs API? or exist some issues.\u00a0\r\n\r\nany quetions can reply me, thanks.",
        "created_at": "2020-11-19T02:49:28.000Z",
        "updated_at": "2020-11-20T08:59:37.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": []
}