{
    "issue": {
        "title": "[Python] Year 2263 or later datetimes get mangled when written using pandas",
        "body": "***Note**: This issue was originally created as [ARROW-8816](https://issues.apache.org/jira/browse/ARROW-8816). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nUsing pyarrow 0.17.0, this\r\n\r\n\u00a0\r\n```java\n\r\nimport datetime\r\nimport pandas as pd\r\n\r\ndef try_with_year(year):\r\n\u00a0 \u00a0 print(f'Year {year:_}:')\r\n\u00a0 \u00a0 df = pd.DataFrame({'x': [datetime.datetime(year, 1, 1)]})\r\n\u00a0 \u00a0 df.to_parquet('foo.parquet', engine='pyarrow', compression=None)\r\n\u00a0 \u00a0 try:\r\n\u00a0 \u00a0 \u00a0 \u00a0 print(pd.read_parquet('foo.parquet', engine='pyarrow'))\r\n\u00a0 \u00a0 except Exception as exc:\r\n\u00a0 \u00a0 \u00a0 \u00a0 print(repr(exc))\r\n\u00a0 \u00a0 print()\r\n\r\ntry_with_year(2_263)\r\ntry_with_year(2_262)\r\n```\r\n\u00a0\r\n\r\nprints\r\n\r\n\u00a0\r\n```\n\r\nYear 2_263:\r\nArrowInvalid('Casting from timestamp[ms] to timestamp[ns] would result in out of bounds timestamp: 9246182400000')\r\n\r\nYear 2_262:\r\n\u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 x\r\n0 2262-01-01\n```\r\nand using pyarrow 0.16.0, it prints\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```\n\r\nYear 2_263:\r\n                              x\r\n0 1678-06-12 00:25:26.290448384\r\n\r\nYear 2_262:\r\n           x\r\n0 2262-01-01\n```\r\nThe issue is that 2263-01-01 is out of bounds for a timestamp stored using epoch nanoseconds, but not out of bounds for a Python datetime.\r\n\r\nWhile pyarrow 0.17.0 refuses to read the erroneous output, it is still possible to read it using other parquet readers (e.g. pyarrow 0.16.0 or fastparquet), yielding the same result as with 0.16.0 above (i.e. only reading has changed in 0.17.0, not writing). It would be better if an error was raised when attempting to write the file instead of silently producing erroneous output.\r\n\r\nThe reason I suspect this is a pyarrow issue instead of a pandas issue is this modified example:\r\n\r\n\u00a0\r\n```java\n\r\nimport datetime\r\nimport pandas as pd\r\nimport pyarrow as pa\r\n\r\ndf = pd.DataFrame({'x': [datetime.datetime(2_263, 1, 1)]})\r\ntable = pa.Table.from_pandas(df)\r\nprint(table[0])\r\ntry:\r\n\u00a0 \u00a0 print(table.to_pandas())\r\nexcept Exception as exc:\r\n\u00a0 \u00a0 print(repr(exc))\r\n```\r\nwhich prints\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```\n\r\n[\r\n\u00a0 [\r\n\u00a0 \u00a0 2263-01-01 00:00:00.000000\r\n\u00a0 ]\r\n]\r\nArrowInvalid('Casting from timestamp[us] to timestamp[ns] would result in out of bounds timestamp: 9246182400000000')\n```\r\non pyarrow 0.17.0 and\r\n\r\n\u00a0\r\n\r\n\u00a0\r\n```\n\r\n[\r\n\u00a0 [\r\n\u00a0 \u00a0 2263-01-01 00:00:00.000000\r\n\u00a0 ]\r\n]\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 x\r\n0 1678-06-12 00:25:26.290448384\n```\r\non pyarrow 0.16.0. Both from_pandas() and to_pandas() are pyarrow methods, pyarrow prints the correct timestamp when asked to produce it as a string (so it was not lost inside pandas), but the\u00a0pa.Table.from_pandas(df).to_pandas() round-trip fails.\r\n\r\n\u00a0\r\n\r\n\u00a0",
        "created_at": "2020-05-15T13:58:59.000Z",
        "updated_at": "2022-10-27T13:26:14.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2022-10-27T13:26:14.000Z"
    },
    "comments": [
        {
            "created_at": "2020-05-15T18:48:40.876Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8816?focusedCommentId=17108548) by Joris Van den Bossche (jorisvandenbossche):*\n> It would be better if an error was raised when attempting to write the file instead of silently producing erroneous output.\r\n\r\nThe file is correct (so we shouldn't error when writing), it is only after reading in that the conversion to pandas causes the issue given pandas' limitation on the range of timestamps.\r\n\r\nAs you can see, in pyarrow 0.17 it was at least fixed to not produces garbage dates but an error is raised instead (which I would say is better than garbage). But it is a known issue that there should be a way to still convert to pandas but with converting to datetime objects instead of to datetime64[ns] dtype. This is covered by ARROW-5359 with the idea to add a `timestamp_as_object` keyword.\r\n\r\n\r\n\r\n"
        },
        {
            "created_at": "2020-05-15T23:09:50.726Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8816?focusedCommentId=17108736) by Rauli Ruohonen (rauli):*\nAh, I see. I thought that the output was wrong, because fastparquet also reads it incorrectly. But using both from pandas is not an independent test, because pandas is shared between the tests. Checking with parquet-tools, the output does look correct (9246182400000 is 2263-01-01 00:00:00, and the extra field gives \"datetime\" for pandas_type and \"object\" for numpy_type; AFAICS the reader has no basis to assume that unchecked cast to datetime64 would be safe).\r\n\r\nStill, it's something of a pitfall that you can successfully save data (using default options), and when you later try to load it using the same software (using default options), it fails. If timestamp_as_object is required to read the data, one could symmetrically also require it to write the data, and avoid surprises upon loading.\r\n\r\nOTOH raising an exception when you actually can produce correct output would also be slightly odd. One solution would be to have a timestamp_as_object='infer' option (instead of just True/False) that would be the default, so that the current writing behavior would be matched with symmetric reading behavior that would produce datetime64 when possible, and datetime when not.\r\n\r\nFrom one pragmatic perspective it'd be safer to raise an exception when trying to write these things unless explicitly requested, because there are readers that fail with them in common use (such as current pyarrow and fastparquet). Maybe the reasoning why write_table defaults to parquet version 1.0 output instead of 2.0 is similar...?\r\n\r\nIMHO the important thing is to always be able to read back in what one wrote (possibly with wider types) if the write was successful, provided that one uses the same pyarrow version and the default options for both reading and writing."
        },
        {
            "created_at": "2020-05-18T07:45:02.541Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8816?focusedCommentId=17109983) by Joris Van den Bossche (jorisvandenbossche):*\n> the extra field gives \"datetime\" for pandas_type and \"object\" for numpy_type;\r\nAh, if there is pandas metadata present and it indicates object dtype, we could indeed use that to avoid conversion to datetime64[ns], but keep datetime objects. That sounds as it should be possible in principle.\r\n> IMHO the important thing is to always be able to read back in what one wrote (possibly with wider types) if the write was successful, provided that one uses the same pyarrow version and the default options for both reading and writing.\r\nYes, but again: we need to distinguish writing parquet and pandas<\\~~>pyarrow conversion. Just for writing parquet from a pyarrow table, fully correct roundtrip works perfectly fine. It's only the pandas<~~>pyarrow conversion that gives problems. \r\n And note that in general, roundtrip is always tricky when a single pyarrow type can map to multiple pandas types (like dates that can be converted to datetime64[D] or datetime.date, or ListArray that can be converted to a column of tuples, a column of lists or a column of numpy arrays)\r\n\r\nBut I agree a roundtrip should be possible: a `timestamp_as_object` keyword should at least help (while still needing to specify a keyword). And with the pandas metadata we could maybe try to automatically choose the good default."
        },
        {
            "created_at": "2022-10-27T13:23:50.850Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8816?focusedCommentId=17625122) by Alenka Frim (alenka):*\nClosing this as it is not relevant anymore (Arrow now errors with `{}ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds timestamp{`}) when converting to pandas.\r\n\r\nI did create a new issue https://issues.apache.org/jira/browse/ARROW-18175 to track work about using the information stored in the metadata:\r\n> Ah, if there is pandas metadata present and it indicates object dtype, we could indeed use that to avoid conversion to datetime64[ns], but keep datetime objects. That sounds as it should be possible in principle."
        }
    ]
}