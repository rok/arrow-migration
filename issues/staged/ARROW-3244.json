{
    "issue": {
        "title": "[Python] Multi-file parquet loading without scan",
        "body": "***Note**: This issue was originally created as [ARROW-3244](https://issues.apache.org/jira/browse/ARROW-3244). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nA number of\u00a0mechanism are possible to avoid having to access and read the parquet footers in a data set consisting of a number of files. In the case of a large number of data files (perhaps split with directory partitioning) and remote storage, this can be a significant overhead. This is significant from the point of view of Dask, which must have the metadata available in the client before setting up computational graphs.\r\n\r\n\u00a0\r\n\r\nHere are some suggestions of what could be done.\r\n\r\n\u00a0\r\n \\* some parquet writing frameworks include a `_metadata` file, which contains all the information from the footers of the various files. If this file is present, then this data can be read from one place, with a single file access. For a large number of files, parsing the thrift information may, by itself, be a non-negligible overhead\u2265\r\n \\* the schema (dtypes) can be found in a `_common_metadata`, or from any one of the data-files, then the schema could be assumed (perhaps at the user's option) to be the same for all of the files. However, the information about the directory partitioning would not be available. Although\u00a0Dask may infer the\u00a0information from the filenames,\u00a0it would be preferable to go through the machinery with parquet-cpp, and view the whole data-set as a single object. Note that the files will still need to have the footer read to access the data, for the bytes offsets, but from Dask's point of view, this would be deferred to tasks running in parallel.\r\n\r\n(please forgive that some of this has already been mentioned elsewhere; this is one of the entries in the list at <https://github.com/dask/fastparquet/issues/374>\u00a0as a feature that is useful in fastparquet)\r\n\r\n\u00a0",
        "created_at": "2018-09-16T23:48:39.000Z",
        "updated_at": "2022-08-29T14:15:12.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2022-08-29T14:13:59.000Z"
    },
    "comments": [
        {
            "created_at": "2018-09-21T00:36:25.015Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3244?focusedCommentId=16622917) by Matthew Rocklin (mrocklin):*\nWhat happens today when someone reads a multi-file parquet dataset with dask dataframe?\u00a0 We read a single file to get the schema and then just build tasks for everything else?\u00a0 Or do we need to read through each of the files in order to find out how many row blocks are in each?\r\n\r\nOn the Arrow side is this in scope?\u00a0 Is this already implemented?\u00a0 Are there mechanisms to construct the metadata files from within Arrow?\u00a0 If not, and if this is in scope then what is the right way / place to add this behavior?"
        },
        {
            "created_at": "2018-09-21T00:43:52.983Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3244?focusedCommentId=16622920) by Wes McKinney (wesm):*\nThe pyarrow perspective is essentially agnostic to the data access pattern, but we'd like to provide APIs to do as the user wishes with the files. The basic pattern of a partitioned dataset read by a single node works fine now (that's the `ParquetDataset` object)\r\n\r\nLet's come up with a concrete API ask and the desired semantics with regards to when precisely the underlying file system is to be accessed, and if this is not available now, we can slate it for one of the upcoming releases. "
        },
        {
            "created_at": "2019-03-26T23:31:39.815Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3244?focusedCommentId=16802274) by Matthew Rocklin (mrocklin):*\nIs this resolved today by the ParquetDataset.metadata attribute?\r\n\r\n```Java\n\r\npq.ParquetDataset(path).metadata\r\n```"
        },
        {
            "created_at": "2019-06-12T16:18:03.524Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-3244?focusedCommentId=16862254) by Wes McKinney (wesm):*\nIt seems we still need a bit more scaffolding to be able to read a multi-file dataset given the _metadata file without having to read the individual file footers. This isn't going to get done for 0.14.0 I don't think but I intend to see this logic implemented in the \"C++ Datasets\" component, so likely in the next release cycle"
        }
    ]
}