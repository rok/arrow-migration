{
    "issue": {
        "title": "[C++] Add \"driver\" option to HdfsClient to choose between libhdfs and libhdfs3 at runtime",
        "body": "***Note**: This issue was originally created as [ARROW-243](https://issues.apache.org/jira/browse/ARROW-243). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nI would like to use for example libhdfs3 from pivotal to read apache parquet files. This would be a small change to the hdfs layer of apache arrow to support this.",
        "created_at": "2016-07-26T22:12:15.000Z",
        "updated_at": "2017-02-13T16:34:37.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2016-12-19T23:26:30.000Z"
    },
    "comments": [
        {
            "created_at": "2016-07-26T22:23:11.609Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-243?focusedCommentId=15394665) by Wes McKinney (wesm):*\nMy preference would be to add an option to `HdfsConnectionConfig` to choose the client library used \u2013 to be loaded at runtime with dlopen \u2013 since libhdfs and libhdfs3 are designed to be roughly ABI-compatible. One reason for this is that binary distributions of Arrow may not know which libraries may be present on the target system.\n\nThis is more work than switching between the libraries at compile time but will be more flexible. "
        },
        {
            "created_at": "2016-07-26T23:25:41.676Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-243?focusedCommentId=15394764) by Ryan Lewis (me@ryanlewis.net):*\nAre you suggesting manually using dlopen? My guess would be that there is a way to achieve what you want without doing this. If the libraries really are ABI compatible, it should be possible to just swap out the underlying SO. "
        },
        {
            "created_at": "2016-08-01T18:34:33.687Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-243?focusedCommentId=15402593) by Wes McKinney (wesm):*\nYes \u2013 part of the reason for using dlopen for libhdfs is that it's typically part of a Hadoop distribution, so `libhdfs.so` will likely not be in LD_LIBRARY_PATH. Since we already have this code in place (https://github.com/apache/arrow/blob/master/cpp/src/arrow/io/libhdfs_shim.cc#L500), making libhdfs3.so also a soft (i.e. not loaded when libarrow_io.so is loaded) dependency isn't too much extra work. \n\nThis code would benefit from some refactoring (for example: there is only one possible set of function pointers available in the shim layer \u2013 you could put these in static shim structs) to make switching between the libraries as seamless as possible. "
        },
        {
            "created_at": "2016-12-19T21:25:11.827Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-243?focusedCommentId=15762348) by Wes McKinney (wesm):*\n[~me@ryanlewis.net] I did the refactoring we were talking about in https://github.com/apache/arrow/pull/244. Give it a spin and let me know how it works for you. "
        },
        {
            "created_at": "2016-12-19T23:26:30.508Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-243?focusedCommentId=15762621) by Wes McKinney (wesm):*\nIssue resolved by pull request 244\n<https://github.com/apache/arrow/pull/244>"
        }
    ]
}