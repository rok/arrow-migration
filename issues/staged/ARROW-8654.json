{
    "issue": {
        "title": "[Python] pyarrow 0.17.0 fails reading \"wide\" parquet files",
        "body": "***Note**: This issue was originally created as [ARROW-8654](https://issues.apache.org/jira/browse/ARROW-8654). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\n```java\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nnum_rows, num_cols = 1000, 45000\r\n\r\ndf = pd.DataFrame(np.random.randint(0, 256, size=(num_rows, num_cols)).astype(np.uint8))\r\n\r\noutfile = \"test.parquet\"\r\ndf.to_parquet(outfile)\r\ndel df\r\n\r\ndf = pd.read_parquet(outfile)\r\n```\r\nYields:\r\n```\n\r\ndf = pd.read_parquet(outfile) \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pandas/io/parquet.py\", line 310, in read_parquet \r\nreturn impl.read(path, columns=columns, kwargs) \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pandas/io/parquet.py\", line 125, in read \r\npath, columns=columns, kwargs \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1530, in read_table \r\npartitioning=partitioning) \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1189, in __init__ \r\nself.validate_schemas() \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1217, in validate_schemas \r\nself.schema = self.pieces[0].get_metadata().schema \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 662, in get_metadata \r\nf = self.open() \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 669, in open \r\nreader = self.open_file_func(self.path) \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1040, in _open_dataset_file \r\nbuffer_size=dataset.buffer_size \r\nFile \"/jupyter/venv/lib/python3.6/site-packages/pyarrow/parquet.py\", line 210, in __init__ \r\nread_dictionary=read_dictionary, metadata=metadata) \r\nFile \"pyarrow/_parquet.pyx\", line 1023, in pyarrow._parquet.ParquetReader.open \r\nFile \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status \r\nOSError: Couldn't deserialize thrift: TProtocolException: Exceeded size limit\r\n```\r\nThis is pandas 1.0.3, and pyarrow 0.17.0.\r\n\r\n\u00a0\r\n\r\nI tried this with pyarrow 0.16.0, and it works. 0.15.1 did as well.\r\n\r\n\u00a0\r\n\r\nI also tried with 40,000 columns aot 45,000 as above, and that does work with 0.17.0.\r\n\r\n\u00a0\r\n\r\nThanks for all your work on this project!",
        "created_at": "2020-04-30T15:55:33.000Z",
        "updated_at": "2020-05-09T17:16:55.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-05-09T17:15:06.000Z"
    },
    "comments": [
        {
            "created_at": "2020-04-30T18:29:22.329Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8654?focusedCommentId=17096858) by Wes McKinney (wesm):*\nFWIW, \"large\" metadata from very wide tables is a problematic area for the Parquet format in general. We'll have to have a closer look to why the metadata got bigger from 0.16.0 to 0.17.0, but there will always be some point where it's too big. I would guess if you keep increasing the number of columns that 0.16.0 will fail, too. "
        },
        {
            "created_at": "2020-04-30T18:30:11.377Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8654?focusedCommentId=17096859) by Wes McKinney (wesm):*\nAlso, the perf of reading very wide Parquet files won't be very good. "
        },
        {
            "created_at": "2020-05-09T16:48:50.436Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8654?focusedCommentId=17103363) by Mike Macpherson (mikemac):*\nThank you for this context, very helpful.\r\n\r\nWhat would you think of adding documentation on parquet file column-number limits, to pandas and/or pyarrow's docs? I'd be interested to contribute the PR/s, if we might clarify what those limits are. That may also be an appropriate place to offer the guidance that performance may decline as column-number grows."
        },
        {
            "created_at": "2020-05-09T17:15:06.352Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8654?focusedCommentId=17103375) by Wes McKinney (wesm):*\nThis is a dup of ARROW-8694, which has been fixed. There is discussion on the mailing list about a patch release 0.17.1\r\n\r\nhttps://github.com/apache/arrow/commit/bc283e33232a382e1f016d57c172c718b06b38ab"
        },
        {
            "created_at": "2020-05-09T17:16:55.003Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-8654?focusedCommentId=17103376) by Wes McKinney (wesm):*\nI agree that adding documentation recommending against very wide Parquet files is a good idea. . I just opened ARROW-8746"
        }
    ]
}