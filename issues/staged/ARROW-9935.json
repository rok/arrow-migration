{
    "issue": {
        "title": "[Python] datasets unable to read empty S3 folders with fsspec' s3fs",
        "body": "***Note**: This issue was originally created as [ARROW-9935](https://issues.apache.org/jira/browse/ARROW-9935). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nWhen an empty \"folder\" is created in S3 using the online bucket explorer tool on the management console then it creates\u00a0a special empty file with the same name as the folder.\r\n\r\n(Some more details here: <https://docs.aws.amazon.com/AmazonS3/latest/user-guide/using-folders.html)>\r\n\r\nIf parquet files are later loaded into one of these directories (with or without partitioning subdirectories) then this dataset cannot be read by the new dataset API.\u00a0 The underlying s3fs `find` method returns a \"file\" object with size 0 that pyarrow then attempts to read.\u00a0 Since this file doesn't truly exist a FileNotFoundError is thrown.\r\n\r\nWould it be safe to simply ignore all files with size 0?\r\n\r\nAs a workaround I can wrap s3fs' find method and strip out these objects with size 0 myself.\r\n\r\nI've attached a script showing the issue and a workaround.\u00a0 It uses a public bucket that I'll leave up for a few months.",
        "created_at": "2020-09-08T02:59:24.000Z",
        "updated_at": "2020-09-08T21:12:50.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: bug"
        ],
        "closed": true,
        "closed_at": "2020-09-08T21:12:50.000Z"
    },
    "comments": [
        {
            "created_at": "2020-09-08T09:48:44.010Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9935?focusedCommentId=17192088) by Antoine Pitrou (apitrou):*\nHave you tried using Arrow's own S3 filesystem implementation?\r\n\r\n```python\n\r\n>>> from pyarrow.fs import S3FileSystem\r\n>>> fs = S3FileSystem()\r\n>>> fs.get_file_info(\"pyarrow-s3-empty-folder-file/mydataset\")\r\n```\r\n\r\n(there may be more S3 configuration to do because this doesn't seem to work here: bad region perhaps?)"
        },
        {
            "created_at": "2020-09-08T21:09:54.944Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9935?focusedCommentId=17192481) by Weston Pace (westonpace):*\nI tried that out and Arrow's own S3 implementation does not run into this issue.\u00a0 This would only affect the s3fs implementation.\u00a0 Either way, this is not a big problem for me since I have the workaround and I might switch to the builtin implementation anyways if the performance is significantly different.\u00a0\u00a0"
        },
        {
            "created_at": "2020-09-08T21:10:59.124Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-9935?focusedCommentId=17192483) by Antoine Pitrou (apitrou):*\nThanks for the feedback. I'll close this issue then."
        }
    ]
}