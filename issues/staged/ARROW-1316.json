{
    "issue": {
        "title": "hdfs connector stand-alone",
        "body": "***Note**: This issue was originally created as [ARROW-1316](https://issues.apache.org/jira/browse/ARROW-1316). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nCurrently, access to hdfs via libhdfs requires the whole of arrow, a java installation and a hadoop installation. This setup is indeed common, such as on \"cluster edge-nodes\".\n\nThis issue is posted with the wish that hdfs file-system access could be done without needing the whole set of installations, above.",
        "created_at": "2017-08-02T14:56:39.000Z",
        "updated_at": "2017-10-19T00:47:25.000Z",
        "labels": [
            "Migrated from Jira",
            "Type: enhancement"
        ],
        "closed": true,
        "closed_at": "2017-09-28T16:37:22.000Z"
    },
    "comments": [
        {
            "created_at": "2017-08-02T15:05:37.323Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-1316?focusedCommentId=16111075) by Wes McKinney (wesm):*\nI am not sure this is possible. To use libhdfs to access an HDFS cluster, you need:\n\n- A JVM installation\n- The Hadoop client libraries in your classpath\n- File system-like API for the libhdfs library\n\nThese are provided respectively by the JDK install, the Hadoop install, and the Arrow libraries. The Arrow interface to HDFS provides a consistent API as other files (https://github.com/apache/arrow/blob/master/cpp/src/arrow/io/hdfs.h). This is the same approach used in TensorFlow (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.h) and other projects. "
        }
    ]
}