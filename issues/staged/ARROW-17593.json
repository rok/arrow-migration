{
    "issue": {
        "title": "[C++] Try and maintain input shape in Acero",
        "body": "***Note**: This issue was originally created as [ARROW-17593](https://issues.apache.org/jira/browse/ARROW-17593). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nData is scanned in large chunks based on the format.  For example, CSV scans chunks based on a chunk_size while parquet scans entire row groups.\r\n\r\nThen, upon entry into Acero, these chunks are sliced into morsels (~L3 size) for parallelism and batches (~L1-L2 size) for cache efficient processing.\r\n\r\nHowever, the way it is currently done, means that the output of Acero is a stream of tiny batches.  This is somewhat undesirable in many cases.\r\n\r\nFor example, if a pyarrow user calls pq.read_table they might expect to get one batch per row group.  If they were to turn around and write out that table to a new parquet file then either they end up with a non-ideal parquet file (tiny row groups) or they are forced to concatenate the batches (which is an allocation + copy).\r\n\r\nEven if the user is doing their own streaming processing (e.g. in pyarrow) these small batch sizes are undesirable as the overhead of python means that streaming processing should be done in larger batches.\r\n\r\nInstead, there should be a configurable max_batch_size, independent of row group size and morsel size, which is configurable, and quite large by default (1Mi or 64Mi rows).  This control exists for users that want to do their own streaming processing and need to be able to tune for RAM usage.\r\n\r\nAcero will read in data based on the format, as it does today (e.g. CSV chunk size, row group size).  If the source data is very large (bigger than max_batch_size) it will be sliced.  From that point on, any morsels or batches should simply be views into this larger output batch.  For example, when doing a projection to add a new column, we should allocate a max_batch_size array and then populate it over many runs of the project node.",
        "created_at": "2022-09-01T15:15:34.000Z",
        "updated_at": "2022-09-02T19:17:06.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: C++",
            "Type: bug"
        ],
        "closed": false
    },
    "comments": [
        {
            "created_at": "2022-09-01T17:11:20.914Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17593?focusedCommentId=17599079) by Will Jones (willjones127):*\nI've been reading through the Parquet implementation, and was surprised to find that you cannot write out a row group with multiple batches. We've decoupled row group sizes and batch size on read (great!), but not on write. Perhaps that should also be part of the solution.\r\n\r\nI'm not deeply familiar with Acero internals yet, but what you've described here seems very sensible. Though it sounds like we may need some helper class to allocate the batch and line up the morsels, IIUC."
        },
        {
            "created_at": "2022-09-01T22:52:51.316Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17593?focusedCommentId=17599181) by Aldrin Montana (octalene):*\nI think the proposal makes a lot of sense. I also think Will's mention of allowing a writer to write multiple batches into a single row group would be really great. I think both will have a large overlap of relevant scenarios, but they will also be separately useful."
        },
        {
            "created_at": "2022-09-02T01:27:44.521Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17593?focusedCommentId=17599216) by Weston Pace (westonpace):*\nThe dataset writer already has this decoupled with `min_rows_per_group` which should handle what you are describing.  I think the write path is in ok shape.  I'm mostly worried about the \"start with datasets but do further downstream work in-memory in python\" path."
        },
        {
            "created_at": "2022-09-02T19:17:06.624Z",
            "body": "***Note**: [Comment](https://issues.apache.org/jira/browse/ARROW-17593?focusedCommentId=17599684) by Aldrin Montana (octalene):*\nNice.\r\n\r\nSince only the final projection would need to worry about the max_batch_size, not the internal plan nodes (I think), this sounds like there wouldn't be too much cost either. I'm mostly curious how efficiently aggregates can size their outputs, since they won't know exactly how many groups there will be until runtime. But, I also guess that would only be inefficient in the last batch, so it would be amortized for larger resultsets."
        }
    ]
}