{
    "issue": {
        "title": "Memory usage RecordBatchStreamWriter",
        "body": "***Note**: This issue was originally created as [ARROW-15920](https://issues.apache.org/jira/browse/ARROW-15920). Please see the [migration documentation](https://gist.github.com/toddfarmer/12aa88361532d21902818a6044fda4c3) for further details.*\n\n### Original Issue Description:\nHi.\r\n\r\nI have a monte-carlo calcuator that yields a couple of hundred Nx1 numpy arrays. I need to develop further functionality on it, and since it can`t be solved easily without having access to the full set I`m pursuing the route of exporting them. Found PyArrow and got exited. First wall I hit, was that the writer could not write \"columns\" (IPC). A stackoverflow post, and two weeks later, I`m writing my arrays to single file-single column with a stream writer ,using write_table and chunksize (write_batch has no such parameter) .I`m then combining all files to a single file by using a reader for every file and reading the corresponding \"part\"-batches. I then combine them to a single recordbatch and write. The whole idea is that I can later pull in parts of the complete set/all columns (which would fit in memory) and\u00a0 process further. Now, everything works, but following along on my task manager, I see that memory simply skyrockets when I write. I would expect memory consumption to stay around the size of my group batches and then some. The whole point of this exercise is having stuff fit in memory, and I can not see how I can achieve this. It makes me wonder if I`m a complete idiot when I read [efficiently-writing-and-reading-arrow-data](https://arrow.apache.org/docs/python/ipc.html#efficiently-writing-and-reading-arrow-data), have I done something wrong or am I looking at it wrong? I have attached a python file with a simple attempt. I have tried the filewriters, doing Tables instead of batches and refactoring in all thinkable ways.\r\n\r\n\u00a0\r\n\r\nA snip:\r\n\u00a0\r\n```java\n\r\nreaders = [pa.ipc.open_stream(file) for file in self.tempfiles]\r\ncombined_schema = pa.unify_schemas([r.schema for r in readers])\r\n\r\nwith pa.ipc.new_stream(os.path.join(self.path, self.outfile_name ),\u00a0 \u00a0 schema=combined_schema,) as writer:\r\n\u00a0 \u00a0 for group in zip(*readers):\r\n\u00a0 \u00a0 \u00a0 \u00a0 combined_batch = pa.RecordBatch.from_arrays(\r\n            [g.column(0) for g in group], names=combined_schema.names)\r\n\u00a0 \u00a0 \u00a0 \u00a0 writer.write_batch(combined_batch)\n```\r\n\u00a0",
        "created_at": "2022-03-11T20:26:03.000Z",
        "updated_at": "2022-04-08T12:23:27.000Z",
        "labels": [
            "Migrated from Jira",
            "Component: Python",
            "Type: enhancement"
        ],
        "closed": false
    },
    "comments": []
}